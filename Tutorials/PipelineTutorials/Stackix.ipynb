{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a5214ae8",
   "metadata": {},
   "source": [
    "# How To Use Stackix\n",
    "\n",
    "Stackix is our implementation of a stacked/hierarchical autoencoder.  \n",
    "This tutorial follows the structure of our `Getting Started - Vanillix`, but is less extensive because  \n",
    "our pipeline works similarly for different architectures. Here, we focus only on Stackix specifics.\n",
    "\n",
    "**AUTOENCODIX** supports far more functionality than shown here, so weâ€™ll also point to advanced tutorials where relevant.  \n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "> This tutorial only shows the specifics of the Stackix pipeline. If you're unfamiliar with general concepts,  \n",
    "> we recommend following the `Getting Started - Vanillix` tutorial first.\n",
    "\n",
    "## Stackix Theory\n",
    "\n",
    "In our implementation, we train one variational autoencoder (VAE) per data modality end-to-end.  \n",
    "The latent spaces of these *outer* autoencoders are then concatenated and used as input to another *inner* autoencoder.  \n",
    "Downstream visualization and evaluation are performed on this **meta-latent space**, which represents a joint embedding of all modalities.  \n",
    "\n",
    "For unpaired data, each outer autoencoder is trained on all available samples of its modality, even if the corresponding sample is missing in other modalities.  \n",
    "When constructing the input for the inner autoencoder, non-overlapping samples are dropped to ensure consistency across modalities.  \n",
    "\n",
    "This approach has two main goals:  \n",
    "1. To produce more informative latent spaces for each modality.  \n",
    "2. To provide a richer, joint representation for the inner autoencoder.  \n",
    "\n",
    "We also store the dropped samples and their indices. This allows us to reconstruct the full dataset: the output of the inner autoencoder can be passed back through the outer autoencoders, where decoding recovers the original modalities.  \n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "Youâ€™ll learn how to:\n",
    "\n",
    "1. **Initialize** the pipeline and run it. <br><br>\n",
    "2. Understand the Stackix-specific **pipeline steps** (paired vs. unpaired data). <br><br>\n",
    "3. Access the Stackix-specific **results** (sub-results for \"outer\" autoencoders). <br><br>\n",
    "4. **Visualize** outputs. <br><br>\n",
    "5. Apply **custom parameters**. <br><br>\n",
    "6. **Save, load, and reuse** a trained pipeline. <br><br>\n",
    "\n",
    "Letâ€™s get started! ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c582ab",
   "metadata": {},
   "source": [
    "## 1) Initialize and Run Stackix\n",
    "\n",
    "We set a few custom parameters of the config file. For a deep dive into the config object, see:  \n",
    "\n",
    "`Tutorials/DeepDives/ConfigTutorial.ipynb`\n",
    "\n",
    "#### 1.1 The Dataset\n",
    "\n",
    "We provide a mock dataset of sequencing data, including proteomics and RNA-seq.  \n",
    "The dataset is accompanied by annotation data containing metadata such as batch, condition, and quality score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff368c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      "multi_bulk:\n",
      "  transcriptomics: 500 samples Ã— 100 features\n",
      "  proteomics: 500 samples Ã— 80 features\n",
      "annotation:\n",
      "  transcriptomics: 500 samples Ã— 3 features\n",
      "  proteomics: 500 samples Ã— 3 features\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Starting Pipeline\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "in handle_direct_user_data with data: <class 'autoencodix.data.datapackage.DataPackage'>\n",
      "anno key: transcriptomics\n",
      "anno key: proteomics\n",
      "Training each modality model...\n",
      "Training modality: transcriptomics\n",
      "Training modality: transcriptomics\n"
     ]
    },
    {
     "ename": "BackendCompilerFailed",
     "evalue": "backend='inductor' raised:\nAssertionError: Device mps not supported\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m     22\u001b[0m stackix \u001b[38;5;241m=\u001b[39m acx\u001b[38;5;241m.\u001b[39mStackix(data\u001b[38;5;241m=\u001b[39mEXAMPLE_MULTI_BULK, config\u001b[38;5;241m=\u001b[39mmy_config)\n\u001b[0;32m---> 23\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mstackix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_pipeline.py:1031\u001b[0m, in \u001b[0;36mBasePipeline.run\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Executes the complete pipeline from preprocessing to visualization.\u001b[39;00m\n\u001b[1;32m   1021\u001b[0m \n\u001b[1;32m   1022\u001b[0m \u001b[38;5;124;03mRuns all pipeline steps in sequence and returns the result.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    Complete pipeline results.\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess()\n\u001b[0;32m-> 1031\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisualize()\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_pipeline.py:484\u001b[0m, in \u001b[0;36mBasePipeline.fit\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    467\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets not built. Please run the preprocess method first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    468\u001b[0m     )\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_type(\n\u001b[1;32m    471\u001b[0m     trainset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets\u001b[38;5;241m.\u001b[39mtrain,\n\u001b[1;32m    472\u001b[0m     validset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets\u001b[38;5;241m.\u001b[39mvalid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m     ),\n\u001b[1;32m    482\u001b[0m )\n\u001b[0;32m--> 484\u001b[0m trainer_result: Result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mupdate(other\u001b[38;5;241m=\u001b[39mtrainer_result)\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/trainers/_stackix_trainer.py:129\u001b[0m, in \u001b[0;36mStackixTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Train the stacked model on the concatenated latent space.\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03mUses the standard BaseTrainer training process but with the stacked model.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m    Training results including losses, latent spaces, and other metrics\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining each modality model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_modalities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinished training each modality model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_type(\n\u001b[1;32m    132\u001b[0m     trainset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_latent_ds,\n\u001b[1;32m    133\u001b[0m     validset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_valid_latent_ds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m     loss_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_type,\n\u001b[1;32m    138\u001b[0m )\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/trainers/_stackix_trainer.py:157\u001b[0m, in \u001b[0;36mStackixTrainer._train_modalities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Trains a Autoencoder for each modality in the dataset.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03mThis method orchestrates the training of individual modality models\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Step 1: Train individual modality models\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modality_trainers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modality_results \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_orchestrator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_modalities\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39msub_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modality_results\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# Step 2: Prepare concatenated latent space datasets\u001b[39;00m\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/trainers/_stackix_orchestrator.py:378\u001b[0m, in \u001b[0;36mStackixOrchestrator.train_modalities\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_distributed(keys\u001b[38;5;241m=\u001b[39mkeys)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_trainers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_results\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/trainers/_stackix_orchestrator.py:269\u001b[0m, in \u001b[0;36mStackixOrchestrator._train_sequential\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m    264\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainset\u001b[38;5;241m.\u001b[39mdatasets[modality]\n\u001b[1;32m    265\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validset\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mget(modality) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validset \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    267\u001b[0m )\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_modality\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodality_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_modality_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modality_latent_dims[modality] \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mget_input_dim()\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/trainers/_stackix_orchestrator.py:159\u001b[0m, in \u001b[0;36mStackixOrchestrator._train_modality\u001b[0;34m(self, modality, modality_dataset, valid_modality_dataset)\u001b[0m\n\u001b[1;32m    149\u001b[0m result \u001b[38;5;241m=\u001b[39m Result()\n\u001b[1;32m    150\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_class(\n\u001b[1;32m    151\u001b[0m     trainset\u001b[38;5;241m=\u001b[39mmodality_dataset,\n\u001b[1;32m    152\u001b[0m     validset\u001b[38;5;241m=\u001b[39mvalid_modality_dataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     loss_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_type,\n\u001b[1;32m    157\u001b[0m )\n\u001b[0;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_results[modality] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodality_trainers[modality] \u001b[38;5;241m=\u001b[39m trainer\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/trainers/_general_trainer.py:153\u001b[0m, in \u001b[0;36mGeneralTrainer.train\u001b[0;34m(self, epochs_overwrite)\u001b[0m\n\u001b[1;32m    150\u001b[0m should_checkpoint: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_checkpoint(epoch)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 153\u001b[0m epoch_loss, epoch_sub_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshould_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshould_checkpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_losses(epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch_loss, epoch_sub_losses)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validset:\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/trainers/_general_trainer.py:193\u001b[0m, in \u001b[0;36mGeneralTrainer._train_epoch\u001b[0;34m(self, should_checkpoint, epoch)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    192\u001b[0m X: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaskix_hook(features)\n\u001b[0;32m--> 193\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m loss, batch_sub_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss_fn(\n\u001b[1;32m    195\u001b[0m     model_output\u001b[38;5;241m=\u001b[39mmodel_outputs,\n\u001b[1;32m    196\u001b[0m     targets\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     ),  \u001b[38;5;66;03m# Pass n_samples for disentangled loss calculations\u001b[39;00m\n\u001b[1;32m    202\u001b[0m )\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fabric\u001b[38;5;241m.\u001b[39mbackward(loss)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/lightning_fabric/wrappers.py:136\u001b[0m, in \u001b[0;36m_FabricModule.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m precision\u001b[38;5;241m.\u001b[39mconvert_input((args, kwargs))\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m precision\u001b[38;5;241m.\u001b[39mforward_context():\n\u001b[0;32m--> 136\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m output \u001b[38;5;241m=\u001b[39m precision\u001b[38;5;241m.\u001b[39mconvert_output(output)\n\u001b[1;32m    140\u001b[0m apply_to_collection(output, dtype\u001b[38;5;241m=\u001b[39mTensor, function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_backward_hook)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:451\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:921\u001b[0m, in \u001b[0;36mcatch_errors_wrapper.<locals>.catch_errors\u001b[0;34m(frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(frame, cache_entry, hooks, frame_state)\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[1;32m    920\u001b[0m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:786\u001b[0m, in \u001b[0;36mconvert_frame.<locals>._convert_frame\u001b[0;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    784\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 786\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43minner_convert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_entry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    789\u001b[0m     counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:400\u001b[0m, in \u001b[0;36mconvert_frame_assert.<locals>._convert_frame_assert\u001b[0;34m(frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    386\u001b[0m compile_id \u001b[38;5;241m=\u001b[39m CompileId(frame_id, frame_compile_id)\n\u001b[1;32m    388\u001b[0m signpost_event(\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_convert_frame_assert._compile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m     },\n\u001b[1;32m    398\u001b[0m )\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_globals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_locals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf_builtins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompiler_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexport_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mframe_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframe_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompile_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompile_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.4-macos-aarch64-none/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:676\u001b[0m, in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, compiler_fn, one_graph, export, export_constraints, hooks, cache_size, frame, frame_state, compile_id, skip)\u001b[0m\n\u001b[1;32m    674\u001b[0m fail_user_frame_lineno: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 676\u001b[0m     guarded_code \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_inner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mone_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhooks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m guarded_code\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\n\u001b[1;32m    679\u001b[0m     Unsupported,\n\u001b[1;32m    680\u001b[0m     TorchRuntimeError,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    687\u001b[0m     BisectValidationException,\n\u001b[1;32m    688\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:535\u001b[0m, in \u001b[0;36m_compile.<locals>.compile_inner\u001b[0;34m(code, one_graph, hooks, transform)\u001b[0m\n\u001b[1;32m    533\u001b[0m CompileContext\u001b[38;5;241m.\u001b[39mget()\u001b[38;5;241m.\u001b[39mattempt \u001b[38;5;241m=\u001b[39m attempt\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 535\u001b[0m     out_code \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_code_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mRestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/bytecode_transformation.py:1036\u001b[0m, in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1033\u001b[0m instructions \u001b[38;5;241m=\u001b[39m cleaned_instructions(code, safe)\n\u001b[1;32m   1034\u001b[0m propagate_line_nums(instructions)\n\u001b[0;32m-> 1036\u001b[0m \u001b[43mtransformations\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:165\u001b[0m, in \u001b[0;36mpreserve_global_state.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m cleanup \u001b[38;5;241m=\u001b[39m setup_compile_debug()\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m     cleanup\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/convert_frame.py:500\u001b[0m, in \u001b[0;36m_compile.<locals>.transform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mtracing_context), tracer\u001b[38;5;241m.\u001b[39mset_current_tx():\n\u001b[0;32m--> 500\u001b[0m         \u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mUnspecializeRestartAnalysis:\n\u001b[1;32m    502\u001b[0m     speculation_log\u001b[38;5;241m.\u001b[39mclear()\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2149\u001b[0m, in \u001b[0;36mInstructionTranslator.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2149\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:810\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mpush_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\n\u001b[1;32m    808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstruction_pointer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mshould_exit\n\u001b[0;32m--> 810\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    811\u001b[0m     ):\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BackendCompilerFailed:\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:773\u001b[0m, in \u001b[0;36mInstructionTranslatorBase.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m         unimplemented(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minst\u001b[38;5;241m.\u001b[39mopname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m     TracingContext\u001b[38;5;241m.\u001b[39mset_current_loc(\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_filename, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineno, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\n\u001b[1;32m    772\u001b[0m     )\n\u001b[0;32m--> 773\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inst\u001b[38;5;241m.\u001b[39mopname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/symbolic_convert.py:2268\u001b[0m, in \u001b[0;36mInstructionTranslator.RETURN_VALUE\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   2263\u001b[0m _step_logger()(\n\u001b[1;32m   2264\u001b[0m     logging\u001b[38;5;241m.\u001b[39mINFO,\n\u001b[1;32m   2265\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchdynamo done tracing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_code\u001b[38;5;241m.\u001b[39mco_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (RETURN_VALUE)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2266\u001b[0m )\n\u001b[1;32m   2267\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE triggered compile\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2268\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_subgraph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2269\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreason\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mGraphCompileReason\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2271\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreturn_value\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m   2272\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39madd_output_instructions([create_instruction(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETURN_VALUE\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1001\u001b[0m, in \u001b[0;36mOutputGraph.compile_subgraph\u001b[0;34m(self, tx, partial_convert, reason)\u001b[0m\n\u001b[1;32m    998\u001b[0m output \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count_calls(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1000\u001b[0m     output\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m-> 1001\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_and_call_fx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_output_vars\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m     )\n\u001b[1;32m   1004\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pass2\u001b[38;5;241m.\u001b[39mgraph_outputs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1005\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(pass2\u001b[38;5;241m.\u001b[39mcreate_store(graph_output_var))\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.4-macos-aarch64-none/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1178\u001b[0m, in \u001b[0;36mOutputGraph.compile_and_call_fx_graph\u001b[0;34m(self, tx, rv, root)\u001b[0m\n\u001b[1;32m   1175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracing_context\u001b[38;5;241m.\u001b[39mfake_mode \u001b[38;5;241m=\u001b[39m backend_fake_mode\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrestore_global_state():\n\u001b[0;32m-> 1178\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_user_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1179\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m disable(compiled_fn)\n\u001b[1;32m   1181\u001b[0m counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstats\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_graphs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1251\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BackendCompilerFailed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiler_fn, e)\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[1;32m   1252\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[1;32m   1253\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m signpost_event(\n\u001b[1;32m   1256\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutputGraph.call_user_compiler\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     },\n\u001b[1;32m   1264\u001b[0m )\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/output_graph.py:1232\u001b[0m, in \u001b[0;36mOutputGraph.call_user_compiler\u001b[0;34m(self, gm)\u001b[0m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mverify_correctness:\n\u001b[1;32m   1231\u001b[0m     compiler_fn \u001b[38;5;241m=\u001b[39m WrapperBackend(compiler_fn)\n\u001b[0;32m-> 1232\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexample_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1233\u001b[0m _step_logger()(logging\u001b[38;5;241m.\u001b[39mINFO, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone compiler function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(compiled_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompiler_fn did not return callable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_dynamo.py:117\u001b[0m, in \u001b[0;36mwrap_backend_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     compiled_gm \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m compiled_gm\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/__init__.py:1731\u001b[0m, in \u001b[0;36m_TorchCompileInductorWrapper.__call__\u001b[0;34m(self, model_, inputs_)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, model_, inputs_):\n\u001b[1;32m   1729\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompile_fx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compile_fx\n\u001b[0;32m-> 1731\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompile_fx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_patches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.4-macos-aarch64-none/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1330\u001b[0m, in \u001b[0;36mcompile_fx\u001b[0;34m(model_, example_inputs_, inner_compile, config_patches, decompositions)\u001b[0m\n\u001b[1;32m   1325\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m inference_compiler(unlifted_gm, example_inputs_)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m V\u001b[38;5;241m.\u001b[39mset_fake_mode(fake_mode), torch\u001b[38;5;241m.\u001b[39m_guards\u001b[38;5;241m.\u001b[39mtracing(\n\u001b[1;32m   1328\u001b[0m     tracing_context\n\u001b[1;32m   1329\u001b[0m ), compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[0;32m-> 1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43maot_autograd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbw_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbw_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1333\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_compiler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_compiler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecompositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecompositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartition_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1336\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_inference_input_mutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs_\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/backends/common.py:58\u001b[0m, in \u001b[0;36maot_autograd.<locals>.compiler_fn\u001b[0;34m(gm, example_inputs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# NB: NOT cloned!\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m enable_aot_logging(), patch_config:\n\u001b[0;32m---> 58\u001b[0m         cg \u001b[38;5;241m=\u001b[39m \u001b[43maot_module_simplified\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m         counters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot_autograd\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m disable(cg)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:903\u001b[0m, in \u001b[0;36maot_module_simplified\u001b[0;34m(mod, args, fw_compiler, bw_compiler, partition_fn, decompositions, keep_inference_input_mutations, inference_compiler)\u001b[0m\n\u001b[1;32m    887\u001b[0m aot_config \u001b[38;5;241m=\u001b[39m AOTConfig(\n\u001b[1;32m    888\u001b[0m     fw_compiler\u001b[38;5;241m=\u001b[39mfw_compiler,\n\u001b[1;32m    889\u001b[0m     bw_compiler\u001b[38;5;241m=\u001b[39mbw_compiler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    899\u001b[0m     no_tangents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    900\u001b[0m )\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m compiled_autograd\u001b[38;5;241m.\u001b[39mdisable():\n\u001b[0;32m--> 903\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_aot_dispatcher_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctional_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;66;03m# TODO: There is something deeply wrong here; compiled_fn running with\u001b[39;00m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;66;03m# the boxed calling convention, but aot_module_simplified somehow\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;66;03m# historically returned a function that was not the boxed calling\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;66;03m# convention.  This should get fixed...\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39mruntime_args):\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_functorch/aot_autograd.py:628\u001b[0m, in \u001b[0;36mcreate_aot_dispatcher_function\u001b[0;34m(flat_fn, flat_args, aot_config)\u001b[0m\n\u001b[1;32m    625\u001b[0m compiler_fn \u001b[38;5;241m=\u001b[39m partial(aot_wrapper_dedupe, compiler_fn\u001b[38;5;241m=\u001b[39mcompiler_fn)\n\u001b[1;32m    626\u001b[0m \u001b[38;5;66;03m# You can put more passes here\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aot_config\u001b[38;5;241m.\u001b[39mis_export:\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;66;03m# During export, we don't get back a callable - we get back the raw fx graph\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# (either a joint or an inference-only graph)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(compiled_fn, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mGraphModule)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:443\u001b[0m, in \u001b[0;36maot_wrapper_dedupe\u001b[0;34m(flat_fn, flat_args, aot_config, compiler_fn, fw_metadata)\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ok:\n\u001b[0;32m--> 443\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleaf_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(leaf_flat_args, fw_metadata):\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    447\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\\\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;124;03mEncountered duplicate inputs that are mutated in the graph, but at least one input/output\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03mto the graph is a tensor subclass. This is not supported today. You can try to\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03mremove the aliasing yourself as a workaround, or otherwise file an issue on github.\"\"\"\u001b[39;00m\n\u001b[1;32m    451\u001b[0m         )\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py:648\u001b[0m, in \u001b[0;36maot_wrapper_synthetic_base\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata, needs_autograd, compiler_fn)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# Happy path: we don't need synthetic bases\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synthetic_base_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maot_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfw_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfw_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[38;5;66;03m# export path: ban synthetic bases for now, add later if requested.\u001b[39;00m\n\u001b[1;32m    651\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_subclass_dispatch(flat_args, fw_metadata):\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_functorch/_aot_autograd/jit_compile_runtime_wrappers.py:352\u001b[0m, in \u001b[0;36maot_dispatch_autograd\u001b[0;34m(flat_fn, flat_args, aot_config, fw_metadata)\u001b[0m\n\u001b[1;32m    349\u001b[0m     tracing_context\u001b[38;5;241m.\u001b[39mfw_metadata \u001b[38;5;241m=\u001b[39m inner_meta\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TracingContext\u001b[38;5;241m.\u001b[39mreport_output_strides() \u001b[38;5;28;01mas\u001b[39;00m fwd_output_strides:\n\u001b[0;32m--> 352\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m \u001b[43maot_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfw_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfw_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madjusted_flat_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(compiled_fw_func, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_boxed_call\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    354\u001b[0m     compiled_fw_func \u001b[38;5;241m=\u001b[39m make_boxed_func(compiled_fw_func)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:1257\u001b[0m, in \u001b[0;36mcompile_fx.<locals>.fw_compiler_base\u001b[0;34m(model, example_inputs, is_inference)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m orig_output_end_idx \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m num_model_outputs\n\u001b[1;32m   1251\u001b[0m     user_visible_outputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1252\u001b[0m         n\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m   1253\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m model_outputs[original_output_start_index:orig_output_end_idx]\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(n, torch\u001b[38;5;241m.\u001b[39mfx\u001b[38;5;241m.\u001b[39mNode)\n\u001b[1;32m   1255\u001b[0m     }\n\u001b[0;32m-> 1257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_fixed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcudagraphs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcudagraphs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgraph_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_inference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_inference\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboxed_forward_device_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforward_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_visible_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_visible_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/repro/after_aot.py:83\u001b[0m, in \u001b[0;36mwrap_compiler_debug.<locals>.debug_wrapper\u001b[0;34m(gm, example_inputs, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# Call the compiler_fn - which is either aot_autograd or inductor\u001b[39;00m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;66;03m# with fake inputs\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m     inner_compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;66;03m# TODO: Failures here are troublesome because no real inputs,\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# need a different serialization strategy\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mrepro_after \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maot\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_inductor/debug.py:304\u001b[0m, in \u001b[0;36mDebugContext.wrap.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m DebugContext():\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.4-macos-aarch64-none/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.10.4-macos-aarch64-none/lib/python3.10/contextlib.py:79\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_recreate_cm():\n\u001b[0;32m---> 79\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:438\u001b[0m, in \u001b[0;36mcompile_fx_inner\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, boxed_forward_device_index, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    434\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m FxGraphCache\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    435\u001b[0m         fx_codegen_and_compile, gm, example_inputs, graph_kwargs\n\u001b[1;32m    436\u001b[0m     )\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     compiled_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfx_codegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFX codegen and compilation took \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# check cudagraph disabling reasons from inductor lowering\u001b[39;00m\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:714\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[0;34m(gm, example_inputs, cudagraphs, num_fixed, is_backward, graph_id, cpp_wrapper, aot_mode, is_inference, user_visible_outputs, layout_opt, extern_node_serializer)\u001b[0m\n\u001b[1;32m    711\u001b[0m             output_strides\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    713\u001b[0m metrics_helper \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39mCachedMetricsHelper()\n\u001b[0;32m--> 714\u001b[0m compiled_fn \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m V\u001b[38;5;241m.\u001b[39maot_compilation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compiled_fn\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_inductor/graph.py:1307\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_fn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28mself\u001b[39m, code, serialized_extern_kernel_nodes, cuda\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcuda\n\u001b[1;32m   1305\u001b[0m     )\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcall\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_dynamo/utils.py:262\u001b[0m, in \u001b[0;36mdynamo_timed.<locals>.dynamo_timed_inner.<locals>.time_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (dynamo_timed)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    261\u001b[0m     t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 262\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     time_spent \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t0\n\u001b[1;32m    264\u001b[0m compilation_time_metrics[key]\u001b[38;5;241m.\u001b[39mappend(time_spent)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_inductor/graph.py:1250\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;129m@dynamo_timed\u001b[39m(phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PyCodeCache\n\u001b[1;32m   1249\u001b[0m     code, linemap \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m-> 1250\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1251\u001b[0m     )\n\u001b[1;32m   1252\u001b[0m     linemap \u001b[38;5;241m=\u001b[39m [(line_no, node\u001b[38;5;241m.\u001b[39mstack_trace) \u001b[38;5;28;01mfor\u001b[39;00m line_no, node \u001b[38;5;129;01min\u001b[39;00m linemap]\n\u001b[1;32m   1253\u001b[0m     key, path \u001b[38;5;241m=\u001b[39m PyCodeCache\u001b[38;5;241m.\u001b[39mwrite(code)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_inductor/graph.py:1203\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcodegen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscheduler\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Scheduler\n\u001b[0;32m-> 1203\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_wrapper_code\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1205\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler \u001b[38;5;241m=\u001b[39m Scheduler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffers)\n\u001b[1;32m   1206\u001b[0m     V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n",
      "File \u001b[0;32m~/development/autoencodix_package/.venv/lib/python3.10/site-packages/torch/_inductor/graph.py:1133\u001b[0m, in \u001b[0;36mGraphLowering.init_wrapper_code\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ops \u001b[38;5;241m=\u001b[39m get_device_op_overrides(device_type)\n\u001b[1;32m   1132\u001b[0m     wrapper_code_gen_cls \u001b[38;5;241m=\u001b[39m get_wrapper_codegen_for_device(device_type)\n\u001b[0;32m-> 1133\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m   1134\u001b[0m         wrapper_code_gen_cls \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code \u001b[38;5;241m=\u001b[39m wrapper_code_gen_cls()\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconst_module:\n\u001b[1;32m   1139\u001b[0m     \u001b[38;5;66;03m# If we have const module, we could reuse the kernels\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m     \u001b[38;5;66;03m# This could avoid duplication and save time on doing recompilation (if Triton.)\u001b[39;00m\n",
      "\u001b[0;31mBackendCompilerFailed\u001b[0m: backend='inductor' raised:\nAssertionError: Device mps not supported\n\nSet TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n\n\nYou can suppress this exception and fall back to eager by setting:\n    import torch._dynamo\n    torch._dynamo.config.suppress_errors = True\n"
     ]
    }
   ],
   "source": [
    "import autoencodix as acx\n",
    "from autoencodix.configs.default_config import DataCase\n",
    "from autoencodix.configs.stackix_config import StackixConfig\n",
    "from autoencodix.utils.example_data import EXAMPLE_MULTI_BULK\n",
    "\n",
    "# Stackix has its own config class\n",
    "# instead of passing a pandas DataFrame, we use a pre-defined DataPackage object directly.\n",
    "# this time with single cell data\n",
    "print(\"Input data:\")\n",
    "print(EXAMPLE_MULTI_BULK)\n",
    "print(\"-\" * 50)\n",
    "my_config = StackixConfig(\n",
    "    epochs=27,\n",
    "    checkpoint_interval=5,\n",
    "    default_vae_loss=\"kl\",  # kl or mmd possible\n",
    "    data_case=DataCase.MULTI_BULK,\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(\"Starting Pipeline\")\n",
    "print(\"-\" * 50)\n",
    "print(\"-\" * 50)\n",
    "stackix = acx.Stackix(data=EXAMPLE_MULTI_BULK, config=my_config)\n",
    "result = stackix.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb84dfb",
   "metadata": {},
   "source": [
    "## 2) Stackix-Specific Steps\n",
    "\n",
    "Stackix does not introduce additional steps to the overall pipeline. However, its preprocessing and training differ slightly from Varix and Vanillix.  \n",
    "Stackix processes each data modality individually and thus supports unpaired data input.  \n",
    "\n",
    "This is enabled by setting the config parameter `requires_paired=False`.  \n",
    "In the following example, we build unpaired data by dropping some samples from our raw_rna data. The outer autoencoder for the protein modality still trains on all protein samples, including those without corresponding RNA. These RNA samples are only excluded when concatenating the latent spaces for the inner autoencoder. Later, we can add these samples back to the inner AE output and feed them into the decoder of the outer protein autoencoder, allowing full reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from autoencodix.data.datapackage import DataPackage\n",
    "from autoencodix.utils.example_data import annotation, raw_protein, raw_rna\n",
    "from autoencodix.configs.stackix_config import StackixConfig\n",
    "from autoencodix.configs.default_config import DataCase\n",
    "import autoencodix as acx\n",
    "\n",
    "rna_anno: pd.DataFrame = annotation.drop(\n",
    "    index=[\"sample_2\", \"sample_8\", \"sample_50\", \"sample_61\"]\n",
    ")\n",
    "print(rna_anno.shape)\n",
    "rna_unpaired: pd.DataFrame = raw_rna.drop(\n",
    "    index=[\"sample_2\", \"sample_8\", \"sample_50\", \"sample_61\"]\n",
    ")\n",
    "print(rna_unpaired.shape)\n",
    "unpaired_dp: DataPackage = DataPackage(\n",
    "    multi_bulk={\"rna\": rna_unpaired, \"protein\": raw_protein},\n",
    "    annotation={\"rna\": rna_anno, \"protein\": annotation},\n",
    ")\n",
    "unpaired_config: StackixConfig = StackixConfig(\n",
    "    data_case=DataCase.MULTI_BULK, requires_paired=False\n",
    ")\n",
    "unpaired_stackix: acx.Stackix = acx.Stackix(data=unpaired_dp, config=unpaired_config)\n",
    "unpaired_result = unpaired_stackix.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131eb195",
   "metadata": {},
   "source": [
    "## 3) Access Stackix-Specific Outputs\n",
    "\n",
    "As explained above, we train *outer* autoencoders for each data modality and one shared *inner* autoencoder.  \n",
    "The results (losses, reconstructions, latent spaces, etc.) for the *inner* autoencoder can be accessed in the same way as for the Vanillix autoencoder (see [1] and [2]).  \n",
    "The results for the *outer* autoencoders are stored in a special `sub_results` structure, as illustrated in the Python code below.  \n",
    "\n",
    "The `sub_results` can be accessed as an attribute of the `result` object.  \n",
    "On the first level, this is a `Dict` with keys corresponding to the data modalities (as defined in Step 1).  \n",
    "The value for each key is a `result` object analogous to the general result object.  \n",
    "These can be accessed via our standard API (see also [2]).\n",
    "\n",
    "[1] `Tutorials/PipelineTutorials/Vanillix.ipynb`  \n",
    "[2] `Tutorials/DeepDives/PipelineOutputTutorial.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e9a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outer_result = result.sub_results\n",
    "print(outer_result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72eafb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outer_result[\"proteomics\"])\n",
    "# access latent space of outer proteomics autoencoder\n",
    "outer_result[\"proteomics\"].latentspaces.get(split=\"test\", epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e4309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the inner result can be accessed directly\n",
    "result.latentspaces.get(split=\"test\", epoch=-1)\n",
    "# or as dataframe with ids\n",
    "result.get_latent_df(split=\"test\", epoch=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af410445",
   "metadata": {},
   "source": [
    "## 4) Visualize Outputs\n",
    "\n",
    "This works exactly as for our other pipelines, by calling the `show_result` method.  \n",
    "For more information, see our Visualization Deep Dive.  \n",
    "\n",
    "We can pass a column from the annotation file using the keyword argument `params` to color the plots according to this column.\n",
    "\n",
    "[3] `Tutorials/DeepDives/VisualizeTutorial.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805e0a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackix.show_result(params=[\"batch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d170d8",
   "metadata": {},
   "source": [
    "## 5) Customize Stackix Parameters\n",
    "\n",
    "Since Stackix is composed of variational autoencoders, there are no Stackix-specific customizations that were not already shown in the `Varix` tutorial.  \n",
    "\n",
    "However, it is possible to change the architecture to a vanilla autoencoder via the `model_type` attribute when initializing Stackix.  \n",
    "Please note that this attribute expects a **class type**, not an instance of the class.  \n",
    "Therefore, you need to import the appropriate `Architecture` (see [API reference](https://maxjoas.github.io/autoencodix_package/api/modeling)) and the corresponding loss type (e.g., `VanillixLoss` if using a vanilla autoencoder).  \n",
    "\n",
    "See the code below for a clearer example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1951bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencodix as acx\n",
    "from autoencodix.configs.default_config import DataCase\n",
    "from autoencodix.configs.stackix_config import StackixConfig\n",
    "from autoencodix.utils.example_data import EXAMPLE_MULTI_BULK\n",
    "from autoencodix.modeling import VanillixArchitecture\n",
    "from autoencodix.utils import VanillixLoss\n",
    "\n",
    "my_config = StackixConfig(\n",
    "    epochs=27,\n",
    "    checkpoint_interval=5,\n",
    "    data_case=DataCase.MULTI_BULK,\n",
    ")\n",
    "stackix_custom = acx.Stackix(\n",
    "    data=EXAMPLE_MULTI_BULK,\n",
    "    config=my_config,\n",
    "    model_type=VanillixArchitecture,\n",
    "    loss_type=VanillixLoss,\n",
    ")\n",
    "result_custom = stackix_custom.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c1282",
   "metadata": {},
   "source": [
    "## 6) Saving, Loading and Reusing\n",
    "This works for `Stackix` as for any other models, by using the `save` and `load` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca3393e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "outpath = os.path.join(\"tutorial_res\", \"stackix\")\n",
    "stackix.save(file_path=outpath, save_all=False)\n",
    "\n",
    "folder = os.path.dirname(outpath)\n",
    "pkl_files = glob.glob(os.path.join(folder, \"*.pkl\"))\n",
    "model_files = glob.glob(os.path.join(folder, \"*.pth\"))\n",
    "\n",
    "print(\"PKL files:\", pkl_files)\n",
    "print(\"Model files:\", model_files)\n",
    "\n",
    "# The load functionality will automatically rebuild the pipeline object\n",
    "# from the saved files.\n",
    "stackix_loaded = acx.Stackix.load(outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893bf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackix_loaded.predict(data=EXAMPLE_MULTI_BULK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88410743",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackix_loaded.show_result(params=[\"batch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb17ab",
   "metadata": {},
   "source": [
    "#### Generate New Data\n",
    "For a variational autoencoder, the generate or sample_latent_space step draws new latent vectors from the modelâ€™s learned latent distribution.\n",
    "\n",
    "**Latent Sampling:** \n",
    "The model first aggregates the posterior over all encoded latent vectors in the chosen split and epoch by computing the mean (global_mu) and log-variance (global_logvar). It then samples new latent points from a diagonal Gaussian defined by these aggregate statistics, using the reparameterization trick to inject Gaussian noise.\n",
    "\n",
    "**Number of Samples (n_samples):** \n",
    "Users can specify how many latent points to generate. The method expands the aggregated mean and log-variance to match the requested number of samples before sampling.\n",
    "\n",
    "**Custom Latent Prior:** \n",
    "Optionally, a custom latent_prior can be provided (a tensor or NumPy array with shape (n_samples, latent_dim)), which will be used directly instead of the aggregated posterior. This is basically the `decode` step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7a7423",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_reconstructions = stackix_loaded.generate(n_samples=5)\n",
    "print(generated_reconstructions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencodix_package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
