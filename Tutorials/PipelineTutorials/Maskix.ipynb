{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c3420e",
   "metadata": {},
   "source": [
    "\n",
    "# How To Use Maskix\n",
    "\n",
    "Maskix is our implementation of a variational autoencoder.  \n",
    "This tutorial follows the structure of our `Getting Started - Vanillix`, but is much less extensive, because  \n",
    "our pipeline works similarly for different architectures, so here we focus only on Maskix specifics.\n",
    "\n",
    "**AUTOENCODIX** supports far more functionality than shown here, so we’ll also point to advanced tutorials where relevant.  \n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "> This tutorial only shows the specifics of the Maskix pipeline. If you're unfamiliar with general concepts,  \n",
    "> we recommend following the `Getting Started - Vanillix` tutorial first.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "You’ll learn how to:\n",
    "\n",
    "1. **Theory primer** of the Maskix architecture. <br><br>\n",
    "2. **Initialize** the pipeline and run the pipeline. <br><br>\n",
    "3. Understand Maskix-specific **config parameters*. <br><br>\n",
    "4. **Access & Visualize** the results effectively. <br><br>\n",
    "5. Pass **custom masking fucntions**. <br><br>\n",
    "6. How to use Maskix for **data imputation** <br><br>\n",
    "7. **Save, load, and reuse** a trained pipeline. <br><br>\n",
    "\n",
    "\n",
    "## 1) Theory Primer\n",
    "\n",
    "Maskix adapts the scMAE (single-cell Masked Autoencoder) framework from Fang et al. (2024) for single-cell RNA-seq analysis. The model learns useful representations by corrupting the input expression matrix and training the network both to reconstruct the original data and to identify which entries were perturbed. This encourages the model to capture gene–gene (or feature–feature) relationships in high-dimensional, noisy datasets. Although originally designed for single-cell data, the same approach applies to other domains.\n",
    "\n",
    "Each training iteration follows this corruption process:\n",
    "\n",
    "1. Sample a Bernoulli distribution to determine which entries should be perturbed, producing a binary mask with the same shape as the input.\n",
    "2. For each gene (or feature), generate a random permutation of sample indices.\n",
    "3. For all positions marked as masked, replace the original value with the value from the permuted index for that feature.\n",
    "\n",
    "The corrupted input is encoded into a low-dimensional latent representation. In parallel, a mask predictor takes the latent space as input and estimates which positions were masked. The predicted mask is then concatenated with the latent representation and passed into a decoder that attempts to reconstruct the original values.\n",
    "\n",
    "Training uses two loss components. The mask predictor is optimized with binary cross-entropy against the true mask. Reconstruction uses a weighted mean-squared error where masked entries receive higher weight, controlled by the hyperparameter `delta_mask_corrupted`. The total loss is the weighted sum of these components, balanced by the hyperparameter `delta_mask_predictor`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebc5d2d",
   "metadata": {},
   "source": [
    "## Requirements 1: Be in the correct directory (execute below)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84fe753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "p = os.getcwd()\n",
    "d = \"autoencodix_package\"\n",
    "if d not in p:\n",
    "    raise FileNotFoundError(f\"'{d}' not found in path: {p}\")\n",
    "os.chdir(os.sep.join(p.split(os.sep)[: p.split(os.sep).index(d) + 1]))\n",
    "print(f\"Changed to: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2ee90",
   "metadata": {},
   "source": [
    "## Requirements 2: Obtain tutorial data or use own data\n",
    "We use a dataset from the [Fang et al. (2024)](https://doi.org/10.1093/bioinformatics/btae020), with can be donwloaded [here](https://cloud.scadsai.uni-leipzig.de/index.php/s/BP8YzDef4nSfgwj/download/GSE84133_human_combined_final.h5ad), this needs to be moved to `data/raw`. Alternatively you can use your own single-cell dataset (Maskix works with other datatypes, but in this example we expect single cell data: `h5ad`) file and replace the variable `sc_path`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7d7f48",
   "metadata": {},
   "source": [
    "## 2) Initialize and Run Maskix\n",
    "As for every other pipline, we need to perform the following steps:\n",
    "- import relevant classes\n",
    "- define a config\n",
    "- init the pipeline\n",
    "- call the run step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414f7f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencodix as acx\n",
    "from autoencodix.configs import MaskixConfig\n",
    "from autoencodix.configs.default_config import DataInfo, DataConfig, DataCase\n",
    "\n",
    "sc_path = os.path.join(\"data/raw\", \"GSE84133_human_combined_final.h5ad\")\n",
    "config = MaskixConfig(\n",
    "    epochs=30,\n",
    "    checkpoint_interval=10,\n",
    "    k_filter=1000,\n",
    "    batch_size=64,\n",
    "    data_config=DataConfig(\n",
    "        annotation_columns=[\"multi_sc:assigned_cluster\"],\n",
    "        data_info={\n",
    "            \"multi_sc\": DataInfo(\n",
    "                file_path=sc_path, is_single_cell=True, data_type=\"NUMERIC\"\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    "    data_case=DataCase.MULTI_SINGLE_CELL,\n",
    ")\n",
    "maskix = acx.Maskix(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6132d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskix_result = maskix.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1136bec1",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Understanding Maskix-specific Config Parameters\n",
    "\n",
    "As described in the [theory section](#1-theory-primer), we implemented the architecture from [Fang et al. (2024)](https://doi.org/10.1093/bioinformatics/btae020), with weighted loss terms and a specific encoder–decoder design. To make this adaptable within our framework, we expose the following configuration parameters:\n",
    "\n",
    "* **maskix_hidden_dim**: Hidden dimension used in the Maskix encoder and decoder, matching the scMAE reference architecture by default.\n",
    "* **maskix_swap_prob**: Bernoulli probability controlling how often feature values are swapped during input corruption.\n",
    "* **delta_mask_predictor**: Weighting factor for the mask prediction loss in the total training objective.\n",
    "* **delta_mask_corrupted**: Weighting factor that increases the reconstruction penalty on corrupted entries.\n",
    "* **maskix_architecture**: Selects between the default scMAE architecture or a custom architecture configured through `n_layers` and `enc_factor`.\n",
    "\n",
    "You can find the default values by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e6529e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MaskixConfig.print_schema(\n",
    "    filter_params=[\n",
    "        \"maskix_hidden_dim\",\n",
    "        \"maskix_swap_prob\",\n",
    "        \"delta_mask_predictor\",\n",
    "        \"delta_mask_corrupted\",\n",
    "        \"maskix_architecture\",\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9a4bf8",
   "metadata": {},
   "source": [
    "## 4) Access & Visualize Results Effectively\n",
    "\n",
    "In addition to the results that the Vanillix pipeline provided, we can access:\n",
    "- `total`, `reconstruction`, and `masked` losses  \n",
    "\n",
    "A note on the different loss types:  \n",
    "For our maksed autoencoder, the total loss consists of a reconstruction loss and a mask predictor los\n",
    "To investigate these losses, the `result` object has the attribute `sub_losses`.  \n",
    "This is a `LossRegistry` with the name of the loss as the key, and the value is a `TrainingDynamics` object, which can be accessed in the same way as for the Vanillix results.\n",
    "\n",
    "For more details, check `Tutorials/DeepDives/PipelineOutputTutorial.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908a62ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_losses = maskix_result.sub_losses\n",
    "print(\"Sub Losses:\")\n",
    "print(f\"keys: {sub_losses.keys()}\")\n",
    "print(\"\\n\")\n",
    "recon_dyn = sub_losses.get(key=\"recon_loss\")\n",
    "print(\"Value of reconstruction loss in epoch 4 for train split\")\n",
    "print(recon_dyn.get(split=\"train\", epoch=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbac4e4",
   "metadata": {},
   "source": [
    "As for our other pipelines we can visualize the loss with `.show_result()`\n",
    "For more infos on visualization, see: `Tutorials/DeepDives/VisualizeTutorial.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe788a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "maskix.show_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f18cfcd",
   "metadata": {},
   "source": [
    "## 5) Adding a Custom Masking Function to `MaskixTrainer`\n",
    "\n",
    "`MaskixTrainer` supports replacing its default corruption mechanism with a user-defined masking function. This enables experimentation with alternative masking strategies while ensuring compatibility with the trainer’s data flow.\n",
    "\n",
    "#### How to Add a Custom Masking Function\n",
    "\n",
    "Provide your masking function at initialization:\n",
    "\n",
    "```python\n",
    "# We assume that config is defined and other imports are done (see above)\n",
    "\n",
    "def my_masking_fn(x: torch.Tensor, strength: float = 0.2):\n",
    "    noise = torch.randn_like(x) * strength\n",
    "    return x + noise  # must return ONLY a single tensor in shape of input tensor\n",
    "\n",
    "masking_fn_kwargs = {\"strength\": 0.1}\n",
    "maskix = acx.Maskix(config=config, masking_fn, masking_fn_kwargs\n",
    ")\n",
    "```\n",
    "#### Requirements for a Custom Masking Function\n",
    "\n",
    "A custom masking function must satisfy the following constraints:\n",
    "\n",
    "1. **It must accept a `torch.Tensor` as the first positional argument.**  \n",
    "   The trainer passes the input mini-batch `X` directly into the function.\n",
    "\n",
    "2. **It must return exactly one value: a `torch.Tensor`.**  \n",
    "   The trainer does not consume or propagate additional outputs.  \n",
    "   Returning tuples or multiple values is not allowed.\n",
    "\n",
    "3. **The returned tensor must have the same shape as the input tensor.**  \n",
    "   Any shape mismatch will raise a validation error.\n",
    "\n",
    "4. **The function must operate on the device of the input tensor.**  \n",
    "   The function must not assume the tensor resides on the CPU; it must operate on the device of `x`.\n",
    "\n",
    "5. **Any additional parameters must be passed via `masking_fn_kwargs`.**  \n",
    "   These keyword arguments provide a clean separation between trainer configuration and masking logic.\n",
    "\n",
    "##### Example\n",
    "This is our default masking method:\n",
    "```python\n",
    "    def _maskix_hook(\n",
    "        self, X: torch.Tensor\n",
    "    ) -> torch.Tensor\n",
    "        # expand probablities for bernoulli sampling to match input shape\n",
    "        probs = self._mask_probas.expand(X.shape)\n",
    "\n",
    "        # Create the Boolean Mask (1 = Swap, 0 = Keep)\n",
    "        should_swap = torch.bernoulli(probs).bool()\n",
    "\n",
    "        # COLUMN-WISE SHUFFLING\n",
    "        # We generate a random float matrix and argsort it along dim=0.\n",
    "        # This gives us independent random indices for every column.\n",
    "        rand_indices = torch.rand(X.shape, device=X.device).argsort(dim=0)\n",
    "\n",
    "        # Use gather to reorder X based on these random indices\n",
    "        shuffled_X = torch.gather(X, 0, rand_indices)\n",
    "        corrupted_X = torch.where(should_swap, shuffled_X, X)\n",
    "\n",
    "        return corrupted_X\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4c2b4",
   "metadata": {},
   "source": [
    "##### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241c71ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import autoencodix as acx\n",
    "from autoencodix.configs import MaskixConfig\n",
    "from autoencodix.configs.default_config import DataInfo, DataConfig, DataCase\n",
    "\n",
    "\n",
    "def my_masking_fn(x: torch.Tensor, strength: float = 0.2):\n",
    "    # Noise is created with the same shape, dtype, and device as `x`\n",
    "    # Because of randn_lie, if you use other function, take care of \n",
    "    # device and dtype casting.\n",
    "    noise = torch.randn_like(x) * strength\n",
    "    return x + noise\n",
    "\n",
    "\n",
    "kwargs = {\"strength\": 0.5}\n",
    "\n",
    "\n",
    "sc_path = os.path.join(\"data/raw\", \"GSE84133_human_combined_final.h5ad\")\n",
    "config = MaskixConfig(\n",
    "    epochs=5,\n",
    "    checkpoint_interval=2,\n",
    "    batch_size=64,\n",
    "    data_config=DataConfig(\n",
    "        annotation_columns=[\"multi_sc:assigned_cluster\"],\n",
    "        data_info={\n",
    "            \"multi_sc\": DataInfo(\n",
    "                file_path=sc_path, is_single_cell=True, data_type=\"NUMERIC\"\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    "    data_case=DataCase.MULTI_SINGLE_CELL,\n",
    ")\n",
    "maskix = acx.Maskix(config=config, masking_fn=my_masking_fn, masking_fn_kwargs=kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6dbc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = maskix.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db22c8",
   "metadata": {},
   "source": [
    "## 6) Use Maskix to Impute Data\n",
    "\n",
    "You can also input corrupted/missing data and use `Maskix` to impute the data. Here we recommend using a custom masking function. For example, if you want to impute missing values, an imputer could randomly replace values with zeros.\n",
    "\n",
    "Then you could use `Maskix` to clean your data and use the cleaned data to run your analysis, for example another autoencodix pipeline or anything else. In our mock example we will do the following:\n",
    "\n",
    "- Create corrupted data with missing values\n",
    "- Remove the \"missing\" data\n",
    "- Train Maskix with clean data, but with a custom imputer that mimics missing data\n",
    "- Feed corrupted data into trained Maskix and obtain imputed data\n",
    "- Train Varix with:\n",
    "    - Original data with missing values\n",
    "    - Imputed data\n",
    "- Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3397d5ff",
   "metadata": {},
   "source": [
    "#### Create Corrupted Data\n",
    "We will use our single-cell example from before and use maskix to preprocess the data, which makes the artificall corruption process more robust, because we make sure to corrupt informative features/samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0822c7e",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ed344",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import autoencodix as acx\n",
    "from autoencodix.configs import MaskixConfig\n",
    "from autoencodix.configs.default_config import DataInfo, DataConfig, DataCase\n",
    "\n",
    "\n",
    "sc_path = os.path.join(\"data/raw\", \"GSE84133_human_combined_final.h5ad\")\n",
    "config = MaskixConfig(\n",
    "    epochs=30,\n",
    "    checkpoint_interval=2,\n",
    "    k_filter=1000,\n",
    "    batch_size=64,\n",
    "    data_config=DataConfig(\n",
    "        annotation_columns=[\"multi_sc:assigned_cluster\"],\n",
    "        data_info={\n",
    "            \"multi_sc\": DataInfo(\n",
    "                file_path=sc_path, is_single_cell=True, data_type=\"NUMERIC\"\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    "    data_case=DataCase.MULTI_SINGLE_CELL,\n",
    ")\n",
    "maskix_orig = acx.Maskix(config=config)\n",
    "maskix_orig.preprocess()\n",
    "data = maskix_orig.result.datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7422022b",
   "metadata": {},
   "source": [
    "Now we will randomly set data do zero with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import copy\n",
    "from autoencodix.data._numeric_dataset import NumericDataset\n",
    "def drop_samples(ds: NumericDataset):\n",
    "    \"\"\"Randomly drops samples in .data and according .metadata (pd.DataFrame) and sample_ids\"\"\"\n",
    "    data = ds.data\n",
    "    n_samples = data.shape[0]\n",
    "    drop_prob = 0.3\n",
    "    keep_mask = torch.bernoulli((1 - drop_prob) * torch.ones(n_samples)).bool()\n",
    "    # replace entries with zero\n",
    "    imputed_data = data.clone()\n",
    "    imputed_data[~keep_mask] = 0\n",
    "\n",
    "    missing_data = data[keep_mask]\n",
    "    missing_metadata = ds.metadata.iloc[keep_mask.cpu().numpy()].reset_index(drop=True)\n",
    "    missing_sample_ids = [sid for i, sid in enumerate(ds.sample_ids) if keep_mask[i]]\n",
    "    ds_with_missing = copy.deepcopy(ds)\n",
    "    ds_with_missing.data = missing_data\n",
    "    ds_with_missing.metadata = missing_metadata\n",
    "    ds_with_missing.sample_ids = missing_sample_ids\n",
    "\n",
    "\n",
    "    ds_with_zero = copy.deepcopy(ds)\n",
    "    ds_with_zero.data = imputed_data\n",
    "    ds_with_zero.metadata = ds.metadata\n",
    "    ds_with_zero.sample_ids = ds.sample_ids\n",
    "    return ds_with_missing, ds_with_zero\n",
    "\n",
    "ds_with_missing = copy.deepcopy(data)\n",
    "ds_with_zero = copy.deepcopy(data)\n",
    "missing_train, ds_with_zero_train = drop_samples(ds_with_missing.train)\n",
    "\n",
    "print(f\"Original train data shape: {data.train.data.shape}, shape after corruption: {missing_train.data.shape}\")\n",
    "missing_test, ds_with_zero_test = drop_samples(data.test)\n",
    "print(f\"Original test data shape: {data.test.data.shape}, shape after corruption: {missing_test.data.shape}\")\n",
    "missing_valid, ds_with_zero_valid    = drop_samples(data.valid)\n",
    "print(f\"Original valid data shape: {data.valid.data.shape}, shape after corruption: {missing_valid.data.shape}\")\n",
    "ds_with_missing.train= missing_train\n",
    "ds_with_missing.test = missing_test\n",
    "ds_with_missing.valid = missing_valid\n",
    "\n",
    "\n",
    "ds_with_zero.train= ds_with_zero_train\n",
    "ds_with_zero.test = ds_with_zero_test\n",
    "ds_with_zero.valid = ds_with_zero_valid\n",
    "# clean corruped samples by removing zero values from dataset (drop samples) and also \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77b5de4",
   "metadata": {},
   "source": [
    "Now we train our Maskix with this clean data, but we will pass a custom imputer that will simulate missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e93035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import autoencodix as acx\n",
    "from autoencodix.configs import MaskixConfig\n",
    "from autoencodix.configs.default_config import DataInfo, DataConfig, DataCase\n",
    "\n",
    "\n",
    "def my_imputer(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"randomly replaces value with zero\"\n",
    "    rand_mask = torch.bernoulli(0.3 * torch.ones(x.shape, device=x.device)).bool()\n",
    "    rand_mask.to(x.device)\n",
    "    imputed_x = torch.where(rand_mask, torch.zeros_like(x, device=x.device), x)\n",
    "    return imputed_x\n",
    "\n",
    "\n",
    "sc_path = os.path.join(\"data/raw\", \"GSE84133_human_combined_final.h5ad\")\n",
    "config = MaskixConfig(\n",
    "    epochs=30,\n",
    "    checkpoint_interval=2,\n",
    "    k_filter=3000,\n",
    "    skip_preprocessing=True,\n",
    "    batch_size=64,\n",
    "    data_config=DataConfig(\n",
    "        annotation_columns=[\"multi_sc:assigned_cluster\"],\n",
    "        data_info={\n",
    "            \"multi_sc\": DataInfo(\n",
    "                 is_single_cell=True, data_type=\"NUMERIC\"\n",
    "            )\n",
    "        },\n",
    "    ),\n",
    "    data_case=DataCase.MULTI_SINGLE_CELL,\n",
    ")\n",
    "maskix = acx.Maskix(config=config, masking_fn=my_imputer, data=ds_with_missing)\n",
    "result = maskix.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6030d5d3",
   "metadata": {},
   "source": [
    "No we use the trained maskix to impute our missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70770b67",
   "metadata": {},
   "source": [
    "Now, we can use the fitted model and use a corrupted input with missing to get a reconstruction without missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba2053c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo_train = maskix.impute(ds_with_zero.train.data)\n",
    "recons_train = mo_train.reconstruction\n",
    "\n",
    "mo_test = maskix.impute(ds_with_zero.test.data)\n",
    "recons_test = mo_test.reconstruction\n",
    "\n",
    "mo_valid = maskix.impute(ds_with_zero.valid.data)\n",
    "recons_valid = mo_valid.reconstruction\n",
    "\n",
    "\n",
    "ds_imputed = copy.deepcopy(ds_with_zero)\n",
    "ds_imputed.train.data = recons_train\n",
    "ds_imputed.test.data = recons_test\n",
    "ds_imputed.valid.data = recons_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d67502",
   "metadata": {},
   "source": [
    "We can compare the loss between the imputed data and the original and the reconstructed data and the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# ground truth without missing values\n",
    "result_orig = maskix_orig.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c58f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import mse_loss\n",
    "original_train = result_orig.datasets.train.data\n",
    "original_recon = result_orig.reconstructions.get(split=\"train\", epoch=-1)\n",
    "original_recon_tensor = torch.from_numpy(original_recon)\n",
    "loss = mse_loss(original_recon_tensor, original_train)\n",
    "print(f\"MSE Loss original reconstruction: {loss}\")\n",
    "loss_imputed = mse_loss(recons_train.to(\"cpu\"), original_train)\n",
    "print(f\"MSE Loss imputed reconstruction: {loss_imputed}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c07d8a",
   "metadata": {},
   "source": [
    "Finally, you can use this data as input for other autoencoders like `varix` or `vanillix`, but also use it for maskix.\n",
    "We will train two Varix models, one with the missing data and one with the imputed data and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4491fff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dda64e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "recons_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76ff1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencodix as acx\n",
    "from autoencodix.configs import VarixConfig\n",
    "from autoencodix.configs.default_config import DataInfo, DataConfig, DataCase\n",
    "from autoencodix.data._datasetcontainer import DatasetContainer\n",
    "import copy\n",
    "\n",
    "config = VarixConfig(\n",
    "    epochs=50,\n",
    "    checkpoint_interval=10,\n",
    "    batch_size=64,\n",
    "    skip_preprocessing=True,\n",
    "    data_config=DataConfig(\n",
    "        annotation_columns=[\"multi_sc:assigned_cluster\"],\n",
    "        data_info={\"multi_sc\": DataInfo(is_single_cell=True, data_type=\"NUMERIC\")},\n",
    "    ),\n",
    "    data_case=DataCase.MULTI_SINGLE_CELL,\n",
    ")\n",
    "\n",
    "varix_imputed = acx.Varix(config=config, data=ds_imputed)\n",
    "varix_missing = acx.Varix(config=config, data=ds_with_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c66338",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_imputed = varix_imputed.run()\n",
    "result_missing = varix_missing.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be6d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "varix_imputed.show_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b5d500",
   "metadata": {},
   "outputs": [],
   "source": [
    "varix_imputed.show_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43748449",
   "metadata": {},
   "outputs": [],
   "source": [
    "varix_imputed.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad506f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "varix_missing.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7ec489",
   "metadata": {},
   "source": [
    "## 7) Save, Load and Re-Use Maskix\n",
    "\n",
    "There are not `Maskix` specific steps here. See the `Tutorials/PipelineTutorials/Vanillix.ipynb\n",
    "`  or `Tutorials/DeepDives/MemoryEfficientSaving.ipynb` for details. Below is a basic save/load usecase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc6900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "# use a filename without extension, we handle this internally\n",
    "outpath = os.path.join(\"tutorial_res\", \"maskix\")\n",
    "maskix.save(file_path=outpath, save_all=False)\n",
    "\n",
    "folder = os.path.dirname(outpath)\n",
    "pkl_files = glob.glob(os.path.join(folder, \"*.pkl\"))\n",
    "model_files = glob.glob(os.path.join(folder, \"*.pth\"))\n",
    "\n",
    "print(\"PKL files:\", pkl_files)\n",
    "print(\"Model files:\", model_files)\n",
    "\n",
    "# the load functionality automatically will build the pipeline object out of the three saved files\n",
    "varix_loaded = acx.Maskix.load(outpath)\n",
    "varix_loaded.predict(data=maskix_result.datasets)\n",
    "varix_loaded.visualize()\n",
    "varix_loaded.show_result()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencodix_package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
