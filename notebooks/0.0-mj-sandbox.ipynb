{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODIX PACKAGE HANDBOOK\n",
    "This notebook demonstrates the usage of the autoencodix package.\n",
    "For now it serves as an internal guideline with the goal to:\n",
    "- test the package from a user perspective\n",
    "- serve as a first draft of user documentation\n",
    "- serve a developer guideline \n",
    "  - developer guide will be derrived from this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 Generate mock data\n",
    "When  development proceeds this section should be used to  show how to use different datatypes\n",
    "for now we only use a mock numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_data = np.random.rand(100, 10)\n",
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 General Pipeline Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1036f1750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/maximilianjoas/development/autoencodix_package/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import autoencodix as acx\n",
    "from autoencodix.utils.default_config import DefaultConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 0, Loss: 2.3556206822395325\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 1, Loss: 2.308934450149536\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 2, Loss: 2.213356852531433\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "output.reconstruction: torch.Size([20, 10])\n",
      "cpu not relevant here\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 0, Loss: 2.3556206822395325\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 1, Loss: 2.308934450149536\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 2, Loss: 2.213356852531433\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "\n",
      "Warning: The following parameters are not valid for predict:\n",
      "Invalid parameters: data\n",
      "Valid parameters are: config\n",
      "output.reconstruction: torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "#### --------------------------------------------\n",
    "# TODO user prepares data or config\n",
    "### INITIALIZATION ### --------------------------\n",
    "# Use Vanillix Pipeline interface\n",
    "# needs to be initialized with data\n",
    "# data should be a numpy array, pandas dataframe or AnnData object\n",
    "# possible to pass a custom Config object\n",
    "van = acx.Vanillix(data=sample_data)\n",
    "# ------------------------------------------------\n",
    "### DATA PROCESSING ### --------------------------\n",
    "# job of old make data\n",
    "# populates self._features attrbute with torch tensor\n",
    "# populates self._datasets attribute with torch dataset\n",
    "# (important for training with dataloader)\n",
    "# possible to pass a custom Config object, or keyword arguments\n",
    "van.preprocess()\n",
    "# ------------------------------------------------\n",
    "### MODEL TRAINING ### --------------------------\n",
    "# job of old make model\n",
    "# calls self.Trainer class to init and train model\n",
    "# populates self._model attribute with trained model\n",
    "# populates self.result attribute with training results (model, losses, etc)\n",
    "van.fit()\n",
    "# ------------------------------------------------\n",
    "### PREDICTION ### -------------------------------\n",
    "# job of old make predict\n",
    "# if no data is passed, used the test split from preprocessing\n",
    "# otherwise, uses the data passed, and preprocesses it\n",
    "# updates self.result attribute with predictions (latent space, reconstructions, etc)\n",
    "van.predict()\n",
    "# ------------------------------------------------\n",
    "### EVALUATION ### -------------------------------\n",
    "# job of old make ml_task\n",
    "# populates self.result attribute with ml task results\n",
    "van.evaluate()  # not implemented yet\n",
    "# ------------------------------------------------\n",
    "### VISUALIZATION ### ---------------------------\n",
    "# job of old make visualize\n",
    "# populates self.result attribute with visualizations\n",
    "van.visualize()\n",
    "# show visualizations for notebook use\n",
    "van.show_result()\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# run all steps in the pipeline\n",
    "result_object = van.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 10) (10, 10) (20, 10)\n"
     ]
    }
   ],
   "source": [
    "recons = result_object.reconstructions.get(split=\"train\", epoch=2)\n",
    "recons_val = result_object.reconstructions.get(split=\"valid\", epoch=2)\n",
    "recons_test = result_object.reconstructions.get(split=\"test\", epoch=-1)\n",
    "print(recons.shape, recons_val.shape, recons_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 16) (10, 16) (20, 16)\n"
     ]
    }
   ],
   "source": [
    "latents = result_object.latentspaces.get(split=\"train\", epoch=2)\n",
    "latents_val = result_object.latentspaces.get(split=\"valid\", epoch=2)\n",
    "latents_test = result_object.latentspaces.get(split=\"test\", epoch=-1)\n",
    "print(latents.shape, latents_val.shape, latents_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a custom train, test, valid split\n",
    "When you pass the data to the pipeline, autoencodix, internally splits the data for you based on the train,test, valid ratios provided in the config (defaults are 70%/10%/20% train/valid/test).\n",
    "You can either pass custom ratios (see next section) or provide the indices directly as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.367237627506256\n",
      "Epoch: 1, Loss: 2.1364158391952515\n",
      "Epoch: 2, Loss: 2.140314042568207\n"
     ]
    }
   ],
   "source": [
    "sample_data = np.random.rand(100, 10)\n",
    "custom_train_indices = np.arange(75)  # we won't allow overlap between splits\n",
    "custom_valid_indices = np.arange(75, 80)\n",
    "custom_test_indices = np.arange(80, 100)\n",
    "\n",
    "# the custom split needs to be a dictionary with keys \"train\", \"valid\", and \"test\" and indices of the samples to be included in each split as numpy arrays\n",
    "custom_split = {\n",
    "    \"train\": custom_train_indices,\n",
    "    \"valid\": custom_valid_indices,\n",
    "    \"test\": custom_test_indices,\n",
    "}\n",
    "van = acx.Vanillix(data=sample_data, custom_splits=custom_split)\n",
    "van.preprocess()\n",
    "van.fit(epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to pass empty splits, but depending on how you'll use the autoencodix pipeline, this will throw an error at some point. So it is possible to call `fit` with only training data, but if you want to call `predict` and don't provide new data, this won't work without a data in the test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using predict with new data\n",
    "The standard case is to train the model with the train data and then predict with the test split.\n",
    "However, it is possible to pass new data to the predict method to perform inference on this data with the already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: The following parameters are not valid for predict:\n",
      "Invalid parameters: data\n",
      "Valid parameters are: config\n"
     ]
    }
   ],
   "source": [
    "new_unseen_data = np.random.rand(10, 10)\n",
    "van.predict(data=new_unseen_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the result of the pipeline\n",
    "Each step in the pipeline writes its results in the result object of the Vanillix instance.\n",
    "In this section we explore how to access and make sense of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Object Public Attributes:\n",
      "------------------------------\n",
      "latentspaces: TrainingDynamics object\n",
      "reconstructions: TrainingDynamics object\n",
      "mus: TrainingDynamics object\n",
      "sigmas: TrainingDynamics object\n",
      "losses: TrainingDynamics object\n",
      "preprocessed_data: Tensor of shape (100, 10)\n",
      "model: _FabricModule\n",
      "model_checkpoints: TrainingDynamics object\n",
      "datasets: DatasetContainer(train=<autoencodix.data._numeric_dataset.NumericDataset object at 0x1053f1e70>, valid=<autoencodix.data._numeric_dataset.NumericDataset object at 0x10626cd90>, test=<autoencodix.data._numeric_dataset.NumericDataset object at 0x10626ce20>)\n"
     ]
    }
   ],
   "source": [
    "result = van.result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TrainingDynamics object in result\n",
    "The training dynamics object has the followinf form:\n",
    "<epoch><split><data>\n",
    "So if you want to access the train loss for the 5th epoch, you would:\n",
    "`result.lossss.get(epoch=5, split=\"train\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7134380141894022\n",
      "[0.27492604 0.24243712 0.23720634]\n",
      "{0: {'train': array(0.78907921), 'valid': array(0.27492604)}, 1: {'train': array(0.71213861), 'valid': array(0.24243712)}, 2: {'train': array(0.71343801), 'valid': array(0.23720634)}}\n"
     ]
    }
   ],
   "source": [
    "loss_train_ep2 = result.losses.get(epoch=2, split=\"train\")\n",
    "print(loss_train_ep2)\n",
    "valid_loss = result.losses.get(split=\"valid\")\n",
    "print(valid_loss)\n",
    "print(result.losses.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this schema works for every TrainingDynamics instance in the results object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Pipeline usage with custom parameters\n",
    "Here we show how to customize the above shown pipeline with a user config or with keyword arguments.\n",
    "In future iterations we want to allow to read a config from a file, this will be also demonstrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n",
      "Epoch: 0, Loss: 1.9614873230457306\n",
      "Epoch: 1, Loss: 1.382771223783493\n",
      "Epoch: 2, Loss: 0.977891355752945\n",
      "Epoch: 3, Loss: 0.6883653551340103\n",
      "Epoch: 4, Loss: 0.5179779827594757\n",
      "cpu not relevant here\n",
      "Epoch: 0, Loss: 8780462592.772186\n",
      "Epoch: 1, Loss: 1348435297.7003174\n",
      "Epoch: 2, Loss: 5917.90837097168\n",
      "Epoch: 3, Loss: 10487.504081726074\n",
      "Epoch: 4, Loss: 3092.8131675720215\n"
     ]
    }
   ],
   "source": [
    "# Use Vanillix Pipeline interface\n",
    "# needs to be initialized with data\n",
    "# data should be a numpy array, pandas dataframe or AnnData object\n",
    "# possible to pass a custom Config object\n",
    "van = acx.Vanillix(data=sample_data)\n",
    "# job of old make data\n",
    "# populates self._features attrbute with torch tensor\n",
    "# populates self._datasets attribute with torch dataset\n",
    "# (important for training with dataloader)\n",
    "# possible to pass a custom Config object, or keyword arguments\n",
    "van.preprocess()\n",
    "# job of old make model\n",
    "# calls self.Trainer class to init and train model\n",
    "# populates self._model attribute with trained model\n",
    "# populates self.result attribute with training results (losses, etc)\n",
    "# van.fit()\n",
    "\"\"\" \n",
    "Each step can be run separately, with custom parameters, these parameters\n",
    "can be passed as keyword arguments, or as a Config object\n",
    "\"\"\"\n",
    "van.fit(learning_rate=0.01, batch_size=32, epochs=5)  # or like this:\n",
    "my_config = DefaultConfig(learning_rate=130.0, batch_size=32, epochs=5)\n",
    "van.fit(config=my_config)  # config has to be an keyword argument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.1  How to relevant keyword arguments for pipeline methods\n",
    "It can be hard to know what keyword arguments are valid for each step,\n",
    "so we show:\n",
    "- how to get a list of allowed keyword arguments\n",
    "- what happens if you pass non-allowed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size',\n",
      " 'checkpoint_interval',\n",
      " 'config',\n",
      " 'device',\n",
      " 'epochs',\n",
      " 'global_seed',\n",
      " 'gpu_strategy',\n",
      " 'learning_rate',\n",
      " 'n_gpus',\n",
      " 'n_workers',\n",
      " 'reconstruction_loss',\n",
      " 'reproducible',\n",
      " 'weight_decay'}\n"
     ]
    }
   ],
   "source": [
    "# for each config method, we can call a valid_params method\n",
    "van = acx.Vanillix(data=sample_data)\n",
    "fit_params = (\n",
    "    van.fit.valid_params\n",
    ")  # returns a set of keyword arguments that are actually used in the fit method\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get even more verbose info about the keyword args, you can run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Keyword Arguments:\n",
      "--------------------------------------------------\n",
      "\n",
      "learning_rate:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.001\n",
      "  Description: Learning rate for optimization\n",
      "\n",
      "batch_size:\n",
      "  Type: <class 'int'>\n",
      "  Default: 32\n",
      "  Description: Number of samples per batch\n",
      "\n",
      "epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of training epochs\n",
      "\n",
      "weight_decay:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.01\n",
      "  Description: L2 regularization factor\n",
      "\n",
      "reconstruction_loss:\n",
      "  Type: typing.Literal['mse', 'bce']\n",
      "  Default: mse\n",
      "  Description: Type of reconstruction loss\n",
      "\n",
      "device:\n",
      "  Type: typing.Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']\n",
      "  Default: auto\n",
      "  Description: Device to use\n",
      "\n",
      "n_gpus:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Number of GPUs to use\n",
      "\n",
      "n_workers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 2\n",
      "  Description: Number of data loading workers\n",
      "\n",
      "checkpoint_interval:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Interval for saving checkpoints\n",
      "\n",
      "gpu_strategy:\n",
      "  Type: typing.Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']\n",
      "  Default: auto\n",
      "  Description: GPU parallelization strategy\n",
      "\n",
      "reproducible:\n",
      "  Type: <class 'bool'>\n",
      "  Default: True\n",
      "  Description: Whether to ensure reproducibility\n",
      "\n",
      "global_seed:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Global random seed\n"
     ]
    }
   ],
   "source": [
    "# when you want to have more info about the params, you can get type hints from the config object\n",
    "my_config = DefaultConfig()\n",
    "conig_values = my_config.get_params()\n",
    "my_config.print_schema(filter_params=fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass not supported parameters you get a warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: The following parameters are not valid for fit:\n",
      "Invalid parameters: epochds\n",
      "Valid parameters are: batch_size, checkpoint_interval, config, device, epochs, global_seed, gpu_strategy, learning_rate, n_gpus, n_workers, reconstruction_loss, reproducible, weight_decay\n",
      "cpu not relevant here\n",
      "Epoch: 0, Loss: 2.2837421894073486\n",
      "Epoch: 1, Loss: 2.2954598665237427\n",
      "Epoch: 2, Loss: 2.3972206711769104\n"
     ]
    }
   ],
   "source": [
    "# if you use an unsupported keyword argument, you will get a warning\n",
    "# as you see the default value from the DefaultConfig is not overwritten and the training will take 100 epochs (not 10)\n",
    "van.preprocess()\n",
    "van.fit(epochds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.2 How to get information about the default config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DefaultConfig Configuration Parameters:\n",
      "--------------------------------------------------\n",
      "\n",
      "latent_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 16\n",
      "  Description: Dimension of the latent space\n",
      "\n",
      "n_layers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of layers in encoder/decoder\n",
      "\n",
      "enc_factor:\n",
      "  Type: <class 'int'>\n",
      "  Default: 4\n",
      "  Description: Scaling factor for encoder dimensions\n",
      "\n",
      "input_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 10000\n",
      "  Description: Input dimension\n",
      "\n",
      "drop_p:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Dropout probability\n",
      "\n",
      "learning_rate:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.001\n",
      "  Description: Learning rate for optimization\n",
      "\n",
      "batch_size:\n",
      "  Type: <class 'int'>\n",
      "  Default: 32\n",
      "  Description: Number of samples per batch\n",
      "\n",
      "epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of training epochs\n",
      "\n",
      "weight_decay:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.01\n",
      "  Description: L2 regularization factor\n",
      "\n",
      "reconstruction_loss:\n",
      "  Type: typing.Literal['mse', 'bce']\n",
      "  Default: mse\n",
      "  Description: Type of reconstruction loss\n",
      "\n",
      "default_vae_loss:\n",
      "  Type: typing.Literal['kl']\n",
      "  Default: kl\n",
      "  Description: Type of VAE loss\n",
      "\n",
      "min_samples_per_split:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Minimum number of samples per split\n",
      "\n",
      "device:\n",
      "  Type: typing.Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']\n",
      "  Default: auto\n",
      "  Description: Device to use\n",
      "\n",
      "n_gpus:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Number of GPUs to use\n",
      "\n",
      "n_workers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 2\n",
      "  Description: Number of data loading workers\n",
      "\n",
      "checkpoint_interval:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Interval for saving checkpoints\n",
      "\n",
      "float_precision:\n",
      "  Type: typing.Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16']\n",
      "  Default: 32\n",
      "  Description: Floating point precision\n",
      "\n",
      "gpu_strategy:\n",
      "  Type: typing.Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']\n",
      "  Default: auto\n",
      "  Description: GPU parallelization strategy\n",
      "\n",
      "train_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.7\n",
      "  Description: Ratio of data for training\n",
      "\n",
      "test_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.2\n",
      "  Description: Ratio of data for testing\n",
      "\n",
      "valid_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Ratio of data for validation\n",
      "\n",
      "reproducible:\n",
      "  Type: <class 'bool'>\n",
      "  Default: True\n",
      "  Description: Whether to ensure reproducibility\n",
      "\n",
      "global_seed:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Global random seed\n"
     ]
    }
   ],
   "source": [
    "# if you want to see what config parameters are used in the default config you can do it like:\n",
    "default_config = DefaultConfig()\n",
    "default_config.print_schema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.3 Documentation Config class\n",
    "You can update the config with your own values by:\n",
    "- passing arguments as:\n",
    "    - dict\n",
    "    - single arguments\n",
    "- passing a file (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "\n",
    "# METHOD 1: override the default config with a dictionary\n",
    "my_args = {\"learning_rate\": 0.0234, \"batch_size\": 13, \"epochs\": 12}\n",
    "my_config = DefaultConfig(**my_args)\n",
    "# METHOD 2: override signle parameters\n",
    "my_new_conig = DefaultConfig(latent_dim=23, n_gpus=13)\n",
    "\n",
    "# METHOD 3: from a file: TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Use the Varix model\n",
    "Now we show how easy it is to use a variational autoencoder instead of a vanilla version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n",
      "Epoch: 0, Loss: 1.3720149099826813\n",
      "Epoch: 1, Loss: 1.1976171135902405\n",
      "Epoch: 2, Loss: 1.221748024225235\n",
      "\n",
      "Warning: The following parameters are not valid for predict:\n",
      "Invalid parameters: data\n",
      "Valid parameters are: config\n"
     ]
    }
   ],
   "source": [
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "import autoencodix as acx\n",
    "import numpy as np\n",
    "\n",
    "sample_data = np.random.rand(100, 10)\n",
    "my_config = DefaultConfig(learning_rate=0.001, epochs=3, checkpoint_interval=1)\n",
    "varix = acx.Varix(data=sample_data, config=my_config)\n",
    "result = varix.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine Variational result\n",
    "Here, we have more info in our results object than in the Vanillix case. We have the learned paramters mu and logvar of the normal distirbution, in addition to the losses and reconstructions. We provide also the sampled latentspaces at each epoch and split.\n",
    "\n",
    "You can resample new latenspaces (shown in next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 16)\n"
     ]
    }
   ],
   "source": [
    "# we did not train for the test split, so we don't need to pass an epoch\n",
    "# technically the epoch is -1\n",
    "mu_test_ep_last = result.latentspaces.get(split=\"test\")\n",
    "print(mu_test_ep_last.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different loss types\n",
    "For our variation autoencoder, the total loss consists of a reconstruction loss and a distribution loss i.e. kl-divergence. To investigate these losses, the result_obj has the attribute `sub_losses`. This is a `LossRegistry` withe the name of the loss as key and the value is of class `TrainingDynamics` and can be accessed as shown for the Vanillix part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['recon_loss', 'var_loss'])\n",
      "[0.42741003 0.44090185 0.37895077]\n"
     ]
    }
   ],
   "source": [
    "sub_losses = result.sub_losses\n",
    "print(f\"keys: {sub_losses.keys()}\")\n",
    "recon_dyn = sub_losses.get(key=\"recon_loss\")\n",
    "print(recon_dyn.get(split=\"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample new latentspaces\n",
    "You might want to use the trained model and the fitted parameters mu, and logvar to sample latentspaces. Therefore, the Varix pipeline has the additional method `sample_latent_space`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4227, -1.1264, -0.5146, -0.6423,  1.3555,  0.1267,  1.1256, -0.8374,\n",
      "          1.2268, -1.7507,  0.9605, -0.4691,  0.9137, -0.8082, -0.5185, -0.1135],\n",
      "        [ 0.0871, -0.7650, -0.7087,  1.9782,  0.5053,  0.0163,  0.5845, -0.4165,\n",
      "          0.2073,  1.6614, -0.8132,  0.6487, -1.5413, -0.4115,  0.0680,  0.1220],\n",
      "        [ 0.5332,  0.1249,  1.5426,  1.3922,  0.3982,  0.5452,  0.5251, -0.1709,\n",
      "          0.2392, -0.6901,  0.6993, -2.4067,  0.7687,  0.4694, -1.3835, -1.3028],\n",
      "        [ 0.4925,  0.2083, -0.3846,  0.7595, -0.3062, -0.2996, -0.4743,  1.3784,\n",
      "          1.0943, -0.0518, -0.3465, -0.3751,  0.3677,  0.3754, -0.2108, -0.4754],\n",
      "        [-0.1330,  1.1649,  0.2671,  0.2378, -3.0322, -1.7903, -0.1169,  1.0738,\n",
      "         -0.9661,  0.0313,  0.2592, -1.3385,  0.7877, -0.0342,  1.4808,  2.4948]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "sampled = varix.sample_latent_space()\n",
    "\n",
    "print(sampled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3581,  0.8946,  2.3437, -1.0793,  1.5991, -0.6819,  1.3511,  0.7896,\n",
      "         -0.6627,  1.4723,  1.3740, -0.8405, -0.3541, -0.3976,  1.8037, -0.3309],\n",
      "        [-2.7742,  1.3001, -0.7447, -1.5788, -1.3535,  0.3174, -0.5517,  0.9802,\n",
      "         -1.0824,  1.4315, -0.8219, -1.6757,  0.4232, -1.0295, -1.2279,  0.2959],\n",
      "        [-0.5726,  1.2144,  1.3754, -0.1848,  1.2447, -0.2668, -1.7038,  1.4772,\n",
      "          0.6938,  0.8111,  0.5638, -1.3457,  1.3015, -2.0530, -1.4406, -0.6207],\n",
      "        [-0.5844,  1.0119,  1.3542,  0.8185,  0.4677,  0.6580, -0.4376, -1.5302,\n",
      "          0.3264,  0.5532,  1.9664, -1.2441,  1.4664, -1.4824,  1.2571, -1.7927],\n",
      "        [ 1.2330, -0.1874, -0.8378,  1.5088, -0.8935, -0.0678, -0.0589, -0.2272,\n",
      "          0.4520, -1.1687,  0.4746,  0.1345, -0.5463, -0.4620,  0.9897, -0.3327]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# you can also select a specific epoch and split to sample from (default is last epoch and test split)\n",
    "sampled = varix.sample_latent_space(epoch=2, split=\"valid\")\n",
    "print(sampled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8121, -0.2599, -1.1656],\n",
      "        [-1.5269,  0.5212, -0.3416],\n",
      "        [ 1.1872,  1.8373,  0.4524]], device='mps:0')\n",
      "tensor([[ 2.3787,  0.1305, -0.7975],\n",
      "        [-0.3502, -0.2842,  1.0856],\n",
      "        [-2.3673,  1.2124,  1.5476]], device='mps:0')\n",
      "tensor([[ 0.0609,  2.3645,  0.2055],\n",
      "        [-0.3573, -1.5928,  0.5545],\n",
      "        [-2.5494, -0.6906, -0.8733]], device='mps:0')\n",
      "tensor([[ 0.9513,  1.5320,  1.5767],\n",
      "        [ 0.4684, -1.0381,  0.9011],\n",
      "        [-1.0837,  1.9147,  0.4768]], device='mps:0')\n",
      "tensor([[-0.0046, -1.3439, -0.2800],\n",
      "        [ 1.0443, -0.2889, -0.1052],\n",
      "        [ 0.5079,  1.7101,  0.0815]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# or sample multiple times\n",
    "for _ in range(5):\n",
    "    sampled = varix.sample_latent_space()\n",
    "    print(sampled[:3, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 How to work with custom omics data\n",
    "In the above steps we showed how to use `Autoencodix` with mock data. Nowe we demonstrate how to use real-world data. We'll cover:\n",
    "1. combining multi-omics data from bulk sequencing (e.g. mRNA and methylation)\n",
    "2. combining multi-omics data from single cell sequencing\n",
    "3. \"Translating\" between multi-omics data e.g. scRNA <-> scATAC, or bulkmRNA <-> bulkmiRNA\n",
    "4. Working with image data\n",
    "5. \"translating\" between data-modalities\n",
    "  - one bulk-omics modality to another\n",
    "  - omics to image an vice versa\n",
    "\n",
    "### 04.1 Combining mulit-omics data from bulk-sequencing\n",
    "First we need to prepare our config object. We can (a) directly provide an object in python, or (b) provide an YAML file. We show both\n",
    "\n",
    "#### YAML config\n",
    "Assume we have the file in `./config.yaml`.\n",
    "We can keep the yaml file structure to define our input data like:\n",
    "```yaml\n",
    "data_config: # has to be named data_config\n",
    "  data_info: # has to be named data_infor\n",
    "   RNA: # name can be chosen by user\n",
    "     file_path: \"data/raw/data_mrna_seq_v2_rsem_formatted.parquet\"\n",
    "     is_single_cell: false # default false, added for verbosity\n",
    "   METHYLATION: # can be chosen by user\n",
    "     file_path: \"data/raw/data_methylation_per_gene_formatted.parquet\"\n",
    "     is_single_cell: false # default false, added for verbosity\n",
    "   CLINICAL: # can be chosen by user\n",
    "     file_path: \"data/raw/data_clinical_formatted.parquet\"\n",
    "     data_type: \"ANNOTATION\" # default NUMERIC (as for RNA and METHYLATION)\n",
    "```\n",
    "ATTENTION:\n",
    "If you use `.txt` or `.csv` files, it is best practice to add the `sep` parameter. If none is given, the reader will try to auto-detect the separator, which is error prone.\n",
    "This would loke like:\n",
    "```YAML\n",
    "    RNA:\n",
    "      ...\n",
    "      sep: \"\\t\" # for tab, \";\" or \",\" would be also possible (as in pandas)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maximilianjoas/development/autoencodix_package\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "import os\n",
    "\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())\n",
    "# this fills the data_config attribute of the DefaultConfig object\n",
    "# we can also change the default values in the config.yaml file\n",
    "# or via the DefaultConfig object\n",
    "config = DefaultConfig.model_validate(yaml.safe_load(Path(\"config.yaml\").read_text()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with custom values\n",
    "custom_config = DefaultConfig.model_validate(\n",
    "    {**yaml.safe_load(Path(\"config.yaml\").read_text()), \"learning_rate\": 0.77}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_info={'RNA': DataInfo(file_path='data/raw/data_mrna_seq_v2_rsem_formatted.parquet', data_type='NUMERIC', scaling='STANDARD', filtering='VAR', is_single_cell=False, data_object=None, min_cells=None, min_genes=None, translate_direction=None, img_root=None, is_X=None, sep=None, extra_anno_file=None), 'METHYLATION': DataInfo(file_path='data/raw/data_methylation_per_gene_formatted.parquet', data_type='NUMERIC', scaling='STANDARD', filtering='VAR', is_single_cell=False, data_object=None, min_cells=None, min_genes=None, translate_direction=None, img_root=None, is_X=None, sep=None, extra_anno_file=None), 'CLINICAL': DataInfo(file_path='data/raw/data_clinical_formatted.parquet', data_type='ANNOTATION', scaling='STANDARD', filtering='VAR', is_single_cell=False, data_object=None, min_cells=None, min_genes=None, translate_direction=None, img_root=None, is_X=None, sep=None, extra_anno_file=None)} patient_id_column='patient_id' output_h5ad='multiomics.h5ad'\n",
      "0.001\n",
      "0.77\n"
     ]
    }
   ],
   "source": [
    "print(config.data_config)\n",
    "print(config.learning_rate)\n",
    "print(custom_config.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create DataConfig in Python\n",
    "We will only use one way of config creation for the next examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.utils.default_config import DataConfig, DataInfo\n",
    "\n",
    "root_dir = os.path.join(\"data/raw\")\n",
    "meth_file = \"data_methylation_per_gene_formatted.parquet\"\n",
    "mrna_file = \"data_mrna_seq_v2_rsem_formatted.parquet\"\n",
    "clin_file = \"data_clinical_formatted.parquet\"\n",
    "\n",
    "bulk_config = DefaultConfig(\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"RNA\": DataInfo(file_path=os.path.join(root_dir, mrna_file)),\n",
    "            \"METHYLATION\": DataInfo(file_path=os.path.join(root_dir, meth_file)),\n",
    "            \"CLINICAL\": DataInfo(\n",
    "                file_path=os.path.join(root_dir, clin_file), data_type=\"ANNOTATION\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the input data\n",
    "Practically we would be done with the above step. We pass the config to the pipeline as shown multiple times before. For cleared documentation (esepcially for Devs) we show what happens inside the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNA data from data/raw/data_mrna_seq_v2_rsem_formatted.parquet\n",
      "Loading METHYLATION data from data/raw/data_methylation_per_gene_formatted.parquet\n",
      "Loading CLINICAL data from data/raw/data_clinical_formatted.parquet\n",
      "10001\n"
     ]
    }
   ],
   "source": [
    "from autoencodix.utils._bulkreader import BulkDataReader\n",
    "\n",
    "reader = BulkDataReader()\n",
    "common_samples, config = reader.load_and_intersect_bulkdata(config=bulk_config)\n",
    "print(len(common_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 10001 × 16313\n",
      "    obs: 'PATIENT_ID', 'ONCOTREE_CODE', 'CANCER_TYPE', 'CANCER_TYPE_DETAILED', 'TUMOR_TYPE', 'GRADE', 'TISSUE_PROSPECTIVE_COLLECTION_INDICATOR', 'TISSUE_RETROSPECTIVE_COLLECTION_INDICATOR', 'TISSUE_SOURCE_SITE_CODE', 'TUMOR_TISSUE_SITE', 'ANEUPLOIDY_SCORE', 'SAMPLE_TYPE', 'MSI_SCORE_MANTIS', 'MSI_SENSOR_SCORE', 'SOMATIC_STATUS', 'TMB_NONSYNONYMOUS', 'TISSUE_SOURCE_SITE', 'SUBTYPE', 'CANCER_TYPE_ACRONYM', 'OTHER_PATIENT_ID', 'AGE', 'SEX', 'AJCC_PATHOLOGIC_TUMOR_STAGE', 'AJCC_STAGING_EDITION', 'DAYS_LAST_FOLLOWUP', 'DAYS_TO_BIRTH', 'DAYS_TO_INITIAL_PATHOLOGIC_DIAGNOSIS', 'ETHNICITY', 'FORM_COMPLETION_DATE', 'HISTORY_NEOADJUVANT_TRTYN', 'ICD_10', 'ICD_O_3_HISTOLOGY', 'ICD_O_3_SITE', 'INFORMED_CONSENT_VERIFIED', 'NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT', 'PATH_M_STAGE', 'PATH_N_STAGE', 'PATH_T_STAGE', 'PERSON_NEOPLASM_CANCER_STATUS', 'PRIMARY_LYMPH_NODE_PRESENTATION_ASSESSMENT', 'PRIOR_DX', 'RACE', 'RADIATION_THERAPY', 'WEIGHT', 'IN_PANCANPATHWAYS_FREEZE', 'OS_STATUS', 'OS_MONTHS', 'DSS_STATUS', 'DSS_MONTHS', 'DFS_STATUS', 'DFS_MONTHS', 'PFS_STATUS', 'PFS_MONTHS', 'AJCC_PATHOLOGIC_TUMOR_STAGE_SHORT'\n",
      "    obsm: 'RNA', 'METHYLATION'\n"
     ]
    }
   ],
   "source": [
    "bulk_anndata = reader.build_bulk_anndata(common_samples=common_samples, config=config)\n",
    "print(bulk_anndata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explanation of AnnData\n",
    "The Annotation data is saved in `obs` in the AnnData object. The different omics types are (not typical for single cell, but we use bulk here) in `.obsm`. The `.X` will be populated based on either by the first dataset. Internally we only work with `obs` and `obsm`\n",
    "The AnnData object will then be passed to the preprocessor and eventually will be transformed to a torch Dataset to use in training.\n",
    "You can either pass the config with the location of your datafiles to the `Autoencodix` pipeline, or you can create the data yourself and pass the AnnData object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.2 Working with single cell data from different sequencing processes\n",
    "First we define our config again, then we use the reader object to build the AnnData (this will look more familar for single cell practioners)\n",
    "\n",
    "We can provide a config yaml like:\n",
    "```YAML\n",
    "# config.yaml\n",
    "data_config:\n",
    " data_info:\n",
    "   RNA:\n",
    "     file_path: \"data/raw/Sc-1.h5ad\" # we request h5ad files\n",
    "     is_single_cell: true\n",
    "     min_cells: 0.01\n",
    "     min_genes: 0.01\n",
    "     is_X: true\n",
    "   METH:\n",
    "     file_path: \"data/raw/Sc-2.h5ad\"\n",
    "     is_single_cell: true\n",
    "     min_cells: 0.01\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.utils import DefaultConfig\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "scconfig = DefaultConfig.model_validate(\n",
    "    yaml.safe_load(Path(\"scconfig.yaml\").read_text())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.utils._screader import SingleCellDataReader\n",
    "\n",
    "reader = SingleCellDataReader()\n",
    "adata, scconfig = reader.load_and_build_sc_anndata(config=scconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 45549 × 17425\n",
      "    obs: 'author_cell_type', 'age_group', 'donor_id', 'nCount_RNA', 'nFeature_RNA', 'nCount_ATAC', 'nFeature_ATAC', 'TSS_percentile', 'nucleosome_signal', 'percent_mt', 'assay_ontology_term_id', 'cell_type_ontology_term_id', 'development_stage_ontology_term_id', 'disease_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'organism_ontology_term_id', 'sex_ontology_term_id', 'tissue_ontology_term_id', 'suspension_type', 'is_primary_data', 'batch', 'tissue_type', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage', 'observation_joinid', 'RNA_author_cell_type', 'RNA_age_group', 'RNA_donor_id', 'RNA_nCount_RNA', 'RNA_nFeature_RNA', 'RNA_nCount_ATAC', 'RNA_nFeature_ATAC', 'RNA_TSS_percentile', 'RNA_nucleosome_signal', 'RNA_percent_mt', 'RNA_assay_ontology_term_id', 'RNA_cell_type_ontology_term_id', 'RNA_development_stage_ontology_term_id', 'RNA_disease_ontology_term_id', 'RNA_self_reported_ethnicity_ontology_term_id', 'RNA_organism_ontology_term_id', 'RNA_sex_ontology_term_id', 'RNA_tissue_ontology_term_id', 'RNA_suspension_type', 'RNA_is_primary_data', 'RNA_batch', 'RNA_tissue_type', 'RNA_cell_type', 'RNA_assay', 'RNA_disease', 'RNA_organism', 'RNA_sex', 'RNA_tissue', 'RNA_self_reported_ethnicity', 'RNA_development_stage', 'RNA_observation_joinid', 'METH_author_cell_type', 'METH_age_group', 'METH_donor_id', 'METH_nCount_RNA', 'METH_nFeature_RNA', 'METH_nCount_ATAC', 'METH_nFeature_ATAC', 'METH_TSS_percentile', 'METH_nucleosome_signal', 'METH_percent_mt', 'METH_assay_ontology_term_id', 'METH_cell_type_ontology_term_id', 'METH_development_stage_ontology_term_id', 'METH_disease_ontology_term_id', 'METH_self_reported_ethnicity_ontology_term_id', 'METH_organism_ontology_term_id', 'METH_sex_ontology_term_id', 'METH_tissue_ontology_term_id', 'METH_suspension_type', 'METH_is_primary_data', 'METH_batch', 'METH_tissue_type', 'METH_cell_type', 'METH_assay', 'METH_disease', 'METH_organism', 'METH_sex', 'METH_tissue', 'METH_self_reported_ethnicity', 'METH_development_stage', 'METH_observation_joinid'\n",
      "    var: 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype', 'feature_length', 'feature_type'\n",
      "    uns: 'batch_condition', 'citation', 'schema_reference', 'schema_version', 'title'\n",
      "    obsm: 'X_joint_wnn_umap', 'X_umap'\n",
      "    layers: 'RNA', 'METH'\n"
     ]
    }
   ],
   "source": [
    "print(adata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.3 Translating between omics data\n",
    "We only allow bulk to bulk and single-cell to single cell. The config is almost identical to the case before, we only add the direction of the translation like:\n",
    "```YAML\n",
    "data_config:\n",
    " data_info:\n",
    "   RNA:\n",
    "     file_path: \"data/raw/Sc-1.h5ad\"\n",
    "     is_single_cell: true\n",
    "     min_cells: 0.01\n",
    "     min_genes: 0.01\n",
    "     is_X: true\n",
    "     translate_direction: \"FROM\"\n",
    "   METH:\n",
    "     file_path: \"data/raw/Sc-2.h5ad\"\n",
    "     is_single_cell: true\n",
    "     min_cells: 0.01\n",
    "     translate_direction: \"TO\"\n",
    "\n",
    "```\n",
    "\n",
    "For the bulk case we can keep annotation data without including it in the translation like:\n",
    "```YAML\n",
    "# config.yaml\n",
    "data_config:\n",
    "  data_info:\n",
    "   RNA:\n",
    "     file_path: \"data/raw/data_mrna_seq_v2_rsem_formatted.parquet\"\n",
    "     is_single_cell: false\n",
    "     translate_direction: \"FROM\"\n",
    "   METHYLATION:\n",
    "     file_path: \"data/raw/data_methylation_per_gene_formatted.parquet\"\n",
    "     is_single_cell: false\n",
    "     translate_direction: \"TO\"\n",
    "   CLINICAL:\n",
    "     file_path: \"data/raw/data_clinical_formatted.parquet\"\n",
    "     data_type: \"ANNOTATION\"\n",
    "     # default translate_direction is  NONE, so we don't need to specify it here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04.4 Working with images\n",
    "When working with images, we need have two cases:\n",
    "- pure image, without translating\n",
    "- translating between omics and images\n",
    "\n",
    "In the first case we need to provide the folder path of the images. In the second case we need to provide the folder path of the images and an annotation file that maps the metadata for the images to the image filenames. In this file we also need to map the sample_ids of the other data modality to the image filename and metadata. Later we will add support for an unpaired case, where we only provide image metadata without mapping to the other data modality.\n",
    "The file should look like this:\n",
    "**important**: this file needs to contain the columns `sample_ids` and `img_paths`\n",
    "```text\n",
    "sample_ids\timg_paths\tMETADATA1\tMETADATA2\n",
    "TCGA-05-4244-01\t0_label_1.png\tNon-Small Cell Lung Cancer\tLUAD\n",
    "TCGA-05-4249-01\t1_label_1.png\tNon-Small Cell Lung Cancer\tLUAD\n",
    "TCGA-05-4250-01\t2_label_1.png\tNon-Small Cell Lung Cancer\tLUAD\n",
    "TCGA-05-4382-01\t3_label_1.png\tNon-Small Cell Lung Cancer\tLUAD\n",
    "TCGA-05-4384-01\t4_label_1.png\tNon-Small Cell Lung Cancer\tLUAD\n",
    "TCGA-05-4389-01\t5_label_1.png\tNon-Small Cell Lung Cancer\tLUAD\n",
    "TCGA-05-4390-01\t6_label_1.png\tNon-Small Cell Lung Cancer\tLUAD\n",
    "TCGA-05-4395-01\t7_label_1.png\tNon-Small Cell Lung Cancer\tLUAD\n",
    "TCGA-05-4396-01\t8_label_1.png\tNon-Small Cell Lung Cancer\tLUAD\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image only case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.utils.default_config import DataConfig, DataInfo\n",
    "\n",
    "img_config = DefaultConfig(\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"IMG\": DataInfo(file_path=\"data/raw/images/tcga_fake\", data_type=\"IMG\"),\n",
    "            \"ANNO\": DataInfo(\n",
    "                file_path=\"data/raw/tcga_mappings.txt\", data_type=\"ANNOTATION\"\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images and omics data\n",
    "If you want to translate between image and another data modality you need to provide the same files as above. For the Annotation files you have two possibilities: (a) you provide one annotation file (as shown above), in this file you match the metadata of the two data modalites, by an shared sample_id / mapping of sample_id and image_path and other metadata. (b) you can have supply an extra annotation file for the images with the attribute `img_anno_file`. If None is given, we will use the shared file. This is only allowed for unpaired translation.\n",
    "So for the unpaired translation the `DataConfig` should look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.utils.default_config import DataConfig, DataInfo, DefaultConfig\n",
    "\n",
    "img_config = DefaultConfig(\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"IMG\": DataInfo(\n",
    "                file_path=\"data/raw/images/tcga_fake\",\n",
    "                data_type=\"IMG\",\n",
    "                translate_direction=\"to\",\n",
    "            ),\n",
    "            \"RNA\": DataInfo(\n",
    "                file_path=\"data/raw/data_mrna_seq_v2_rsem_formatted.parquet\",\n",
    "                data_type=\"NUMERIC\",\n",
    "                translate_direction=\"from\",\n",
    "            ),\n",
    "            \"ANNO\": DataInfo(\n",
    "                file_path=\"data/raw/tcga_mappings.txt\",\n",
    "                data_type=\"ANNOTATION\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_info = img_config.data_config.data_info\n",
    "numeric_datasets = {k: v for k, v in data_info.items() if v.data_type == \"NUMERIC\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'RNA': DataInfo(file_path='data/raw/data_mrna_seq_v2_rsem_formatted.parquet', data_type='NUMERIC', scaling='STANDARD', filtering='VAR', is_single_cell=False, data_object=None, min_cells=None, min_genes=None, translate_direction='from', img_root=None, is_X=None, sep=None, extra_anno_file=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpaired case\n",
    "This case is not implemented in the old version of `autoencodix` and will be added after the other translating features, we still show how the config can look like:\n",
    "```python\n",
    "from autoencodix.utils.default_config import DataConfig, DataInfo\n",
    "img_config = DefaultConfig(\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"IMG\": DataInfo(\n",
    "                file_path=\"data/raw/images/tcga_fake.txt\",\n",
    "                data_type=\"IMG\",\n",
    "                translate_direction=\"to\"\n",
    "                extra_anno_file=\"path/to/file\"\n",
    "            ),\n",
    "            \"RNA\": DataInfo(\n",
    "                file_path=\"data/raw/data_mrna_seq_v2_rsem_formatted.parquet\",\n",
    "                data_type=\"NUMERIC\",\n",
    "                translate_direction=\"from\"\n",
    "            ),\n",
    "            \"ANNO\": DataInfo(\n",
    "                file_path=\"data_clinical_formatted.parquet\",\n",
    "                data_type=\"ANNOTATION\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Internal handling of image data\n",
    "Image and bulk case (paired)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_case(config:DefaultConfig) -> str:\n",
    "    # No translation\n",
    "    \"Multi Single Cell\"\n",
    "    # Description: if all numeric is single cell and no translation, we use this case\n",
    "    # RULES:\n",
    "    # ANNO  skipped, since ANNO comes from h5ad file and we don't allow mixed and sc data\n",
    "    # print warning when skipping\n",
    "    # All numeric data has to be single cell, raise error if not\n",
    "    # If translate_direction is not set, raise when other data types are present\n",
    "    # IMG, and other skipped\n",
    "    # -------------------------\n",
    "    \"Mulit Bulk\"\n",
    "    # Description: if all numeric is bulk and no translation, we use this case\n",
    "    # RULES:\n",
    "    # All numeric data has to be bulk, raise error if not\n",
    "    # ANNO is required and saved in obs\n",
    "    # each modality as an entry in obsm with the key being the name of the modality as defined in the config\n",
    "    # -------------------------\n",
    "\n",
    "    # Translation paired\n",
    "    # General description:\n",
    "    # if we have two different datasets and annotation, we can translate between them\n",
    "    # we can translate between bulk(1) <-> bulk(2)bulk <-> img, sc(1) <-> sc(2), sc <-> img\n",
    "    \"Bulk-Bulk\"\n",
    "    # Description: if all numeric is bulk and translation is set, we use this case\n",
    "    # RULES:\n",
    "    # if we have only 1 datase where translate_direction is set, we raise an error\n",
    "    # if we have more than 2 datasets, we raise an error (except ANNOTATION)\n",
    "    # we require ANNOTATION\n",
    "    # we read like standard bulk data\n",
    "    \"IMG<->Bulk\"\n",
    "    \"Single Cell to Single Cell\"\n",
    "    \"Bulk to Bulk\"\n",
    "    \"Bulk <-> Img\"\"\n",
    "    J\"Img <-> Bulk\" # one IMG allowed, one NUMERIC allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ImageDataReader' object has no attribute 'load_and_build_img_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mautoencodix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_imgreader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ImageDataReader\n\u001b[1;32m      2\u001b[0m imgreader \u001b[38;5;241m=\u001b[39m ImageDataReader()\n\u001b[0;32m----> 3\u001b[0m imgs\u001b[38;5;241m=\u001b[39m \u001b[43mimgreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_build_img_data\u001b[49m(config\u001b[38;5;241m=\u001b[39mimg_config)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageDataReader' object has no attribute 'load_and_build_img_data'"
     ]
    }
   ],
   "source": [
    "from autoencodix.utils._imgreader import ImageDataReader\n",
    "imgreader = ImageDataReader()\n",
    "bulkreader = BulkDataReader()\n",
    "screader = SingleCellDataReader()\n",
    "imgs= imgreader.read_all_images_from_dir(img_dir=)\n",
    "seen_bulk = False\n",
    "seen_sc = False\n",
    "for k,v in img_config.data_config.data_info.items():\n",
    "    if v.data_type == \"IMG\":\n",
    "        scaling = v.scaling\n",
    "        img_dir = v.file_path\n",
    "        to_h = img_config.img_height_resize\n",
    "        to_w = img_config.img_width_resize\n",
    "\n",
    "        imgs = imgreader.read_all_images_from_dir(img_dir=img_dir, scaling=\"MINMAX\", to_h=to_h, to_w=to_w)\n",
    "    if v.data_type == \"NUMERIC\":\n",
    "        all_one_data_modality = True\n",
    "        if v.is_single_cell:\n",
    "            seen_sc = True\n",
    "            adata, img_config = screader.load_and_build_sc_anndata(config=img_config)\n",
    "            all\n",
    "        else:\n",
    "            seen_bulk = True\n",
    "            common_samples, img_config = bulkreader.load_and_intersect_bulkdata(config=img_config)\n",
    "            bulk_anndata = bulkreader.build_bulk_anndata(common_samples=common_samples, config=img_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgage shape after transpose(28, 28, 3)\n",
      "imgage shape after resize(28, 28, 3)\n",
      "imgage shape after normalize(28, 28, 3)\n",
      "shape after np.expand_dims: (28, 28, 1)\n",
      "shape after transpose: (1, 28, 28)\n",
      "imgage shape after transpose(28, 28, 3)\n",
      "imgage shape after resize(28, 28, 3)\n",
      "imgage shape after normalize(28, 28, 3)\n",
      "shape after np.expand_dims: (28, 28, 1)\n",
      "shape after transpose: (1, 28, 28)\n",
      "imgage shape after transpose(28, 28, 3)\n",
      "imgage shape after resize(28, 28, 3)\n",
      "imgage shape after normalize(28, 28, 3)\n",
      "shape after np.expand_dims: (28, 28, 1)\n",
      "shape after transpose: (1, 28, 28)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = imgs[0]\n",
    "imgs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1223c5ea0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reshape to show (remove channel dimension)\n",
    "sample = sample.squeeze()\n",
    "plt.imshow(sample, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from typing import Union, Literal, List, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class ImageProcessingError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def read_all_images_from_dir(\n",
    "    img_dir: str,\n",
    "    scaling: str,\n",
    "    to_h: Optional[int],\n",
    "    to_w: Optional[int],\n",
    ") -> List[torch.Tensor]:\n",
    "    SUPPORTED_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n",
    "    paths = [\n",
    "        os.path.join(img_dir, f)\n",
    "        for f in os.listdir(img_dir)\n",
    "        if f.endswith(tuple(SUPPORTED_EXTENSIONS))\n",
    "    ]\n",
    "    imgs = [\n",
    "        parse_image_to_tensor(image_path=p, scaling=scaling, to_h=to_h, to_w=to_w)\n",
    "        for p in paths[0:3]\n",
    "    ]\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def normalize_image(\n",
    "    image: np.ndarray, method: Literal[\"STANDARD\", \"MINMAX\", \"ROBUST\", \"NONE\"]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize image data using specified method with cv2.\n",
    "\n",
    "    Args:\n",
    "        image (np.ndarray): Input image array of shape (C, H, W)\n",
    "        method (Literal[\"STANDARD\", \"MINMAX\", \"ROBUST\", \"NONE\"]): Normalization method\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized image array\n",
    "\n",
    "    Raises:\n",
    "        ImageProcessingError: If normalization fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if method == \"NONE\":\n",
    "            return image\n",
    "\n",
    "        if method == \"MINMAX\":\n",
    "            # Scale to [0,1] range\n",
    "            return cv2.normalize(\n",
    "                image,\n",
    "                None,\n",
    "                alpha=0,\n",
    "                beta=1,\n",
    "                norm_type=cv2.NORM_MINMAX,\n",
    "                dtype=cv2.CV_32F,\n",
    "            )\n",
    "\n",
    "        elif method == \"STANDARD\":\n",
    "            # Standardize to zero mean and unit variance\n",
    "            mean = np.mean(image, axis=(1, 2), keepdims=True)\n",
    "            std = np.std(image, axis=(1, 2), keepdims=True)\n",
    "            return (image - mean) / (\n",
    "                std + 1e-8\n",
    "            )  # add epsilon to prevent division by zero\n",
    "\n",
    "        elif method == \"ROBUST\":\n",
    "            # Use median and IQR instead of mean/std for robustness to outliers\n",
    "            median = np.median(image, axis=(1, 2), keepdims=True)\n",
    "            q75, q25 = np.percentile(image, [75, 25], axis=(1, 2), keepdims=True)\n",
    "            iqr = q75 - q25\n",
    "            return (image - median) / (iqr + 1e-8)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported normalization method: {method}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Failed to normalize image: {str(e)}\")\n",
    "\n",
    "\n",
    "def parse_image_to_tensor(\n",
    "    image_path: Union[str, Path],\n",
    "    scaling: str,\n",
    "    to_h: Optional[int] = None,\n",
    "    to_w: Optional[int] = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Parses an image from a given path to a PyTorch tensor.\n",
    "\n",
    "    This function performs the following operations:\n",
    "    1. Reads an image from the specified path\n",
    "    2. Resizes it to the specified dimensions\n",
    "    3. Converts it to grayscale if it's a color image\n",
    "    4. Transposes the dimensions to (C, H, W)\n",
    "    5. Normalizes the pixel values using the specified method\n",
    "    6. Converts it to a PyTorch tensor\n",
    "\n",
    "    Args:\n",
    "        image_path (Union[str, Path]): Path to the image file\n",
    "        config (DefaultConfig): Configuration object with image processing settings\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Processed image tensor with shape (C, H, W)\n",
    "\n",
    "    Raises:\n",
    "        ImageProcessingError: If there are issues with image format, shape, or processing\n",
    "        FileNotFoundError: If the image file doesn't exist\n",
    "        ValueError: If input dimensions are invalid\n",
    "    \"\"\"\n",
    "    # INPUT VALIDATION ----------------------------------------------\n",
    "    if not validate_image_path(image_path):\n",
    "        raise FileNotFoundError(f\"Invalid image path: {image_path}\")\n",
    "    image_path = Path(image_path)\n",
    "    SUPPORTED_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n",
    "    if image_path.suffix.lower() not in SUPPORTED_EXTENSIONS:\n",
    "        raise ImageProcessingError(\n",
    "            f\"Unsupported image format: {image_path.suffix}. \"\n",
    "            f\"Supported formats are: {', '.join(SUPPORTED_EXTENSIONS)}\"\n",
    "        )\n",
    "    # IMAGE PROCESSING ----------------------------------------------\n",
    "    try:\n",
    "        # Read image based on format\n",
    "        if image_path.suffix.lower() in {\".tif\", \".tiff\"}:\n",
    "            image = cv2.imread(str(image_path), cv2.IMREAD_UNCHANGED)\n",
    "        else:\n",
    "            image = cv2.imread(str(image_path))\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Failed to read image: {image_path}\")\n",
    "\n",
    "        (h, w, _) = image.shape[:3]\n",
    "        if to_h is None:\n",
    "            to_h = h\n",
    "        if to_w is None:\n",
    "            to_w = w\n",
    "\n",
    "        if not (2 <= len(image.shape) <= 3):\n",
    "            raise ImageProcessingError(\n",
    "                f\"Image has unsupported shape: {image.shape}. \"\n",
    "                \"Supported shapes are 2D and 3D.\"\n",
    "            )\n",
    "\n",
    "        print(f\"imgage shape after transpose{image.shape}\")\n",
    "        try:\n",
    "            image = cv2.resize(image, (to_w, to_h), interpolation=cv2.INTER_AREA)\n",
    "            print(f\"imgage shape after resize{image.shape}\")\n",
    "        except Exception as e:\n",
    "            raise ImageProcessingError(f\"Failed to resize image: {str(e)}\")\n",
    "\n",
    "        image = normalize_image(image=image, method=scaling)\n",
    "\n",
    "        print(f\"imgage shape after normalize{image.shape}\")\n",
    "        if len(image.shape) == 3:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        # check if format is (C, H, W)\n",
    "        if len(image.shape) == 2:\n",
    "            # add channel dimension\n",
    "            image = np.expand_dims(image, axis=2)\n",
    "            print(f\"shape after np.expand_dims: {image.shape}\")\n",
    "\n",
    "        image = image.transpose(2, 0, 1)\n",
    "        print(f\"shape after transpose: {image.shape}\")\n",
    "        return image\n",
    "\n",
    "    except Exception as e:\n",
    "        if isinstance(e, (FileNotFoundError, ImageProcessingError, ValueError)):\n",
    "            raise\n",
    "        raise ImageProcessingError(\n",
    "            f\"Unexpected error during image processing: {str(e)}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def validate_image_path(image_path: Union[str, Path]) -> bool:\n",
    "    \"\"\"\n",
    "    Validates if the given image path exists and has a supported format.\n",
    "\n",
    "    Args:\n",
    "        image_path (Union[str, Path]): Path to validate\n",
    "\n",
    "    Returns:\n",
    "        bool: True if path is valid, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    path = Path(image_path) if isinstance(image_path, str) else image_path\n",
    "    return (\n",
    "        path.exists()\n",
    "        and path.is_file()\n",
    "        and path.suffix.lower() in {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 05 How to add a new architecture\n",
    "### High level workflow\n",
    "To add a new autoencoder architecture, you need to at least code two things:\n",
    "- the model architecture itself in  `src/autoencodix/modeling/` - anlogous to `_varix_architecture.py`\n",
    "- the pipeline itself in `src/autoencodix/` - analgous to `varix.py`\n",
    "Depending on the complexity and data requirements you might also want to provide the following:\n",
    "- a new custom dataset class in `src/autoencodix/data` - analogous to `numeric_dataset.py`\n",
    "- a new custom preprocessor in `src/autoencodix/data` - as in `preprocessor.py`\n",
    "- a new custom trainer in `src/autoencodix/trainers` - as in `_general_trainer.py`\n",
    "  - including a custom predict method of your trainer\n",
    "- a custom loss for your model\n",
    "- a custom visualizer (no example implemented yet)\n",
    "- a custom evaluator for downstream tasks (no example implemented yet)\n",
    "- a custom tuner (not sure if this will be part of the package)\n",
    "### High level structure\n",
    "- Each autoencodix model in our family is based on our base classes in `src/autoencodix/base`. Here we have (often abstract) classes that define the general structure of each step (preprocess, fit, predict, evaluate, visualize) in our pipeline, as well as additional classes e.g. losses.\n",
    "- In these base classes we've implemented shared functionalities, like calling the corresponding trainer, or preprocessor.\n",
    "- The base classes also guide you to the structure of your new class. The methods of the base classes should not be changed. Rather overwrite the method in the implementation of your child class in case you need to make changes.\n",
    "\n",
    "### Must-do files details\n",
    "We'll illustrate this by an example. We want to add the new architecture with the name MySpecial to our package. First we add the actual architecture:\n",
    "- create the file `src/autoencodix/modeling/_myspecialix_architecture.py` (note files that should not be imported at end-user lever have a leading underscore).\n",
    "- create the file `tests/test_modelling/test_myspecialix_architecture`\n",
    "- we write the class itself that might look like:\n",
    "```python\n",
    "from autoencodix.base._base_autoencoder import BaseAutoencoder\n",
    "# your imports\n",
    "# TODO\n",
    "\n",
    "\n",
    "# needs to inherit from BaseAutoencoder\n",
    "class MySpecialArchitecture(BaseAutoencoder):\n",
    "    \"\"\"\n",
    "    MySpecial implementation accroding to (cite paper, yourself, etc)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.input_dim : int\n",
    "        number of input features\n",
    "    self.config: DefaultConfig\n",
    "        Configuration object containing model architecture parameters\n",
    "    self._encoder: nn.Module\n",
    "        Encoder network of the autoencoder\n",
    "    self._decoder: nn.Module\n",
    "        Decoder network of the autoencoder\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    my_super_special_method():\n",
    "        does cool stuff\n",
    "    _build_network()\n",
    "        Construct the encoder and decoder networks via the LayerFactory\n",
    "    encode(x: torch.Tensor) -> torch.Tensor\n",
    "        Encode the input tensor x\n",
    "    decode(x: torch.Tensor) -> torch.Tensor\n",
    "        Decode the latent tensor x\n",
    "    forward(x: torch.Tensor) -> ModelOutput\n",
    "        Forward pass of the model, fills in the reconstruction and latentspace attributes of ModelOutput class.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, config: Optional[Union[None, DefaultConfig]], input_dim: int\n",
    "    ) -> None:\n",
    "        if config is None:\n",
    "            config = DefaultConfig()\n",
    "        self._config = config\n",
    "        super().__init__(config, input_dim)\n",
    "        self.input_dim = input_dim # we always base the input dimension (usually number of features in your dataset)\n",
    "\n",
    "        # populate self.encoder and self.decoder\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self) -> None:\n",
    "        \"\"\"\n",
    "        Construct the encoder and decoder networks.\n",
    "        See your _layer_factory.py file that could help you here.\n",
    "        Also check other implementation to see how to use _layer_factory.py\n",
    "        \"\"\"\n",
    "        self._encoder = TODO\n",
    "        self._decoder = TODO\n",
    "\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Encode the input tensor x\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Encoded tensor\n",
    "\n",
    "        \"\"\"\n",
    "        encoded = self._encoder(x)\n",
    "        return encoded\n",
    "\n",
    "\n",
    "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode the latent tensor x\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Latent tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Decoded tensor\n",
    "\n",
    "        \"\"\"\n",
    "        return self._decoder(x)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> ModelOutput:\n",
    "        \"\"\"\n",
    "        Forward pass of the model, fill\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            Input tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ModelOutput\n",
    "            ModelOutput object containing the reconstructed tensor and latent tensor\n",
    "\n",
    "        \"\"\"\n",
    "        latent = self.encode(x)\n",
    "        x_hat = self.decode(z)\n",
    "        # fill model output arcodding to your needs (we need reconstruction and a latentspace, see ModelOuput class for required output)\n",
    "        return ModelOutput(\n",
    "            reconstruction=x_hat,\n",
    "            latentspace=latent\n",
    "            latent_mean=None,\n",
    "            latent_logvar=None,\n",
    "            additional_info=None,\n",
    "        )\n",
    "\n",
    "```\n",
    "- adjust the `__init__.py` in `src/autoencodix/modelling` to import `MySpecialArchitecture\n",
    "- next we write tests for the newly created file in the test file\n",
    "- lastly, we need to create the pipeline file:\n",
    "  - create `src/autoencodix/myspecialix.py`\n",
    "  - create `tests/test_myspecialix.py`\n",
    "- the `myspecialix.py` might look like this\n",
    "```python\n",
    "# your imports\n",
    "# TODO\n",
    "\n",
    "class MySpecialix(BasePipeline): # must inhertit from BasePipeline\n",
    "    \"\"\"\n",
    "    MySpecialix specific version of the BasePipeline class.\n",
    "    Inherits preprocess, fit, predict, evaluate, and visualize methods from BasePipeline.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    data : Union[np.ndarray, AnnData, pd.DataFrame]\n",
    "        Input data from the user\n",
    "    config : Optional[Union[None, DefaultConfig]]\n",
    "        Configuration object containing customizations for the pipeline\n",
    "    _preprocessor : Preprocessor\n",
    "        Preprocessor object to preprocess the input data (custom for Vanillix)\n",
    "    _visualizer : Visualizer\n",
    "        Visualizer object to visualize the model output (custom for Vanillix)\n",
    "    _trainer : GeneralTrainer\n",
    "        Trainer object that trains the model (custom for Vanillix)\n",
    "    _evaluator : Evaluator\n",
    "        Evaluator object that evaluates the model performance or downstream tasks (custom for Vanillix)\n",
    "    result : Result\n",
    "        Result object to store the pipeline results\n",
    "    _datasets : Optional[DatasetContainer]\n",
    "        Container for train, validation, and test datasets (preprocessed)\n",
    "    data_splitter : DataSplitter\n",
    "        DataSplitter object to split the data into train, validation, and test sets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data: Union[np.ndarray, AnnData, pd.DataFrame],\n",
    "        # this can be also a custom Type like MySpecialixDataset\n",
    "        dataset_type: Type[BaseDataset] = NumericDataset,\n",
    "        # This will be the Type MySpecialixArchitecture that we created before\n",
    "        model_type: Type[BaseAutoencoder] = MySpecialixArchitecture,\n",
    "        # This can be a custom Loss class, or an exisiting one see _losses.py\n",
    "        loss_type: Type[BaseLoss] = VanillixLoss,\n",
    "        preprocessor: Optional[Preprocessor] = None,\n",
    "        visualizer: Optional[BaseVisualizer] = None,\n",
    "        evaluator: Optional[Evaluator] = None,\n",
    "        result: Optional[Result] = None,\n",
    "        datasplitter_type: Type[DataSplitter] = DataSplitter,\n",
    "        custom_splits: Optional[Dict[str, np.ndarray]] = None,\n",
    "        config: Optional[DefaultConfig] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize MySpecialix pipeline with customizable components.\n",
    "\n",
    "        Some components are passed as types rather than instances because they require\n",
    "        data that is only available after preprocessing.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : Union[np.ndarray, AnnData, pd.DataFrame]\n",
    "            Input data to be processed\n",
    "        trainer_type : Type[BaseTrainer]\n",
    "            Type of trainer to be instantiated during fit step, default is GeneralTrainer\n",
    "        dataset_type : Type[BaseDataset]\n",
    "            Type of dataset to be instantiated post-preprocessing, default is NumericDataset\n",
    "        loss_type : Type[BaseLoss], which loss to use for Vanillix, default is VanillaAutoencoderLoss\n",
    "        preprocessor : Optional[Preprocessor]\n",
    "            For data preprocessing, default creates new Preprocessor\n",
    "        visualizer : Optional[Visualizer]\n",
    "            For result visualization, default creates new Visualizer\n",
    "        evaluator : Optional[Evaluator]\n",
    "            For model evaluation, default creates new Evaluator\n",
    "        result : Optional[Result]\n",
    "            Container for pipeline results, default creates new Result\n",
    "        datasplitter_type : Type[DataSplitter], optional\n",
    "            Type of splitter to be instantiated during preprocessing, default is DataSplitter\n",
    "        custom_splits : Optional[Dict[str, np.ndarray]]\n",
    "            Custom train/valid/test split indices\n",
    "        config : Optional[DefaultConfig]\n",
    "            Configuration for all pipeline components\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            data=data,\n",
    "            dataset_type=dataset_type,\n",
    "            trainer_type=trainer_type,\n",
    "            model_type=model_type,\n",
    "            loss_type=loss_type,\n",
    "            preprocessor=preprocessor or Preprocessor(),\n",
    "            visualizer=visualizer or Visualizer(),\n",
    "            evaluator=evaluator or Evaluator(),\n",
    "            result=result or Result(),\n",
    "            datasplitter_type=datasplitter_type,\n",
    "            config=config or DefaultConfig(),\n",
    "            custom_split=custom_splits,\n",
    "        )\n",
    "\n",
    "```\n",
    "##### More explaination to the passing of Types instead of classes:\n",
    "Most functionality of the pipeline comes from the BasePipeline. To make the methods custom to our specific architecture that we use in our `MySpecial` pipeline, we need to pass our specializes subclasses. Since we don't have all required parameters for this subclasses when calling the init method of the parent class, we pass only the type of the subclasses. These types need to be childs of the corresponding base class. Inside the BasePipeline we instantiate the specific classes with the required paramters as soon as we have them\n",
    "### Optional files details\n",
    "The optional files work from the same principle as the mandatory files, so we can always create a special class based on the baseclass and then we pass the type of our special class to our MySpecialix Pipeline e.g MySpecialTrainer n the init mehtod of MySpecialix (same as we did with MySpecialArchitecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS\n",
    "- show how to update and work with the config object (later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANDBOX \n",
    "testing Varix and losses, especially sub_losses in result_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Goal: 4 cases unserer alten Implemntation abbilden\n",
    "- single cell (s. Beispiel Jan)\n",
    "- image (tcga beispiel)\n",
    "- multi TCGA omics (mini Beispiel)\n",
    "- ontologie Beispiel\n",
    "\n",
    "Hauptsächlich get es darum wie wir Daten und Metadaten von verschiedenen Omics Typen speichern und wie der Nutzer die Information über die Daten ans package weitergeben.\n",
    "Im ersten Schritt der Sanbox werden wir nun versuchen jeden case in einem AnnData Objekt abzubilden und dann auf Grundlage des AnnData Objekts das perprocessing machen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated config structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate from object\n",
    "Alternatively, we can just create our DefaultConfig Object and initialize it with the data info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Single Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DefaultConfig.model_validate(yaml.safe_load(Path(\"scconfig.yaml\").read_text()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XModalix with images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### images and bulk-seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add the rest of the code\n",
    "img_folder = \"images/tcga_mini\"\n",
    "xmodal_config = DefaultConfig(\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"RNA\": DataInfo(\n",
    "                file_path=os.path.join(root_dir, mrna_file),\n",
    "                data_type=DataType.NUMERIC,\n",
    "            ),\n",
    "            \"IMG\": DataInfo(\n",
    "                file_path=os.path.join(root_dir, \"tcga_mappings.txt\"),\n",
    "                data_type=DataType.IMG,\n",
    "                img_root=os.path.join(root_dir, img_folder),\n",
    "            ),\n",
    "            \"CLINICAL\": DataInfo(\n",
    "                file_path=os.path.join(root_dir, clin_file),\n",
    "                data_type=DataType.ANNOTATION,\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading RNA data from data/raw/data_mrna_seq_v2_rsem_formatted.parquet\n",
      "Loading IMG data from data/raw/tcga_mappings.txt\n",
      "Skipping image data\n",
      "Loading CLINICAL data from data/raw/data_clinical_formatted.parquet\n",
      "10059\n",
      "Skipping image data\n"
     ]
    }
   ],
   "source": [
    "# bulk seq standard\n",
    "common_samples, config_filled = load_and_intersect_bulkdata(config=xmodal_config)\n",
    "print(len(common_samples))\n",
    "adata = build_bulk_anndata(config=config_filled, common_samples=common_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 10059 × 16313\n",
      "    obs: 'PATIENT_ID', 'ONCOTREE_CODE', 'CANCER_TYPE', 'CANCER_TYPE_DETAILED', 'TUMOR_TYPE', 'GRADE', 'TISSUE_PROSPECTIVE_COLLECTION_INDICATOR', 'TISSUE_RETROSPECTIVE_COLLECTION_INDICATOR', 'TISSUE_SOURCE_SITE_CODE', 'TUMOR_TISSUE_SITE', 'ANEUPLOIDY_SCORE', 'SAMPLE_TYPE', 'MSI_SCORE_MANTIS', 'MSI_SENSOR_SCORE', 'SOMATIC_STATUS', 'TMB_NONSYNONYMOUS', 'TISSUE_SOURCE_SITE', 'SUBTYPE', 'CANCER_TYPE_ACRONYM', 'OTHER_PATIENT_ID', 'AGE', 'SEX', 'AJCC_PATHOLOGIC_TUMOR_STAGE', 'AJCC_STAGING_EDITION', 'DAYS_LAST_FOLLOWUP', 'DAYS_TO_BIRTH', 'DAYS_TO_INITIAL_PATHOLOGIC_DIAGNOSIS', 'ETHNICITY', 'FORM_COMPLETION_DATE', 'HISTORY_NEOADJUVANT_TRTYN', 'ICD_10', 'ICD_O_3_HISTOLOGY', 'ICD_O_3_SITE', 'INFORMED_CONSENT_VERIFIED', 'NEW_TUMOR_EVENT_AFTER_INITIAL_TREATMENT', 'PATH_M_STAGE', 'PATH_N_STAGE', 'PATH_T_STAGE', 'PERSON_NEOPLASM_CANCER_STATUS', 'PRIMARY_LYMPH_NODE_PRESENTATION_ASSESSMENT', 'PRIOR_DX', 'RACE', 'RADIATION_THERAPY', 'WEIGHT', 'IN_PANCANPATHWAYS_FREEZE', 'OS_STATUS', 'OS_MONTHS', 'DSS_STATUS', 'DSS_MONTHS', 'DFS_STATUS', 'DFS_MONTHS', 'PFS_STATUS', 'PFS_MONTHS', 'AJCC_PATHOLOGIC_TUMOR_STAGE_SHORT'\n",
      "    obsm: 'RNA'\n"
     ]
    }
   ],
   "source": [
    "print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 45549 × 19475\n",
      "    obs: 'author_cell_type', 'age_group', 'donor_id', 'nCount_RNA', 'nFeature_RNA', 'nCount_ATAC', 'nFeature_ATAC', 'TSS_percentile', 'nucleosome_signal', 'percent_mt', 'assay_ontology_term_id', 'cell_type_ontology_term_id', 'development_stage_ontology_term_id', 'disease_ontology_term_id', 'self_reported_ethnicity_ontology_term_id', 'organism_ontology_term_id', 'sex_ontology_term_id', 'tissue_ontology_term_id', 'suspension_type', 'is_primary_data', 'batch', 'tissue_type', 'cell_type', 'assay', 'disease', 'organism', 'sex', 'tissue', 'self_reported_ethnicity', 'development_stage', 'observation_joinid'\n",
      "    var: 'feature_is_filtered', 'feature_name', 'feature_reference', 'feature_biotype', 'feature_length', 'feature_type'\n",
      "    uns: 'batch_condition', 'citation', 'schema_reference', 'schema_version', 'title'\n",
      "    obsm: 'X_joint_wnn_umap', 'X_umap'\n",
      "    layers: 'RNA'\n"
     ]
    }
   ],
   "source": [
    "print(scadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_img(file_path):\n",
    "    ext = file_path.split(\".\")[-1]\n",
    "    if not ext in [\"png\", \"jpg\", \"jpeg\"]:\n",
    "        raise ValueError(f\"Unsupported image format: {ext}\")\n",
    "    img = cv2.imread(file_path)\n",
    "    # read image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image folder: data/raw/images/tcga_mini\n"
     ]
    }
   ],
   "source": [
    "for k, v in xmodal_config.data_config.data_info.items():\n",
    "    if v.data_type == DataType.IMG:\n",
    "        print(f\"Image folder: {v.file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class XModaleData:\n",
    "    mapping_file: str\n",
    "    img_folder: str\n",
    "    _img_array: List[np.ndarray]\n",
    "    _labels: Dict[str, List[Union[str, int, float]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import cv2\n",
    "\n",
    "adata = ad.AnnData()\n",
    "data_dict = defaultdict(dict)\n",
    "adata = ad.AnnData(\n",
    "    X=config.data_config.data_info[\"RNA\"].data_object.loc[list(common_samples)].values,\n",
    "    obs=pd.DataFrame(index=list(common_samples)),\n",
    ")\n",
    "\n",
    "\n",
    "previous = {}\n",
    "for k, v in config.data_config.data_info.items():\n",
    "    if v.data_type == DataType.IMG:\n",
    "        img_folder = v.img_root\n",
    "        mappings = pd.read_csv(v.file_path, sep=\"\\t\")\n",
    "        file_paths = list(mappings[\"img_path\"].values)\n",
    "        imgs = [read_img(os.path.join(img_folder, img)) for img in file_paths]\n",
    "        config.data_config.data_info[k].data_object = {\n",
    "            \"imgs\": imgs,\n",
    "            \"mappings\": mappings,\n",
    "        }\n",
    "    if v.data_type == DataType.ANNOTATION:\n",
    "        ann_df = pd.read_parquet(os.path.join(v.file_path))\n",
    "        adata.obs = ann_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next TODOS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- write docu of new data handling:\n",
    "  - how to fill config\n",
    "  - how different scenarios work\n",
    "  - how to proceed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_losses = result.sub_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.utils._result import Result, LossRegistry\n",
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "from autoencodix.utils._traindynamics import TrainingDynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_result = Result()\n",
    "sample_model = None\n",
    "sample_datasets = {\"train\": None, \"valid\": None, \"test\": None}\n",
    "sample_recon_data = {\n",
    "    0: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "    100: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "}\n",
    "sample_var_loss_data = {\n",
    "    0: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "    2: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "    100: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "}\n",
    "sample_recon_data_dyn = TrainingDynamics(_data=sample_recon_data)\n",
    "sample_var_loss_data_dyn = TrainingDynamics(_data=sample_var_loss_data)\n",
    "sample_losses = LossRegistry(\n",
    "    _losses={\"recon_loss\": sample_recon_data_dyn, \"var_loss\": sample_var_loss_data_dyn}\n",
    ")\n",
    "filled_result = Result(\n",
    "    model=sample_model, datasets=sample_datasets, sub_losses=sample_losses\n",
    ")\n",
    "\n",
    "recon_loss_data1 = {\n",
    "    0: {\"valid\": 0.1, \"train\": 0.2, \"test\": 0.3},\n",
    "    1: {\"valid\": 0.4, \"train\": 0.5, \"test\": 0.6},\n",
    "    2: {\"valid\": 0.7, \"train\": 0.8, \"test\": 0.9},\n",
    "}\n",
    "var_loss_data1 = {\n",
    "    0: {\"valid\": 0.11, \"train\": 0.21, \"test\": 0.31},\n",
    "    1: {\"valid\": 0.31, \"train\": 0.41, \"test\": 0.51},\n",
    "    2: {\"valid\": 0.51, \"train\": 0.61, \"test\": 0.71},\n",
    "}\n",
    "\n",
    "recon_dynamics1 = TrainingDynamics(_data=recon_loss_data1)\n",
    "var_dynamics1 = TrainingDynamics(_data=var_loss_data1)\n",
    "sample_registry = LossRegistry(\n",
    "    _losses={\"recon_loss\": recon_dynamics1, \"var_loss\": var_dynamics1}\n",
    ")\n",
    "sample_result1 = Result(sub_losses=sample_registry)\n",
    "recon_loss_data2 = {\n",
    "    0: {\"valid\": 0.15, \"train\": 0.25},\n",
    "    1: {\"valid\": 0.45, \"train\": 0.55, \"test\": None},\n",
    "    2: {\"valid\": None, \"train\": 0.85, \"test\": 0.95},\n",
    "}\n",
    "var_loss_data2 = {\n",
    "    0: {\"valid\": 0.16, \"train\": 0.26, \"test\": 0.36},\n",
    "    1: {\"valid\": 0.36, \"train\": 0.46},\n",
    "    2: None,\n",
    "}  # should not be updated\n",
    "\n",
    "recon_dynamics2 = TrainingDynamics(_data=recon_loss_data2)\n",
    "var_dynamics2 = TrainingDynamics(_data=var_loss_data2)\n",
    "sample_registry2 = LossRegistry(\n",
    "    _losses={\"recon_loss\": recon_dynamics2, \"var_loss\": var_dynamics2}\n",
    ")\n",
    "sample_result2 = Result(sub_losses=sample_registry2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_update = {\n",
    "    0: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},  # totally overwrite\n",
    "    2: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},  # keep all\n",
    "    3: {},  # update with partial data\n",
    "    4: None,  # udate with all\n",
    "    5: {\n",
    "        \"train\": None,\n",
    "        \"valid\": 0.0,\n",
    "        \"test\": 0.0,\n",
    "    },  # update with partial data (keep test)\n",
    "    6: {\"train\": 0.0},  # update with partial data (keep train), add vlaid and test\n",
    "    7: {\"train\": 0},  # update with partial data {train: 0.1}\n",
    "    100: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},  # not in to update\n",
    "}\n",
    "\n",
    "update_with = {\n",
    "    0: {\"train\": 0.1, \"valid\": 0.2, \"test\": 0.3},\n",
    "    1: {\"train\": 0.4, \"valid\": 0.5, \"test\": 0.6},\n",
    "    2: {},\n",
    "    3: {\"train\": 0.7, \"valid\": 0.8},\n",
    "    4: {\"train\": 0.9, \"valid\": 1.0, \"test\": 1.1},\n",
    "    5: {\"train\": 0.12, \"valid\": 0.22},\n",
    "    6: {\"valid\": 0.32, \"test\": 0.42},\n",
    "    7: {\"train\": 0.1},\n",
    "}\n",
    "\n",
    "expected_result = {\n",
    "    0: {\"train\": np.array(0.1), \"valid\": np.array(0.2), \"test\": np.array(0.3)},\n",
    "    1: {\"train\": np.array(0.4), \"valid\": np.array(0.5), \"test\": np.array(0.6)},\n",
    "    2: {\"train\": np.array(0.0), \"valid\": np.array(0.0), \"test\": np.array(0.0)},\n",
    "    3: {\"train\": np.array(0.7), \"valid\": np.array(0.8)},\n",
    "    4: {\"train\": np.array(0.9), \"valid\": np.array(1.0), \"test\": np.array(1.1)},\n",
    "    5: {\"train\": np.array(0.12), \"valid\": np.array(0.22), \"test\": np.array(0.0)},\n",
    "    6: {\"train\": np.array(0.0), \"valid\": np.array(0.32), \"test\": np.array(0.42)},\n",
    "    7: {\"train\": np.array(0.1)},\n",
    "    100: {\"train\": np.array(0.0), \"valid\": np.array(0.0), \"test\": np.array(0.0)},\n",
    "}\n",
    "\n",
    "to_update_dyn = TrainingDynamics(_data=to_update)\n",
    "to_update_registry = LossRegistry(\n",
    "    _losses={\n",
    "        \"recon_loss\": to_update_dyn,\n",
    "        \"var_loss\": to_update_dyn,\n",
    "        \"more_loss\": TrainingDynamics(_data={}),\n",
    "    }\n",
    ")\n",
    "to_update_result = Result(sub_losses=to_update_registry)\n",
    "\n",
    "\n",
    "update_with_dyn = TrainingDynamics(_data=update_with)\n",
    "update_with_registry = LossRegistry(\n",
    "    _losses={\n",
    "        \"recon_loss\": update_with_dyn,\n",
    "        \"var_loss\": TrainingDynamics(_data={}),\n",
    "        \"other_loss\": TrainingDynamics(_data={}),\n",
    "    }\n",
    ")\n",
    "update_with_result = Result(sub_losses=update_with_registry)\n",
    "to_update_result.update(update_with_result)\n",
    "after_update = dict(\n",
    "    sorted(to_update_result.sub_losses.get(key=\"recon_loss\").get().items())\n",
    ")\n",
    "expected_recon_dyn = TrainingDynamics(_data=expected_result)\n",
    "expected_varloss_dyn = TrainingDynamics(_data=to_update)\n",
    "expected_moreloss_dyn = TrainingDynamics(_data={})\n",
    "expected_other_dyn = TrainingDynamics(_data={})\n",
    "expected_loss_registry = LossRegistry(\n",
    "    _losses={\n",
    "        \"recon_loss\": expected_recon_dyn,\n",
    "        \"var_loss\": expected_varloss_dyn,\n",
    "        \"more_loss\": expected_moreloss_dyn,\n",
    "        \"other_loss\": expected_other_dyn,\n",
    "    }\n",
    ")\n",
    "expected_result = Result(sub_losses=expected_loss_registry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_loss\n",
      "<class 'dict'>\n",
      "var_loss\n",
      "<class 'dict'>\n",
      "more_loss\n",
      "<class 'dict'>\n",
      "other_loss\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "to_update_result.sub_losses._losses\n",
    "for lossname, dynamics in expected_result.sub_losses.losses():\n",
    "    print(lossname)\n",
    "    print(type(dynamics._data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_loss\n",
      "{0: {'train': array(0.1), 'valid': array(0.2), 'test': array(0.3)}, 1: {'train': array(0.4), 'valid': array(0.5), 'test': array(0.6)}, 2: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}, 3: {'valid': array(0.8), 'train': array(0.7)}, 4: {'valid': array(1.), 'test': array(1.1), 'train': array(0.9)}, 5: {'valid': array(0.22), 'test': array(0.), 'train': array(0.12)}, 6: {'train': array(0.), 'valid': array(0.32), 'test': array(0.42)}, 7: {'train': array(0.1)}, 100: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}}\n",
      "var_loss\n",
      "{0: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}, 2: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}, 5: {'valid': array(0.), 'test': array(0.)}, 6: {'train': array(0.)}, 7: {'train': array(0)}, 100: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}}\n",
      "more_loss\n",
      "{}\n",
      "other_loss\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "for lossname, dynamics in to_update_result.sub_losses.losses():\n",
    "    print(lossname)\n",
    "    print(dynamics.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_loss\n",
      "{0: {'train': array(0.1), 'valid': array(0.2), 'test': array(0.3)}, 1: {'train': array(0.4), 'valid': array(0.5), 'test': array(0.6)}, 2: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}, 3: {'train': array(0.7), 'valid': array(0.8)}, 4: {'train': array(0.9), 'valid': array(1.), 'test': array(1.1)}, 5: {'train': array(0.12), 'valid': array(0.22), 'test': array(0.)}, 6: {'train': array(0.), 'valid': array(0.32), 'test': array(0.42)}, 7: {'train': array(0.1)}, 100: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}}\n",
      "var_loss\n",
      "{0: {'train': 0.0, 'valid': 0.0, 'test': 0.0}, 2: {'train': 0.0, 'valid': 0.0, 'test': 0.0}, 3: {}, 4: None, 5: {'train': None, 'valid': 0.0, 'test': 0.0}, 6: {'train': 0.0}, 7: {'train': 0}, 100: {'train': 0.0, 'valid': 0.0, 'test': 0.0}}\n",
      "more_loss\n",
      "{}\n",
      "other_loss\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "for lossname, dynamics in expected_result.sub_losses.losses():\n",
    "    print(lossname)\n",
    "    print(dynamics.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'train': array(0.26), 'valid': array(0.16), 'test': array(0.36)},\n",
       " 2: {'train': 0.0, 'valid': 0.0, 'test': 0.0},\n",
       " 100: {'train': 0.0, 'valid': 0.0, 'test': 0.0},\n",
       " 1: {'valid': array(0.36), 'train': array(0.46)}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(filled_result.sub_losses._losses[\"var_loss\"].get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetContainer(train=<autoencodix.data._numeric_dataset.NumericDataset object at 0x15ff96b30>, valid=<autoencodix.data._numeric_dataset.NumericDataset object at 0x15ff96bc0>, test=<autoencodix.data._numeric_dataset.NumericDataset object at 0x15ff96e00>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty does not overwrite anything\n",
    "result.update(empty_result)\n",
    "result.losses.get()\n",
    "result.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'valid': array(0.1), 'train': array(0.2), 'test': array(0.3)},\n",
       " 1: {'valid': array(0.4), 'train': array(0.5), 'test': array(0.6)},\n",
       " 2: {'valid': array(0.7), 'train': array(0.8), 'test': array(0.9)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case one update empty result\n",
    "# with: - empty result, - sample result1, - sample result2\n",
    "empty_result.update(other=sample_result1)\n",
    "empty_result.sub_losses._losses[\"recon_loss\"].get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_result.update(other=sample_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case two update filled result\n",
    "# with: - filled result, - sample result1, - sample result2, empty result\n",
    "filled_result.update(other=sample_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_losses == updated_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LossRegistry(_losses={'recon_loss': TrainingDynamics(), 'var_loss': TrainingDynamics()})\n"
     ]
    }
   ],
   "source": [
    "updated_losses = result.sub_losses\n",
    "print(updated_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_loss: TrainingDynamics()\n",
      "{0: {'train': array(0.44560597), 'valid': array(0.24305786)}, 1: {'train': array(0.40050308), 'valid': array(0.23968849)}, 2: {'train': array(0.39850321), 'valid': array(0.2438124)}}\n",
      "var_loss: TrainingDynamics()\n",
      "{0: {'train': array(0.03078574), 'valid': array(0.00519827)}, 1: {'train': array(0.02884311), 'valid': array(0.00536202)}, 2: {'train': array(0.02645038), 'valid': array(0.00561051)}}\n"
     ]
    }
   ],
   "source": [
    "for name, loss in updated_losses.losses():\n",
    "    print(f\"{name}: {loss}\")\n",
    "    print(loss.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_FabricModule(\n",
       "  (_forward_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (_original_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varix._trainer._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DefaultConfig Configuration Parameters:\n",
      "--------------------------------------------------\n",
      "\n",
      "latent_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 16\n",
      "  Description: Dimension of the latent space\n",
      "\n",
      "n_layers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of layers in encoder/decoder\n",
      "\n",
      "enc_factor:\n",
      "  Type: <class 'int'>\n",
      "  Default: 4\n",
      "  Description: Scaling factor for encoder dimensions\n",
      "\n",
      "input_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 10000\n",
      "  Description: Input dimension\n",
      "\n",
      "drop_p:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Dropout probability\n",
      "\n",
      "learning_rate:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.001\n",
      "  Description: Learning rate for optimization\n",
      "\n",
      "batch_size:\n",
      "  Type: <class 'int'>\n",
      "  Default: 32\n",
      "  Description: Number of samples per batch\n",
      "\n",
      "epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of training epochs\n",
      "\n",
      "weight_decay:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.01\n",
      "  Description: L2 regularization factor\n",
      "\n",
      "reconstruction_loss:\n",
      "  Type: typing.Literal['mse', 'bce']\n",
      "  Default: mse\n",
      "  Description: Type of reconstruction loss\n",
      "\n",
      "default_vae_loss:\n",
      "  Type: typing.Literal['kl', 'mmd']\n",
      "  Default: kl\n",
      "  Description: Type of VAE loss\n",
      "\n",
      "loss_reduction:\n",
      "  Type: typing.Literal['sum', 'mean']\n",
      "  Default: mean\n",
      "  Description: Loss reduction in PyTorch i.e in torch.nn.functional.binary_cross_entropy_with_logits(reduction=loss_reduction)\n",
      "\n",
      "beta:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Beta weighting factor for VAE loss\n",
      "\n",
      "min_samples_per_split:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Minimum number of samples per split\n",
      "\n",
      "device:\n",
      "  Type: typing.Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']\n",
      "  Default: auto\n",
      "  Description: Device to use\n",
      "\n",
      "n_gpus:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Number of GPUs to use\n",
      "\n",
      "n_workers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 2\n",
      "  Description: Number of data loading workers\n",
      "\n",
      "checkpoint_interval:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Interval for saving checkpoints\n",
      "\n",
      "float_precision:\n",
      "  Type: typing.Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16']\n",
      "  Default: 32\n",
      "  Description: Floating point precision\n",
      "\n",
      "gpu_strategy:\n",
      "  Type: typing.Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']\n",
      "  Default: auto\n",
      "  Description: GPU parallelization strategy\n",
      "\n",
      "train_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.7\n",
      "  Description: Ratio of data for training\n",
      "\n",
      "test_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.2\n",
      "  Description: Ratio of data for testing\n",
      "\n",
      "valid_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Ratio of data for validation\n",
      "\n",
      "reproducible:\n",
      "  Type: <class 'bool'>\n",
      "  Default: True\n",
      "  Description: Whether to ensure reproducibility\n",
      "\n",
      "global_seed:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Global random seed\n"
     ]
    }
   ],
   "source": [
    "DefaultConfig().print_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = DefaultConfig()\n",
    "config.n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model architecture layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "varix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_FabricModule(\n",
       "  (_forward_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (_original_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varix._trainer._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = DefaultConfig()\n",
    "config.n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n",
      "[100, 50, 25, 12, 2]\n",
      "Epoch: 0, Loss: 0.5027862191200256\n",
      "_FabricModule(\n",
      "  (_forward_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (_logvar): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=50, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=12, bias=True)\n",
      "      (9): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "      (11): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=12, bias=True)\n",
      "      (1): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=12, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=50, bias=True)\n",
      "      (9): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "      (11): ReLU()\n",
      "      (12): Linear(in_features=50, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (_logvar): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=50, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=12, bias=True)\n",
      "      (9): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "      (11): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=12, bias=True)\n",
      "      (1): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=12, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=50, bias=True)\n",
      "      (9): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "      (11): ReLU()\n",
      "      (12): Linear(in_features=50, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ENCODER FACTOR: 1\n",
      "cpu not relevant here\n",
      "[100, 100, 100, 16]\n",
      "Epoch: 0, Loss: 0.5166383385658264\n",
      "_FabricModule(\n",
      "  (_forward_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ENCODER FACTOR: 2\n",
      "cpu not relevant here\n",
      "[100, 50, 25, 16]\n",
      "Epoch: 0, Loss: 0.46474137902259827\n",
      "_FabricModule(\n",
      "  (_forward_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=25, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=25, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=50, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=25, bias=True)\n",
      "      (1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=25, out_features=50, bias=True)\n",
      "      (5): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=50, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=25, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=25, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=50, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=25, bias=True)\n",
      "      (1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=25, out_features=50, bias=True)\n",
      "      (5): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=50, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ENCODER FACTOR: 3\n",
      "cpu not relevant here\n",
      "[100, 33, 16, 16]\n",
      "Epoch: 0, Loss: 0.5135508179664612\n",
      "_FabricModule(\n",
      "  (_forward_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=33, bias=True)\n",
      "      (1): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=33, out_features=16, bias=True)\n",
      "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=16, out_features=33, bias=True)\n",
      "      (5): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=33, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=33, bias=True)\n",
      "      (1): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=33, out_features=16, bias=True)\n",
      "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=16, out_features=33, bias=True)\n",
      "      (5): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=33, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from autoencodix.trainers._general_trainer import GeneralTrainer\n",
    "import autoencodix as acx\n",
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "import numpy as np\n",
    "\n",
    "sample_data = np.random.rand(10, 100)\n",
    "\n",
    "varix = acx.Varix(\n",
    "    data=sample_data,\n",
    "    config=DefaultConfig(n_layers=3, epochs=1, enc_factor=2, latent_dim=2),\n",
    ")\n",
    "varix.preprocess()\n",
    "varix.fit()\n",
    "print(varix._trainer._model)\n",
    "for enc_factor in [1, 2, 3]:\n",
    "    print(f\"ENCODER FACTOR: {enc_factor}\")\n",
    "    varix = acx.Varix(\n",
    "        data=sample_data,\n",
    "        config=DefaultConfig(n_layers=2, epochs=1, enc_factor=enc_factor),\n",
    "    )\n",
    "    varix.preprocess()\n",
    "    varix.fit()\n",
    "    print(varix._trainer._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(varix._trainer._model._decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "empty = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5317338109016418\n",
      "_FabricModule(\n",
      "  (_forward_module): VanillixArchitecture(\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=16, bias=True)\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VanillixArchitecture(\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=16, bias=True)\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "van = acx.Vanillix(data=sample_data, config=DefaultConfig(n_layers=0, epochs=1))\n",
    "van.preprocess()\n",
    "van.fit()\n",
    "print(van._trainer._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
