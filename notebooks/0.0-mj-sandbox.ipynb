{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maximilianjoas/development/autoencodix_package/notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 400)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Generate completely random data\n",
    "def generate_random_data(n_samples=1000, n_features=400):\n",
    "    \"\"\"\n",
    "    Generate random float data for testing neural networks\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int, optional (default=1000)\n",
    "        Number of samples in the dataset\n",
    "    n_features : int, optional (default=400)\n",
    "        Number of features per sample\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Random dataset of shape (n_samples, n_features)\n",
    "    \"\"\"\n",
    "    return np.random.rand(n_samples, n_features).astype(np.float32)\n",
    "data = generate_random_data()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianjoas/development/autoencodix_package/src/autoencodix/data/_numeric_dataset.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.data = torch.tensor(data, dtype=torch.float32)\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args and kwargs: (), {}\n",
      "No user config found\n",
      "default config: DefaultConfig(learning_rate=0.001, batch_size=32, epochs=100, latent_dim=16, n_layers=3, enc_factor=4, input_dim=10000, drop_p=0.1, weight_decay=0.01, use_gpu=False, n_devices=1, float_precision='bf16-mixed', gpu_strategy='auto', checkpoint_interval=10, reconstruction_loss='mse', default_vae_loss='kl', reproducible=True, global_seed=1)\n",
      "Method config after updates: DefaultConfig(learning_rate=0.001, batch_size=32, epochs=100, latent_dim=16, n_layers=3, enc_factor=4, input_dim=10000, drop_p=0.1, weight_decay=0.01, use_gpu=False, n_devices=1, float_precision='bf16-mixed', gpu_strategy='auto', checkpoint_interval=10, reconstruction_loss='mse', default_vae_loss='kl', reproducible=True, global_seed=1)\n",
      "args and kwargs: (), {}\n",
      "No user config found\n",
      "default config: DefaultConfig(learning_rate=0.001, batch_size=32, epochs=100, latent_dim=16, n_layers=3, enc_factor=4, input_dim=10000, drop_p=0.1, weight_decay=0.01, use_gpu=False, n_devices=1, float_precision='bf16-mixed', gpu_strategy='auto', checkpoint_interval=10, reconstruction_loss='mse', default_vae_loss='kl', reproducible=True, global_seed=1)\n",
      "Method config after updates: DefaultConfig(learning_rate=0.001, batch_size=32, epochs=100, latent_dim=16, n_layers=3, enc_factor=4, input_dim=10000, drop_p=0.1, weight_decay=0.01, use_gpu=False, n_devices=1, float_precision='bf16-mixed', gpu_strategy='auto', checkpoint_interval=10, reconstruction_loss='mse', default_vae_loss='kl', reproducible=True, global_seed=1)\n",
      "input_dim 400\n",
      "model: _FabricModule(\n",
      "  (_forward_module): VanillixArchitecture(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=400, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=16, bias=True)\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=25, bias=True)\n",
      "      (1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=25, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=100, out_features=400, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VanillixArchitecture(\n",
      "    (encoder): Sequential(\n",
      "      (0): Linear(in_features=400, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=16, bias=True)\n",
      "    )\n",
      "    (decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=25, bias=True)\n",
      "      (1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=25, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=100, out_features=400, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([32, 400])\n",
      "features shape: torch.Size([28, 400])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected value to be of type numpy.ndarray, got <class 'float'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m van\u001b[38;5;241m.\u001b[39mpreprocess()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# job of old make model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# calls self.Trainer class to init and train model\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# populates self._model attribute with trained model\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# populates self.result attribute with training results (model, losses, etc)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mvan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# job of old make predict\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# if no data is passed, used the test split from preprocessing\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# otherwise, uses the data passed, and preprocesses it\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# updates self.result attribute with predictions (latent space, reconstructions, etc)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m van\u001b[38;5;241m.\u001b[39mpredict()\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/utils/_utils.py:53\u001b[0m, in \u001b[0;36mconfig_method.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod config after updates: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Forward the updated config to the wrapped function\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_pipeline.py:204\u001b[0m, in \u001b[0;36mBasePipeline.fit\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasets not built. Please run the preprocess method first.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m     )\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_class(\n\u001b[1;32m    198\u001b[0m     trainset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets\u001b[38;5;241m.\u001b[39mtrain,\n\u001b[1;32m    199\u001b[0m     validset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets\u001b[38;5;241m.\u001b[39mvalid,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m     called_from\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id,\n\u001b[1;32m    203\u001b[0m )\n\u001b[0;32m--> 204\u001b[0m trainer_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mupdate(trainer_result)\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/trainers/simple_trainer.py:51\u001b[0m, in \u001b[0;36mSimpleTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# capture epoch loss  --------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_result\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch_loss\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# valid loop -----------------------------------------------------------\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validset:\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/utils/_traindynamics.py:30\u001b[0m, in \u001b[0;36mTrainingDynamics.add\u001b[0;34m(self, epoch, data, split)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mAdd a numpy array for a specific epoch and split.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    The data split (default: 'train').\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected value to be of type numpy.ndarray, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[epoch] \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected value to be of type numpy.ndarray, got <class 'float'>."
     ]
    }
   ],
   "source": [
    "# import module\n",
    "import autoencodix as acx\n",
    "# Use Vanillix Pipeline interface\n",
    "# needs to be initialized with data\n",
    "# data should be a numpy array, pandas dataframe or AnnData object\n",
    "# possible to pass a custom Config object\n",
    "van = acx.Vanillix(data=data)\n",
    "# job of old make data\n",
    "# populates self._features attrbute with torch tensor\n",
    "# populates self._datasets attribute with torch dataset\n",
    "# (important for training with dataloader)\n",
    "# possible to pass a custom Config object, or keyword arguments\n",
    "van.preprocess()\n",
    "# job of old make model\n",
    "# calls self.Trainer class to init and train model\n",
    "# populates self._model attribute with trained model\n",
    "# populates self.result attribute with training results (model, losses, etc)\n",
    "van.fit()\n",
    "# job of old make predict\n",
    "# if no data is passed, used the test split from preprocessing\n",
    "# otherwise, uses the data passed, and preprocesses it\n",
    "# updates self.result attribute with predictions (latent space, reconstructions, etc)\n",
    "van.predict()\n",
    "# job of old make ml_task\n",
    "# populates self.result attribute with ml task results\n",
    "van.evaluate() # not implemented yet\n",
    "# job of old make visualize\n",
    "# populates self.result attribute with visualizations\n",
    "van.visualize()\n",
    "# show visualizations for notebook use\n",
    "van.show_result()\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# run all steps in the pipeline\n",
    "result_object = van.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'autoencodix.data._numeric_dataset.NumericDataset'>\n",
      "<class 'torch.Tensor'>\n",
      "shape: torch.Size([700, 400])\n"
     ]
    }
   ],
   "source": [
    "traindata = van._datasets.train\n",
    "print(type(traindata))\n",
    "print(type(traindata.data))\n",
    "print(f\"shape: {traindata.data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainnloader = van._trainer._trainloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([32, 400])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(trainnloader):\n",
    "    print(i, data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Usage Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autoencodix.src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import module\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautoencodix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01macx\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mautoencodix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdefault_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DefaultConfig\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Use Vanillix Pipeline interface\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# needs to be initialized with data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# data should be a numpy array, pandas dataframe or AnnData object\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# possible to pass a custom Config object\u001b[39;00m\n\u001b[1;32m      8\u001b[0m van \u001b[38;5;241m=\u001b[39m acx\u001b[38;5;241m.\u001b[39mVanillix(data\u001b[38;5;241m=\u001b[39msample_data)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autoencodix.src'"
     ]
    }
   ],
   "source": [
    "\n",
    "# import module\n",
    "import autoencodix as acx\n",
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "# Use Vanillix Pipeline interface\n",
    "# needs to be initialized with data\n",
    "# data should be a numpy array, pandas dataframe or AnnData object\n",
    "# possible to pass a custom Config object\n",
    "van = acx.Vanillix(data=sample_data)\n",
    "# job of old make data\n",
    "# populates self._features attrbute with torch tensor\n",
    "# populates self._datasets attribute with torch dataset\n",
    "# (important for training with dataloader)\n",
    "# possible to pass a custom Config object, or keyword arguments\n",
    "van.preprocess()\n",
    "# job of old make model\n",
    "# calls self.Trainer class to init and train model\n",
    "# populates self._model attribute with trained model\n",
    "# populates self.result attribute with training results (losses, etc)\n",
    "van.fit()\n",
    "\"\"\" \n",
    "Each step can be run separately, with custom parameters, these parameters\n",
    "can be passed as keyword arguments, or as a Config object\n",
    "\"\"\"\n",
    "van.fit(learning_rate=0.01, batch_size=32, epochs=100) # or like this:\n",
    "my_config = DefaultConfig(learning_rate=130.0, batch_size=32, epochs=100)\n",
    "van.fit(config=my_config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric is instance of BaseDataset\n"
     ]
    }
   ],
   "source": [
    "from autoencodix.data._numeric_dataset import NumericDataset\n",
    "from autoencodix.base._base_dataset import BaseDataset\n",
    "numeric = NumericDataset(sample_data)\n",
    "base = BaseDataset(sample_data)\n",
    "\n",
    "if isinstance(numeric, BaseDataset):\n",
    "    print('numeric is instance of BaseDataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lightning_fabric import Fabric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightning_fabric.accelerators.mps.MPSAccelerator at 0x163418f70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fabric = Fabric(accelerator=\"auto\")\n",
    "fabric.accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation Config class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
