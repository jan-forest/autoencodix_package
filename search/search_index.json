{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Documentation","text":"<p>You can find our code documentation in our <code>API Reference</code> (see navbar on the right). For a general overview, refer to our <code>README</code> below.</p>"},{"location":"#readme","title":"README","text":"<p>Autoencoders are deep-learning-based networks for dimension reduction and embedding by a combination of a compressing encoder and decoder structure for non-linear and multi-modal data integration, with promising applications to complex biological data from large-scale omics measurements. Current ongoing research and publications provide many exciting architectures and implementations of autoencoders. However, there is a lack of easy-to-use and unified implementations covering the whole pipeline of autoencoder applications. Consequently, we present <code>AUTOENCODIX</code> with the following features:</p> <ol> <li>Multi-modal data integration for any numerical or categorical data  </li> <li>Different autoencoder architectures:  </li> <li>vanilla <code>vanillix</code> </li> <li>variational <code>varix</code></li> <li>disentangled variational <code>disentanglix</code> </li> <li>hierarchical/stacked <code>stackix</code> </li> <li>ontology-based <code>ontix</code> </li> <li>masking <code>maskix</code> </li> <li>Image VAE (2D) <code>imagix</code> </li> <li>cross-modal autoencoder (translation between different data modalities) <code>x-modalix</code> (works for multiple modalities paired and unpaired)  </li> <li>A Python package with a scikit-learn-like interface </li> </ol>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python&gt;=3.8 &lt;3.13</li> <li>uv or another package manager (we recommend uv)</li> <li>git or gh</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ul> <li><code>gh repo clone jan-forest/autoencodix_package</code></li> <li><code>cd autoencodix_package</code></li> <li><code>uv venv --python 3.10</code></li> <li><code>source .venv/bin/activate</code></li> <li><code>uv sync</code></li> </ul>"},{"location":"#sample-usage","title":"Sample Usage","text":"<pre><code>import autoencodix as acx\nfrom autoencodix.data.datapackage import DataPackage\nfrom autoencodix.configs.vanillix_config import VanillixConfig\nfrom autoencodix.configs.default_config import DataCase\n\n# If your data is stored in pandas DataFrames, you can easily pass them to our custom DataPackage.\n# For any tabular data that is not single-cell, provide it as a dictionary to the \"multi_bulk\" attribute of DataPackage.\n# Note: \"multi\" might be misleading \u2014 it's valid to provide just one modality (1\u2013n data modalities).\n# Here, we assume paired metadata. If you have separate metadata for each modality, use the same dict keys as in multi_bulk, e.g.:\n# annotation = {\"rna\": rna_annotation, \"protein\": protein_annotation}\nmy_datapackage: DataPackage = DataPackage(\n    multi_bulk={\"rna\": raw_rna, \"protein\": raw_protein},\n    annotation={\"paired\": annotation},\n)\n\nmyconfig: VanillixConfig = VanillixConfig(data_case=DataCase.MULTI_BULK, epochs=30, device=\"cpu\")\nvanillix = acx.Vanillix(data=my_datapackage, config=myconfig)\nresult = vanillix.run()\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>We provide extensive tutorials for all of our use cases. The best place to start is the <code>Vanillix Tutorial</code>. Here, we explain the design and features of our pipeline, which applies to other pipelines. From there, you can explore the tutorials for the more specialized architectures (<code>Varix</code>, <code>Ontix</code>, etc). We also provide tutorials for each pipeline, but the <code>Vanillix Tutorial</code> explains the general concepts, while the other tutorials go into the specifics of the corresponding pipeline. For even more details on extra functionality, such as visualizing or customizing we provide deep dive tutorials for these topics. You can find the tutorials here:</p> <ul> <li>Best to get started: Vanillix Tutorial</li> <li>Tutorials for each specific pipeline (more advanced, do Vanillix first): Pipeline Tutorials</li> <li>Tutorials for specific extra functionality, see Deep Dives</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>Whether you have a feature request, found a bug, or have any other idea, we're always happy. For more details, refer to our Guide</p>"},{"location":"#read-the-docs","title":"Read The Docs","text":"<p>You can find our documentation here.</p>"},{"location":"#cite","title":"Cite","text":"<p>TODO</p>"},{"location":"#license","title":"License","text":"<p>Copyright [2024][Maximilian Josef Joas &amp; Jan Ewald, ScaDS.AI, Leipzig University]</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#tutorials","title":"Tutorials","text":"<p>Comming Soon!</p>"},{"location":"api/base/","title":"Base Module","text":""},{"location":"api/base/#autoencodix.base.BaseAutoencoder","title":"<code>BaseAutoencoder</code>","text":"<p>               Bases: <code>ABC</code>, <code>Module</code></p> <p>Interface for building autoencoder models.</p> <p>Defines standard methods for encoding data to a latent space and decoding back to the original space. Includes a weight initialization method for stable training. Intended to be extended by specific autoencoder variants like VAE.</p> <p>Attributes:</p> Name Type Description <code>input_dim</code> <p>Number of input features.</p> <code>config</code> <p>Configuration object containing model architecture parameters.</p> <code>_encoder</code> <code>Optional[Module]</code> <p>Encoder network.</p> <code>_decoder</code> <code>Optional[Module]</code> <p>Decoder network.</p> <code>ontologies</code> <p>Ontology information, if provided for Ontix</p> <code>feature_order</code> <p>For Ontix</p> Source code in <code>src/autoencodix/base/_base_autoencoder.py</code> <pre><code>class BaseAutoencoder(ABC, nn.Module):\n    \"\"\"Interface for building autoencoder models.\n\n    Defines standard methods for encoding data to a latent space and decoding\n    back to the original space. Includes a weight initialization method for\n    stable training. Intended to be extended by specific autoencoder variants\n    like VAE.\n\n    Attributes:\n        input_dim: Number of input features.\n        config: Configuration object containing model architecture parameters.\n        _encoder: Encoder network.\n        _decoder: Decoder network.\n        ontologies: Ontology information, if provided for Ontix\n        feature_order: For Ontix\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[DefaultConfig],\n        input_dim: Union[int, Tuple[int, ...]],\n        ontologies: Optional[Union[Tuple, Dict]] = None,\n        feature_order: Optional[Union[Tuple, Dict]] = None,\n    ):\n        \"\"\"Initializes the BaseAutoencoder.\n\n        Args:\n            config: Configuration object containing model parameters.\n                If None, a default configuration will be used.\n            input_dim: Number of input features.\n            ontologies: Ontology information, if provided for Ontix\n            feature_order: For Ontix\n        \"\"\"\n        super().__init__()\n        if config is None:\n            config = DefaultConfig()\n        self.input_dim = input_dim\n        self._encoder: Optional[nn.Module] = None\n        self._decoder: Optional[nn.Module] = None\n        self.config = config\n        self.ontologies = ontologies\n        self.feature_order = feature_order\n        self.init_args = dict(\n            config=config,\n            input_dim=input_dim,\n            ontologies=ontologies,\n            feature_order=feature_order,\n        )\n\n    @abstractmethod\n    def _build_network(self) -&gt; None:\n        \"\"\"Builds the encoder and decoder networks for the autoencoder model.\n\n        Populates the self._encoder and self._decoder attributes.\n        This method should be implemented by subclasses to define\n        the architecture of the encoder and decoder networks.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def encode(\n        self, x: torch.Tensor\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Encodes the input into the latent space.\n\n        Args:\n            x: The input tensor to be encoded.\n\n        Returns:\n            The encoded latent space representation, or mu and logvar for VAEs.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Returns the latent space representation of the input.\n\n        Method for unification of getting a latent space between Variational\n        and Vanilla Autoencoders. This method is a wrapper around the encode\n        method, or the reparameterization method for VAE.\n\n        Args:\n            x: The input tensor to be encoded.\n\n        Returns:\n            The latent space representation of the input tensor.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decodes the latent representation back to the input space.\n\n        Args:\n            x: The latent tensor to be decoded.\n\n        Returns:\n            The decoded tensor, reconstructed from the latent space.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n        \"\"\"Combines encoding and decoding steps for the autoencoder.\n\n        Args:\n            x: The input tensor to be processed.\n\n        Returns:\n            The reconstructed input tensor and any additional information,\n            depending on the model type.\n        \"\"\"\n        pass\n\n    def _init_weights(self, m):\n        \"\"\"Initializes weights using Xavier uniform initialization.\n\n        This weight initialization method helps maintain the variance of\n        activations across layers, preventing gradients from vanishing or\n        exploding during training. This approach ensures stable and efficient\n        training of the autoencoder model.\n\n        Args:\n            m: The module to initialize.\n        \"\"\"\n        if isinstance(m, nn.Linear):\n            torch.nn.init.xavier_uniform_(m.weight)\n            m.bias.data.fill_(0.01)\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseAutoencoder.__init__","title":"<code>__init__(config, input_dim, ontologies=None, feature_order=None)</code>","text":"<p>Initializes the BaseAutoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[DefaultConfig]</code> <p>Configuration object containing model parameters. If None, a default configuration will be used.</p> required <code>input_dim</code> <code>Union[int, Tuple[int, ...]]</code> <p>Number of input features.</p> required <code>ontologies</code> <code>Optional[Union[Tuple, Dict]]</code> <p>Ontology information, if provided for Ontix</p> <code>None</code> <code>feature_order</code> <code>Optional[Union[Tuple, Dict]]</code> <p>For Ontix</p> <code>None</code> Source code in <code>src/autoencodix/base/_base_autoencoder.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[DefaultConfig],\n    input_dim: Union[int, Tuple[int, ...]],\n    ontologies: Optional[Union[Tuple, Dict]] = None,\n    feature_order: Optional[Union[Tuple, Dict]] = None,\n):\n    \"\"\"Initializes the BaseAutoencoder.\n\n    Args:\n        config: Configuration object containing model parameters.\n            If None, a default configuration will be used.\n        input_dim: Number of input features.\n        ontologies: Ontology information, if provided for Ontix\n        feature_order: For Ontix\n    \"\"\"\n    super().__init__()\n    if config is None:\n        config = DefaultConfig()\n    self.input_dim = input_dim\n    self._encoder: Optional[nn.Module] = None\n    self._decoder: Optional[nn.Module] = None\n    self.config = config\n    self.ontologies = ontologies\n    self.feature_order = feature_order\n    self.init_args = dict(\n        config=config,\n        input_dim=input_dim,\n        ontologies=ontologies,\n        feature_order=feature_order,\n    )\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseAutoencoder.decode","title":"<code>decode(x)</code>  <code>abstractmethod</code>","text":"<p>Decodes the latent representation back to the input space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The latent tensor to be decoded.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The decoded tensor, reconstructed from the latent space.</p> Source code in <code>src/autoencodix/base/_base_autoencoder.py</code> <pre><code>@abstractmethod\ndef decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decodes the latent representation back to the input space.\n\n    Args:\n        x: The latent tensor to be decoded.\n\n    Returns:\n        The decoded tensor, reconstructed from the latent space.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseAutoencoder.encode","title":"<code>encode(x)</code>  <code>abstractmethod</code>","text":"<p>Encodes the input into the latent space.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to be encoded.</p> required <p>Returns:</p> Type Description <code>Union[Tensor, Tuple[Tensor, Tensor]]</code> <p>The encoded latent space representation, or mu and logvar for VAEs.</p> Source code in <code>src/autoencodix/base/_base_autoencoder.py</code> <pre><code>@abstractmethod\ndef encode(\n    self, x: torch.Tensor\n) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"Encodes the input into the latent space.\n\n    Args:\n        x: The input tensor to be encoded.\n\n    Returns:\n        The encoded latent space representation, or mu and logvar for VAEs.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseAutoencoder.forward","title":"<code>forward(x)</code>  <code>abstractmethod</code>","text":"<p>Combines encoding and decoding steps for the autoencoder.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to be processed.</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>The reconstructed input tensor and any additional information,</p> <code>ModelOutput</code> <p>depending on the model type.</p> Source code in <code>src/autoencodix/base/_base_autoencoder.py</code> <pre><code>@abstractmethod\ndef forward(self, x: torch.Tensor) -&gt; ModelOutput:\n    \"\"\"Combines encoding and decoding steps for the autoencoder.\n\n    Args:\n        x: The input tensor to be processed.\n\n    Returns:\n        The reconstructed input tensor and any additional information,\n        depending on the model type.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseAutoencoder.get_latent_space","title":"<code>get_latent_space(x)</code>  <code>abstractmethod</code>","text":"<p>Returns the latent space representation of the input.</p> <p>Method for unification of getting a latent space between Variational and Vanilla Autoencoders. This method is a wrapper around the encode method, or the reparameterization method for VAE.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>The input tensor to be encoded.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The latent space representation of the input tensor.</p> Source code in <code>src/autoencodix/base/_base_autoencoder.py</code> <pre><code>@abstractmethod\ndef get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Returns the latent space representation of the input.\n\n    Method for unification of getting a latent space between Variational\n    and Vanilla Autoencoders. This method is a wrapper around the encode\n    method, or the reparameterization method for VAE.\n\n    Args:\n        x: The input tensor to be encoded.\n\n    Returns:\n        The latent space representation of the input tensor.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseDataset","title":"<code>BaseDataset</code>","text":"<p>               Bases: <code>ABC</code>, <code>Dataset</code></p> <p>Interface to guide implementation for custom PyTorch datasets.</p> <p>Attributes:</p> Name Type Description <code>data</code> <p>The dataset content (can be a torch.Tensor or other data structure).</p> <code>config</code> <p>Optional configuration object.</p> <code>sample_ids</code> <p>Optional list of identifiers for each sample.</p> <code>feature_ids</code> <p>Optional list of identifiers for each feature.</p> <code>mytype</code> <code>Enum</code> <p>Enum indicating the dataset type (should be set in subclasses).</p> Source code in <code>src/autoencodix/base/_base_dataset.py</code> <pre><code>class BaseDataset(abc.ABC, Dataset):\n    \"\"\"Interface to guide implementation for custom PyTorch datasets.\n\n    Attributes:\n        data: The dataset content (can be a torch.Tensor or other data structure).\n        config: Optional configuration object.\n        sample_ids: Optional list of identifiers for each sample.\n        feature_ids: Optional list of identifiers for each feature.\n        mytype: Enum indicating the dataset type (should be set in subclasses).\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Union[torch.Tensor, List[ImgData], sp.sparse.spmatrix],\n        config: Optional[Any] = None,\n        sample_ids: Optional[List[Any]] = None,\n        feature_ids: Optional[List[Any]] = None,\n    ):\n        \"\"\"Initializes the dataset.\n\n        Args:\n            data: The data to be used by the dataset.\n            config: Optional configuration parameters.\n            sample_ids: Optional identifiers for each sample.\n            feature_ids: Optional identifiers for each feature.\n            mytype: Enum indicating the dataset type (should be set in subclasses).\n        \"\"\"\n        self.data = data\n        self.raw_data = data  # for child class ImageDataset\n        self.config = config\n        self.sample_ids = sample_ids\n        self.feature_ids = feature_ids\n        self.mytype: Enum  # Should be set in subclasses to indicate the dataset type (e.g., DataSetTypes.NUM or DataSetTypes.IMG)\n\n        self.metadata: Optional[Union[pd.Series, pd.DataFrame]] = (None,)\n        self.datasets: Dict[str, BaseDataset] = {}  # for xmodalix child\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of samples in the dataset.\n\n        Returns:\n            The number of samples in the dataset.\n        \"\"\"\n        if isinstance(self.data, list):\n            return len(self.data)\n        else:\n            return self.data.shape[0]\n\n    def get_input_dim(self) -&gt; Union[int, Tuple[int, ...]]:\n        \"\"\"Gets the input dimension of the dataset (n_features)\n\n        Returns:\n            The input dimension of the dataset's feature space.\n        \"\"\"\n        if isinstance(self.data, (torch.Tensor, sp.sparse.spmatrix)):\n            return self.data.shape[1]\n\n        elif isinstance(self.data, list):\n            if len(self.data) == 0:\n                raise ValueError(\n                    \"Dataset is ImgData, and the list of ImgData is empty, cannot determine input dimension.\"\n                )\n            if isinstance(self.data[0], ImgData):\n                return self.data[0].img.shape[0]\n            else:\n                raise ValueError(\n                    \"List data is not of type ImgData, cannot determine input dimension.\"\n                )\n        else:\n            raise ValueError(\"Unsupported data type for input dimension retrieval.\")\n\n    def _to_df(self, modality: Optional[str] = None) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert the dataset to a pandas DataFrame.\n\n        Returns:\n            DataFrame representation of the dataset\n        \"\"\"\n        if isinstance(self.data, torch.Tensor):\n            return pd.DataFrame(\n                self.data.numpy(), columns=self.feature_ids, index=self.sample_ids\n            )\n        else:\n            raise TypeError(\n                \"Data is not a torch.Tensor and cannot be converted to DataFrame.\"\n            )\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseDataset.__init__","title":"<code>__init__(data, config=None, sample_ids=None, feature_ids=None)</code>","text":"<p>Initializes the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Tensor, List[ImgData], spmatrix]</code> <p>The data to be used by the dataset.</p> required <code>config</code> <code>Optional[Any]</code> <p>Optional configuration parameters.</p> <code>None</code> <code>sample_ids</code> <code>Optional[List[Any]]</code> <p>Optional identifiers for each sample.</p> <code>None</code> <code>feature_ids</code> <code>Optional[List[Any]]</code> <p>Optional identifiers for each feature.</p> <code>None</code> <code>mytype</code> <p>Enum indicating the dataset type (should be set in subclasses).</p> required Source code in <code>src/autoencodix/base/_base_dataset.py</code> <pre><code>def __init__(\n    self,\n    data: Union[torch.Tensor, List[ImgData], sp.sparse.spmatrix],\n    config: Optional[Any] = None,\n    sample_ids: Optional[List[Any]] = None,\n    feature_ids: Optional[List[Any]] = None,\n):\n    \"\"\"Initializes the dataset.\n\n    Args:\n        data: The data to be used by the dataset.\n        config: Optional configuration parameters.\n        sample_ids: Optional identifiers for each sample.\n        feature_ids: Optional identifiers for each feature.\n        mytype: Enum indicating the dataset type (should be set in subclasses).\n    \"\"\"\n    self.data = data\n    self.raw_data = data  # for child class ImageDataset\n    self.config = config\n    self.sample_ids = sample_ids\n    self.feature_ids = feature_ids\n    self.mytype: Enum  # Should be set in subclasses to indicate the dataset type (e.g., DataSetTypes.NUM or DataSetTypes.IMG)\n\n    self.metadata: Optional[Union[pd.Series, pd.DataFrame]] = (None,)\n    self.datasets: Dict[str, BaseDataset] = {}  # for xmodalix child\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of samples in the dataset.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of samples in the dataset.</p> Source code in <code>src/autoencodix/base/_base_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of samples in the dataset.\n\n    Returns:\n        The number of samples in the dataset.\n    \"\"\"\n    if isinstance(self.data, list):\n        return len(self.data)\n    else:\n        return self.data.shape[0]\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseDataset.get_input_dim","title":"<code>get_input_dim()</code>","text":"<p>Gets the input dimension of the dataset (n_features)</p> <p>Returns:</p> Type Description <code>Union[int, Tuple[int, ...]]</code> <p>The input dimension of the dataset's feature space.</p> Source code in <code>src/autoencodix/base/_base_dataset.py</code> <pre><code>def get_input_dim(self) -&gt; Union[int, Tuple[int, ...]]:\n    \"\"\"Gets the input dimension of the dataset (n_features)\n\n    Returns:\n        The input dimension of the dataset's feature space.\n    \"\"\"\n    if isinstance(self.data, (torch.Tensor, sp.sparse.spmatrix)):\n        return self.data.shape[1]\n\n    elif isinstance(self.data, list):\n        if len(self.data) == 0:\n            raise ValueError(\n                \"Dataset is ImgData, and the list of ImgData is empty, cannot determine input dimension.\"\n            )\n        if isinstance(self.data[0], ImgData):\n            return self.data[0].img.shape[0]\n        else:\n            raise ValueError(\n                \"List data is not of type ImgData, cannot determine input dimension.\"\n            )\n    else:\n        raise ValueError(\"Unsupported data type for input dimension retrieval.\")\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseEvaluator","title":"<code>BaseEvaluator</code>","text":"<p>               Bases: <code>ABC</code></p> Source code in <code>src/autoencodix/base/_base_evaluator.py</code> <pre><code>class BaseEvaluator(abc.ABC):\n    @abc.abstractmethod\n    def evaluate(self, *args):\n        \"\"\"\n        Evaluate the Autoencodix pipeline on defined machine learning tasks.\n\n        Subclasses must implement this method to perform evaluation using the provided arguments.\n\n        Args:\n            *args: Variable length argument list for evaluation parameters.\n\n        Returns:\n            Result: The evaluation result.\n        \"\"\"\n        pass\n\n    @staticmethod\n    def _expand_reference_methods(reference_methods: list, result: Result) -&gt; list:\n        \"\"\"\n        Expands the list of reference methods if needed for evaluation.\n\n        Args:\n            reference_methods (list): The list of reference methods to potentially expand.\n            result (Result): The evaluation result object.\n\n        Returns:\n            list: The (possibly expanded) list of reference methods.\n        \"\"\"\n        return reference_methods\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseEvaluator.evaluate","title":"<code>evaluate(*args)</code>  <code>abstractmethod</code>","text":"<p>Evaluate the Autoencodix pipeline on defined machine learning tasks.</p> <p>Subclasses must implement this method to perform evaluation using the provided arguments.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list for evaluation parameters.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Result</code> <p>The evaluation result.</p> Source code in <code>src/autoencodix/base/_base_evaluator.py</code> <pre><code>@abc.abstractmethod\ndef evaluate(self, *args):\n    \"\"\"\n    Evaluate the Autoencodix pipeline on defined machine learning tasks.\n\n    Subclasses must implement this method to perform evaluation using the provided arguments.\n\n    Args:\n        *args: Variable length argument list for evaluation parameters.\n\n    Returns:\n        Result: The evaluation result.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseLoss","title":"<code>BaseLoss</code>","text":"<p>               Bases: <code>Module</code>, <code>ABC</code></p> <p>Provides common loss computation functionality for autoencoders.</p> <p>Implements standard loss calculations including reconstruction loss, KL divergence, and Maximum Mean Discrepancy (MMD), while requiring subclasses to implement the specific forward method.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration parameters for the loss function.</p> <code>recon_loss</code> <code>Module</code> <p>Module for computing reconstruction loss (MSE or BCE).</p> <code>reduction_fn</code> <p>Function to apply reduction (mean or sum).</p> <code>compute_kernel</code> <p>Function to compute kernel for MMD loss.</p> <code>annealing_scheduler</code> <p>Helper for loss calculation with annealing.</p> Source code in <code>src/autoencodix/base/_base_loss.py</code> <pre><code>class BaseLoss(nn.Module, ABC):\n    \"\"\"Provides common loss computation functionality for autoencoders.\n\n    Implements standard loss calculations including reconstruction loss,\n    KL divergence, and Maximum Mean Discrepancy (MMD), while requiring\n    subclasses to implement the specific forward method.\n\n    Attributes:\n        config: Configuration parameters for the loss function.\n        recon_loss: Module for computing reconstruction loss (MSE or BCE).\n        reduction_fn: Function to apply reduction (mean or sum).\n        compute_kernel: Function to compute kernel for MMD loss.\n        annealing_scheduler: Helper for loss calculation with annealing.\n    \"\"\"\n\n    def __init__(self, config: DefaultConfig, annealing_scheduler=None):\n        \"\"\"Initializes the loss module with the specified configuration.\n\n        Args:\n            config: Configuration parameters for the loss function.\n            annealing_scheduler: Helper class for loss calculation with annealing.\n\n        Raises:\n            NotImplementedError: If unsupported loss reduction or reconstruction\n                loss type is specified.\n        \"\"\"\n        super().__init__()\n        self.annealing_scheduler = annealing_scheduler or AnnealingScheduler()\n        self.config = config\n        self.recon_loss: nn.Module\n\n        if self.config.loss_reduction == \"mean\":\n            self.reduction_fn = torch.mean\n        elif self.config.loss_reduction == \"sum\":\n            self.reduction_fn = torch.sum\n        else:\n            raise NotImplementedError(\n                f\"Invalid loss reduction type: {self.config.loss_reduction}. \"\n                f\"Only 'mean' and 'sum' are supported.\"\n            )\n\n        if self.config.reconstruction_loss == \"mse\":\n            self.recon_loss = nn.MSELoss(reduction=config.loss_reduction)\n        elif self.config.reconstruction_loss == \"bce\":\n            self.recon_loss = nn.BCEWithLogitsLoss(reduction=config.loss_reduction)\n        else:\n            raise NotImplementedError(\n                f\"Invalid reconstruction loss type: {self.config.reconstruction_loss}. \"\n                f\"Only 'mse' and 'bce' are supported. Please check the value of \"\n                f\"'config.reconstruction_loss' for typos or unsupported types.\"\n            )\n\n        self.compute_kernel = self._mmd_kernel\n\n    def _mmd_kernel(self, x: torch.Tensor, y: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes Gaussian kernel for Maximum Mean Discrepancy calculation.\n\n        Calculates the kernel matrix between two sets of samples, using a\n        Gaussian kernel with normalization by feature dimension.\n\n        Args:\n            x: First set of input samples.\n            y: Second set of input samples.\n\n        Returns:\n            Kernel matrix of shape (x.shape[0], y.shape[0]).\n        \"\"\"\n        x_size = x.size(0)\n        y_size = y.size(0)\n        dim = x.size(1)\n\n        x = x.unsqueeze(1)\n        y = y.unsqueeze(0)\n        tiled_x = x.expand(x_size, y_size, dim)\n        tiled_y = y.expand(x_size, y_size, dim)\n\n        kernel_input = (tiled_x - tiled_y).pow(2).mean(2) / float(dim)\n        return torch.exp(-kernel_input)\n\n    def compute_mmd_loss(\n        self, z: torch.Tensor, true_samples: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Computes Maximum Mean Discrepancy loss.\n\n        Args:\n            z: Samples from the encoded distribution.\n            true_samples: Samples from the prior distribution.\n\n        Returns:\n            The MMD loss value.\n\n        Raises:\n            NotImplementedError: If unsupported loss reduction type is specified.\n        \"\"\"\n        true_samples_kernel = self.compute_kernel(x=true_samples, y=true_samples)\n        z_device = z.device\n        true_samples = true_samples.to(z_device)\n        z_kernel = self.compute_kernel(z, z)\n        ztr_kernel = self.compute_kernel(x=true_samples, y=z)\n\n        if self.config.loss_reduction == \"mean\":\n            return true_samples_kernel.mean() + z_kernel.mean() - 2 * ztr_kernel.mean()\n        elif self.config.loss_reduction == \"sum\":\n            return true_samples_kernel.sum() + z_kernel.sum() - 2 * ztr_kernel.sum()\n        else:\n            raise NotImplementedError(\n                f\"Invalid loss reduction type: {self.config.loss_reduction}. \"\n                f\"Only 'mean' and 'sum' are supported.\"\n            )\n\n    def compute_kl_loss(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Computes KL divergence loss between N(mu, logvar) and N(0, 1).\n\n        Args:\n            mu: Mean tensor.\n            logvar: Log variance tensor.\n\n        Returns:\n            The KL divergence loss value.\n\n        Raises:\n            ValueError: If mu and logvar do not have the same shape.\n        \"\"\"\n        if mu.shape != logvar.shape:\n            raise ValueError(\n                f\"Shape mismatch: mu has shape {mu.shape}, but logvar has shape {logvar.shape}.\"\n            )\n        return -0.5 * self.reduction_fn(1 + logvar - mu.pow(2) - logvar.exp())\n\n    def compute_variational_loss(\n        self,\n        mu: Optional[torch.Tensor],\n        logvar: Optional[torch.Tensor],\n        z: Optional[torch.Tensor] = None,\n        true_samples: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Computes either KL or MMD loss based on configuration.\n\n        Args:\n            mu: Mean tensor for variational loss.\n            logvar: Log variance tensor for variational loss.\n            z: Encoded samples for MMD loss.\n            true_samples: Prior samples for MMD loss.\n\n        Returns:\n            The computed variational loss.\n\n        Raises:\n            ValueError: If required parameters are missing or if mu and logvar have shape mismatch.\n            NotImplementedError: If unsupported VAE loss type is specified.\n        \"\"\"\n\n        if self.config.default_vae_loss == \"kl\":\n            if mu is None:\n                raise ValueError(\"mu must be provided for VAE loss\")\n            if logvar is None:\n                raise ValueError(\"logvar must be provided for VAE loss\")\n            if mu.shape != logvar.shape:\n                raise ValueError(\n                    f\"Shape mismatch: mu has shape {mu.shape}, but logvar has shape {logvar.shape}\"\n                )\n\n            return self.compute_kl_loss(mu=mu, logvar=logvar)\n\n        elif self.config.default_vae_loss == \"mmd\":\n            if z is None:\n                raise ValueError(\"z must be provided for MMD loss\")\n            if true_samples is None:\n                raise ValueError(\"true_samples must be provided for MMD loss\")\n            return self.compute_mmd_loss(z=z, true_samples=true_samples)\n        else:\n            raise NotImplementedError(\n                f\"VAE loss type {self.config.default_vae_loss} is not implemented. \"\n                f\"Only 'kl' and 'mmd' are supported.\"\n            )\n\n    def compute_paired_loss(\n        self,\n        latentspaces: dict[str, torch.Tensor],\n        sample_ids: dict[str, list],\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Calculates the paired distance loss across all pairs of modalities in a batch.\n\n        Args:\n            latentspaces: A dictionary mapping modality names to their latent space tensors.\n                        e.g., {'RNA': tensor_rna, 'ATAC': tensor_atac}\n            sample_ids: A dictionary mapping modality names to their list of sample IDs.\n\n        Returns:\n            A single scalar tensor representing the total paired loss.\n        \"\"\"\n\n        loss_helper = []\n        modality_names = list(latentspaces.keys())\n\n        # 1. Iterate through all unique pairs of modalities\n        for mod_a, mod_b in itertools.combinations(modality_names, 2):\n            ids_a = sample_ids[mod_a]\n            ids_b = sample_ids[mod_b]\n\n            # 2. Find the intersection of sample IDs\n            common_ids = set(ids_a) &amp; set(ids_b)\n\n            if not common_ids:\n                print(\"no common ids\")\n                continue\n\n            # 3. Create a mapping from sample ID to index for efficient lookup\n            id_to_idx_a = {sample_id: i for i, sample_id in enumerate(ids_a)}\n            id_to_idx_b = {sample_id: i for i, sample_id in enumerate(ids_b)}\n\n            # Get the corresponding indices for the common samples\n            indices_a = [id_to_idx_a[common_id] for common_id in common_ids]\n            indices_b = [id_to_idx_b[common_id] for common_id in common_ids]\n\n            # 4. Select the latent vectors for the paired samples\n            paired_latents_a = latentspaces[mod_a][indices_a]\n            paired_latents_b = latentspaces[mod_b][indices_b]\n\n            # 5. Calculate the distance between the aligned latent vectors\n            # L1 distance, averaged over latent dimensions and then over samples\n            distance = torch.abs(paired_latents_a - paired_latents_b).mean(dim=1)\n            pair_loss = self.reduction_fn(distance)\n            loss_helper.append(pair_loss)\n        if not loss_helper:\n            return torch.tensor(0.0)\n        return torch.stack(loss_helper).mean()\n\n    @staticmethod\n    def _compute_log_gauss_dense(\n        z: torch.Tensor, mu: torch.Tensor, logvar: torch.Tensor\n    ) -&gt; torch.Tensor:\n        \"\"\"Computes the log probability of a Gaussian distribution.\n\n        Args:\n            z: Latent variable tensor.\n            mu: Mean tensor.\n            logvar: Log variance tensor.\n\n        Returns:\n            Log probability of the Gaussian distribution.\n        \"\"\"\n        return -0.5 * (\n            torch.log(torch.tensor([2 * torch.pi]).to(z.device))\n            + logvar\n            + (z - mu) ** 2 * torch.exp(-logvar)\n        )\n\n    @staticmethod\n    def _compute_log_import_weight_mat(batch_size: int, n_samples: int) -&gt; torch.Tensor:\n        \"\"\"Computes the log import weight matrix for disentangled loss.\n           Similar to: https://github.com/rtqichen/beta-tcvae\n        Args:\n            batch_size: Number of samples in the batch.\n            n_samples: Total number of samples in the dataset.\n\n        Returns:\n            Log import weight matrix of shape (batch_size, n_samples).\n        \"\"\"\n\n        N = n_samples\n        M = batch_size - 1\n        strat_weight = (N - M) / (N * M)\n        W = torch.Tensor(batch_size, batch_size).fill_(1 / M)\n        W.view(-1)[:: M + 1] = 1 / N\n        W.view(-1)[1 :: M + 1] = strat_weight\n        W[M - 1, 0] = strat_weight\n        return W.log()\n\n    @abstractmethod\n    def forward(\n        self,\n        *args,\n        **kwargs,\n    ) -&gt; Any:\n        \"\"\"Calculates the loss for the autoencoder.\n\n        This method must be implemented by subclasses to define the specific\n        loss computation logic for the autoencoder. The implementation should\n        compute the total loss as well as any individual loss components\n        (e.g., reconstruction loss, KL divergence, etc.) based on the model's\n        output and the provided targets.\n\n        Args:\n            *kwargs depending on the loss type and pipeline\n\n\n        Returns:\n            - The total loss value as a scalar tensor.\n            - A dictionary of individual loss components, where the keys are\n                descriptive strings (e.g., \"reconstruction_loss\", \"kl_loss\") and\n                the values are the corresponding loss tensors.\n            - Implementation in subclasses is flexible, so for new loss classes this can differ.\n\n        Note:\n            Subclasses must implement this method to define the specific loss\n            computation logic for their use case.\n        \"\"\"\n        # TODO maybe standardize the return types more i.e. request a scalar and a dict\n        pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseLoss.__init__","title":"<code>__init__(config, annealing_scheduler=None)</code>","text":"<p>Initializes the loss module with the specified configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DefaultConfig</code> <p>Configuration parameters for the loss function.</p> required <code>annealing_scheduler</code> <p>Helper class for loss calculation with annealing.</p> <code>None</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If unsupported loss reduction or reconstruction loss type is specified.</p> Source code in <code>src/autoencodix/base/_base_loss.py</code> <pre><code>def __init__(self, config: DefaultConfig, annealing_scheduler=None):\n    \"\"\"Initializes the loss module with the specified configuration.\n\n    Args:\n        config: Configuration parameters for the loss function.\n        annealing_scheduler: Helper class for loss calculation with annealing.\n\n    Raises:\n        NotImplementedError: If unsupported loss reduction or reconstruction\n            loss type is specified.\n    \"\"\"\n    super().__init__()\n    self.annealing_scheduler = annealing_scheduler or AnnealingScheduler()\n    self.config = config\n    self.recon_loss: nn.Module\n\n    if self.config.loss_reduction == \"mean\":\n        self.reduction_fn = torch.mean\n    elif self.config.loss_reduction == \"sum\":\n        self.reduction_fn = torch.sum\n    else:\n        raise NotImplementedError(\n            f\"Invalid loss reduction type: {self.config.loss_reduction}. \"\n            f\"Only 'mean' and 'sum' are supported.\"\n        )\n\n    if self.config.reconstruction_loss == \"mse\":\n        self.recon_loss = nn.MSELoss(reduction=config.loss_reduction)\n    elif self.config.reconstruction_loss == \"bce\":\n        self.recon_loss = nn.BCEWithLogitsLoss(reduction=config.loss_reduction)\n    else:\n        raise NotImplementedError(\n            f\"Invalid reconstruction loss type: {self.config.reconstruction_loss}. \"\n            f\"Only 'mse' and 'bce' are supported. Please check the value of \"\n            f\"'config.reconstruction_loss' for typos or unsupported types.\"\n        )\n\n    self.compute_kernel = self._mmd_kernel\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseLoss.compute_kl_loss","title":"<code>compute_kl_loss(mu, logvar)</code>","text":"<p>Computes KL divergence loss between N(mu, logvar) and N(0, 1).</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>Mean tensor.</p> required <code>logvar</code> <code>Tensor</code> <p>Log variance tensor.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The KL divergence loss value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mu and logvar do not have the same shape.</p> Source code in <code>src/autoencodix/base/_base_loss.py</code> <pre><code>def compute_kl_loss(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Computes KL divergence loss between N(mu, logvar) and N(0, 1).\n\n    Args:\n        mu: Mean tensor.\n        logvar: Log variance tensor.\n\n    Returns:\n        The KL divergence loss value.\n\n    Raises:\n        ValueError: If mu and logvar do not have the same shape.\n    \"\"\"\n    if mu.shape != logvar.shape:\n        raise ValueError(\n            f\"Shape mismatch: mu has shape {mu.shape}, but logvar has shape {logvar.shape}.\"\n        )\n    return -0.5 * self.reduction_fn(1 + logvar - mu.pow(2) - logvar.exp())\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseLoss.compute_mmd_loss","title":"<code>compute_mmd_loss(z, true_samples)</code>","text":"<p>Computes Maximum Mean Discrepancy loss.</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Tensor</code> <p>Samples from the encoded distribution.</p> required <code>true_samples</code> <code>Tensor</code> <p>Samples from the prior distribution.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>The MMD loss value.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If unsupported loss reduction type is specified.</p> Source code in <code>src/autoencodix/base/_base_loss.py</code> <pre><code>def compute_mmd_loss(\n    self, z: torch.Tensor, true_samples: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Computes Maximum Mean Discrepancy loss.\n\n    Args:\n        z: Samples from the encoded distribution.\n        true_samples: Samples from the prior distribution.\n\n    Returns:\n        The MMD loss value.\n\n    Raises:\n        NotImplementedError: If unsupported loss reduction type is specified.\n    \"\"\"\n    true_samples_kernel = self.compute_kernel(x=true_samples, y=true_samples)\n    z_device = z.device\n    true_samples = true_samples.to(z_device)\n    z_kernel = self.compute_kernel(z, z)\n    ztr_kernel = self.compute_kernel(x=true_samples, y=z)\n\n    if self.config.loss_reduction == \"mean\":\n        return true_samples_kernel.mean() + z_kernel.mean() - 2 * ztr_kernel.mean()\n    elif self.config.loss_reduction == \"sum\":\n        return true_samples_kernel.sum() + z_kernel.sum() - 2 * ztr_kernel.sum()\n    else:\n        raise NotImplementedError(\n            f\"Invalid loss reduction type: {self.config.loss_reduction}. \"\n            f\"Only 'mean' and 'sum' are supported.\"\n        )\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseLoss.compute_paired_loss","title":"<code>compute_paired_loss(latentspaces, sample_ids)</code>","text":"<p>Calculates the paired distance loss across all pairs of modalities in a batch.</p> <p>Parameters:</p> Name Type Description Default <code>latentspaces</code> <code>dict[str, Tensor]</code> <p>A dictionary mapping modality names to their latent space tensors.         e.g., {'RNA': tensor_rna, 'ATAC': tensor_atac}</p> required <code>sample_ids</code> <code>dict[str, list]</code> <p>A dictionary mapping modality names to their list of sample IDs.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>A single scalar tensor representing the total paired loss.</p> Source code in <code>src/autoencodix/base/_base_loss.py</code> <pre><code>def compute_paired_loss(\n    self,\n    latentspaces: dict[str, torch.Tensor],\n    sample_ids: dict[str, list],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Calculates the paired distance loss across all pairs of modalities in a batch.\n\n    Args:\n        latentspaces: A dictionary mapping modality names to their latent space tensors.\n                    e.g., {'RNA': tensor_rna, 'ATAC': tensor_atac}\n        sample_ids: A dictionary mapping modality names to their list of sample IDs.\n\n    Returns:\n        A single scalar tensor representing the total paired loss.\n    \"\"\"\n\n    loss_helper = []\n    modality_names = list(latentspaces.keys())\n\n    # 1. Iterate through all unique pairs of modalities\n    for mod_a, mod_b in itertools.combinations(modality_names, 2):\n        ids_a = sample_ids[mod_a]\n        ids_b = sample_ids[mod_b]\n\n        # 2. Find the intersection of sample IDs\n        common_ids = set(ids_a) &amp; set(ids_b)\n\n        if not common_ids:\n            print(\"no common ids\")\n            continue\n\n        # 3. Create a mapping from sample ID to index for efficient lookup\n        id_to_idx_a = {sample_id: i for i, sample_id in enumerate(ids_a)}\n        id_to_idx_b = {sample_id: i for i, sample_id in enumerate(ids_b)}\n\n        # Get the corresponding indices for the common samples\n        indices_a = [id_to_idx_a[common_id] for common_id in common_ids]\n        indices_b = [id_to_idx_b[common_id] for common_id in common_ids]\n\n        # 4. Select the latent vectors for the paired samples\n        paired_latents_a = latentspaces[mod_a][indices_a]\n        paired_latents_b = latentspaces[mod_b][indices_b]\n\n        # 5. Calculate the distance between the aligned latent vectors\n        # L1 distance, averaged over latent dimensions and then over samples\n        distance = torch.abs(paired_latents_a - paired_latents_b).mean(dim=1)\n        pair_loss = self.reduction_fn(distance)\n        loss_helper.append(pair_loss)\n    if not loss_helper:\n        return torch.tensor(0.0)\n    return torch.stack(loss_helper).mean()\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseLoss.compute_variational_loss","title":"<code>compute_variational_loss(mu, logvar, z=None, true_samples=None)</code>","text":"<p>Computes either KL or MMD loss based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Optional[Tensor]</code> <p>Mean tensor for variational loss.</p> required <code>logvar</code> <code>Optional[Tensor]</code> <p>Log variance tensor for variational loss.</p> required <code>z</code> <code>Optional[Tensor]</code> <p>Encoded samples for MMD loss.</p> <code>None</code> <code>true_samples</code> <code>Optional[Tensor]</code> <p>Prior samples for MMD loss.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>The computed variational loss.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required parameters are missing or if mu and logvar have shape mismatch.</p> <code>NotImplementedError</code> <p>If unsupported VAE loss type is specified.</p> Source code in <code>src/autoencodix/base/_base_loss.py</code> <pre><code>def compute_variational_loss(\n    self,\n    mu: Optional[torch.Tensor],\n    logvar: Optional[torch.Tensor],\n    z: Optional[torch.Tensor] = None,\n    true_samples: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Computes either KL or MMD loss based on configuration.\n\n    Args:\n        mu: Mean tensor for variational loss.\n        logvar: Log variance tensor for variational loss.\n        z: Encoded samples for MMD loss.\n        true_samples: Prior samples for MMD loss.\n\n    Returns:\n        The computed variational loss.\n\n    Raises:\n        ValueError: If required parameters are missing or if mu and logvar have shape mismatch.\n        NotImplementedError: If unsupported VAE loss type is specified.\n    \"\"\"\n\n    if self.config.default_vae_loss == \"kl\":\n        if mu is None:\n            raise ValueError(\"mu must be provided for VAE loss\")\n        if logvar is None:\n            raise ValueError(\"logvar must be provided for VAE loss\")\n        if mu.shape != logvar.shape:\n            raise ValueError(\n                f\"Shape mismatch: mu has shape {mu.shape}, but logvar has shape {logvar.shape}\"\n            )\n\n        return self.compute_kl_loss(mu=mu, logvar=logvar)\n\n    elif self.config.default_vae_loss == \"mmd\":\n        if z is None:\n            raise ValueError(\"z must be provided for MMD loss\")\n        if true_samples is None:\n            raise ValueError(\"true_samples must be provided for MMD loss\")\n        return self.compute_mmd_loss(z=z, true_samples=true_samples)\n    else:\n        raise NotImplementedError(\n            f\"VAE loss type {self.config.default_vae_loss} is not implemented. \"\n            f\"Only 'kl' and 'mmd' are supported.\"\n        )\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseLoss.forward","title":"<code>forward(*args, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>Calculates the loss for the autoencoder.</p> <p>This method must be implemented by subclasses to define the specific loss computation logic for the autoencoder. The implementation should compute the total loss as well as any individual loss components (e.g., reconstruction loss, KL divergence, etc.) based on the model's output and the provided targets.</p> <p>Returns:</p> Type Description <code>Any</code> <ul> <li>The total loss value as a scalar tensor.</li> </ul> <code>Any</code> <ul> <li>A dictionary of individual loss components, where the keys are descriptive strings (e.g., \"reconstruction_loss\", \"kl_loss\") and the values are the corresponding loss tensors.</li> </ul> <code>Any</code> <ul> <li>Implementation in subclasses is flexible, so for new loss classes this can differ.</li> </ul> Note <p>Subclasses must implement this method to define the specific loss computation logic for their use case.</p> Source code in <code>src/autoencodix/base/_base_loss.py</code> <pre><code>@abstractmethod\ndef forward(\n    self,\n    *args,\n    **kwargs,\n) -&gt; Any:\n    \"\"\"Calculates the loss for the autoencoder.\n\n    This method must be implemented by subclasses to define the specific\n    loss computation logic for the autoencoder. The implementation should\n    compute the total loss as well as any individual loss components\n    (e.g., reconstruction loss, KL divergence, etc.) based on the model's\n    output and the provided targets.\n\n    Args:\n        *kwargs depending on the loss type and pipeline\n\n\n    Returns:\n        - The total loss value as a scalar tensor.\n        - A dictionary of individual loss components, where the keys are\n            descriptive strings (e.g., \"reconstruction_loss\", \"kl_loss\") and\n            the values are the corresponding loss tensors.\n        - Implementation in subclasses is flexible, so for new loss classes this can differ.\n\n    Note:\n        Subclasses must implement this method to define the specific loss\n        computation logic for their use case.\n    \"\"\"\n    # TODO maybe standardize the return types more i.e. request a scalar and a dict\n    pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline","title":"<code>BasePipeline</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Provides a standardized interface for building model pipelines.</p> <p>Implements methods for preprocessing data, training models, making predictions, evaluating performance, and visualizing results. Subclasses customize behavior by providing specific implementations for processing, training, evaluation, and visualization. For example when using the Stackix Model, we would use the StackixPreprocessor Type for preprocessing.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration for the pipeline's components and behavior.</p> <code>preprocessed_data</code> <code>Optional[DatasetContainer]</code> <p>Pre-split and processed data that can be provided by user.</p> <code>raw_user_data</code> <code>Union[DataPackage, AnnData, MuData, DataFrame, dict]</code> <p>Raw input data for processing (DataFrames, MuData, etc.).</p> <code>result</code> <p>Storage container for all pipeline outputs.</p> <code>_preprocessor</code> <p>Component that filters, scales, and cleans data.</p> <code>_visualizer</code> <p>Component that generates visual representations of results.</p> <code>_dataset_type</code> <p>Base class for dataset implementations.</p> <code>_trainer_type</code> <p>Base class for trainer implementations.</p> <code>_model_type</code> <p>Base class for model architecture implementations.</p> <code>_loss_type</code> <p>Base class for loss function implementations.</p> <code>_datasets</code> <code>Optional[DatasetContainer]</code> <p>Split datasets after preprocessing.</p> <code>_evaluator</code> <code>Optional[DatasetContainer]</code> <p>Component that assesses model performance. Not implemented yet</p> <code>_data_splitter</code> <p>Component that divides data into train/validation/test sets.</p> <code>_ontologies</code> <p>Tuple of dictionaries containing the ontologies to be used to construct sparse decoder layers. If a list is provided, it is assumed to be a list of file paths to ontology files. First item in list or tuple will be treated as first layer (after latent space) and so on.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>class BasePipeline(abc.ABC):\n    \"\"\"Provides a standardized interface for building model pipelines.\n\n    Implements methods for preprocessing data, training models, making predictions,\n    evaluating performance, and visualizing results. Subclasses customize behavior\n    by providing specific implementations for processing, training, evaluation,\n    and visualization. For example when using the Stackix Model, we would use\n    the StackixPreprocessor Type for preprocessing.\n\n    Attributes:\n        config: Configuration for the pipeline's components and behavior.\n        preprocessed_data: Pre-split and processed data that can be provided by user.\n        raw_user_data: Raw input data for processing (DataFrames, MuData, etc.).\n        result: Storage container for all pipeline outputs.\n        _preprocessor: Component that filters, scales, and cleans data.\n        _visualizer: Component that generates visual representations of results.\n        _dataset_type: Base class for dataset implementations.\n        _trainer_type: Base class for trainer implementations.\n        _model_type: Base class for model architecture implementations.\n        _loss_type: Base class for loss function implementations.\n        _datasets: Split datasets after preprocessing.\n        _evaluator: Component that assesses model performance. Not implemented yet\n        _data_splitter: Component that divides data into train/validation/test sets.\n        _ontologies: Tuple of dictionaries containing the ontologies to be used to construct sparse decoder layers.\n            If a list is provided, it is assumed to be a list of file paths to ontology files.\n            First item in list or tuple will be treated as first layer (after latent space) and so on.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_type: Type[BaseDataset],\n        trainer_type: Type[BaseTrainer],\n        model_type: Type[BaseAutoencoder],\n        loss_type: Type[BaseLoss],\n        datasplitter_type: Type[DataSplitter],\n        preprocessor_type: Type[BasePreprocessor],\n        data: Optional[\n            Union[DataPackage, DatasetContainer, ad.AnnData, MuData, pd.DataFrame, dict]  # type: ignore[invalid-type-form]\n        ],\n        visualizer: Optional[BaseVisualizer] = None,\n        evaluator: Optional[BaseEvaluator] = None,\n        result: Optional[Result] = None,\n        config: Optional[DefaultConfig] = None,\n        custom_split: Optional[Dict[str, np.ndarray]] = None,\n        ontologies: Optional[Union[Tuple, Dict[Any, Any]]] = None,\n        masking_fn: Optional[Callable] = None,\n        masking_fn_kwargs: Dict[str, Any] = {},\n        **kwargs: dict,\n    ) -&gt; None:  # ty: ignore[call-non-callable]\n        \"\"\"Initializes the pipeline with components and configuration.\n\n        Args:\n            dataset_type: Class for dataset implementations.\n            trainer_type: Class for model training implementations.\n            model_type: Class for model architecture implementations.\n            loss_type: Class for loss function implementations.\n            datasplitter_type: Class for data splitting implementation.\n            preprocessor_type: Class for data preprocessing implementation.\n            visualizer: Component for generating visualizations.\n            data: Input data to be processed or already processed data.\n            evaluator: Component for assessing model performance.\n            result: Storage container for pipeline outputs.\n            config: Configuration parameters for all pipeline components.\n            custom_split: User-provided data splits (train/validation/test).\n            **kwargs: Additional keyword arguments.\n\n        Raises:\n            TypeError: If inputs have incorrect types.\n        \"\"\"\n        if not hasattr(self, \"_default_config\"):\n            raise ValueError(\n                \"\"\"\n                            The _default_config attribute has not been specified in your pipeline class.\n\n                            Example:\n                            self._default_config = XModalixConfig()\n\n                            This error typically occurs when a new architecture is added without setting the\n                            _default_config in its corresponding pipeline class.\n\n                            For more details, please refer to the 'how to add a new architecture' section in our documentation.\n                            \"\"\"\n            )\n\n        self._validate_config(config=config)\n        self._validate_user_input(data=data)\n        self.masking_fn = masking_fn\n        self.masking_fn_kwargs = masking_fn_kwargs\n        processed_data = data if isinstance(data, DatasetContainer) else None\n        raw_user_data = (\n            data\n            if isinstance(data, (DataPackage, ad.AnnData, MuData, pd.DataFrame, dict))\n            else None\n        )\n        if processed_data is not None and not isinstance(\n            processed_data, DatasetContainer\n        ):\n            raise TypeError(\n                f\"Expected data type to be DatasetContainer, got {type(processed_data)}.\"\n            )\n\n        self.preprocessed_data: Optional[DatasetContainer] = processed_data\n        self.raw_user_data: Union[\n            DataPackage, ad.AnnData, MuData, pd.DataFrame, dict  # type: ignore[invalid-type-form]\n        ] = raw_user_data\n        self._trainer_type = trainer_type\n        self._trainer: Optional[BaseTrainer] = None\n        self._model_type = model_type\n        self._loss_type = loss_type\n        self._preprocessor_type = preprocessor_type\n        if self.raw_user_data is not None:\n            self.raw_user_data, datacase = self._handle_direct_user_data(\n                data=self.raw_user_data,\n            )\n            self.config.data_case = datacase\n            self._fill_data_info()\n\n        self.ontologies = ontologies\n        self._preprocessor = self._preprocessor_type(\n            config=self.config, ontologies=self.ontologies\n        )\n\n        self.visualizer = (\n            visualizer()  # ty: ignore[call-non-callable]\n            if visualizer is not None\n            else BaseVisualizer()  # ty: ignore[call-non-callable]\n        )  # ty: ignore[call-non-callable]\n        self.evaluator = (\n            evaluator()  # ty: ignore[call-non-callable]\n            if evaluator is not None\n            else BaseEvaluator()  # ty: ignore[call-non-callable]\n        )  # ty: ignore[call-non-callable]\n        self.result = result if result is not None else Result()\n        self._dataset_type = dataset_type\n        self._data_splitter = datasplitter_type(\n            config=self.config, custom_splits=custom_split\n        )\n\n        self._datasets: Optional[DatasetContainer] = (\n            processed_data  # None, or user input\n        )\n\n    def _validate_config(self, config: Any) -&gt; None:\n        \"\"\"Sets config to default if None, or validates its type.\n        Args:\n            config: Configuration object to validate or set to default.\n        Raises:\n            TypeError: If config is not of type DefaultConfig\n        \"\"\"\n        if config is None:\n            self.config = self._default_config  # type: ignore\n        else:\n            if not isinstance(config, DefaultConfig):\n                raise TypeError(\n                    f\"Expected config type to be DefaultConfig, got {type(config)}.\"\n                )\n            if not isinstance(config, type(self._default_config)):  # type: ignore\n                warnings.warn(\n                    f\"Your config is of type: {type(config)}, for this pipeline the default params of: {type(self._default_config)} work best\"\n                )\n            self.config = config\n\n    def _validate_user_input(self, data: Any) -&gt; None:\n        \"\"\"Ensures that user-provided data is of a valid type.\n        Args:\n            data: User-provided data to validate.\n        Raises:\n            TypeError: If data is not of a supported type.\n        \"\"\"\n        if not isinstance(\n            data,\n            (\n                DataPackage,\n                ad.AnnData,\n                MuData,\n                pd.DataFrame,\n                dict,\n                type(None),\n                DatasetContainer,\n            ),\n        ):\n            raise TypeError(\n                f\"Expected data type to be one of [DataPackage, AnnData, MuData, \"\n                f\"pd.DataFrame, dict, DatasetContainer], got {type(data)}.\"\n            )\n\n    def _handle_direct_user_data(\n        self,\n        data,\n    ) -&gt; Tuple[DataPackage, DataCase]:\n        \"\"\"Converts raw user data into a standardized DataPackage format.\n\n        Args:\n            data: Raw input data in various formats.\n\n        Returns:\n            DataPackage containing the standardized data\n            DataCase, muliti_single_cell or multi_bulk, etc.\n\n        Raises:\n            TypeError: If data format is not supported.\n            ValueError: If data doesn't meet format requirements or data_case\n                cannot be inferred.\n        \"\"\"\n        print(f\"in handle_direct_user_data with data: {type(data)}\")\n        data_case = self.config.data_case\n        if isinstance(data, DataPackage):\n            data_package = data\n            data_case = self.config.data_case\n        elif isinstance(data, ad.AnnData):\n            mudata = MuData({\"user-data\": data})\n            data_package = DataPackage(multi_sc={\"multi_sc\": mudata})\n            if self.config.data_case is None:\n                data_case = DataCase.MULTI_SINGLE_CELL\n        elif isinstance(data, MuData):\n            data_package = DataPackage(multi_sc={\"multi_sc\": data})\n            if self.config.data_case is None:\n                data_case = DataCase.MULTI_SINGLE_CELL\n        elif isinstance(data, pd.DataFrame):\n            data_package = DataPackage(multi_bulk={\"user-data\": data})\n            if self.config.data_case is None:\n                data_case = DataCase.MULTI_BULK\n        elif isinstance(data, dict):\n            # Check if all values in the dictionary are pandas DataFrames\n            if all(isinstance(value, pd.DataFrame) for value in data.values()):\n                data_package = DataPackage(multi_bulk=data)\n                if self.config.data_case is None:\n                    data_case = DataCase.MULTI_BULK\n            else:\n                raise ValueError(\n                    \"All values in the dictionary must be pandas DataFrames.\"\n                )\n        if data_case is None:\n            raise ValueError(\"data_case must be provided if it cannot be inferred.\")\n\n        return data_package, data_case\n\n    def _validate_raw_user_data(self) -&gt; None:\n        \"\"\"Validates the format and content of user-provided raw data.\n\n        Ensures that raw_user_data is a valid DataPackage with properly formatted\n        attributes.\n\n        Raises:\n            TypeError: If raw_user_data is not a DataPackage.\n            ValueError: If DataPackage attributes aren't dictionaries or all are None.\n        \"\"\"\n        if not isinstance(self.raw_user_data, DataPackage):\n            raise TypeError(\n                f\"Expected raw_user_data to be of type DataPackage, got \"\n                f\"{type(self.raw_user_data)}.\"\n            )\n\n        all_none = True\n        for attr_name in self.raw_user_data.__annotations__:\n            attr_value = getattr(self.raw_user_data, attr_name)\n            if attr_value is not None:\n                all_none = False\n                if not isinstance(attr_value, dict):\n                    raise ValueError(\n                        f\"Attribute '{attr_name}' of raw_user_data must be a dictionary, \"\n                        f\"got {type(attr_value)}.\"\n                    )\n\n        if all_none:\n            raise ValueError(\n                \"All attributes of raw_user_data are None. At least one must be non-None.\"\n            )\n\n    def _fill_data_info(self) -&gt; None:\n        \"\"\"Populates the config's data_info with entries for all data keys.\n\n        Creates DataInfo objects for each data key found in raw_user_data\n        if they don't already exist in the configuration.\n        This method is needed, when the user provides data via the Pipeline and\n        not via the config.\n        \"\"\"\n        all_keys = []\n        for k in self.raw_user_data.__annotations__:\n            attr_value = getattr(self.raw_user_data, k)\n            all_keys.append(k)\n            if isinstance(attr_value, dict):\n                all_keys.extend(attr_value.keys())\n                for k, v in attr_value.items():\n                    if isinstance(v, MuData):\n                        all_keys.extend(v.mod.keys())\n        for k in all_keys:\n            if self.config.data_config.data_info.get(k) is None:\n                self.config.data_config.data_info[k] = DataInfo()\n\n    def _validate_user_data(self):\n        \"\"\"Validates user-provided data based on its source and format.\n\n        Performs different validation based on whether the user provided\n        preprocessed data, raw data, or a data configuration.\n\n        Raises:\n            Various exceptions depending on validation results.\n        \"\"\"\n        if self.raw_user_data is None:\n            if self._datasets is not None:  # case when user passes preprocessed data\n                self._validate_container()\n            else:  # user passes data via config\n                self._validate_config_data()\n        else:\n            self._validate_raw_user_data()\n\n    def _validate_container(self):\n        \"\"\"Validates that a DatasetContainer has at least one valid dataset.\n\n        Ensures the container has properly formatted datasets and at least\n        one split is present.\n\n        Raises:\n            ValueError: If container validation fails.\n        \"\"\"\n        if self.preprocessed_data is None:\n            raise ValueError(\"DatasetContainer is None. Please provide valid datasets.\")\n        none_count = 0\n        if not isinstance(self.preprocessed_data.train, Dataset):\n            if self.preprocessed_data.train is not None:\n                raise ValueError(\n                    f\"Train dataset has to be either None or Dataset, got \"\n                    f\"{type(self.preprocessed_data.train)}\"\n                )\n            none_count += 1\n        if not isinstance(self.preprocessed_data.test, Dataset):\n            if self.preprocessed_data.test is not None:\n                raise ValueError(\n                    f\"Test dataset has to be either None or Dataset, got \"\n                    f\"{type(self.preprocessed_data.test)}\"\n                )\n            none_count += 1\n\n        if not isinstance(self.preprocessed_data.valid, Dataset):\n            if self.preprocessed_data.valid is not None:\n                raise ValueError(\n                    f\"Valid dataset has to be either None or Dataset, got \"\n                    f\"{type(self.preprocessed_data.valid)}\"\n                )\n            none_count += 1\n        if none_count == 3:\n            raise ValueError(\"At least one split needs to be provided\")\n\n    def _validate_config_data(self):\n        \"\"\"Validates the data configuration provided via config.\n\n        Ensures the data configuration has the necessary components based\n        on the data types being processed.\n\n        Raises:\n            ValueError: If data configuration validation fails.\n        \"\"\"\n        data_info_dict = self.config.data_config.data_info\n        if not data_info_dict:\n            raise ValueError(\"data_info dictionary is empty.\")\n\n        # Check if there's at least one non-annotation file\n        non_annotation_files = {\n            key: info\n            for key, info in data_info_dict.items()\n            if info.data_type != \"ANNOTATION\"\n        }\n\n        if not non_annotation_files:\n            raise ValueError(\"At least one non-annotation file must be provided.\")\n\n        # Check if there's any non-single-cell data\n        non_single_cell_data = {\n            key: info\n            for key, info in data_info_dict.items()\n            if not info.is_single_cell and info.data_type != \"ANNOTATION\"\n        }\n\n        # If there's non-single-cell data, check for annotation file\n        if non_single_cell_data:\n            annotation_files = {\n                key: info\n                for key, info in data_info_dict.items()\n                if info.data_type == \"ANNOTATION\"\n            }\n\n            if not annotation_files:\n                raise ValueError(\n                    \"When working with non-single-cell data, an annotation file must be \"\n                    \"provided.\"\n                )\n\n    def preprocess(self, config: Optional[Union[None, DefaultConfig]] = None, **kwargs):\n        \"\"\"Filters, normalizes and prepares data for model training.\n\n        Processes raw input data into the format required by the model and creates\n        train/validation/test splits as needed.\n\n        Args:\n            config: Optional custom configuration for preprocessing.\n            **kwargs: Additional configuration parameters as keyword arguments.\n\n        Raises:\n            NotImplementedError: If preprocessor is not initialized.\n        \"\"\"\n        if self._preprocessor_type is None:\n            raise NotImplementedError(\"Preprocessor not initialized\")\n        self._validate_user_data()\n        if self.preprocessed_data is None:\n            self.preprocessed_data = self._preprocessor.preprocess(\n                raw_user_data=self.raw_user_data,  # type: ignore\n            )\n            self.result.datasets = self.preprocessed_data\n            self._datasets = self.preprocessed_data\n        else:\n            self._datasets = self.preprocessed_data\n            self.result.datasets = self.preprocessed_data\n\n    def fit(self, config: Optional[Union[None, DefaultConfig]] = None, **kwargs):\n        \"\"\"Trains the model on preprocessed data.\n\n        Creates and configures a trainer instance, then executes the training\n        process using the preprocessed datasets.\n\n        Args:\n            config: Optional custom configuration for training.\n            **kwargs: Additional configuration parameters as keyword arguments.\n\n        Raises:\n            ValueError: If datasets aren't available for training.\n        \"\"\"\n        if self._datasets is None:\n            raise ValueError(\n                \"Datasets not built. Please run the preprocess method first.\"\n            )\n\n        self._trainer = self._trainer_type(\n            trainset=self._datasets.train,\n            validset=self._datasets.valid,\n            result=self.result,\n            config=self.config,\n            model_type=self._model_type,\n            loss_type=self._loss_type,\n            ontologies=self.ontologies,  # Ontix\n            masking_fn=self.masking_fn if hasattr(self, \"masking_fn\") else None,\n            masking_fn_kwargs=(\n                self.masking_fn_kwargs if hasattr(self, \"masking_fn_kwargs\") else None\n            ),\n        )\n\n        trainer_result: Result = self._trainer.train()\n        self.result.update(other=trainer_result)\n\n    def predict(\n        self,\n        data: Optional[\n            Union[\n                DataPackage,\n                DatasetContainer,\n                ad.AnnData,\n                MuData,  # ty: ignore[invalid-type-form]\n            ]  # ty: ignore[invalid-type-form]\n        ] = None,  # ty: ignore[invalid-type-form]\n        config: Optional[Union[None, DefaultConfig]] = None,\n        from_key: Optional[str] = None,\n        to_key: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"Generates predictions using the trained model.\n\n        Uses the trained model to make predictions on test data or new data\n        provided by the user. Processes the results and stores them in the\n        result container.\n\n        Args:\n            data: Optional new data for predictions.\n            config: Optional custom configuration for prediction.\n            **kwargs: Additional configuration parameters as keyword arguments.\n\n        Raises:\n            NotImplementedError: If required components aren't initialized.\n            ValueError: If no test data is available or data format is invalid.\n        \"\"\"\n        self._validate_prediction_requirements()\n        if self._trainer is None:\n            raise ValueError(\n                \"Trainer not initialized, call fit first. If you used .save and .load, then you shoul not call .fit, then this is a bug.\"\n                \"In this case please submit an issue.\"\n            )\n\n        self._trainer.setup_trainer(old_model=self.result.model)\n        original_input = data\n        predict_data = self._prepare_prediction_data(data=data)\n\n        predictor_results = self._generate_predictions(\n            predict_data=predict_data,\n        )\n\n        self._process_latent_results(\n            predictor_results=predictor_results, predict_data=predict_data\n        )\n        self._postprocess_reconstruction(\n            predictor_results=predictor_results,\n            original_input=original_input,\n            predict_data=predict_data,\n        )\n        self.result.update(predictor_results)\n        return self.result\n\n    def _validate_prediction_requirements(self):\n        \"\"\"Validate that required components are initialized.\"\"\"\n        if self._preprocessor is None:\n            raise NotImplementedError(\"Preprocessor not initialized\")\n        if self.result.model is None:\n            raise NotImplementedError(\n                \"Model not trained. Please run the fit method first\"\n            )\n\n    def _prepare_prediction_data(\n        self,\n        data: Optional[\n            Union[\n                DataPackage,\n                DatasetContainer,\n                ad.AnnData,\n                MuData,  # ty: ignore[invalid-type-form]\n            ]  # ty: ignore[invalid-type-form]\n        ] = None,  # ty: ignore[invalid-type-form]\n    ) -&gt; DatasetContainer:\n        \"\"\"Prepare and validate input data for prediction.\n        Args:\n            data: Optional new data for predictions. If None, uses existing datasets.\n        Returns:\n            DatasetContainer: The prepared dataset container for predictions.\n        Raises:\n            ValueError: If data type is unsupported or no test data is available.\n        \"\"\"\n        if data is None:\n            return self._get_existing_datasets()\n        elif isinstance(data, DatasetContainer):\n            return self._handle_dataset_container(data=data)\n        elif isinstance(data, (DataPackage, ad.AnnData, MuData, dict, pd.DataFrame)):\n            return self._handle_user_data(data=data)\n        else:\n            raise ValueError(f\"Unsupported data type: {type(data)}\")\n\n    def _get_existing_datasets(self) -&gt; DatasetContainer:\n        \"\"\"Get existing preprocessed datasets and validate them for prediction.\n        Returns:\n            DatasetContainer: The preprocessed datasets available for prediction.\n        Raises:\n            ValueError: If no datasets are available or no test data is present.\n        \"\"\"\n        if self._datasets is None:\n            raise ValueError(\n                \"No data provided for prediction and no preprocessed datasets \"\n                \"available. Please run the preprocess method first or provide \"\n                \"data for prediction.\"\n            )\n        if self._datasets.test is None:\n            raise ValueError(\"No test data available for prediction\")\n        return self._datasets\n\n    def _handle_dataset_container(self, data: DatasetContainer) -&gt; DatasetContainer:\n        \"\"\"Handle DatasetContainer input for prediction.\n        Args:\n            data: DatasetContainer containing preprocessed datasets.\n        Returns:\n            DatasetContainer: The processed dataset container for predictions.\n        \"\"\"\n        self.result.new_datasets = data\n\n        if hasattr(self._preprocessor, \"_dataset_container\"):\n            self._preprocessor._dataset_container = data\n\n        return data\n\n    def _handle_user_data(self, data: Any) -&gt; DatasetContainer:\n        \"\"\"Handle user-provided data (DataPackage, AnnData, etc.).\n        Args:\n            data: Raw user data in various formats (DataPackage, AnnData, etc.).\n        Returns:\n            DatasetContainer: The processed dataset container for predictions.\n        Raises:\n            ValueError: If data type is unsupported or no test data is available.\n        \"\"\"\n        processed_data, _ = self._handle_direct_user_data(data=data)\n        predict_data = self._preprocessor.preprocess(\n            raw_user_data=processed_data, predict_new_data=True\n        )\n        self.result.new_datasets = predict_data\n        return predict_data\n\n    def _validate_prediction_data(self, predict_data: DatasetContainer):\n        \"\"\"Validate that prediction data has required test split.\"\"\"\n        if predict_data.test is None:\n            raise ValueError(\n                f\"The data for prediction need to be a DatasetContainer with a test \"\n                f\"attribute, got: {predict_data}\"\n            )\n\n    def _generate_predictions(\n        self,\n        predict_data: DatasetContainer,\n    ):\n        \"\"\"Generate predictions using the trained model.\n        Args:\n            predict_data: DatasetContainer with preprocessed datasets for prediction.\n        Returns:\n            Predictor results containing latent spaces and reconstructions.\n\n        \"\"\"\n        self._validate_prediction_data(predict_data=predict_data)\n        return self._trainer.predict(\n            data=predict_data.test,\n            model=self.result.model,\n        )  # type: ignore\n\n    def _process_latent_results(\n        self, predictor_results, predict_data: DatasetContainer\n    ):\n        \"\"\"Process and store latent space results.\n        Args:\n            predictor_results: Results from the prediction step containing latents.\n            predict_data: DatasetContainer with preprocessed datasets for prediction.\n        \"\"\"\n        latent = predictor_results.latentspaces.get(epoch=-1, split=\"test\")\n        if isinstance(latent, dict):\n            print(\"Detected dictionary in latent results, extracting array...\")\n            latent = next(iter(latent.values()))  # TODO better adjust for xmodal\n        self.result.adata_latent = ad.AnnData(latent)\n        self.result.adata_latent.obs_names = predict_data.test.sample_ids  # type: ignore\n        self.result.adata_latent.uns[\"var_names\"] = predict_data.test.feature_ids  # type: ignore\n        self.result.update(predictor_results)\n\n    def _postprocess_reconstruction(\n        self, predictor_results, original_input, predict_data: DatasetContainer\n    ):\n        \"\"\"Postprocess reconstruction results based on input type.\n\n        This outpus the reconstruction in the same format as the original input data,\n        whether it is a DatasetContainer, DataPackage, AnnData, MuData, or other formats.\n\n        Args:\n            predictor_results: Results from the prediction step containing reconstructions.\n            original_input: Original input data format (if provided).\n            predict_data: DatasetContainer with preprocessed datasets for prediction.\n        Raises:\n            ValueError: If reconstruction fails or data types are incompatible.\n        \"\"\"\n        raw_recon: Union[Dict, np.ndarray, torch.Tensor] = (\n            self.result.reconstructions.get(epoch=-1, split=\"test\")\n        )\n        if isinstance(raw_recon, np.ndarray):\n            raw_recon = torch.from_numpy(raw_recon)  # type: ignore\n        elif isinstance(raw_recon, dict):\n            raw_recon = raw_recon.get(\"translation\")  # type: ignore\n            if raw_recon is None:\n                raise ValueError(\n                    f\"Raw recon is dict, but has no translation key, this should not happen: {raw_recon}\"\n                )\n            raw_recon = torch.from_numpy(raw_recon)  # type: ignore\n        else:\n            raise ValueError(\n                f\"type of raw_recon has to be 'dict' or 'np.ndarray', got: {type(raw_recon)}\"\n            )\n\n        if original_input is None:\n            # Using existing datasets\n            self._handle_dataset_container_reconstruction(\n                raw_recon=raw_recon,  # type: ignore\n                dataset_container=predict_data,\n                context=\"existing datasets\",\n            )\n        elif isinstance(original_input, DatasetContainer):\n            self._handle_dataset_container_reconstruction(\n                raw_recon=raw_recon,  # type: ignore\n                dataset_container=original_input,\n                context=\"provided DatasetContainer\",\n            )\n        elif self.config.data_case == DataCase.MULTI_SINGLE_CELL:\n            self._handle_multi_single_cell_reconstruction(\n                raw_recon=raw_recon,\n                predictor_results=predictor_results,  # type: ignore\n            )\n        elif isinstance(\n            original_input, (DataPackage, ad.AnnData, MuData, dict, pd.DataFrame)\n        ):\n            self._handle_user_data_reconstruction(\n                raw_recon=raw_recon, predictor_results=predictor_results\n            )\n        else:\n            self._handle_unsupported_reconstruction()\n\n    def _handle_dataset_container_reconstruction(\n        self,\n        raw_recon: torch.Tensor,\n        dataset_container: DatasetContainer,\n        context: str = \"DatasetContainer\",\n    ):\n        \"\"\"Handle reconstruction for DatasetContainer input.\n        Args:\n            raw_recon: Raw reconstruction tensor from the model.\n            dataset_container: Original DatasetContainer provided by the user.\n            context: Description of the data context for error messages.\n        Raises:\n            ValueError: If no test data is available in the container.\n        \"\"\"\n\n        # if dataset_container.test is None:\n        #     raise ValueError(f\"No test data available in {context} for reconstruction.\")\n        # temp = copy.deepcopy(dataset_container.test)\n        # temp.data = raw_recon\n        # self.result.final_reconstruction = temp\n        pass\n\n    def _handle_multi_single_cell_reconstruction(\n        self, raw_recon: torch.Tensor, predictor_results: Result\n    ):\n        \"\"\"Handle reconstruction for multi-single-cell data\n        Args:\n            raw_recon: Raw reconstruction tensor from the model.\n            predictor_results: Results from the prediction step containing reconstructions.\n        Raises:\n            ValueError: If reconstruction formatting fails or data types are incompatible.\n        \"\"\"\n        pkg = self._preprocessor.format_reconstruction(\n            reconstruction=raw_recon, result=predictor_results\n        )\n        if not isinstance(pkg.multi_sc, dict):\n            raise ValueError(\n                \"Expected pkg.multi_sc to be a dictionary, got \"\n                f\"{type(pkg.multi_sc)} instead.\"\n            )\n        self.result.final_reconstruction = pkg.multi_sc[\"multi_sc\"]\n\n    def _handle_user_data_reconstruction(\n        self, raw_recon: torch.Tensor, predictor_results\n    ):\n        \"\"\"Handle reconstruction for user-provided data formats.\n        Args:\n            raw_recon: Raw reconstruction tensor from the model.\n            predictor_results: Results from the prediction step containing reconstructions.\n        \"\"\"\n        pkg = self._preprocessor.format_reconstruction(\n            reconstruction=raw_recon, result=predictor_results\n        )\n        self.result.final_reconstruction = pkg\n\n    def _handle_unsupported_reconstruction(self):\n        \"\"\"Handle cases where reconstruction formatting is not available.\"\"\"\n        print(\n            \"Reconstruction Formatting (the process of using the reconstruction \"\n            \"output of the autoencoder models and combine it with metadata to get \"\n            \"the exact same data structure as the raw input data i.e, a DataPackage, \"\n            \"DatasetContainer, or AnnData) not available for this data type or case.\"\n        )\n\n    def decode(\n        self, latent: Union[torch.Tensor, ad.AnnData, pd.DataFrame]\n    ) -&gt; Union[torch.Tensor, ad.AnnData, pd.DataFrame]:\n        \"\"\"Transforms latent space representations back to input space.\n\n        Handles various input formats for the latent representation and\n        returns the decoded data in a matching format.\n\n        Args:\n            latent: Latent space representation to decode.\n\n        Returns:\n            Decoded data in a format matching the input.\n\n        Raises:\n            TypeError: If no model has been trained or input type is invalid.\n            ValueError: If latent dimensions are incompatible with the model.\n        \"\"\"\n        if self.result.model is None:\n            raise TypeError(\"No model trained yet, use fit() or run() method first\")\n        recons: torch.Tensor\n        if isinstance(latent, ad.AnnData):\n            latent_data = torch.tensor(\n                latent.X, dtype=torch.float32\n            )  # Ensure float for compatibility\n\n            expected_latent_dim = self.config.latent_dim\n            if not latent_data.shape[1] == expected_latent_dim:\n                raise ValueError(\n                    f\"Input AnnData's .X has shape {latent_data.shape}, but the model \"\n                    f\"expects a latent vector of size {expected_latent_dim}. Consider \"\n                    f\"projecting the AnnData to the correct latent space first.\"\n                )\n            latent_tensor = latent_data\n\n            recons = self._trainer.decode(x=latent_tensor)\n            if self._datasets is None:\n                raise ValueError(\n                    \"No datasets available in the DatasetContainer to reconstruct \"\n                    \"AnnData objects. Please provide a valid DatasetContainer.\"\n                )\n            if self._datasets.train is None:\n                raise ValueError(\n                    \"The train dataset in the DatasetContainer is None. \"\n                    \"Please provide a valid train dataset to reconstruct AnnData objects.\"\n                )\n            if not isinstance(self._datasets.train, BaseDataset):\n                raise TypeError(\n                    \"The train dataset in the DatasetContainer must be a BaseDataset \"\n                    \"to reconstruct AnnData objects.\"\n                )\n            recons_adata = ad.AnnData(\n                X=recons.to(\"cpu\").detach().numpy(),\n                obs=pd.DataFrame(index=latent.obs_names),\n                var=pd.DataFrame(index=self._datasets.train.feature_ids),\n            )\n\n            return recons_adata\n        elif isinstance(latent, pd.DataFrame):\n            latent_tensor = torch.tensor(latent.values, dtype=torch.float32)\n            recons = self._trainer.decode(x=latent_tensor)\n            return pd.DataFrame(\n                recons.to(\"cpu\").detach().numpy(),\n                index=latent.index,\n                columns=latent.columns,\n            )\n        elif isinstance(latent, torch.Tensor):\n            # Check size compatibility\n            expected_latent_dim = self.config.latent_dim\n            if not latent.shape[1] == expected_latent_dim:\n                if self._trainer._model._mu.out_features == latent.shape[1]:\n                    warnings.warn(\n                        f\"latent_prior has latent dimension {latent.shape[1]}, \"\n                        \"which matches the input feature dimension of the model. Did you \"\n                        \"mean to provide latent vectors of dimension \"\n                        \"For Ontix this is the default behaviour and the warning can be ignored. \"\n                        f\"{self.config.latent_dim}?\"\n                    )\n                else:\n                    raise ValueError(\n                        f\"latent_prior has incompatible latent dimension {latent.shape[1]}, \"\n                        f\"expected {self.config.latent_dim}. or {self._trainer._model._mu.out_features}.\"\n                    )\n\n            latent_tensor = latent\n        else:\n            raise TypeError(\n                f\"Input 'latent' must be either a torch.Tensor or an AnnData object, \"\n                f\"not {type(latent)}.\"\n            )\n\n        return self._trainer.decode(x=latent_tensor)\n\n    def evaluate(\n        self,\n        ml_model_class: ClassifierMixin = linear_model.LogisticRegression(),\n        ml_model_regression: RegressorMixin = linear_model.LinearRegression(),\n        params: Union[\n            list, str\n        ] = [],  # Default empty list, to use all parameters use string \"all\"\n        metric_class: str = \"roc_auc_ovo\",  # Default is 'roc_auc_ovo' via https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-string-names\n        metric_regression: str = \"r2\",  # Default is 'r2'\n        reference_methods: list = [],  # Default [], Options are \"PCA\", \"UMAP\", \"TSNE\", \"RandomFeature\"\n        split_type: Literal[\n            \"use-split\", \"CV-5\", \"LOOC\"\n        ] = \"use-split\",  # Default is \"use-split\", other options: \"CV-5\", ... \"LOOCV\"?\n        n_downsample: Optional[int] = 10000,\n    ) -&gt; Result:\n        \"\"\"TODO\"\"\"\n        if self.evaluator is None:\n            raise NotImplementedError(\"Evaluator not initialized\")\n        if self.result.model is None:\n            raise NotImplementedError(\n                \"Model not trained. Please run the fit method first\"\n            )\n        if not is_classifier(ml_model_class):\n            warnings.warn(\n                \"The provided model is not a sklearn-type classifier. \"\n                \"Evaluation continues but may produce incorrect results or errors.\"\n            )\n        if not is_regressor(ml_model_regression):\n            warnings.warn(\n                \"The provided model is not a sklearn-type regressor. \"\n                \"Evaluation continues but may produce incorrect results or errors.\"\n            )\n\n        if len(params) == 0:\n            if self.config.data_config.annotation_columns is None:\n                params = []  # type: ignore\n            else:\n                params = self.config.data_config.annotation_columns  # type: ignore\n\n        if len(params) == 0:\n            raise ValueError(\n                \"No parameters specified for evaluation. Please provide a list of \"\n                \"parameters or ensure that annotation_columns are set in the config.\"\n            )\n\n        if \"RandomFeature\" in reference_methods:\n            if self._datasets is None:\n                raise ValueError(\n                    \"Datasets not available for adding RandomFeature. Please keep \"\n                    \"preprocessed data available before evaluation.\"\n                )\n\n        if len(self.result.latentspaces._data) == 0:\n            raise ValueError(\n                \"No latent spaces found in results. Please run predict() to \"\n                \"calculate embeddings before evaluation.\"\n            )\n\n        self.result = self.evaluator.evaluate(\n            datasets=self._datasets,\n            result=self.result,\n            ml_model_class=ml_model_class,\n            ml_model_regression=ml_model_regression,\n            params=params,\n            metric_class=metric_class,\n            metric_regression=metric_regression,\n            reference_methods=reference_methods,\n            split_type=split_type,\n            n_downsample=n_downsample,\n        )\n\n        _: Any = self.visualizer._plot_evaluation(result=self.result)\n\n        return self.result\n\n    def visualize(self, config: Optional[Union[None, DefaultConfig]] = None, **kwargs):\n        \"\"\"Creates visualizations of model results and performance.\n\n        Args:\n            config: Optional custom configuration for visualization.\n            **kwargs: Additional configuration parameters.\n\n        Raises:\n            NotImplementedError: If visualizer is not initialized.\n        \"\"\"\n        if self.visualizer is None:\n            raise NotImplementedError(\"Visualizer not initialized\")\n\n        self.visualizer.visualize(result=self.result, config=self.config)\n\n    def show_result(self, split: str = \"all\", **kwargs):\n        \"\"\"Displays key visualizations of model results.\n\n        This method generates the following visualizations:\n        1. Loss Curves: Displays the absolute loss curves to provide insights into\n           the model's training and validation performance over epochs.\n        2. Latent Space Ridgeline Plot: Visualizes the distribution of the latent\n           space representations across different dimensions, offering a high-level\n           overview of the learned embeddings.\n        3. Latent Space 2D Scatter Plot: Projects the latent space into two dimensions\n           for a detailed view of the clustering or separation of data points.\n\n        These visualizations help in understanding the model's performance and\n        the structure of the latent space representations.\n        \"\"\"\n        print(\"Creating plots ...\")\n\n        params: Optional[Union[List[str], str]] = kwargs.pop(\"params\", None)\n        # Check if params are empty and annotation columns are available in config\n        if params is None and self.config.data_config.annotation_columns:\n            params = self.config.data_config.annotation_columns\n\n        if len(self.result.losses._data) != 0:\n            self.visualizer.show_loss(plot_type=\"absolute\")\n        else:\n            warnings.warn(\n                \"No loss data found in results. Skipping loss curve visualization.\"\n            )\n\n        if len(self.result.latentspaces._data) != 0:\n            self.visualizer.show_latent_space(\n                result=self.result, plot_type=\"Ridgeline\", split=split, param=params\n            )\n            self.visualizer.show_latent_space(\n                result=self.result, plot_type=\"2D-scatter\", split=split, param=params\n            )\n        else:\n            warnings.warn(\n                \"No latent spaces found in results. Please run predict() to \"\n                \"calculate embeddings.\"\n            )\n\n    def run(\n        self, data: Optional[Union[DatasetContainer, DataPackage]] = None\n    ) -&gt; Result:\n        \"\"\"Executes the complete pipeline from preprocessing to visualization.\n\n        Runs all pipeline steps in sequence and returns the result.\n\n        Args:\n            data: Optional data for prediction (overrides test data).\n\n        Returns:\n            Complete pipeline results.\n        \"\"\"\n        self.preprocess()\n        self.fit()\n        self.predict(data=data)\n        self.visualize()\n        return self.result\n\n    def save(self, file_path: str, save_all: bool = False):\n        \"\"\"Saves the pipeline to a file.\n\n        Args:\n            file_path: Path where the pipeline should be saved.\n        \"\"\"\n        saver = Saver(file_path, save_all=save_all)\n        saver.save(self)\n\n    @classmethod\n    def load(cls, file_path) -&gt; Any:\n        \"\"\"Loads a pipeline from a file.\n\n        Args:\n            file_path: Path to the saved pipeline.\n\n        Returns:\n            The loaded pipeline instance.\n        \"\"\"\n        loader = Loader(file_path)\n        return loader.load()\n\n    def sample_latent_space(\n        self,\n        n_samples: int,\n        split: str = \"test\",\n        epoch: int = -1,\n    ) -&gt; torch.Tensor:\n        \"\"\"Samples latent space points from the learned distribution.\n\n        If `n_samples` is not provided, this method returns one latent point per\n        sample in the specified split (legacy behavior). If `n_samples` is given,\n        it draws samples from the aggregated posterior distribution of the split.\n\n        Args:\n            split: The split to sample from (train, valid, test), default is test.\n            epoch: The epoch to sample from, default is the last epoch (-1).\n            n_samples: Optional number of latent points to sample. If None,\n                returns one latent point per available sample in the split.\n\n        Returns:\n            z: torch.Tensor - The sampled latent space points.\n\n        Raises:\n            ValueError: If the model has not been trained or latent statistics\n                have not been computed.\n            TypeError: If mu or logvar are not numpy arrays.\n        \"\"\"\n\n        if not hasattr(self, \"_trainer\") or self._trainer is None:\n            raise ValueError(\"Model is not trained yet. Please train the model first.\")\n        if self.result.mus is None or self.result.sigmas is None:\n            raise ValueError(\"Model has not learned the latent space distribution yet.\")\n        if not isinstance(n_samples, int) or n_samples &lt;= 0:\n            raise ValueError(\"n_samples must be a positive integer.\")\n\n        mu = self.result.mus.get(split=split, epoch=epoch)\n        logvar = self.result.sigmas.get(split=split, epoch=epoch)\n\n        if not isinstance(mu, np.ndarray):\n            raise TypeError(\n                f\"Expected value to be of type numpy.ndarray, got {type(mu)}.\"\n                \"This can happen if the model was not trained with VAE loss or if you forgot to run predict()\"\n            )\n        if not isinstance(logvar, np.ndarray):\n            raise TypeError(\n                f\"Expected value to be of type numpy.ndarray, got {type(logvar)}.\"\n            )\n\n        mu_t = torch.from_numpy(mu).to(\n            device=self._trainer._model.device, dtype=self._trainer._model.dtype\n        )\n        logvar_t = torch.from_numpy(logvar).to(\n            device=self._trainer._model.device, dtype=self._trainer._model.dtype\n        )\n\n        with torch.no_grad():\n            global_mu = mu_t.mean(dim=0)\n            global_logvar = logvar_t.mean(dim=0)\n\n            mu_exp = global_mu.expand(n_samples, -1)\n            logvar_exp = global_logvar.expand(n_samples, -1)\n\n            z = self._trainer._model.reparameterize(mu_exp, logvar_exp)\n            return z\n\n    def generate(\n        self,\n        n_samples: Optional[int] = None,\n        latent_prior: Optional[Union[np.ndarray, torch.Tensor]] = None,\n        split: str = \"test\",\n        epoch: int = -1,\n    ) -&gt; torch.Tensor:\n        \"\"\"Generates new samples from the model's latent space.\n\n        This method allows for the generation of new data samples by sampling\n        from the model's latent space. Users can either provide a custom latent\n        prior or specify the number of samples to generate. If a custom latent\n        prior is provided, its batch dimension must be compatible with n_samples.\n\n        Args:\n            n_samples: The number of samples to generate.\n            latent_prior: Optional custom latent prior distribution. If provided,\n                this will be used for sampling instead of the learned distribution.\n                The prior must either be a single latent vector or a batch of\n                latent vectors matching n_samples.\n            split: The split to sample from (train, valid, test), default is test.\n            epoch: The epoch to sample from, default is the last epoch (-1).\n\n        Returns:\n            torch.Tensor: The generated samples in the input space.\n\n        Raises:\n            ValueError: If n_samples is not a positive integer or if the latent\n                prior has incompatible dimensions.\n            TypeError: If latent_prior is not a numpy array or tensor.\n        \"\"\"\n        if not isinstance(n_samples, int) or n_samples &lt;= 0:\n            if latent_prior is None:\n                raise ValueError(\n                    \"n_samples must be a positive integer or latent_prior provided.\"\n                )\n\n        if latent_prior is None:\n            latent_prior = self.sample_latent_space(\n                n_samples=n_samples, split=split, epoch=epoch\n            )\n\n        if isinstance(latent_prior, np.ndarray):\n            latent_prior = torch.from_numpy(latent_prior).to(\n                device=self._trainer._model.device,\n                dtype=self._trainer._model.dtype,\n            )\n        if not isinstance(latent_prior, torch.Tensor):\n            raise TypeError(\n                f\"latent_prior must be numpy.ndarray or torch.Tensor, got {type(latent_prior)}.\"\n            )\n        if not latent_prior.shape[1] == self.config.latent_dim:\n            if self._trainer._model._mu.out_features == latent_prior.shape[1]:\n                warnings.warn(\n                    f\"latent_prior has latent dimension {latent_prior.shape[1]}, \"\n                    \"which matches the input feature dimension of the model. Did you \"\n                    \"mean to provide latent vectors of dimension \"\n                    \"For Ontix this is the default behaviour and the warning can be ignored. \"\n                    f\"{self.config.latent_dim}?\"\n                )\n            else:\n                raise ValueError(\n                    f\"latent_prior has incompatible latent dimension {latent_prior.shape[1]}, \"\n                    f\"expected {self.config.latent_dim}.\"\n                )\n\n        with torch.no_grad():\n            generated = self.decode(latent=latent_prior)\n            return generated\n\n    def explain(\n        self,\n        explainer: Any,\n        baseline_type: Literal[\"mean\", \"random_sample\"] = \"mean\",\n        n_subset: int = 100,\n        llm_explain: bool = False,\n        llm_client: Literal[\"ollama\", \"mistral\"] = \"mistral\",\n        llm_model: str = \"mistral-medium-latest\",\n    ):  # TODO Vincent: add return type\n        my_converter = AnnDataConverter()\n        dataset: Optional[DatasetContainer] = get_dataset(self.result)\n        if dataset is None:\n            raise ValueError(\n                \"No dataset available for explanation.\"\n                \"This happens if you used .save and .load, and did not run .predict before.\"\n                \"This can also happen if you run .explain before .preprocess or .fit.\"\n            )\n        adata_train: Optional[Dict[str, ad.AnnData]] = my_converter.dataset_to_adata(\n            dataset, split=\"train\"\n        )\n        adata_test: Optional[Dict[str, ad.AnnData]] = my_converter.dataset_to_adata(\n            dataset, split=\"test\"\n        )\n        adata_valid: Optional[Dict[str, ad.AnnData]] = my_converter.dataset_to_adata(\n            dataset, split=\"valid\"\n        )\n        model = self.result.model\n        if model is None:\n            raise ValueError(\n                \"No model available for explanation.\"\n                \"This happens if you used .save and .load, and did not run .fit before.\"\n                \"This can also happen if you run .explain before .fit.\"\n            )\n        # TODO Vincent: Implement feature importance explanation\n        # Best with Explainer class that gets initialized here and has a method\n        # Maybe like:\n        # explainer = FeatureImportanceExplainer(adata_train, adata_test, model, explainer, ...)\n        # output = explainer.explain()\n        # also note tha adata_&lt;split&gt; can be None, if the split is not available\n        # so best to check this before concatenating or using them\n\n        if llm_explain:\n            # TODO Vincent:\n            # Je nachdem wie die Gene Liste aussieht, m\u00fcsstet du noch in src/autoencodix/utils/_llm_explainer.py\n            # in _init_prompt anpassen, wie der prompt gebaut wird. Ich gehe jetzt von einer Liste aus String aus, aber\n            # ich wusste nicht genau was dein return Typ ist.\n\n            llm_explainer = LLMExplainer(\n                client_name=llm_client,\n                model_name=llm_model,\n                gene_list=[\"GeneA\", \"GeneB\", \"GeneC\"],  # Example gene list\n            )\n            explanation = llm_explainer.explain()\n            print(\"LLM Explanation:\")\n            print(explanation)\n            return explanation\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.__init__","title":"<code>__init__(dataset_type, trainer_type, model_type, loss_type, datasplitter_type, preprocessor_type, data, visualizer=None, evaluator=None, result=None, config=None, custom_split=None, ontologies=None, masking_fn=None, masking_fn_kwargs={}, **kwargs)</code>","text":"<p>Initializes the pipeline with components and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_type</code> <code>Type[BaseDataset]</code> <p>Class for dataset implementations.</p> required <code>trainer_type</code> <code>Type[BaseTrainer]</code> <p>Class for model training implementations.</p> required <code>model_type</code> <code>Type[BaseAutoencoder]</code> <p>Class for model architecture implementations.</p> required <code>loss_type</code> <code>Type[BaseLoss]</code> <p>Class for loss function implementations.</p> required <code>datasplitter_type</code> <code>Type[DataSplitter]</code> <p>Class for data splitting implementation.</p> required <code>preprocessor_type</code> <code>Type[BasePreprocessor]</code> <p>Class for data preprocessing implementation.</p> required <code>visualizer</code> <code>Optional[BaseVisualizer]</code> <p>Component for generating visualizations.</p> <code>None</code> <code>data</code> <code>Optional[Union[DataPackage, DatasetContainer, AnnData, MuData, DataFrame, dict]]</code> <p>Input data to be processed or already processed data.</p> required <code>evaluator</code> <code>Optional[BaseEvaluator]</code> <p>Component for assessing model performance.</p> <code>None</code> <code>result</code> <code>Optional[Result]</code> <p>Storage container for pipeline outputs.</p> <code>None</code> <code>config</code> <code>Optional[DefaultConfig]</code> <p>Configuration parameters for all pipeline components.</p> <code>None</code> <code>custom_split</code> <code>Optional[Dict[str, ndarray]]</code> <p>User-provided data splits (train/validation/test).</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If inputs have incorrect types.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def __init__(\n    self,\n    dataset_type: Type[BaseDataset],\n    trainer_type: Type[BaseTrainer],\n    model_type: Type[BaseAutoencoder],\n    loss_type: Type[BaseLoss],\n    datasplitter_type: Type[DataSplitter],\n    preprocessor_type: Type[BasePreprocessor],\n    data: Optional[\n        Union[DataPackage, DatasetContainer, ad.AnnData, MuData, pd.DataFrame, dict]  # type: ignore[invalid-type-form]\n    ],\n    visualizer: Optional[BaseVisualizer] = None,\n    evaluator: Optional[BaseEvaluator] = None,\n    result: Optional[Result] = None,\n    config: Optional[DefaultConfig] = None,\n    custom_split: Optional[Dict[str, np.ndarray]] = None,\n    ontologies: Optional[Union[Tuple, Dict[Any, Any]]] = None,\n    masking_fn: Optional[Callable] = None,\n    masking_fn_kwargs: Dict[str, Any] = {},\n    **kwargs: dict,\n) -&gt; None:  # ty: ignore[call-non-callable]\n    \"\"\"Initializes the pipeline with components and configuration.\n\n    Args:\n        dataset_type: Class for dataset implementations.\n        trainer_type: Class for model training implementations.\n        model_type: Class for model architecture implementations.\n        loss_type: Class for loss function implementations.\n        datasplitter_type: Class for data splitting implementation.\n        preprocessor_type: Class for data preprocessing implementation.\n        visualizer: Component for generating visualizations.\n        data: Input data to be processed or already processed data.\n        evaluator: Component for assessing model performance.\n        result: Storage container for pipeline outputs.\n        config: Configuration parameters for all pipeline components.\n        custom_split: User-provided data splits (train/validation/test).\n        **kwargs: Additional keyword arguments.\n\n    Raises:\n        TypeError: If inputs have incorrect types.\n    \"\"\"\n    if not hasattr(self, \"_default_config\"):\n        raise ValueError(\n            \"\"\"\n                        The _default_config attribute has not been specified in your pipeline class.\n\n                        Example:\n                        self._default_config = XModalixConfig()\n\n                        This error typically occurs when a new architecture is added without setting the\n                        _default_config in its corresponding pipeline class.\n\n                        For more details, please refer to the 'how to add a new architecture' section in our documentation.\n                        \"\"\"\n        )\n\n    self._validate_config(config=config)\n    self._validate_user_input(data=data)\n    self.masking_fn = masking_fn\n    self.masking_fn_kwargs = masking_fn_kwargs\n    processed_data = data if isinstance(data, DatasetContainer) else None\n    raw_user_data = (\n        data\n        if isinstance(data, (DataPackage, ad.AnnData, MuData, pd.DataFrame, dict))\n        else None\n    )\n    if processed_data is not None and not isinstance(\n        processed_data, DatasetContainer\n    ):\n        raise TypeError(\n            f\"Expected data type to be DatasetContainer, got {type(processed_data)}.\"\n        )\n\n    self.preprocessed_data: Optional[DatasetContainer] = processed_data\n    self.raw_user_data: Union[\n        DataPackage, ad.AnnData, MuData, pd.DataFrame, dict  # type: ignore[invalid-type-form]\n    ] = raw_user_data\n    self._trainer_type = trainer_type\n    self._trainer: Optional[BaseTrainer] = None\n    self._model_type = model_type\n    self._loss_type = loss_type\n    self._preprocessor_type = preprocessor_type\n    if self.raw_user_data is not None:\n        self.raw_user_data, datacase = self._handle_direct_user_data(\n            data=self.raw_user_data,\n        )\n        self.config.data_case = datacase\n        self._fill_data_info()\n\n    self.ontologies = ontologies\n    self._preprocessor = self._preprocessor_type(\n        config=self.config, ontologies=self.ontologies\n    )\n\n    self.visualizer = (\n        visualizer()  # ty: ignore[call-non-callable]\n        if visualizer is not None\n        else BaseVisualizer()  # ty: ignore[call-non-callable]\n    )  # ty: ignore[call-non-callable]\n    self.evaluator = (\n        evaluator()  # ty: ignore[call-non-callable]\n        if evaluator is not None\n        else BaseEvaluator()  # ty: ignore[call-non-callable]\n    )  # ty: ignore[call-non-callable]\n    self.result = result if result is not None else Result()\n    self._dataset_type = dataset_type\n    self._data_splitter = datasplitter_type(\n        config=self.config, custom_splits=custom_split\n    )\n\n    self._datasets: Optional[DatasetContainer] = (\n        processed_data  # None, or user input\n    )\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.decode","title":"<code>decode(latent)</code>","text":"<p>Transforms latent space representations back to input space.</p> <p>Handles various input formats for the latent representation and returns the decoded data in a matching format.</p> <p>Parameters:</p> Name Type Description Default <code>latent</code> <code>Union[Tensor, AnnData, DataFrame]</code> <p>Latent space representation to decode.</p> required <p>Returns:</p> Type Description <code>Union[Tensor, AnnData, DataFrame]</code> <p>Decoded data in a format matching the input.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If no model has been trained or input type is invalid.</p> <code>ValueError</code> <p>If latent dimensions are incompatible with the model.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def decode(\n    self, latent: Union[torch.Tensor, ad.AnnData, pd.DataFrame]\n) -&gt; Union[torch.Tensor, ad.AnnData, pd.DataFrame]:\n    \"\"\"Transforms latent space representations back to input space.\n\n    Handles various input formats for the latent representation and\n    returns the decoded data in a matching format.\n\n    Args:\n        latent: Latent space representation to decode.\n\n    Returns:\n        Decoded data in a format matching the input.\n\n    Raises:\n        TypeError: If no model has been trained or input type is invalid.\n        ValueError: If latent dimensions are incompatible with the model.\n    \"\"\"\n    if self.result.model is None:\n        raise TypeError(\"No model trained yet, use fit() or run() method first\")\n    recons: torch.Tensor\n    if isinstance(latent, ad.AnnData):\n        latent_data = torch.tensor(\n            latent.X, dtype=torch.float32\n        )  # Ensure float for compatibility\n\n        expected_latent_dim = self.config.latent_dim\n        if not latent_data.shape[1] == expected_latent_dim:\n            raise ValueError(\n                f\"Input AnnData's .X has shape {latent_data.shape}, but the model \"\n                f\"expects a latent vector of size {expected_latent_dim}. Consider \"\n                f\"projecting the AnnData to the correct latent space first.\"\n            )\n        latent_tensor = latent_data\n\n        recons = self._trainer.decode(x=latent_tensor)\n        if self._datasets is None:\n            raise ValueError(\n                \"No datasets available in the DatasetContainer to reconstruct \"\n                \"AnnData objects. Please provide a valid DatasetContainer.\"\n            )\n        if self._datasets.train is None:\n            raise ValueError(\n                \"The train dataset in the DatasetContainer is None. \"\n                \"Please provide a valid train dataset to reconstruct AnnData objects.\"\n            )\n        if not isinstance(self._datasets.train, BaseDataset):\n            raise TypeError(\n                \"The train dataset in the DatasetContainer must be a BaseDataset \"\n                \"to reconstruct AnnData objects.\"\n            )\n        recons_adata = ad.AnnData(\n            X=recons.to(\"cpu\").detach().numpy(),\n            obs=pd.DataFrame(index=latent.obs_names),\n            var=pd.DataFrame(index=self._datasets.train.feature_ids),\n        )\n\n        return recons_adata\n    elif isinstance(latent, pd.DataFrame):\n        latent_tensor = torch.tensor(latent.values, dtype=torch.float32)\n        recons = self._trainer.decode(x=latent_tensor)\n        return pd.DataFrame(\n            recons.to(\"cpu\").detach().numpy(),\n            index=latent.index,\n            columns=latent.columns,\n        )\n    elif isinstance(latent, torch.Tensor):\n        # Check size compatibility\n        expected_latent_dim = self.config.latent_dim\n        if not latent.shape[1] == expected_latent_dim:\n            if self._trainer._model._mu.out_features == latent.shape[1]:\n                warnings.warn(\n                    f\"latent_prior has latent dimension {latent.shape[1]}, \"\n                    \"which matches the input feature dimension of the model. Did you \"\n                    \"mean to provide latent vectors of dimension \"\n                    \"For Ontix this is the default behaviour and the warning can be ignored. \"\n                    f\"{self.config.latent_dim}?\"\n                )\n            else:\n                raise ValueError(\n                    f\"latent_prior has incompatible latent dimension {latent.shape[1]}, \"\n                    f\"expected {self.config.latent_dim}. or {self._trainer._model._mu.out_features}.\"\n                )\n\n        latent_tensor = latent\n    else:\n        raise TypeError(\n            f\"Input 'latent' must be either a torch.Tensor or an AnnData object, \"\n            f\"not {type(latent)}.\"\n        )\n\n    return self._trainer.decode(x=latent_tensor)\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.evaluate","title":"<code>evaluate(ml_model_class=linear_model.LogisticRegression(), ml_model_regression=linear_model.LinearRegression(), params=[], metric_class='roc_auc_ovo', metric_regression='r2', reference_methods=[], split_type='use-split', n_downsample=10000)</code>","text":"<p>TODO</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def evaluate(\n    self,\n    ml_model_class: ClassifierMixin = linear_model.LogisticRegression(),\n    ml_model_regression: RegressorMixin = linear_model.LinearRegression(),\n    params: Union[\n        list, str\n    ] = [],  # Default empty list, to use all parameters use string \"all\"\n    metric_class: str = \"roc_auc_ovo\",  # Default is 'roc_auc_ovo' via https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-string-names\n    metric_regression: str = \"r2\",  # Default is 'r2'\n    reference_methods: list = [],  # Default [], Options are \"PCA\", \"UMAP\", \"TSNE\", \"RandomFeature\"\n    split_type: Literal[\n        \"use-split\", \"CV-5\", \"LOOC\"\n    ] = \"use-split\",  # Default is \"use-split\", other options: \"CV-5\", ... \"LOOCV\"?\n    n_downsample: Optional[int] = 10000,\n) -&gt; Result:\n    \"\"\"TODO\"\"\"\n    if self.evaluator is None:\n        raise NotImplementedError(\"Evaluator not initialized\")\n    if self.result.model is None:\n        raise NotImplementedError(\n            \"Model not trained. Please run the fit method first\"\n        )\n    if not is_classifier(ml_model_class):\n        warnings.warn(\n            \"The provided model is not a sklearn-type classifier. \"\n            \"Evaluation continues but may produce incorrect results or errors.\"\n        )\n    if not is_regressor(ml_model_regression):\n        warnings.warn(\n            \"The provided model is not a sklearn-type regressor. \"\n            \"Evaluation continues but may produce incorrect results or errors.\"\n        )\n\n    if len(params) == 0:\n        if self.config.data_config.annotation_columns is None:\n            params = []  # type: ignore\n        else:\n            params = self.config.data_config.annotation_columns  # type: ignore\n\n    if len(params) == 0:\n        raise ValueError(\n            \"No parameters specified for evaluation. Please provide a list of \"\n            \"parameters or ensure that annotation_columns are set in the config.\"\n        )\n\n    if \"RandomFeature\" in reference_methods:\n        if self._datasets is None:\n            raise ValueError(\n                \"Datasets not available for adding RandomFeature. Please keep \"\n                \"preprocessed data available before evaluation.\"\n            )\n\n    if len(self.result.latentspaces._data) == 0:\n        raise ValueError(\n            \"No latent spaces found in results. Please run predict() to \"\n            \"calculate embeddings before evaluation.\"\n        )\n\n    self.result = self.evaluator.evaluate(\n        datasets=self._datasets,\n        result=self.result,\n        ml_model_class=ml_model_class,\n        ml_model_regression=ml_model_regression,\n        params=params,\n        metric_class=metric_class,\n        metric_regression=metric_regression,\n        reference_methods=reference_methods,\n        split_type=split_type,\n        n_downsample=n_downsample,\n    )\n\n    _: Any = self.visualizer._plot_evaluation(result=self.result)\n\n    return self.result\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.fit","title":"<code>fit(config=None, **kwargs)</code>","text":"<p>Trains the model on preprocessed data.</p> <p>Creates and configures a trainer instance, then executes the training process using the preprocessed datasets.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[None, DefaultConfig]]</code> <p>Optional custom configuration for training.</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration parameters as keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If datasets aren't available for training.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def fit(self, config: Optional[Union[None, DefaultConfig]] = None, **kwargs):\n    \"\"\"Trains the model on preprocessed data.\n\n    Creates and configures a trainer instance, then executes the training\n    process using the preprocessed datasets.\n\n    Args:\n        config: Optional custom configuration for training.\n        **kwargs: Additional configuration parameters as keyword arguments.\n\n    Raises:\n        ValueError: If datasets aren't available for training.\n    \"\"\"\n    if self._datasets is None:\n        raise ValueError(\n            \"Datasets not built. Please run the preprocess method first.\"\n        )\n\n    self._trainer = self._trainer_type(\n        trainset=self._datasets.train,\n        validset=self._datasets.valid,\n        result=self.result,\n        config=self.config,\n        model_type=self._model_type,\n        loss_type=self._loss_type,\n        ontologies=self.ontologies,  # Ontix\n        masking_fn=self.masking_fn if hasattr(self, \"masking_fn\") else None,\n        masking_fn_kwargs=(\n            self.masking_fn_kwargs if hasattr(self, \"masking_fn_kwargs\") else None\n        ),\n    )\n\n    trainer_result: Result = self._trainer.train()\n    self.result.update(other=trainer_result)\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.generate","title":"<code>generate(n_samples=None, latent_prior=None, split='test', epoch=-1)</code>","text":"<p>Generates new samples from the model's latent space.</p> <p>This method allows for the generation of new data samples by sampling from the model's latent space. Users can either provide a custom latent prior or specify the number of samples to generate. If a custom latent prior is provided, its batch dimension must be compatible with n_samples.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>Optional[int]</code> <p>The number of samples to generate.</p> <code>None</code> <code>latent_prior</code> <code>Optional[Union[ndarray, Tensor]]</code> <p>Optional custom latent prior distribution. If provided, this will be used for sampling instead of the learned distribution. The prior must either be a single latent vector or a batch of latent vectors matching n_samples.</p> <code>None</code> <code>split</code> <code>str</code> <p>The split to sample from (train, valid, test), default is test.</p> <code>'test'</code> <code>epoch</code> <code>int</code> <p>The epoch to sample from, default is the last epoch (-1).</p> <code>-1</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The generated samples in the input space.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_samples is not a positive integer or if the latent prior has incompatible dimensions.</p> <code>TypeError</code> <p>If latent_prior is not a numpy array or tensor.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def generate(\n    self,\n    n_samples: Optional[int] = None,\n    latent_prior: Optional[Union[np.ndarray, torch.Tensor]] = None,\n    split: str = \"test\",\n    epoch: int = -1,\n) -&gt; torch.Tensor:\n    \"\"\"Generates new samples from the model's latent space.\n\n    This method allows for the generation of new data samples by sampling\n    from the model's latent space. Users can either provide a custom latent\n    prior or specify the number of samples to generate. If a custom latent\n    prior is provided, its batch dimension must be compatible with n_samples.\n\n    Args:\n        n_samples: The number of samples to generate.\n        latent_prior: Optional custom latent prior distribution. If provided,\n            this will be used for sampling instead of the learned distribution.\n            The prior must either be a single latent vector or a batch of\n            latent vectors matching n_samples.\n        split: The split to sample from (train, valid, test), default is test.\n        epoch: The epoch to sample from, default is the last epoch (-1).\n\n    Returns:\n        torch.Tensor: The generated samples in the input space.\n\n    Raises:\n        ValueError: If n_samples is not a positive integer or if the latent\n            prior has incompatible dimensions.\n        TypeError: If latent_prior is not a numpy array or tensor.\n    \"\"\"\n    if not isinstance(n_samples, int) or n_samples &lt;= 0:\n        if latent_prior is None:\n            raise ValueError(\n                \"n_samples must be a positive integer or latent_prior provided.\"\n            )\n\n    if latent_prior is None:\n        latent_prior = self.sample_latent_space(\n            n_samples=n_samples, split=split, epoch=epoch\n        )\n\n    if isinstance(latent_prior, np.ndarray):\n        latent_prior = torch.from_numpy(latent_prior).to(\n            device=self._trainer._model.device,\n            dtype=self._trainer._model.dtype,\n        )\n    if not isinstance(latent_prior, torch.Tensor):\n        raise TypeError(\n            f\"latent_prior must be numpy.ndarray or torch.Tensor, got {type(latent_prior)}.\"\n        )\n    if not latent_prior.shape[1] == self.config.latent_dim:\n        if self._trainer._model._mu.out_features == latent_prior.shape[1]:\n            warnings.warn(\n                f\"latent_prior has latent dimension {latent_prior.shape[1]}, \"\n                \"which matches the input feature dimension of the model. Did you \"\n                \"mean to provide latent vectors of dimension \"\n                \"For Ontix this is the default behaviour and the warning can be ignored. \"\n                f\"{self.config.latent_dim}?\"\n            )\n        else:\n            raise ValueError(\n                f\"latent_prior has incompatible latent dimension {latent_prior.shape[1]}, \"\n                f\"expected {self.config.latent_dim}.\"\n            )\n\n    with torch.no_grad():\n        generated = self.decode(latent=latent_prior)\n        return generated\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.load","title":"<code>load(file_path)</code>  <code>classmethod</code>","text":"<p>Loads a pipeline from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <p>Path to the saved pipeline.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The loaded pipeline instance.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>@classmethod\ndef load(cls, file_path) -&gt; Any:\n    \"\"\"Loads a pipeline from a file.\n\n    Args:\n        file_path: Path to the saved pipeline.\n\n    Returns:\n        The loaded pipeline instance.\n    \"\"\"\n    loader = Loader(file_path)\n    return loader.load()\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.predict","title":"<code>predict(data=None, config=None, from_key=None, to_key=None, **kwargs)</code>","text":"<p>Generates predictions using the trained model.</p> <p>Uses the trained model to make predictions on test data or new data provided by the user. Processes the results and stores them in the result container.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[Union[DataPackage, DatasetContainer, AnnData, MuData]]</code> <p>Optional new data for predictions.</p> <code>None</code> <code>config</code> <code>Optional[Union[None, DefaultConfig]]</code> <p>Optional custom configuration for prediction.</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration parameters as keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If required components aren't initialized.</p> <code>ValueError</code> <p>If no test data is available or data format is invalid.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def predict(\n    self,\n    data: Optional[\n        Union[\n            DataPackage,\n            DatasetContainer,\n            ad.AnnData,\n            MuData,  # ty: ignore[invalid-type-form]\n        ]  # ty: ignore[invalid-type-form]\n    ] = None,  # ty: ignore[invalid-type-form]\n    config: Optional[Union[None, DefaultConfig]] = None,\n    from_key: Optional[str] = None,\n    to_key: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"Generates predictions using the trained model.\n\n    Uses the trained model to make predictions on test data or new data\n    provided by the user. Processes the results and stores them in the\n    result container.\n\n    Args:\n        data: Optional new data for predictions.\n        config: Optional custom configuration for prediction.\n        **kwargs: Additional configuration parameters as keyword arguments.\n\n    Raises:\n        NotImplementedError: If required components aren't initialized.\n        ValueError: If no test data is available or data format is invalid.\n    \"\"\"\n    self._validate_prediction_requirements()\n    if self._trainer is None:\n        raise ValueError(\n            \"Trainer not initialized, call fit first. If you used .save and .load, then you shoul not call .fit, then this is a bug.\"\n            \"In this case please submit an issue.\"\n        )\n\n    self._trainer.setup_trainer(old_model=self.result.model)\n    original_input = data\n    predict_data = self._prepare_prediction_data(data=data)\n\n    predictor_results = self._generate_predictions(\n        predict_data=predict_data,\n    )\n\n    self._process_latent_results(\n        predictor_results=predictor_results, predict_data=predict_data\n    )\n    self._postprocess_reconstruction(\n        predictor_results=predictor_results,\n        original_input=original_input,\n        predict_data=predict_data,\n    )\n    self.result.update(predictor_results)\n    return self.result\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.preprocess","title":"<code>preprocess(config=None, **kwargs)</code>","text":"<p>Filters, normalizes and prepares data for model training.</p> <p>Processes raw input data into the format required by the model and creates train/validation/test splits as needed.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[None, DefaultConfig]]</code> <p>Optional custom configuration for preprocessing.</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration parameters as keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If preprocessor is not initialized.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def preprocess(self, config: Optional[Union[None, DefaultConfig]] = None, **kwargs):\n    \"\"\"Filters, normalizes and prepares data for model training.\n\n    Processes raw input data into the format required by the model and creates\n    train/validation/test splits as needed.\n\n    Args:\n        config: Optional custom configuration for preprocessing.\n        **kwargs: Additional configuration parameters as keyword arguments.\n\n    Raises:\n        NotImplementedError: If preprocessor is not initialized.\n    \"\"\"\n    if self._preprocessor_type is None:\n        raise NotImplementedError(\"Preprocessor not initialized\")\n    self._validate_user_data()\n    if self.preprocessed_data is None:\n        self.preprocessed_data = self._preprocessor.preprocess(\n            raw_user_data=self.raw_user_data,  # type: ignore\n        )\n        self.result.datasets = self.preprocessed_data\n        self._datasets = self.preprocessed_data\n    else:\n        self._datasets = self.preprocessed_data\n        self.result.datasets = self.preprocessed_data\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.run","title":"<code>run(data=None)</code>","text":"<p>Executes the complete pipeline from preprocessing to visualization.</p> <p>Runs all pipeline steps in sequence and returns the result.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[Union[DatasetContainer, DataPackage]]</code> <p>Optional data for prediction (overrides test data).</p> <code>None</code> <p>Returns:</p> Type Description <code>Result</code> <p>Complete pipeline results.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def run(\n    self, data: Optional[Union[DatasetContainer, DataPackage]] = None\n) -&gt; Result:\n    \"\"\"Executes the complete pipeline from preprocessing to visualization.\n\n    Runs all pipeline steps in sequence and returns the result.\n\n    Args:\n        data: Optional data for prediction (overrides test data).\n\n    Returns:\n        Complete pipeline results.\n    \"\"\"\n    self.preprocess()\n    self.fit()\n    self.predict(data=data)\n    self.visualize()\n    return self.result\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.sample_latent_space","title":"<code>sample_latent_space(n_samples, split='test', epoch=-1)</code>","text":"<p>Samples latent space points from the learned distribution.</p> <p>If <code>n_samples</code> is not provided, this method returns one latent point per sample in the specified split (legacy behavior). If <code>n_samples</code> is given, it draws samples from the aggregated posterior distribution of the split.</p> <p>Parameters:</p> Name Type Description Default <code>split</code> <code>str</code> <p>The split to sample from (train, valid, test), default is test.</p> <code>'test'</code> <code>epoch</code> <code>int</code> <p>The epoch to sample from, default is the last epoch (-1).</p> <code>-1</code> <code>n_samples</code> <code>int</code> <p>Optional number of latent points to sample. If None, returns one latent point per available sample in the split.</p> required <p>Returns:</p> Name Type Description <code>z</code> <code>Tensor</code> <p>torch.Tensor - The sampled latent space points.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been trained or latent statistics have not been computed.</p> <code>TypeError</code> <p>If mu or logvar are not numpy arrays.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def sample_latent_space(\n    self,\n    n_samples: int,\n    split: str = \"test\",\n    epoch: int = -1,\n) -&gt; torch.Tensor:\n    \"\"\"Samples latent space points from the learned distribution.\n\n    If `n_samples` is not provided, this method returns one latent point per\n    sample in the specified split (legacy behavior). If `n_samples` is given,\n    it draws samples from the aggregated posterior distribution of the split.\n\n    Args:\n        split: The split to sample from (train, valid, test), default is test.\n        epoch: The epoch to sample from, default is the last epoch (-1).\n        n_samples: Optional number of latent points to sample. If None,\n            returns one latent point per available sample in the split.\n\n    Returns:\n        z: torch.Tensor - The sampled latent space points.\n\n    Raises:\n        ValueError: If the model has not been trained or latent statistics\n            have not been computed.\n        TypeError: If mu or logvar are not numpy arrays.\n    \"\"\"\n\n    if not hasattr(self, \"_trainer\") or self._trainer is None:\n        raise ValueError(\"Model is not trained yet. Please train the model first.\")\n    if self.result.mus is None or self.result.sigmas is None:\n        raise ValueError(\"Model has not learned the latent space distribution yet.\")\n    if not isinstance(n_samples, int) or n_samples &lt;= 0:\n        raise ValueError(\"n_samples must be a positive integer.\")\n\n    mu = self.result.mus.get(split=split, epoch=epoch)\n    logvar = self.result.sigmas.get(split=split, epoch=epoch)\n\n    if not isinstance(mu, np.ndarray):\n        raise TypeError(\n            f\"Expected value to be of type numpy.ndarray, got {type(mu)}.\"\n            \"This can happen if the model was not trained with VAE loss or if you forgot to run predict()\"\n        )\n    if not isinstance(logvar, np.ndarray):\n        raise TypeError(\n            f\"Expected value to be of type numpy.ndarray, got {type(logvar)}.\"\n        )\n\n    mu_t = torch.from_numpy(mu).to(\n        device=self._trainer._model.device, dtype=self._trainer._model.dtype\n    )\n    logvar_t = torch.from_numpy(logvar).to(\n        device=self._trainer._model.device, dtype=self._trainer._model.dtype\n    )\n\n    with torch.no_grad():\n        global_mu = mu_t.mean(dim=0)\n        global_logvar = logvar_t.mean(dim=0)\n\n        mu_exp = global_mu.expand(n_samples, -1)\n        logvar_exp = global_logvar.expand(n_samples, -1)\n\n        z = self._trainer._model.reparameterize(mu_exp, logvar_exp)\n        return z\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.save","title":"<code>save(file_path, save_all=False)</code>","text":"<p>Saves the pipeline to a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path where the pipeline should be saved.</p> required Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def save(self, file_path: str, save_all: bool = False):\n    \"\"\"Saves the pipeline to a file.\n\n    Args:\n        file_path: Path where the pipeline should be saved.\n    \"\"\"\n    saver = Saver(file_path, save_all=save_all)\n    saver.save(self)\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.show_result","title":"<code>show_result(split='all', **kwargs)</code>","text":"<p>Displays key visualizations of model results.</p> <p>This method generates the following visualizations: 1. Loss Curves: Displays the absolute loss curves to provide insights into    the model's training and validation performance over epochs. 2. Latent Space Ridgeline Plot: Visualizes the distribution of the latent    space representations across different dimensions, offering a high-level    overview of the learned embeddings. 3. Latent Space 2D Scatter Plot: Projects the latent space into two dimensions    for a detailed view of the clustering or separation of data points.</p> <p>These visualizations help in understanding the model's performance and the structure of the latent space representations.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def show_result(self, split: str = \"all\", **kwargs):\n    \"\"\"Displays key visualizations of model results.\n\n    This method generates the following visualizations:\n    1. Loss Curves: Displays the absolute loss curves to provide insights into\n       the model's training and validation performance over epochs.\n    2. Latent Space Ridgeline Plot: Visualizes the distribution of the latent\n       space representations across different dimensions, offering a high-level\n       overview of the learned embeddings.\n    3. Latent Space 2D Scatter Plot: Projects the latent space into two dimensions\n       for a detailed view of the clustering or separation of data points.\n\n    These visualizations help in understanding the model's performance and\n    the structure of the latent space representations.\n    \"\"\"\n    print(\"Creating plots ...\")\n\n    params: Optional[Union[List[str], str]] = kwargs.pop(\"params\", None)\n    # Check if params are empty and annotation columns are available in config\n    if params is None and self.config.data_config.annotation_columns:\n        params = self.config.data_config.annotation_columns\n\n    if len(self.result.losses._data) != 0:\n        self.visualizer.show_loss(plot_type=\"absolute\")\n    else:\n        warnings.warn(\n            \"No loss data found in results. Skipping loss curve visualization.\"\n        )\n\n    if len(self.result.latentspaces._data) != 0:\n        self.visualizer.show_latent_space(\n            result=self.result, plot_type=\"Ridgeline\", split=split, param=params\n        )\n        self.visualizer.show_latent_space(\n            result=self.result, plot_type=\"2D-scatter\", split=split, param=params\n        )\n    else:\n        warnings.warn(\n            \"No latent spaces found in results. Please run predict() to \"\n            \"calculate embeddings.\"\n        )\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePipeline.visualize","title":"<code>visualize(config=None, **kwargs)</code>","text":"<p>Creates visualizations of model results and performance.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[None, DefaultConfig]]</code> <p>Optional custom configuration for visualization.</p> <code>None</code> <code>**kwargs</code> <p>Additional configuration parameters.</p> <code>{}</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If visualizer is not initialized.</p> Source code in <code>src/autoencodix/base/_base_pipeline.py</code> <pre><code>def visualize(self, config: Optional[Union[None, DefaultConfig]] = None, **kwargs):\n    \"\"\"Creates visualizations of model results and performance.\n\n    Args:\n        config: Optional custom configuration for visualization.\n        **kwargs: Additional configuration parameters.\n\n    Raises:\n        NotImplementedError: If visualizer is not initialized.\n    \"\"\"\n    if self.visualizer is None:\n        raise NotImplementedError(\"Visualizer not initialized\")\n\n    self.visualizer.visualize(result=self.result, config=self.config)\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePreprocessor","title":"<code>BasePreprocessor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Contains logic for data preprocessing in the Autoencodix framework.</p> <p>This class defines the general preprocessing workflow and provides methods for handling different data modalities and data cases. Subclasses should implement the <code>preprocess</code> method to perform specific preprocessing steps.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>A DefaultConfig object containing preprocessing configurations.</p> <code>processed_data</code> <p>A dictionary to store processed DataPackage objects for each data split.</p> <code>bulk_genes_to_keep</code> <code>Optional[Dict[str, List[str]]]</code> <p>Optional list of genes to keep for bulk data.</p> <code>bulk_scalers</code> <code>Optional[Dict[str, Any]]</code> <p>Optional dictionary of scalers for bulk data.</p> <code>sc_genes_to_keep</code> <code>Optional[Dict[str, List[str]]]</code> <p>Optional dictionary mapping modality keys to lists of genes to keep for single-cell data.</p> <code>sc_scalers</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Optional dictionary mapping modality keys to scalers for single-cell data.</p> <code>sc_general_genes_to_keep</code> <code>Optional[Dict[str, List]]</code> <p>Optional dictionary mapping modality keys to lists of genes to keep filtered by non-SC specific methods.</p> <code>data_readers</code> <code>Dict[Enum, Any]</code> <p>A dictionary mapping DataCase enum values to data reader instances for different modalities.</p> <code>_dataset_container</code> <code>Optional[DatasetContainer]</code> <p>Optional DatasetContainer to hold the processed datasets.</p> Source code in <code>src/autoencodix/base/_base_preprocessor.py</code> <pre><code>class BasePreprocessor(abc.ABC):\n    \"\"\"Contains logic for data preprocessing in the Autoencodix framework.\n\n    This class defines the general preprocessing workflow and provides\n    methods for handling different data modalities and data cases.\n    Subclasses should implement the `preprocess` method to perform\n    specific preprocessing steps.\n\n    Attributes:\n        config: A DefaultConfig object containing preprocessing configurations.\n        processed_data: A dictionary to store processed DataPackage objects for each data split.\n        bulk_genes_to_keep: Optional list of genes to keep for bulk data.\n        bulk_scalers: Optional dictionary of scalers for bulk data.\n        sc_genes_to_keep: Optional dictionary mapping modality keys to lists of genes to keep for single-cell data.\n        sc_scalers: Optional dictionary mapping modality keys to scalers for single-cell data.\n        sc_general_genes_to_keep: Optional dictionary mapping modality keys to lists of genes to keep filtered by non-SC specific methods.\n        data_readers: A dictionary mapping DataCase enum values to data reader instances for different modalities.\n        _dataset_container: Optional DatasetContainer to hold the processed datasets.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: DefaultConfig,\n        ontologies: Optional[Union[Tuple[Any, Any], Dict[Any, Any]]] = None,\n    ):\n        \"\"\"Initializes the BasePreprocessor with a configuration object.\n\n        Args :\n            config: A DefaultConfig object containing preprocessing configurations.\n            ontologies: Ontology information, if provided for Ontix.\n        \"\"\"\n        self.config = config\n        self._dataset_container: Optional[DatasetContainer] = None\n        self.processed_data = Dict[str, Dict[str, Union[Any, DataPackage]]]\n        self.bulk_genes_to_keep: Optional[Dict[str, List[str]]] = None\n        self.bulk_scalers: Optional[Dict[str, Any]] = None\n        self.sc_genes_to_keep: Optional[Dict[str, List[str]]] = None\n        self.sc_scalers: Optional[Dict[str, Dict[str, Any]]] = None\n        self.sc_general_genes_to_keep: Optional[Dict[str, List]] = None\n        self._ontologies: Optional[Union[Tuple[Any, Any], Dict[Any, Any]]] = ontologies\n        self.data_readers: Dict[Enum, Any] = {\n            DataCase.MULTI_SINGLE_CELL: SingleCellDataReader(),\n            DataCase.MULTI_BULK: BulkDataReader(config=self.config),\n            DataCase.BULK_TO_BULK: BulkDataReader(config=self.config),\n            DataCase.SINGLE_CELL_TO_SINGLE_CELL: SingleCellDataReader(),\n            DataCase.IMG_TO_BULK: {\n                \"bulk\": BulkDataReader(config=self.config),\n                \"img\": ImageDataReader(config=self.config),\n            },\n            DataCase.SINGLE_CELL_TO_IMG: {\n                \"sc\": SingleCellDataReader(),\n                \"img\": ImageDataReader(config=self.config),\n            },\n            DataCase.IMG_TO_IMG: ImageDataReader(config=self.config),\n        }\n\n    @abc.abstractmethod\n    def preprocess(\n        self,\n        raw_user_data: Optional[DataPackage] = None,\n        predict_new_data: bool = False,\n    ) -&gt; DatasetContainer:\n        \"\"\"To be implemented by subclasses for specific preprocessing steps.\n        Args:\n            raw_user_data: Users can provide raw data. This is an alternative way of\n                providing data via filepaths in the config. If this param is passed, we skip the data reading step.\n            predict_new_data: Indicates whether the user wants to predict with unseen data.\n                If this is the case, we don't split the data and only prerpocess.\n        \"\"\"\n        pass\n\n    def _general_preprocess(\n        self,\n        raw_user_data: Optional[DataPackage] = None,\n        predict_new_data: bool = False,\n    ) -&gt; Dict[str, Dict[str, Union[Any, DataPackage]]]:\n        \"\"\"Orchestrates the preprocessing steps.\n\n        This method determines the data case from the configuration and calls\n        the appropriate processing function for that data case.\n\n        Args:\n            raw_user_data: Optional DataPackage containing user-provided data.\n                If provided, the data reading step is skipped.\n            predict_new_data: Boolean indicating whether to preprocess new unseen data\n                without splitting it into train/validation/test sets.\n\n        Returns:\n            A dictionary containing processed DataPackage objects for each data split\n            (e.g., 'train', 'validation', 'test').\n\n        Raises:\n            ValueError: If an unsupported data case is encountered.\n        \"\"\"\n        self.predict_new_data = predict_new_data\n        datacase = self.config.data_case\n        if datacase is None:\n            raise TypeError(\n                \"datacase can't be None. Please ensure the configuration specifies a valid DataCase.\"\n            )\n        if raw_user_data is None:\n            self.from_key, self.to_key = self._get_translation_keys()\n        else:\n            self.from_key, self.to_key = self._get_user_translation_keys(\n                raw_user_data=raw_user_data\n            )\n        process_function = self._get_process_function(datacase=datacase)\n        if process_function:\n            return process_function(raw_user_data=raw_user_data)\n        else:\n            raise ValueError(f\"Unsupported data case: {datacase}\")\n\n    def _get_process_function(self, datacase: DataCase) -&gt; Any:\n        \"\"\"Returns the appropriate processing function based on the data case.\n\n        Args:\n            datacase: The DataCase enum value representing the current data case.\n\n        Returns:\n            A callable function that performs the preprocessing for the given data case,\n            or None if the data case is not supported.\n        \"\"\"\n        process_map = {\n            DataCase.MULTI_SINGLE_CELL: self._process_multi_single_cell,\n            DataCase.MULTI_BULK: self._process_multi_bulk_case,\n            DataCase.BULK_TO_BULK: self._process_multi_bulk_case,\n            DataCase.SINGLE_CELL_TO_SINGLE_CELL: self._process_multi_single_cell,\n            DataCase.IMG_TO_BULK: self._process_img_to_bulk_case,\n            DataCase.SINGLE_CELL_TO_IMG: self._process_sc_to_img_case,\n            DataCase.IMG_TO_IMG: self._process_img_to_img_case,\n        }\n        return process_map.get(datacase)\n\n    def _process_data_case(\n        self, data_package: DataPackage, modality_processors: Dict[Any, Any]\n    ) -&gt; Union[Dict[str, Dict[str, Union[Any, DataPackage]]], Dict[str, Any]]:\n        \"\"\"Processes the data package based on the provided modality processors.\n\n        This method handles the common preprocessing steps for different data cases,\n        including splitting the data package, removing NaNs, and applying\n        modality-specific processors.\n\n        Args::\n            data_package: The DataPackage object to be processed.\n            modality_processors: A dictionary mapping modality keys (e.g., 'multi_sc', 'from_modality')\n                to callable processor functions that will be applied to the corresponding modality data.\n\n        Returns:\n            A dictionary containing processed DataPackage objects for each data split.\n        \"\"\"\n        if self.predict_new_data:\n            # we get the data modality keys from this structure in postsplit processing\n            # for predict_new data there do not exits real splits, because all is \"test\" data\n            # but the preprocessing code expects this splits, so we mock them\n            # use train, because processing logic expects train split\n            mock_split: Dict[str, Dict[str, Union[Any, DataPackage]]] = {\n                \"test\": {\n                    \"data\": data_package,\n                    \"indices\": {\"paired\": np.array([])},\n                },\n                \"valid\": {\"data\": None, \"indices\": {\"paired\": np.array([])}},\n                \"train\": {\"data\": data_package, \"indices\": {\"paired\": np.array([])}},\n            }\n            if self.config.skip_preprocessing:\n                return mock_split\n\n            clean_package = self._remove_nans(data_package=data_package)\n            mock_split[\"test\"][\"data\"] = clean_package\n            for modality_key, (\n                presplit_processor,\n                postsplit_processor,\n            ) in modality_processors.items():\n                modality_data = clean_package[modality_key]\n                if modality_data:\n                    processed_modality_data = presplit_processor(modality_data)\n                    # mock the split\n                    clean_package[modality_key] = processed_modality_data\n                    mock_split[\"test\"][\"data\"] = clean_package\n                    mock_split = postsplit_processor(mock_split)\n            return mock_split\n        # normal case without new data -----------------------------------\n        if self.config.skip_preprocessing:\n            split_packages, _ = self._split_data_package(data_package=data_package)\n            return split_packages\n        clean_package = self._remove_nans(data_package=data_package)\n        for modality_key, (presplit_processor, _) in modality_processors.items():\n            modality_data = clean_package[modality_key]\n            if modality_data:\n                processed_modality_data = presplit_processor(modality_data)\n                clean_package[modality_key] = processed_modality_data\n        split_packages, indices = self._split_data_package(data_package=clean_package)\n        processed_splits = {}\n        for modality_key, (_, postsplit_processor) in modality_processors.items():\n            split_packages = postsplit_processor(split_packages)\n        for split_name, split_package in split_packages.items():\n            split_indices = {\n                name: {\n                    split: idx\n                    for split, idx in indices[name].items()\n                    if split == split_name\n                }\n                for name in indices.keys()\n            }\n            processed_splits[split_name] = {\n                \"data\": split_package[\"data\"],\n                \"indices\": split_indices,\n            }\n        return processed_splits\n\n    def _process_multi_single_cell(\n        self, raw_user_data: Optional[DataPackage] = None\n    ) -&gt; Dict[str, Dict[str, Union[Any, DataPackage]]]:\n        \"\"\"Process MULTI_SINGLE_CELL case\n\n        Reads multi-single-cell data, performs data splitting, NaN removal,\n        and applies single-cell specific filtering.\n        Args:\n            raw_user_data: Optional DataPackage containing user-provided data.\n\n        Returns:\n            A dictionary containing processed DataPackage objects for each data split.\n        Raises:\n            ValueError: If multi_sc in data_package is None.\n\n        \"\"\"\n        if raw_user_data is None:\n            screader = self.data_readers[DataCase.MULTI_SINGLE_CELL]  # type: ignore\n\n            mudata = screader.read_data(config=self.config)\n            data_package: DataPackage = DataPackage()\n            data_package.multi_sc = mudata\n        else:\n            data_package = raw_user_data\n        if self.config.requires_paired:\n            common_ids = data_package.get_common_ids()\n            if data_package.multi_sc is None:\n                raise ValueError(\"multi_sc in data_package is None\")\n            data_package.multi_sc = {\n                \"multi_sc\": data_package.multi_sc[\"multi_sc\"][common_ids]\n            }\n\n        def presplit_processor(modality_data: Any) -&gt; Any:\n            if modality_data is None:\n                return modality_data\n            sc_filter = SingleCellFilter(\n                data_info=self.config.data_config.data_info, config=self.config\n            )\n            return sc_filter.presplit_processing(multi_sc=modality_data)\n\n        def postsplit_processor(\n            split_data: Dict[str, Dict[str, Any]],\n        ) -&gt; Dict[str, Dict[str, Any]]:\n            return self._postsplit_multi_single_cell(\n                split_data=split_data, datapackage_key=\"multi_sc\"\n            )\n\n        return self._process_data_case(\n            data_package,\n            modality_processors={\"multi_sc\": (presplit_processor, postsplit_processor)},\n        )\n\n    def _process_multi_bulk_case(\n        self,\n        raw_user_data: Optional[DataPackage] = None,\n    ) -&gt; Dict[str, Dict[str, Union[Any, DataPackage]]]:\n        \"\"\"\n        Process MULTI_BULK case.\n\n        Reads multi-bulk data, performs data splitting, NaN removal,\n        and applies filtering and scaling to bulk dataframes.\n        Args:\n            raw_user_data: Optional DataPackage containing user-provided data.\n\n        Returns:\n            A dictionary containing processed DataPackage objects for each data split.\n        \"\"\"\n        if raw_user_data is None:\n            bulkreader = self.data_readers[DataCase.MULTI_BULK]\n            bulk_dfs, annotation = bulkreader.read_data()\n\n            data_package = DataPackage(multi_bulk=bulk_dfs, annotation=annotation)\n        else:\n            data_package = raw_user_data\n        if self.config.requires_paired:\n            common_ids = data_package.get_common_ids()\n            unpaired_data = data_package.multi_bulk\n            unpaired_anno = data_package.annotation\n            if unpaired_anno is None:\n                raise ValueError(\"annotation attribute of datapackge cannot be None\")\n            if unpaired_data is None:\n                raise ValueError(\"multi_bulk attribute of datapackge cannot be None\")\n            data_package.multi_bulk = {\n                k: v.loc[common_ids] for k, v in unpaired_data.items()\n            }\n\n            data_package.annotation = {\n                k: v.loc[common_ids]  # ty: ignore\n                for k, v in unpaired_anno.items()  # ty: ignore\n            }\n\n        def presplit_processor(\n            modality_data: Dict[str, Union[pd.DataFrame, None]],\n        ) -&gt; Dict[str, Union[pd.DataFrame, None]]:\n            \"\"\"For the multi_bulk modality we perform all operations after splitting at the moment.\"\"\"\n            return modality_data\n\n        def postsplit_processor(\n            split_data: Dict[str, Dict[str, Any]],\n        ) -&gt; Dict[str, Dict[str, Any]]:\n            return self._postsplit_multi_bulk(split_data=split_data)\n\n        return self._process_data_case(\n            data_package,\n            modality_processors={\n                \"multi_bulk\": (presplit_processor, postsplit_processor)\n            },\n        )\n\n    def _calc_k_filter(\n        self, i: int, remainder: int, base_features: int\n    ) -&gt; Optional[int]:\n        if self.config.k_filter is None:\n            return None\n        extra = 1 if i &lt; remainder else 0\n        return base_features + extra\n\n    def _postsplit_multi_single_cell(\n        self,\n        split_data: Dict[str, Dict[str, Any]],\n        datapackage_key: str = \"multi_sc\",\n        modality_key: Optional[str] = None,\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Post-split processing for multi-single-cell data.\n        This method applies filtering and scaling to the single-cell data after it has been split.\n        Now supports multiple MuData objects in the input dictionary.\n\n        Args:\n            split_data: A dictionary containing the split data for each data split.\n            datapackage_key: The key in the DataPackage that contains the multi-single-cell data.\n            modality_key: Optional specific modality key for backward compatibility.\n                        If provided, only processes that specific modality.\n                        If None, processes all modalities in the dictionary.\n\n        Returns:\n            A dictionary containing processed DataPackage objects for each data split.\n\n        Raises:\n            ValueError: If the train split data is None.\n        \"\"\"\n        processed_splits: Dict[str, Dict[str, Any]] = {}\n        train_split: Optional[Dict[str, Any]] = split_data.get(\"train\")\n\n        if train_split is None:\n            raise ValueError(\n                \"Train split data is None. Ensure that the data package contains valid train data.\"\n            )\n\n        train_data: Optional[Any] = train_split.get(\"data\")\n        if train_data is None:\n            raise ValueError(\n                \"Train split data is None. Ensure that the data package contains valid train data.\"\n            )\n\n        # Get all modality keys from the train data\n        mudata_dict = train_data[datapackage_key]\n\n        if modality_key is not None:\n            if modality_key not in mudata_dict:\n                raise ValueError(\n                    f\"Specified modality_key '{modality_key}' not found in {list(mudata_dict.keys())}\"\n                )\n            modality_keys = [modality_key]\n            print(\n                f\"Processing single modality (backward compatibility): {modality_key}\"\n            )\n        else:\n            modality_keys = list(mudata_dict.keys())\n            print(f\"Processing {len(modality_keys)} MuData objects: {modality_keys}\")\n\n        # Initialize storage for scalers and gene filters for each modality\n        # if we do this for the first time, we need a train split and we dont\n        # fitted any scalers or features to keep yet.\n        # that's why in the predict_new case we can keep the mocksplit for train None\n        # because we never get in this if\n        if (\n            self.sc_scalers is None\n            and self.sc_genes_to_keep is None\n            and self.sc_general_genes_to_keep is None\n        ) or (\"modality\" in datapackage_key):\n            # Process each MuData object in the train split\n            processed_mudata_dict = {}\n            all_scalers = {}\n            all_sc_genes_to_keep = {}\n            all_general_genes_to_keep = {}\n\n            for current_modality_key in modality_keys:\n                print(f\"Processing train modality: {current_modality_key}\")\n\n                sc_filter = SingleCellFilter(\n                    data_info=self.config.data_config.data_info, config=self.config\n                )\n\n                # Single-cell specific filtering\n                filtered_train, sc_genes_to_keep = sc_filter.sc_postsplit_processing(\n                    mudata=mudata_dict[current_modality_key]\n                )\n\n                # General post-processing\n                processed_train, general_genes_to_keep, scalers = (\n                    sc_filter.general_postsplit_processing(\n                        mudata=filtered_train, scaler_map=None, gene_map=None\n                    )\n                )\n\n                # Store processed data and filters for this modality\n                processed_mudata_dict[current_modality_key] = processed_train\n                all_scalers[current_modality_key] = scalers\n                all_sc_genes_to_keep[current_modality_key] = sc_genes_to_keep\n                all_general_genes_to_keep[current_modality_key] = general_genes_to_keep\n\n            # Store all scalers and gene filters\n            self.sc_scalers = all_scalers\n            self.sc_genes_to_keep = all_sc_genes_to_keep\n            self.sc_general_genes_to_keep = all_general_genes_to_keep\n\n            # Update train data with processed MuData objects\n            train_data[datapackage_key] = processed_mudata_dict\n\n        else:\n            # Use existing scalers and gene filters\n            all_scalers = self.sc_scalers  # type: ignore\n            all_sc_genes_to_keep = self.sc_genes_to_keep  # type: ignore\n            all_general_genes_to_keep = self.sc_general_genes_to_keep  # type: ignore\n\n        # Store processed train split\n        processed_splits[\"train\"] = {\n            \"data\": train_data,\n            \"indices\": split_data[\"train\"][\"indices\"],\n        }\n\n        # Process other splits (val, test, etc.)\n        for split, split_package in split_data.items():\n            if split == \"train\":\n                continue\n\n            data_package = split_package[\"data\"]\n            if data_package is None:\n                processed_splits[split] = split_package\n                continue\n\n            print(f\"Processing {split} split\")\n            processed_mudata_dict = {}\n\n            # Process each MuData object in this split\n            for current_modality_key in modality_keys:\n                print(f\"Processing {split} modality: {current_modality_key}\")\n\n                sc_filter = SingleCellFilter(\n                    data_info=self.config.data_config.data_info, config=self.config\n                )\n\n                # Apply single-cell filtering using train-derived gene map\n                filtered_sc_data, _ = sc_filter.sc_postsplit_processing(\n                    mudata=data_package[datapackage_key][current_modality_key],\n                    gene_map=all_sc_genes_to_keep[current_modality_key],\n                )\n\n                # Apply general processing using train-derived scalers and gene map\n                processed_general_data, _, _ = sc_filter.general_postsplit_processing(\n                    mudata=filtered_sc_data,\n                    gene_map=all_general_genes_to_keep[current_modality_key],\n                    scaler_map=all_scalers[current_modality_key],\n                )\n\n                processed_mudata_dict[current_modality_key] = processed_general_data\n\n            # Update data package with all processed MuData objects\n            data_package[datapackage_key] = processed_mudata_dict\n\n            processed_splits[split] = {\n                \"data\": data_package,\n                \"indices\": split_package[\"indices\"],\n            }\n\n        return processed_splits\n\n    def _postsplit_multi_bulk(\n        self,\n        split_data: Dict[str, Dict[str, Any]],\n        datapackage_key: str = \"multi_bulk\",\n    ) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"Post-split processing for multi-bulk data.\n\n        This method applies filtering and scaling to the bulk dataframes after they have been split.\n\n        Args:\n            split_data: A dictionary containing the split data for each data split.\n            datapackage_key: The key in the DataPackage that contains the multi-bulk data.\n        Returns:\n            A dictionary containing processed DataPackage objects for each data split.\n        Raises:\n            ValueError: If the train split data is None.\n        \"\"\"\n\n        train_split: Optional[Dict[str, Any]] = split_data.get(\"train\")\n        if train_split is None:\n            raise ValueError(\n                \"Train split data is None. Ensure that the data package contains valid train data.\"\n            )\n        train_data: Optional[Any] = train_split.get(\"data\")\n        genes_to_keep_map: Dict[str, List[str]] = {}\n        scalers: Dict[str, Any] = {}\n        processed_splits: Dict[str, Dict[str, Any]] = {}\n\n        if (self.bulk_scalers is None and self.bulk_genes_to_keep is None) or (\n            \"modality\" in datapackage_key\n        ):\n            if train_data is None:\n                raise ValueError(\n                    \"Train split data is None. Ensure that the data package contains valid train data.\"\n                )\n            n_modalities: int = len(train_data[datapackage_key].keys())\n            remainder: int = 0\n            base_features = 0\n            if self.config.k_filter is not None:\n                base_features = self.config.k_filter // n_modalities\n                remainder = self.config.k_filter % n_modalities\n\n            # Get valid modality keys (those that are not None)\n            modality_keys = [\n                k for k, v in train_data[datapackage_key].items() if v is not None\n            ]\n\n            for i, k in enumerate(modality_keys):\n                v = train_data[datapackage_key][k]\n                cur_k_filter = self._calc_k_filter(\n                    i=i, base_features=base_features, remainder=remainder\n                )\n                self.config.data_config.data_info[k].k_filter = cur_k_filter\n\n                data_processor = DataFilter(\n                    data_info=self.config.data_config.data_info[k],\n                    config=self.config,\n                    ontologies=self._ontologies,\n                )\n                filtered_df, genes_to_keep = data_processor.filter(df=v)\n                scaler = data_processor.fit_scaler(df=filtered_df)\n                genes_to_keep_map[k] = genes_to_keep\n                scalers[k] = scaler\n                scaled_df = data_processor.scale(df=filtered_df, scaler=scaler)\n                train_data[datapackage_key][k] = scaled_df\n                # Check if indices stayed the same after filtering\n                if not filtered_df.index.equals(v.index):\n                    mismatched_indices = filtered_df.index.symmetric_difference(v.index)\n                    raise ValueError(\n                        f\"Indices mismatch after filtering for modality {k}. \"\n                        f\"Mismatched indices: {mismatched_indices}. \"\n                        \"Ensure filtering does not alter the indices.\"\n                    )\n\n            self.bulk_scalers = scalers\n            self.bulk_genes_to_keep = genes_to_keep_map  # type: ignore\n        else:\n            scalers, genes_to_keep_map = self.bulk_scalers, self.bulk_genes_to_keep  # type: ignore\n\n        processed_splits[\"train\"] = {\n            \"data\": train_data,\n            \"indices\": split_data[\"train\"][\"indices\"],\n        }\n\n        for split_name, split_package in split_data.items():\n            if split_name == \"train\":\n                continue\n            if split_package[\"data\"] is None:\n                processed_splits[split_name] = split_data[split_name]\n                continue\n\n            processed_package = split_package[\"data\"]\n            for k, v in processed_package[datapackage_key].items():\n                if v is None:\n                    continue\n                data_processor = DataFilter(\n                    data_info=self.config.data_config.data_info[k],\n                    config=self.config,\n                    ontologies=self._ontologies,\n                )\n                filtered_df, _ = data_processor.filter(\n                    df=v, genes_to_keep=genes_to_keep_map[k]\n                )\n                scaled_df = data_processor.scale(df=filtered_df, scaler=scalers[k])\n                processed_package[datapackage_key][k] = scaled_df\n                if not filtered_df.index.equals(v.index):\n                    raise ValueError(\n                        f\"Indices mismatch after filtering for modality {k}. \"\n                        \"Ensure filtering does not alter the indices.\"\n                    )\n\n            processed_splits[split_name] = {\n                \"data\": processed_package,\n                \"indices\": split_package[\"indices\"],\n            }\n\n        return processed_splits\n\n    def _process_img_to_bulk_case(\n        self, raw_user_data: Optional[DataPackage] = None\n    ) -&gt; Dict[str, Dict[str, Union[Any, DataPackage]]]:\n        \"\"\"Process IMG_TO_BULK case\n\n        Reads image and bulk data, prepares from/to modalities (IMG-&gt;BULK or BULK-&gt;IMG),\n        performs data splitting, NaN removal, and applies normalization to image data\n        and filtering/scaling to bulk dataframes.\n        Args:\n            raw_user_data: Optional DataPackage containing user-provided data.\n                If provided, the data reading step is skipped.\n        Returns:\n            A dictionary containing processed DataPackage objects for each data split.\n        Raises:\n            TypeError: If from_key or to_key is None, indicating that translation keys must be specified.\n        \"\"\"\n\n        if raw_user_data is None:\n            bulkreader = self.data_readers[DataCase.IMG_TO_BULK][\"bulk\"]\n            imgreader = self.data_readers[DataCase.IMG_TO_BULK][\"img\"]\n\n            bulk_dfs, annotation_bulk = bulkreader.read_data()\n            images, annotation_img = imgreader.read_data(config=self.config)\n\n            annotation = {**annotation_bulk, **annotation_img}\n\n            data_package = DataPackage(\n                multi_bulk=bulk_dfs, img=images, annotation=annotation\n            )\n\n        else:\n            data_package = raw_user_data\n\n        if self.config.requires_paired:\n            common_ids = data_package.get_common_ids()\n\n            images = data_package.img\n            if images is None:\n                raise ValueError(\"Images cannot be None\")\n            data_package.img = {\n                k: self.filter_imgdata_list(img_list=v, ids=common_ids)\n                for k, v in images.items()\n            }\n            unpaired_data = data_package.multi_bulk\n            unpaired_anno = data_package.annotation\n            if unpaired_anno is None:\n                raise ValueError(\"annotation attribute of datapackge cannot be None\")\n            if unpaired_data is None:\n                raise ValueError(\"multi_bulk attribute of datapackge cannot be None\")\n            data_package.multi_bulk = {\n                k: v.loc[common_ids] for k, v in unpaired_data.items()\n            }\n\n            data_package.annotation = {\n                k: v.loc[common_ids]  # ty: ignore\n                for k, v in unpaired_anno.items()  # ty: ignore\n            }\n\n        def presplit_processor(\n            modality_data: Dict[str, Union[pd.DataFrame, List[ImgData]]],\n        ) -&gt; Dict[str, Union[pd.DataFrame, List[ImgData]]]:\n            for modality_key, data in modality_data.items():\n                if self._is_image_data(data=data):\n                    modality_data[modality_key] = self._normalize_image_data(\n                        images=data,  # type: ignore\n                        info_key=modality_key,  # type: ignore\n                    )\n            # we don't need to filter bulk data here\n            # because we do it in the postsplit step\n            return modality_data\n\n        def postsplit_processor(\n            split_data: Dict[str, Dict[str, Any]], datapackage_key: str\n        ) -&gt; Dict[str, Dict[str, Any]]:\n            if datapackage_key == \"multi_bulk\":\n                return self._postsplit_multi_bulk(\n                    split_data=split_data, datapackage_key=datapackage_key\n                )\n            return split_data  # for img data we don't need to do anything\n\n        return self._process_data_case(\n            data_package,\n            modality_processors={\n                \"multi_bulk\": (  # TODO change to multi_bulk and img for all translation cases and ajdust processors accordingly\n                    lambda data: presplit_processor(modality_data=data),\n                    lambda data: postsplit_processor(\n                        split_data=data, datapackage_key=\"multi_bulk\"\n                    ),\n                ),\n                \"img\": (\n                    lambda data: presplit_processor(modality_data=data),\n                    lambda data: postsplit_processor(\n                        split_data=data, datapackage_key=\"img\"\n                    ),\n                ),\n            },\n        )\n\n    def _process_sc_to_img_case(\n        self, raw_user_data: Optional[DataPackage] = None\n    ) -&gt; Dict[str, Dict[str, Union[Any, DataPackage]]]:\n        \"\"\"Process SC_TO_IMG case.\n\n        Reads single-cell and image data, prepares from/to modalities (SC-&gt;IMG or IMG-&gt;SC),\n        performs data splitting, NaN removal, and applies single-cell specific filtering\n        to single-cell data and normalization to image data.\n\n        Args:\n            raw_user_data: Optional DataPackage containing user-provided data.\n\n        Returns:\n            A dictionary containing processed DataPackage objects for each data split.\n        \"\"\"\n        if raw_user_data is None:\n            screader = self.data_readers[DataCase.SINGLE_CELL_TO_IMG][\"sc\"]\n            imgreader = self.data_readers[DataCase.SINGLE_CELL_TO_IMG][\"img\"]\n\n            # only one mudata type in this case we know this\n            mudata_dict = screader.read_data(config=self.config)\n            images, annotation = imgreader.read_data(config=self.config)\n\n            data_package = DataPackage(\n                multi_sc=mudata_dict, img=images, annotation=annotation\n            )\n        else:\n            data_package = raw_user_data\n        if self.config.requires_paired:\n            common_ids = data_package.get_common_ids()\n            if data_package.multi_sc is None:\n                raise ValueError(\"multi_sc in data_package is None\")\n            data_package.multi_sc = {\n                \"multi_sc\": data_package.multi_sc[\"multi_sc\"][common_ids]\n            }\n            images = data_package.img\n            if images is None:\n                raise ValueError(\"Images cannot be None\")\n            data_package.img = {\n                k: self.filter_imgdata_list(img_list=v, ids=common_ids)\n                for k, v in images.items()\n            }\n\n        def presplit_processor(\n            modality_data: Dict[str, Union[Any, List[ImgData]]],\n        ) -&gt; Dict[str, Union[Any, List[ImgData]]]:\n            was_image = False\n            for modality_key, data in modality_data.items():\n                if self._is_image_data(data=data):\n                    was_image = True\n                    modality_data[modality_key] = self._normalize_image_data(\n                        images=data,  # type: ignore\n                        info_key=modality_key,  # type: ignore\n                    )\n\n            if was_image:\n                return modality_data\n            else:\n                sc_filter = SingleCellFilter(\n                    data_info=self.config.data_config.data_info, config=self.config\n                )\n                return sc_filter.presplit_processing(multi_sc=modality_data)\n\n        def postsplit_processor(\n            split_data: Dict[str, Dict[str, Any]], datapackage_key: str\n        ) -&gt; Dict[str, Dict[str, Any]]:\n            if datapackage_key == \"multi_sc\":\n                return self._postsplit_multi_single_cell(\n                    split_data=split_data, datapackage_key=datapackage_key\n                )\n            # No postsplit processing needed for image data\n            return split_data\n\n        return self._process_data_case(\n            data_package,\n            modality_processors={\n                \"multi_sc\": (\n                    lambda data: presplit_processor(modality_data=data),\n                    lambda data: postsplit_processor(\n                        split_data=data, datapackage_key=\"multi_sc\"\n                    ),\n                ),\n                \"img\": (\n                    lambda data: presplit_processor(modality_data=data),\n                    lambda data: postsplit_processor(\n                        split_data=data, datapackage_key=\"img\"\n                    ),\n                ),\n            },\n        )\n\n    def _process_img_to_img_case(\n        self, raw_user_data: Optional[DataPackage] = None\n    ) -&gt; Dict[str, DataPackage]:\n        \"\"\"Process IMG_TO_IMG case.\n\n        Reads image data for from/to modalities, performs data splitting,\n        NaN removal, and applies normalization to both from and to image data.\n\n        Args:\n            raw_user_data: Optional DataPackage containing user-provided data.\n                If provided, the data reading step is skipped.\n        Returns:\n            A dictionary containing processed DataPackage objects for each data split.\n        Raises:\n            TypeError: If from_key or to_key is None, indicating that translation keys must be specified.\n        \"\"\"\n        if raw_user_data is None:\n            imgreader = self.data_readers[DataCase.IMG_TO_IMG]\n            images, annotation = imgreader.read_data(config=self.config)\n\n            data_package = DataPackage(img=images, annotation=annotation)\n        else:\n            data_package = raw_user_data\n\n        if self.config.requires_paired:\n            common_ids = data_package.get_common_ids()\n\n            images = data_package.img\n            if images is None:\n                raise ValueError(\"Images cannot be None\")\n            data_package.img = {\n                k: self.filter_imgdata_list(img_list=v, ids=common_ids)\n                for k, v in images.items()\n            }\n\n        def presplit_processor(modality_data: Dict[str, List]) -&gt; Dict[str, List]:\n            \"\"\"Processes img-to-img modality data with normalization for images.\"\"\"\n            print(\"calling normalize image in _process_ing_to_img_case\")\n            return {\n                k: self._normalize_image_data(v, k) for k, v in modality_data.items()\n            }\n\n        def postsplit_processor(\n            split_data: Dict[str, Dict[str, Any]],\n        ) -&gt; Dict[str, Dict[str, Any]]:\n            \"\"\"No postsplit processing needed for image data.\"\"\"\n            return split_data\n\n        return self._process_data_case(\n            data_package,\n            modality_processors={\n                \"img\": (\n                    lambda data: presplit_processor(\n                        data,\n                    ),\n                    postsplit_processor,\n                ),\n            },\n        )\n\n    # This method would be inside your GeneralPreprocessor or a similar class\n    def _split_data_package(\n        self, data_package: DataPackage\n    ) -&gt; Tuple[Dict[str, Optional[Dict[str, Any]]], Dict[str, Any]]:\n        \"\"\"Splits a data package into train/validation/test sets.\n\n        This method first uses PairedUnpairedSplitter to generate a single,\n        synchronized set of indices for all modalities. It then uses\n        DataPackageSplitter to apply these indices to the data.\n\n        Args:\n            data_package: The DataPackage to be split.\n\n        Returns:\n            A tuple containing:\n            1. A dictionary of the split DataPackages.\n            2. A dictionary of the synchronized integer indices used for the split.\n        \"\"\"\n        pairing_splitter = PairedUnpairedSplitter(\n            data_package=data_package, config=self.config\n        )\n        split_indices_config = pairing_splitter.split()\n        data_package_splitter = DataPackageSplitter(\n            data_package=data_package,\n            config=self.config,\n            indices=split_indices_config,\n        )\n        split_datasets = data_package_splitter.split()\n        return split_datasets, split_indices_config\n\n    def _is_image_data(self, data: Any) -&gt; bool:\n        \"\"\"Check if data is image data.\n\n        Determines if the provided data is a list of objects that are considered\n        image data based on having an 'img' attribute.\n\n        Args:\n            data: The data to check.\n\n        Returns:\n            True if the data is image data, False otherwise.\n        \"\"\"\n        if data is None:\n            return False\n        if isinstance(data, list) and hasattr(data[0], \"img\"):\n            return True\n        return False\n\n    def _remove_nans(self, data_package: DataPackage) -&gt; DataPackage:\n        \"\"\"Remove NaN values from the data package.\n\n        Utilizes NaNRemover to identify and remove rows containing NaN values\n        in relevant annotation columns within the DataPackage.\n\n        Args:\n            data_package: The DataPackage from which to remove NaNs.\n\n        Returns:\n            The DataPackage with NaN values removed.\n        \"\"\"\n        nanremover = NaNRemover(\n            config=self.config,\n        )\n        return nanremover.remove_nan(data=data_package)\n\n    def _normalize_image_data(self, images: List, info_key: str) -&gt; List:\n        \"\"\"Process images with normalization.\n\n        Normalizes a list of image data objects using ImageNormalizer based on\n        the scaling method specified in the configuration for the given info_key.\n\n        Args:\n            images: A list of image data objects (each having an 'img' attribute).\n            info_key: The key referencing data information in the configuration to get the scaling method.\n\n        Returns:\n            A list of processed image data objects with normalized image data.\n        \"\"\"\n\n        scaling_method = self.config.data_config.data_info[info_key].scaling\n        if scaling_method == \"NOTSET\":\n            scaling_method = self.config.scaling\n        processed_images = []\n        normalizer = ImageNormalizer()  # Instance created once here\n\n        for img in images:\n            img.img = normalizer.normalize_image(  # Modify directly\n                image=img.img, method=scaling_method\n            )\n            processed_images.append(img)\n\n        return processed_images\n\n    def _get_translation_keys(self) -&gt; Tuple[Optional[str], Optional[str]]:\n        \"\"\"\n        Extract from and to keys from config.\n\n        Retrieves the 'from' and 'to' modality keys from the data configuration\n        based on the 'translate_direction' setting.\n\n        Returns:\n            A tuple containing the from_key and to_key as strings, or None if not found.\n\n        Raises:\n            ValueError: If neither 'from' nor 'to' keys are found in the data configuration.\n            TypeError: If the translate_direction is not set for the data_info.\n        \"\"\"\n        from_key, to_key = None, None\n        for k, v in self.config.data_config.data_info.items():\n            if v.translate_direction is None:\n                continue\n            if v.translate_direction == \"from\":\n                from_key = k\n            if v.translate_direction == \"to\":\n                to_key = k\n        return from_key, to_key\n\n    def _get_user_translation_keys(self, raw_user_data: DataPackage):\n        if len(raw_user_data.from_modality) == 0:  # type: ignore\n            return None, None\n        elif len(raw_user_data.to_modality) == 0:  # type: ignore\n            return None, None\n        else:\n            if raw_user_data.from_modality is None or raw_user_data.to_modality is None:\n                raise TypeError(\n                    \"from_modality and to_modality cannot be None for Translation\"\n                )\n            try:\n                return next(iter(raw_user_data.from_modality.keys())), next(\n                    iter(raw_user_data.to_modality.keys())\n                )\n            except Exception as e:\n                print(\"error getting from or to keys\")\n                print(e)\n                print(\"returning None\")\n                return None, None\n\n    @abstractmethod\n    def format_reconstruction(\n        self, reconstruction: Dict[str, torch.Tensor], result: Optional[Result] = None\n    ) -&gt; DataPackage:\n        pass\n\n    def filter_imgdata_list(self, img_list, ids):\n        filtered = []\n        for imgdata in img_list:\n            if imgdata.sample_id in ids:\n                filtered.append(imgdata)\n        return filtered\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePreprocessor.__init__","title":"<code>__init__(config, ontologies=None)</code>","text":"<p>Initializes the BasePreprocessor with a configuration object.</p> Args <p>config: A DefaultConfig object containing preprocessing configurations. ontologies: Ontology information, if provided for Ontix.</p> Source code in <code>src/autoencodix/base/_base_preprocessor.py</code> <pre><code>def __init__(\n    self,\n    config: DefaultConfig,\n    ontologies: Optional[Union[Tuple[Any, Any], Dict[Any, Any]]] = None,\n):\n    \"\"\"Initializes the BasePreprocessor with a configuration object.\n\n    Args :\n        config: A DefaultConfig object containing preprocessing configurations.\n        ontologies: Ontology information, if provided for Ontix.\n    \"\"\"\n    self.config = config\n    self._dataset_container: Optional[DatasetContainer] = None\n    self.processed_data = Dict[str, Dict[str, Union[Any, DataPackage]]]\n    self.bulk_genes_to_keep: Optional[Dict[str, List[str]]] = None\n    self.bulk_scalers: Optional[Dict[str, Any]] = None\n    self.sc_genes_to_keep: Optional[Dict[str, List[str]]] = None\n    self.sc_scalers: Optional[Dict[str, Dict[str, Any]]] = None\n    self.sc_general_genes_to_keep: Optional[Dict[str, List]] = None\n    self._ontologies: Optional[Union[Tuple[Any, Any], Dict[Any, Any]]] = ontologies\n    self.data_readers: Dict[Enum, Any] = {\n        DataCase.MULTI_SINGLE_CELL: SingleCellDataReader(),\n        DataCase.MULTI_BULK: BulkDataReader(config=self.config),\n        DataCase.BULK_TO_BULK: BulkDataReader(config=self.config),\n        DataCase.SINGLE_CELL_TO_SINGLE_CELL: SingleCellDataReader(),\n        DataCase.IMG_TO_BULK: {\n            \"bulk\": BulkDataReader(config=self.config),\n            \"img\": ImageDataReader(config=self.config),\n        },\n        DataCase.SINGLE_CELL_TO_IMG: {\n            \"sc\": SingleCellDataReader(),\n            \"img\": ImageDataReader(config=self.config),\n        },\n        DataCase.IMG_TO_IMG: ImageDataReader(config=self.config),\n    }\n</code></pre>"},{"location":"api/base/#autoencodix.base.BasePreprocessor.preprocess","title":"<code>preprocess(raw_user_data=None, predict_new_data=False)</code>  <code>abstractmethod</code>","text":"<p>To be implemented by subclasses for specific preprocessing steps. Args:     raw_user_data: Users can provide raw data. This is an alternative way of         providing data via filepaths in the config. If this param is passed, we skip the data reading step.     predict_new_data: Indicates whether the user wants to predict with unseen data.         If this is the case, we don't split the data and only prerpocess.</p> Source code in <code>src/autoencodix/base/_base_preprocessor.py</code> <pre><code>@abc.abstractmethod\ndef preprocess(\n    self,\n    raw_user_data: Optional[DataPackage] = None,\n    predict_new_data: bool = False,\n) -&gt; DatasetContainer:\n    \"\"\"To be implemented by subclasses for specific preprocessing steps.\n    Args:\n        raw_user_data: Users can provide raw data. This is an alternative way of\n            providing data via filepaths in the config. If this param is passed, we skip the data reading step.\n        predict_new_data: Indicates whether the user wants to predict with unseen data.\n            If this is the case, we don't split the data and only prerpocess.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseTrainer","title":"<code>BaseTrainer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>General training logic for all autoencoder models.</p> <p>This class sets up the model, optimizer, and data loaders. It also handles reproducibility and model-specific configurations. Subclasses must implement model training and prediction logic.</p> <p>Attributes:</p> Name Type Description <code>_trainset</code> <p>The dataset used for training.</p> <code>_validset</code> <p>The dataset used for validation, if provided.</p> <code>_result</code> <p>An object to store and manage training results.</p> <code>_config</code> <p>Configuration object containing training hyperparameters and settings.</p> <code>_model_type</code> <p>The autoencoder model class to be trained.</p> <code>_loss_fn</code> <p>Instantiated loss function specific to the model.</p> <code>_trainloader</code> <p>DataLoader for the training dataset.</p> <code>_validloader</code> <p>DataLoader for the validation dataset, if provided.</p> <code>_model</code> <p>The instantiated model architecture.</p> <code>_optimizer</code> <p>The optimizer used for training.</p> <code>_fabric</code> <p>Lightning Fabric wrapper for device and precision management.</p> <code>ontologies</code> <p>Ontology information, if provided for Ontix</p> Source code in <code>src/autoencodix/base/_base_trainer.py</code> <pre><code>class BaseTrainer(abc.ABC):\n    \"\"\"General training logic for all autoencoder models.\n\n    This class sets up the model, optimizer, and data loaders. It also handles\n    reproducibility and model-specific configurations. Subclasses must implement\n    model training and prediction logic.\n\n    Attributes:\n        _trainset: The dataset used for training.\n        _validset: The dataset used for validation, if provided.\n        _result: An object to store and manage training results.\n        _config: Configuration object containing training hyperparameters and settings.\n        _model_type: The autoencoder model class to be trained.\n        _loss_fn: Instantiated loss function specific to the model.\n        _trainloader: DataLoader for the training dataset.\n        _validloader: DataLoader for the validation dataset, if provided.\n        _model: The instantiated model architecture.\n        _optimizer: The optimizer used for training.\n        _fabric: Lightning Fabric wrapper for device and precision management.\n        ontologies: Ontology information, if provided for Ontix\n    \"\"\"\n\n    def __init__(\n        self,\n        trainset: Optional[BaseDataset],\n        validset: Optional[BaseDataset],\n        result: Result,\n        config: DefaultConfig,\n        model_type: Type[BaseAutoencoder],\n        loss_type: Type[BaseLoss],\n        ontologies: Optional[\n            Union[Tuple, List]\n        ] = None,  # Addition to Varix, mandotory for Ontix\n        **kwargs,\n    ):\n        self._trainset = trainset\n        self._model_type = model_type\n        self._validset = validset\n        self._result = result\n        self._config = config\n        self._loss_type = loss_type\n        self.ontologies = ontologies\n        self.setup_trainer()\n\n    def setup_trainer(self, old_model=None):\n        if old_model is None:\n            self._input_validation()\n            self._init_loaders()\n\n        self._loss_fn = self._loss_type(config=self._config)\n\n        self._handle_reproducibility()\n        # Internal data handling\n        self._model: BaseAutoencoder\n        self._fabric = Fabric(\n            accelerator=self._config.device,\n            devices=self._config.n_gpus,\n            precision=self._config.float_precision,\n            strategy=self._config.gpu_strategy,\n        )\n\n        self._fabric.launch()\n        self._setup_fabric(old_model=old_model)\n\n        self._n_cpus = os.cpu_count()\n        if self._n_cpus is None:\n            self._n_cpus = 0\n\n    def _setup_fabric(self, old_model=None):\n        \"\"\"\n        Sets up the model, optimizer, and data loaders with Lightning Fabric.\n        \"\"\"\n        self._init_model_architecture(\n            ontologies=self.ontologies, old_model=old_model\n        )  # Ontix\n\n        self._optimizer = torch.optim.AdamW(\n            params=self._model.parameters(),\n            lr=self._config.learning_rate,\n            weight_decay=self._config.weight_decay,\n        )\n\n        self._model, self._optimizer = self._fabric.setup(self._model, self._optimizer)\n        if old_model is None:\n            self._trainloader = self._fabric.setup_dataloaders(self._trainloader)  # type: ignore\n            if self._validloader is not None:\n                self._validloader = self._fabric.setup_dataloaders(self._validloader)  # type: ignore\n\n    def _init_loaders(self):\n        \"\"\"Initializes the DataLoaders for training and validation datasets.\"\"\"\n        # g = torch.Generator()\n        # g.manual_seed(self._config.global_seed)\n        last_batch_is_one_sample = len(self._trainset) % self._config.batch_size == 1\n        corrected_bs = (\n            self._config.batch_size + 1\n            if last_batch_is_one_sample\n            else self._config.batch_size\n        )\n        if last_batch_is_one_sample:\n            warnings.warn(\n                f\"increased batch_size to {corrected_bs} for trainset, to avoid dropping samples and having batches (makes trainingdynamics messy with missing samples per epoch) of size one (fails for Models with BachNorm)\"\n            )\n\n        self._trainloader = DataLoader(\n            cast(BaseDataset, self._trainset),\n            shuffle=True,\n            batch_size=corrected_bs,\n            worker_init_fn=self._seed_worker,\n            # generator=g,\n        )\n        if self._validset:\n            last_batch_is_one_sample = (\n                len(self._validset) % self._config.batch_size == 1\n            )\n            corrected_bs = (\n                self._config.batch_size + 1\n                if last_batch_is_one_sample\n                else self._config.batch_size\n            )\n            if last_batch_is_one_sample:\n                warnings.warn(\n                    f\"increased batch_size to {corrected_bs} for validset, to avoid dropping samples and having batches (makes trainingdynamics messy with missing samples per epoch) of size one (fails for Models with BachNorm)\"\n                )\n\n            self._validloader = DataLoader(\n                dataset=self._validset,\n                batch_size=self._config.batch_size,\n                shuffle=False,\n            )\n        else:\n            self._validloader = None  # type: ignore\n\n    def _input_validation(self) -&gt; None:\n        if self._trainset is None:\n            raise ValueError(\n                \"Trainset cannot be None. Check the indices you provided with a custom split or be sure that the train_ratio attribute of the config is &gt;0.\"\n            )\n        if not isinstance(self._trainset, BaseDataset):\n            raise TypeError(\n                f\"Expected train type to be an instance of BaseDataset, got {type(self._trainset)}.\"\n            )\n        if self._validset is None:\n            print(\"training without validation\")\n        elif not isinstance(self._validset, BaseDataset):\n            raise TypeError(\n                f\"Expected valid type to be an instance of BaseDataset, got {type(self._validset)}.\"\n            )\n        if self._config is None:\n            raise ValueError(\"Config cannot be None.\")\n\n    def _handle_reproducibility(self) -&gt; None:\n        \"\"\"Sets all relevant seeds for reproducibility\n\n        Raises:\n            NotImplementedError: If the device is set to \"mps\" (Apple Silicon).\n        \"\"\"\n        if self._config.reproducible:\n            torch.use_deterministic_algorithms(True)\n            torch.manual_seed(seed=self._config.global_seed)\n            random.seed(self._config.global_seed)\n            np.random.seed(self._config.global_seed)\n            if torch.cuda.is_available():\n                torch.cuda.manual_seed(seed=self._config.global_seed)\n                torch.cuda.manual_seed_all(seed=self._config.global_seed)\n                torch.backends.cudnn.deterministic = True\n                torch.backends.cudnn.benchmark = False\n            if torch.backends.mps.is_available():\n                torch.mps.manual_seed(seed=self._config.global_seed)\n\n                # torch.use_deterministic_algorithms(True, warn_only=True)\n\n                print(\n                    \"Warning: MPS backend has limited support for deterministic algorithms. \"\n                    \"Seeding is active, but full reproducibility is not guaranteed.\"\n                )\n            else:\n                print(\n                    f\"Reproducibility settings for device {self._config.device} are not implemented or necessary i.e. for cpu.\"\n                )\n\n    def _seed_worker(self, worker_id):\n        worker_seed = self._config.global_seed + worker_id\n        np.random.seed(worker_seed)\n        random.seed(worker_seed)\n\n    def _init_model_architecture(self, ontologies: tuple, old_model=None) -&gt; None:\n        \"\"\"Initializes the model architecture, based on the model type and input dimension.\"\"\"\n        if old_model:\n            self._model = old_model\n            return\n\n        self._input_dim = cast(BaseDataset, self._trainset).get_input_dim()\n        self.feature_order = self._trainset.feature_ids\n        if ontologies is None:\n            self._model = self._model_type(\n                config=self._config, input_dim=self._input_dim\n            )\n\n        else:\n            self._model = self._model_type(\n                config=self._config,\n                input_dim=self._input_dim,\n                ontologies=ontologies,\n                feature_order=self._trainset.feature_ids,  # type: ignore\n            )\n\n    def _should_checkpoint(self, epoch: int) -&gt; bool:\n        return (\n            (epoch + 1) % self._config.checkpoint_interval == 0\n            or epoch == self._config.epochs - 1\n        )\n\n    @abc.abstractmethod\n    def train(self, epochs_overwrite: Optional[int] = None) -&gt; Result:\n        pass\n\n    @abc.abstractmethod\n    def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        pass\n\n    @abc.abstractmethod\n    def predict(\n        self, data: BaseDataset, model: Optional[torch.nn.Module] = None, **kwargs\n    ) -&gt; Result:\n        pass\n\n    @abc.abstractmethod\n    def purge(self) -&gt; None:\n        \"\"\"Cleans up any resources used during training, such as cached data or large attributes.\"\"\"\n        pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseTrainer.purge","title":"<code>purge()</code>  <code>abstractmethod</code>","text":"<p>Cleans up any resources used during training, such as cached data or large attributes.</p> Source code in <code>src/autoencodix/base/_base_trainer.py</code> <pre><code>@abc.abstractmethod\ndef purge(self) -&gt; None:\n    \"\"\"Cleans up any resources used during training, such as cached data or large attributes.\"\"\"\n    pass\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseVisualizer","title":"<code>BaseVisualizer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Defines the interface for visualizing training results.</p> <p>Attributes:</p> Name Type Description <code>plots</code> <p>A nested dictionary to store various plots.</p> Source code in <code>src/autoencodix/base/_base_visualizer.py</code> <pre><code>class BaseVisualizer(abc.ABC):\n    \"\"\"Defines the interface for visualizing training results.\n\n    Attributes:\n        plots: A nested dictionary to store various plots.\n    \"\"\"\n\n    def __init__(self):\n        self.plots = nested_dict()\n\n    def __setitem__(self, key, elem):\n        self.plots[key] = elem\n\n    ### Abstract Methods ###\n    @abc.abstractmethod\n    def visualize(self, result: Result, config: DefaultConfig) -&gt; Result:\n        pass\n\n    @abc.abstractmethod\n    def show_latent_space(\n        self,\n        result: Result,\n        plot_type: str = \"2D-scatter\",\n        labels: Optional[Union[list, pd.Series, None]] = None,\n        param: Optional[Union[list, str]] = None,\n        epoch: Optional[Union[int, None]] = None,\n        split: str = \"all\",\n    ) -&gt; None:\n        pass\n\n    @abc.abstractmethod\n    def show_weights(self) -&gt; None:\n        pass\n\n    ### General Functions used by all Visualizers in similar way ###\n\n    def show_loss(self, plot_type: str = \"absolute\") -&gt; None:\n        \"\"\"\n        Display the loss plot.\n        Args:\n            plot_type: Type of loss plot to display. Options are \"absolute\" or \"relative\". Options are\n                \"absolute\" for the absolute loss plot and\n                \"relative\" for the relative loss plot.\n                Defaults to \"absolute\".\n        Returns:\n            None\n        \"\"\"\n        if plot_type == \"absolute\":\n            if \"loss_absolute\" not in self.plots.keys():\n                print(\"Absolute loss plot not found in the plots dictionary\")\n                print(\n                    \"This happens, when you did not run visualize() or if you saved and loaded the model with `save_all=False`\"\n                )\n            else:\n                fig = self.plots[\"loss_absolute\"]\n                show_figure(fig)\n                plt.show()\n        if plot_type == \"relative\":\n            if \"loss_relative\" not in self.plots.keys():\n                print(\"Relative loss plot not found in the plots dictionary\")\n\n                print(\n                    \"This happens, when you did not run visualize() or if you saved and loaded the model with `save_all=False`\"\n                )\n            else:\n                fig = self.plots[\"loss_relative\"]\n                fig.show()\n                # show_figure(fig)\n                # plt.show()\n\n        if plot_type not in [\"absolute\", \"relative\"]:\n            print(\n                \"Type of loss plot not recognized. Please use 'absolute' or 'relative'\"\n            )\n\n    def show_evaluation(\n        self,\n        param: str,\n        metric: str,\n        ml_alg: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"\n        Displays the evaluation plot for a specific clinical parameter, metric, and optionally ML algorithm.\n        Args:\n            param: clinical parameter to visualize.\n            metric: metric to visualize.\n            ml_alg: ML algorithm to visualize. If None, plots all available algorithms.\n        Returns:\n            None\n        \"\"\"\n        plt.ioff()\n        if \"ML_Evaluation\" not in self.plots.keys():\n            print(\"ML Evaluation plots not found in the plots dictionary\")\n            print(\"You need to run evaluate() method first\")\n            return None\n        if param not in self.plots[\"ML_Evaluation\"].keys():\n            print(f\"Parameter {param} not found in the ML Evaluation plots\")\n            print(f\"Available parameters: {list(self.plots['ML_Evaluation'].keys())}\")\n            return None\n        if metric not in self.plots[\"ML_Evaluation\"][param].keys():\n            print(f\"Metric {metric} not found in the ML Evaluation plots for {param}\")\n            print(\n                f\"Available metrics: {list(self.plots['ML_Evaluation'][param].keys())}\"\n            )\n            return None\n\n        algs = list(self.plots[\"ML_Evaluation\"][param][metric].keys())\n        if ml_alg is not None:\n            if ml_alg not in algs:\n                print(f\"ML algorithm {ml_alg} not found for {param} and {metric}\")\n                print(f\"Available ML algorithms: {algs}\")\n                return None\n            fig = self.plots[\"ML_Evaluation\"][param][metric][ml_alg].figure\n            show_figure(fig)\n            plt.show()\n        else:\n            for alg in algs:\n                print(f\"Showing plot for ML algorithm: {alg}\")\n                fig = self.plots[\"ML_Evaluation\"][param][metric][alg].figure\n                show_figure(fig)\n                plt.show()\n\n    def save_plots(\n        self, path: str, which: Union[str, list] = \"all\", format: str = \"png\"\n    ) -&gt; None:\n        \"\"\"\n        Save specified plots to the given path in the specified format.\n\n        Args:\n            path: The directory path where the plots will be saved.\n            which: A list of plot names to save or a string specifying which plots to save.\n                   If 'all', all plots in the plots dictionary will be saved.\n                   If a single plot name is provided as a string, only that plot will be saved.\n            format: The file format in which to save the plots (e.g., 'png', 'jpg').\n\n        Returns:\n            None\n\n        Raises:\n            ValueError: If the 'which' parameter is not a list or a string.\n        \"\"\"\n        if not os.path.exists(path):\n            os.makedirs(path)\n\n        if not isinstance(which, list):\n            ## Case when which is a string\n            if which == \"all\":\n                ## Case when all plots are to be saved\n                if len(self.plots) == 0:\n                    print(\"No plots found in the plots dictionary\")\n                    print(\"You need to run  visualize() method first\")\n                else:\n                    for item in nested_to_tuple(self.plots):\n                        fig = item[-1]  ## Figure is in last element of the tuple\n                        filename = \"_\".join(str(x) for x in item[0:-1])\n                        fullpath = os.path.join(path, filename)\n                        if hasattr(fig, \"savefig\"):\n                            fig.savefig(f\"{fullpath}.{format}\")\n                        elif hasattr(fig, \"save\"):  # for seaborn objects plots\n                            fig.save(f\"{fullpath}.{format}\")\n            else:\n                ## Case when a single plot is provided as string\n                if which not in self.plots.keys():\n                    print(f\"Plot {which} not found in the plots dictionary\")\n                    print(f\"All available plots are: {list(self.plots.keys())}\")\n                else:\n                    for item in nested_to_tuple(\n                        self.plots[which]\n                    ):  # Plot all epochs and splits of type which\n                        fig = item[-1]  ## Figure is in last element of the tuple\n                        filename = which + \"_\" + \"_\".join(str(x) for x in item[0:-1])  # type: ignore\n                        fullpath = os.path.join(path, filename)\n                        if hasattr(fig, \"savefig\"):\n                            fig.savefig(f\"{fullpath}.{format}\")\n                        elif hasattr(fig, \"save\"):  # for seaborn objects plots\n                            fig.save(f\"{fullpath}.{format}\")\n        else:\n            ## Case when which is a list of plot specified as strings\n            for key in which:\n                if key not in self.plots.keys():\n                    print(f\"Plot {key} not found in the plots dictionary\")\n                    print(f\"All available plots are: {list(self.plots.keys())}\")\n                    continue\n                else:\n                    for item in nested_to_tuple(\n                        self.plots[key]\n                    ):  # Plot all epochs and splits of type key\n                        fig = item[-1]  ## Figure is in last element of the tuple\n                        filename = key + \"_\" + \"_\".join(str(x) for x in item[0:-1])\n                        fullpath = os.path.join(path, filename)\n                        if hasattr(fig, \"savefig\"):\n                            fig.savefig(f\"{fullpath}.{format}\")\n                        elif hasattr(fig, \"save\"):  # for seaborn objects plots\n                            fig.save(f\"{fullpath}.{format}\")\n\n    ### Utilities ###\n\n    @staticmethod\n    def _make_loss_format(result: Result, config: DefaultConfig) -&gt; pd.DataFrame:\n        loss_df_melt = pd.DataFrame()\n        for term in result.sub_losses.keys():\n            # Get the loss values and ensure it's a dictionary\n            loss_values = result.sub_losses.get(key=term).get()\n\n            # Add explicit type checking/conversion\n            if not isinstance(loss_values, dict):\n                # If it's not a dict, try to convert it or handle appropriately\n                if hasattr(loss_values, \"to_dict\"):\n                    loss_values = loss_values.to_dict()  # type: ignore\n                else:\n                    # For non-convertible types, you might need a custom solution\n                    # For numpy arrays, you could do something like:\n                    if hasattr(loss_values, \"shape\"):\n                        # For numpy arrays, create a dict with indices as keys\n                        loss_values = {i: val for i, val in enumerate(loss_values)}\n\n            # Now create the DataFrame\n            loss_df = pd.DataFrame.from_dict(loss_values, orient=\"index\")  # type: ignore\n\n            # Rest of your code remains the same\n            if term == \"var_loss\":\n                loss_df = loss_df * config.beta\n            loss_df[\"Epoch\"] = loss_df.index + 1\n            loss_df[\"Loss Term\"] = term\n\n            loss_df_melt = pd.concat(\n                [\n                    loss_df_melt,\n                    loss_df.melt(\n                        id_vars=[\"Epoch\", \"Loss Term\"],\n                        var_name=\"Split\",\n                        value_name=\"Loss Value\",\n                    ),\n                ],\n                axis=0,\n            ).reset_index(drop=True)\n\n        # Similar handling for the total losses\n        loss_values = result.losses.get()\n        if not isinstance(loss_values, dict):\n            if hasattr(loss_values, \"to_dict\"):\n                loss_values = loss_values.to_dict()  # type: ignore\n            else:\n                if hasattr(loss_values, \"shape\"):\n                    loss_values = {i: val for i, val in enumerate(loss_values)}\n\n        loss_df = pd.DataFrame.from_dict(loss_values, orient=\"index\")  # type: ignore\n        loss_df[\"Epoch\"] = loss_df.index + 1\n        loss_df[\"Loss Term\"] = \"total_loss\"\n\n        loss_df_melt = pd.concat(\n            [\n                loss_df_melt,\n                loss_df.melt(\n                    id_vars=[\"Epoch\", \"Loss Term\"],\n                    var_name=\"Split\",\n                    value_name=\"Loss Value\",\n                ),\n            ],\n            axis=0,\n        ).reset_index(drop=True)\n\n        loss_df_melt[\"Loss Value\"] = loss_df_melt[\"Loss Value\"].astype(float)\n        return loss_df_melt\n\n    @staticmethod\n    def _make_loss_plot(\n        df_plot: pd.DataFrame, plot_type: str\n    ) -&gt; matplotlib.figure.Figure:  # type: ignore\n        \"\"\"\n        Generates a plot for visualizing loss values from a DataFrame.\n\n        Args:\n            df_plot : DataFrame containing the loss values to be plotted. It should have the columns:\n                - \"Loss Term\": The type of loss term (e.g., \"total_loss\", \"reconstruction_loss\").\n                - \"Epoch\": The epoch number.\n                - \"Loss Value\": The value of the loss.\n                - \"Split\": The data split (e.g., \"train\", \"validation\").\n\n            plot_type: The type of plot to generate. It can be either \"absolute\" or \"relative\".\n                - \"absolute\": Generates a line plot for each unique loss term.\n                - \"relative\": Generates a density plot for each data split, excluding the \"total_loss\" term.\n\n        Returns:\n            The generated matplotlib figure containing the loss plots.\n        \"\"\"\n        fig_width_abs = 5 * len(df_plot[\"Loss Term\"].unique())\n        fig_width_rel = 5 * len(df_plot[\"Split\"].unique())\n        if plot_type == \"absolute\":\n            fig, axes = plt.subplots(\n                1,\n                len(df_plot[\"Loss Term\"].unique()),\n                figsize=(fig_width_abs, 5),\n                sharey=False,\n            )\n            ax = 0\n            for term in df_plot[\"Loss Term\"].unique():\n                axes[ax] = sns.lineplot(\n                    data=df_plot[(df_plot[\"Loss Term\"] == term)],\n                    x=\"Epoch\",\n                    y=\"Loss Value\",\n                    hue=\"Split\",\n                    ax=axes[ax],\n                ).set_title(term)\n                ax += 1\n\n            plt.close()\n\n        if plot_type == \"relative\":\n            # Check if loss values are positive\n            if (df_plot[\"Loss Value\"] &lt; 0).any():\n                # Warning\n                warnings.warn(\n                    \"Loss values contain negative values. Check your loss function if correct. Loss will be clipped to zero for plotting.\"\n                )\n                df_plot[\"Loss Value\"] = df_plot[\"Loss Value\"].clip(lower=0)\n\n            # Exclude loss terms where all Loss Value are zero or NaN over all epochs\n            valid_terms = [\n                term\n                for term in df_plot[\"Loss Term\"].unique()\n                if (\n                    (df_plot[df_plot[\"Loss Term\"] == term][\"Loss Value\"].notna().any())\n                    and (df_plot[df_plot[\"Loss Term\"] == term][\"Loss Value\"] != 0).any()\n                )\n            ]\n            exclude = (\n                (df_plot[\"Loss Term\"] != \"total_loss\")\n                &amp; ~(df_plot[\"Loss Term\"].str.contains(\"_factor\"))\n                &amp; (df_plot[\"Loss Term\"].isin(valid_terms))\n            )\n\n            df_plot.loc[exclude, \"Relative Loss Value\"] = (\n                df_plot[exclude]\n                .groupby([\"Split\", \"Epoch\"])[\"Loss Value\"]\n                .transform(lambda x: x / x.sum())\n            )\n            fig = (\n                (\n                    so.Plot(\n                        df_plot[exclude],\n                        \"Epoch\",\n                        \"Relative Loss Value\",\n                        color=\"Loss Term\",\n                    ).add(so.Area(alpha=0.7), so.Stack())\n                )\n                .facet(\"Split\")\n                .layout(size=(fig_width_rel, 5))\n            )\n\n            # fig, axes = plt.subplots(1, 2, figsize=(fig_width_rel, 5), sharey=True)\n\n            # ax = 0\n\n            # for split in df_plot[\"Split\"].unique():\n            #     axes[ax] = sns.kdeplot(\n            #         data=df_plot[exclude &amp; (df_plot[\"Split\"] == split)],\n            #         x=\"Epoch\",\n            #         hue=\"Loss Term\",\n            #         multiple=\"fill\",\n            #         weights=\"Loss Value\",\n            #         clip=[0, df_plot[\"Epoch\"].max()],\n            #         ax=axes[ax],\n            #     ).set_title(split)\n            #     ax += 1\n\n            # plt.close()\n\n        return fig\n\n    @staticmethod\n    def _plot_model_weights(model: torch.nn.Module) -&gt; matplotlib.figure.Figure:  # type: ignore\n        \"\"\"\n        Visualization of model weights in encoder and decoder layers as heatmap for each layer as subplot.\n        Handles non-symmetrical autoencoder architectures.\n        Plots _mu layer for encoder as well.\n        Uses node_names for decoder layers if model has ontologies.\n        ARGS:\n            model (torch.nn.Module): PyTorch model instance.\n        RETURNS:\n            fig (matplotlib.figure): Figure handle (of last plot)\n        \"\"\"\n        all_weights = []\n        names = []\n        node_names = None\n        if hasattr(model, \"ontologies\"):\n            if model.ontologies is not None:\n                node_names = []\n                for ontology in model.ontologies:\n                    node_names.append(list(ontology.keys()))\n                node_names.append(model.feature_order)\n\n        # Collect encoder and decoder weights separately\n        encoder_weights = []\n        encoder_names = []\n        decoder_weights = []\n        decoder_names = []\n        for name, param in model.named_parameters():\n            # print(name)\n            if \"weight\" in name and len(param.shape) == 2:\n                if \"encoder\" in name and \"var\" not in name and \"_mu\" not in name:\n                    encoder_weights.append(param.detach().cpu().numpy())\n                    encoder_names.append(name[:-7])\n                elif \"_mu\" in name:\n                    encoder_weights.append(param.detach().cpu().numpy())\n                    encoder_names.append(name[:-7])\n                elif \"decoder\" in name and \"var\" not in name:\n                    decoder_weights.append(param.detach().cpu().numpy())\n                    decoder_names.append(name[:-7])\n                elif (\n                    \"encoder\" not in name\n                    and \"decoder\" not in name\n                    and \"var\" not in name\n                ):\n                    # fallback for models without explicit encoder/decoder in name\n                    all_weights.append(param.detach().cpu().numpy())\n                    names.append(name[:-7])\n\n        if encoder_weights or decoder_weights:\n            n_enc = len(encoder_weights)\n            n_dec = len(decoder_weights)\n            n_cols = max(n_enc, n_dec)\n            fig, axes = plt.subplots(2, n_cols, sharex=False, figsize=(15 * n_cols, 15))\n            if n_cols == 1:\n                axes = axes.reshape(2, 1)\n            # Plot encoder weights\n            for i in range(n_enc):\n                ax = axes[0, i]\n                sns.heatmap(\n                    encoder_weights[i],\n                    cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                    center=0,\n                    ax=ax,\n                ).set(title=encoder_names[i])\n                ax.set_ylabel(\"Out Node\", size=12)\n            # Hide unused encoder subplots\n            for i in range(n_enc, n_cols):\n                axes[0, i].axis(\"off\")\n            # Plot decoder weights\n            for i in range(n_dec):\n                ax = axes[1, i]\n                heatmap_kwargs = {}\n\n                sns.heatmap(\n                    decoder_weights[i],\n                    cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                    center=0,\n                    ax=ax,\n                    **heatmap_kwargs,\n                ).set(title=decoder_names[i])\n                if model.ontologies is not None:\n                    axes[1, i].set_xticks(\n                        ticks=range(len(node_names[i])),  # type: ignore\n                        labels=node_names[i],  # type: ignore\n                        rotation=90,\n                        fontsize=8,\n                    )\n                    axes[1, i].set_yticks(\n                        ticks=range(len(node_names[i + 1])),  # type: ignore\n                        labels=node_names[i + 1],  # type: ignore\n                        rotation=0,\n                        fontsize=8,\n                    )\n                ax.set_xlabel(\"In Node\", size=12)\n                ax.set_ylabel(\"Out Node\", size=12)\n            # Hide unused decoder subplots\n            for i in range(n_dec, n_cols):\n                axes[1, i].axis(\"off\")\n        else:\n            # fallback: plot all weights in order, split in half for encoder/decoder\n            n_layers = len(all_weights) // 2\n            fig, axes = plt.subplots(\n                2, n_layers, sharex=False, figsize=(5 * n_layers, 10)\n            )\n            for layer in range(n_layers):\n                sns.heatmap(\n                    all_weights[layer],\n                    cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                    center=0,\n                    ax=axes[0, layer],\n                ).set(title=names[layer])\n                sns.heatmap(\n                    all_weights[n_layers + layer],\n                    cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                    center=0,\n                    ax=axes[1, layer],\n                ).set(title=names[n_layers + layer])\n                axes[1, layer].set_xlabel(\"In Node\", size=12)\n                axes[0, layer].set_ylabel(\"Out Node\", size=12)\n                axes[1, layer].set_ylabel(\"Out Node\", size=12)\n\n        fig.suptitle(\"Model Weights\", size=20)\n        plt.close()\n        return fig\n\n    @staticmethod\n    def _collect_all_metadata(result):\n        all_metadata = pd.DataFrame()\n\n        # 1) collect metadata from results.datasets\n\n        # 1a) iterate over splits [train, valid, test] if they exist\n        for split in [\"train\", \"valid\", \"test\"]:\n\n            if hasattr(result.datasets, split) and result.datasets[split] is not None:\n                if hasattr(result.datasets[split], \"metadata\"):\n                    split_metadata = result.datasets[split].metadata\n\n                    # 1b) if result.datasets.split is a dictionary, iterate over keys (modalities)\n                    if isinstance(split_metadata, dict):\n                        for modality, modality_data in split_metadata.items():\n                            all_metadata = pd.concat(\n                                [all_metadata, modality_data], axis=0\n                            )\n                    # 1c) if result.datasets.split is a Dataframe, just collect metadata directly\n                    elif isinstance(split_metadata, pd.DataFrame):\n                        all_metadata = pd.concat([all_metadata, split_metadata], axis=0)\n                else:\n                    split_modalities = result.datasets[split].datasets\n                    if isinstance(split_modalities, dict):\n                        for modality, modality_data in split_modalities.items():\n                            if hasattr(modality_data, \"metadata\"):\n                                modality_metadata = modality_data.metadata\n                                if isinstance(modality_metadata, pd.DataFrame):\n                                    all_metadata = pd.concat(\n                                        [all_metadata, modality_metadata], axis=0\n                                    )\n\n        # 2) collect metadata from results.new_datasets in the same way\n        if hasattr(result, \"new_datasets\"):\n            for split in [\"train\", \"valid\", \"test\"]:\n                if (\n                    hasattr(result.new_datasets, split)\n                    and result.new_datasets[split] is not None\n                ):\n                    if hasattr(result.new_datasets[split], \"metadata\"):\n                        split_metadata = result.new_datasets[split].metadata\n\n                        if isinstance(split_metadata, dict):\n                            for modality, modality_data in split_metadata.items():\n                                all_metadata = pd.concat(\n                                    [all_metadata, modality_data], axis=0\n                                )\n                        elif isinstance(split_metadata, pd.DataFrame):\n                            all_metadata = pd.concat(\n                                [all_metadata, split_metadata], axis=0\n                            )\n                    else:\n                        split_modalities = result.new_datasets[split].datasets\n                        if isinstance(split_modalities, dict):\n                            for modality, modality_data in split_modalities.items():\n                                if hasattr(modality_data, \"metadata\"):\n                                    modality_metadata = modality_data.metadata\n                                    if isinstance(modality_metadata, pd.DataFrame):\n                                        all_metadata = pd.concat(\n                                            [all_metadata, modality_metadata], axis=0\n                                        )\n\n        # Remove duplicate rows if any\n        all_metadata = all_metadata.loc[~all_metadata.index.duplicated(keep=\"first\")]\n\n        return all_metadata\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseVisualizer.save_plots","title":"<code>save_plots(path, which='all', format='png')</code>","text":"<p>Save specified plots to the given path in the specified format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path where the plots will be saved.</p> required <code>which</code> <code>Union[str, list]</code> <p>A list of plot names to save or a string specifying which plots to save.    If 'all', all plots in the plots dictionary will be saved.    If a single plot name is provided as a string, only that plot will be saved.</p> <code>'all'</code> <code>format</code> <code>str</code> <p>The file format in which to save the plots (e.g., 'png', 'jpg').</p> <code>'png'</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'which' parameter is not a list or a string.</p> Source code in <code>src/autoencodix/base/_base_visualizer.py</code> <pre><code>def save_plots(\n    self, path: str, which: Union[str, list] = \"all\", format: str = \"png\"\n) -&gt; None:\n    \"\"\"\n    Save specified plots to the given path in the specified format.\n\n    Args:\n        path: The directory path where the plots will be saved.\n        which: A list of plot names to save or a string specifying which plots to save.\n               If 'all', all plots in the plots dictionary will be saved.\n               If a single plot name is provided as a string, only that plot will be saved.\n        format: The file format in which to save the plots (e.g., 'png', 'jpg').\n\n    Returns:\n        None\n\n    Raises:\n        ValueError: If the 'which' parameter is not a list or a string.\n    \"\"\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n\n    if not isinstance(which, list):\n        ## Case when which is a string\n        if which == \"all\":\n            ## Case when all plots are to be saved\n            if len(self.plots) == 0:\n                print(\"No plots found in the plots dictionary\")\n                print(\"You need to run  visualize() method first\")\n            else:\n                for item in nested_to_tuple(self.plots):\n                    fig = item[-1]  ## Figure is in last element of the tuple\n                    filename = \"_\".join(str(x) for x in item[0:-1])\n                    fullpath = os.path.join(path, filename)\n                    if hasattr(fig, \"savefig\"):\n                        fig.savefig(f\"{fullpath}.{format}\")\n                    elif hasattr(fig, \"save\"):  # for seaborn objects plots\n                        fig.save(f\"{fullpath}.{format}\")\n        else:\n            ## Case when a single plot is provided as string\n            if which not in self.plots.keys():\n                print(f\"Plot {which} not found in the plots dictionary\")\n                print(f\"All available plots are: {list(self.plots.keys())}\")\n            else:\n                for item in nested_to_tuple(\n                    self.plots[which]\n                ):  # Plot all epochs and splits of type which\n                    fig = item[-1]  ## Figure is in last element of the tuple\n                    filename = which + \"_\" + \"_\".join(str(x) for x in item[0:-1])  # type: ignore\n                    fullpath = os.path.join(path, filename)\n                    if hasattr(fig, \"savefig\"):\n                        fig.savefig(f\"{fullpath}.{format}\")\n                    elif hasattr(fig, \"save\"):  # for seaborn objects plots\n                        fig.save(f\"{fullpath}.{format}\")\n    else:\n        ## Case when which is a list of plot specified as strings\n        for key in which:\n            if key not in self.plots.keys():\n                print(f\"Plot {key} not found in the plots dictionary\")\n                print(f\"All available plots are: {list(self.plots.keys())}\")\n                continue\n            else:\n                for item in nested_to_tuple(\n                    self.plots[key]\n                ):  # Plot all epochs and splits of type key\n                    fig = item[-1]  ## Figure is in last element of the tuple\n                    filename = key + \"_\" + \"_\".join(str(x) for x in item[0:-1])\n                    fullpath = os.path.join(path, filename)\n                    if hasattr(fig, \"savefig\"):\n                        fig.savefig(f\"{fullpath}.{format}\")\n                    elif hasattr(fig, \"save\"):  # for seaborn objects plots\n                        fig.save(f\"{fullpath}.{format}\")\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseVisualizer.show_evaluation","title":"<code>show_evaluation(param, metric, ml_alg=None)</code>","text":"<p>Displays the evaluation plot for a specific clinical parameter, metric, and optionally ML algorithm. Args:     param: clinical parameter to visualize.     metric: metric to visualize.     ml_alg: ML algorithm to visualize. If None, plots all available algorithms. Returns:     None</p> Source code in <code>src/autoencodix/base/_base_visualizer.py</code> <pre><code>def show_evaluation(\n    self,\n    param: str,\n    metric: str,\n    ml_alg: Optional[str] = None,\n) -&gt; None:\n    \"\"\"\n    Displays the evaluation plot for a specific clinical parameter, metric, and optionally ML algorithm.\n    Args:\n        param: clinical parameter to visualize.\n        metric: metric to visualize.\n        ml_alg: ML algorithm to visualize. If None, plots all available algorithms.\n    Returns:\n        None\n    \"\"\"\n    plt.ioff()\n    if \"ML_Evaluation\" not in self.plots.keys():\n        print(\"ML Evaluation plots not found in the plots dictionary\")\n        print(\"You need to run evaluate() method first\")\n        return None\n    if param not in self.plots[\"ML_Evaluation\"].keys():\n        print(f\"Parameter {param} not found in the ML Evaluation plots\")\n        print(f\"Available parameters: {list(self.plots['ML_Evaluation'].keys())}\")\n        return None\n    if metric not in self.plots[\"ML_Evaluation\"][param].keys():\n        print(f\"Metric {metric} not found in the ML Evaluation plots for {param}\")\n        print(\n            f\"Available metrics: {list(self.plots['ML_Evaluation'][param].keys())}\"\n        )\n        return None\n\n    algs = list(self.plots[\"ML_Evaluation\"][param][metric].keys())\n    if ml_alg is not None:\n        if ml_alg not in algs:\n            print(f\"ML algorithm {ml_alg} not found for {param} and {metric}\")\n            print(f\"Available ML algorithms: {algs}\")\n            return None\n        fig = self.plots[\"ML_Evaluation\"][param][metric][ml_alg].figure\n        show_figure(fig)\n        plt.show()\n    else:\n        for alg in algs:\n            print(f\"Showing plot for ML algorithm: {alg}\")\n            fig = self.plots[\"ML_Evaluation\"][param][metric][alg].figure\n            show_figure(fig)\n            plt.show()\n</code></pre>"},{"location":"api/base/#autoencodix.base.BaseVisualizer.show_loss","title":"<code>show_loss(plot_type='absolute')</code>","text":"<p>Display the loss plot. Args:     plot_type: Type of loss plot to display. Options are \"absolute\" or \"relative\". Options are         \"absolute\" for the absolute loss plot and         \"relative\" for the relative loss plot.         Defaults to \"absolute\". Returns:     None</p> Source code in <code>src/autoencodix/base/_base_visualizer.py</code> <pre><code>def show_loss(self, plot_type: str = \"absolute\") -&gt; None:\n    \"\"\"\n    Display the loss plot.\n    Args:\n        plot_type: Type of loss plot to display. Options are \"absolute\" or \"relative\". Options are\n            \"absolute\" for the absolute loss plot and\n            \"relative\" for the relative loss plot.\n            Defaults to \"absolute\".\n    Returns:\n        None\n    \"\"\"\n    if plot_type == \"absolute\":\n        if \"loss_absolute\" not in self.plots.keys():\n            print(\"Absolute loss plot not found in the plots dictionary\")\n            print(\n                \"This happens, when you did not run visualize() or if you saved and loaded the model with `save_all=False`\"\n            )\n        else:\n            fig = self.plots[\"loss_absolute\"]\n            show_figure(fig)\n            plt.show()\n    if plot_type == \"relative\":\n        if \"loss_relative\" not in self.plots.keys():\n            print(\"Relative loss plot not found in the plots dictionary\")\n\n            print(\n                \"This happens, when you did not run visualize() or if you saved and loaded the model with `save_all=False`\"\n            )\n        else:\n            fig = self.plots[\"loss_relative\"]\n            fig.show()\n            # show_figure(fig)\n            # plt.show()\n\n    if plot_type not in [\"absolute\", \"relative\"]:\n        print(\n            \"Type of loss plot not recognized. Please use 'absolute' or 'relative'\"\n        )\n</code></pre>"},{"location":"api/configs/","title":"Configs Module","text":""},{"location":"api/configs/#autoencodix.configs.DataInfo","title":"<code>DataInfo</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>SchemaPrinterMixin</code></p> Source code in <code>src/autoencodix/configs/default_config.py</code> <pre><code>class DataInfo(BaseModel, SchemaPrinterMixin):\n    # general -------------------------------------\n    file_path: str = Field(default=\"\", description=\"Path to raw data file\")\n    data_type: Literal[\"NUMERIC\", \"CATEGORICAL\", \"IMG\", \"ANNOTATION\"] = Field(\n        default=\"NUMERIC\"\n    )\n    scaling: Literal[\n        \"STANDARD\", \"MINMAX\", \"ROBUST\", \"MAXABS\", \"NONE\", \"NOTSET\", \"LOG1P\"\n    ] = Field(\n        default=\"NOTSET\",\n        description=\"Setting the scaling here in DataInfo overrides the globally set scaling method for the specific data modality\",\n    )  # can also be set globally, for all data modalities.\n\n    filtering: Literal[\"VAR\", \"MAD\", \"CORR\", \"VARCORR\", \"NOFILT\", \"NONZEROVAR\"] = Field(\n        default=\"VAR\"\n    )\n    sep: Union[str, None] = Field(default=None)  # for pandas read_csv\n    extra_anno_file: Union[str, None] = Field(default=None)\n\n    # single cell specific -------------------------\n    is_single_cell: bool = Field(default=False)\n\n    min_cells: float = Field(\n        default=0.05,\n        ge=0,\n        le=1,\n        description=\"Minimum fraction of cells a gene must be expressed in to be kept. Genes expressed in fewer cells will be filtered out.\",\n    )  # Controls gene filtering based on expression prevalence\n\n    min_genes: float = Field(\n        default=0.02,\n        ge=0,\n        le=1,\n        description=\"Minimum fraction of genes a cell must express to be kept. Cells expressing fewer genes will be filtered out.\",\n    )  # Controls cell quality filtering\n    selected_layers: List[str] = Field(default=[\"X\"])\n\n    is_X: bool = Field(default=False)  # only for single cell data\n    normalize_counts: bool = Field(\n        default=True, description=\"Whether to normalize by total counts\"\n    )\n    log_transform: bool = Field(\n        default=False, description=\"Whether to apply log1p transformation\"\n    )\n    k_filter: Optional[int] = Field(\n        default=None,\n        description=\"Don't set this gets calculated dynamically, based on k_filter in general config \",\n    )\n    # image specific ------------------------------\n    img_width_resize: Union[int, None] = Field(default=64)\n    img_height_resize: Union[int, None] = Field(default=64)\n    # annotation specific -------------------------\n    # xmodalix specific -------------------------\n    translate_direction: Union[Literal[\"from\", \"to\"], None] = Field(default=None)\n    pretrain_epochs: Optional[int] = Field(\n        default=None,\n        description=\"Number of pretraining epochs. This overwrites the global 'pretraining_epochs' in DefaultConfig class to have different number of pretraining epochs for each data modality\",\n    )\n\n    @field_validator(\"selected_layers\")\n    @classmethod\n    def validate_selected_layers(cls, v):\n        if \"X\" not in v:\n            raise ValueError('\"X\" must always be a part of the selected_layers list')\n        return v\n\n    @field_validator(\"k_filter\", mode=\"before\")\n    @classmethod\n    def _forbid_user_k_filter(cls, v: Any, info: ValidationInfo) -&gt; Any:\n        \"\"\"\n        'before'  -&gt; runs only when the value comes from user input.\n        After instantiation we can still do  data_info.k_filter = xx\n        \"\"\"\n        if v is not None:\n            raise ValueError(\n                \"k_filter is computed automatically for each data modality, based on global k_filter \u2013 remove it from your DataInfo configuration.\"\n            )\n        return v\n\n    # # add validation to only allow quadratic image resizing\n    # @field_validator(\"img_width_resize\", \"img_height_resize\")\n    # @classmethod\n    # def validate_image_resize(cls, v, values):\n    #     if v is not None and v &lt;= 0:\n    #         raise ValueError(\"Image resize dimensions must be positive integers\")\n    #     if \"img_width_resize\" in values and \"img_height_resize\" in values:\n    #         if values[\"img_width_resize\"] != values[\"img_height_resize\"]:\n    #             raise ValueError(\"Image width and height must be the same for resizing\")\n    #     return v\n\n    @field_validator(\"img_width_resize\", \"img_height_resize\")\n    @classmethod\n    def validate_image_resize(cls, v, info: ValidationInfo):\n        if v is not None and v &lt;= 0:\n            raise ValueError(\"Image resize dimensions must be positive integers\")\n\n        # Access other field values through info.data\n        data = info.data\n        if \"img_width_resize\" in data and \"img_height_resize\" in data:\n            if data[\"img_width_resize\"] != data[\"img_height_resize\"]:\n                raise ValueError(\"Image width and height must be the same for resizing\")\n        return v\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.DefaultConfig","title":"<code>DefaultConfig</code>","text":"<p>               Bases: <code>BaseModel</code>, <code>SchemaPrinterMixin</code></p> <p>Complete configuration for model, training, hardware, and data handling.</p> Source code in <code>src/autoencodix/configs/default_config.py</code> <pre><code>class DefaultConfig(BaseModel, SchemaPrinterMixin):\n    \"\"\"Complete configuration for model, training, hardware, and data handling.\"\"\"\n\n    # Input validation\n    model_config = ConfigDict(extra=\"forbid\")\n    # Datasets configuration --------------------------------------------------\n    data_config: DataConfig = DataConfig(data_info={})\n    img_path_col: str = Field(\n        default=\"img_paths\",\n        description=\"When working with images, we except a column in your annotation file that specifies the path of the image for a particular sample. Here you can define the name of this column\",\n    )\n    requires_paired: Union[bool, None] = Field(\n        default_factory=lambda: True,\n        description=\"Indicator if the samples for the xmodalix are paired, based on some sample id\",\n    )\n\n    data_case: Union[DataCase, None] = Field(\n        default_factory=lambda: None,\n        description=\"Data case for the model, will be determined automatically\",\n    )\n    k_filter: Union[int, None] = Field(\n        default=None, description=\"Number of features to keep\"\n    )\n    scaling: Literal[\"STANDARD\", \"MINMAX\", \"ROBUST\", \"MAXABS\", \"NONE\", \"LOG1P\"] = Field(\n        default=\"STANDARD\",\n        description=\"Setting the scaling here for all data modalities, can per overruled by setting scaling at data modality level per data modality\",\n    )\n\n    skip_preprocessing: bool = Field(\n        default=False, description=\"If set don't scale, filter or clean the input data.\"\n    )\n\n    class_param: Optional[str] = Field(default=None)\n\n    # Model configuration -----------------------------------------------------\n    latent_dim: int = Field(\n        default=16, ge=1, description=\"Dimension of the latent space\"\n    )\n    hidden_dim: int = Field(\n        default=16,\n        ge=1,\n        description=\"Hidden dimension of image_vae, applies only to image_vae\",\n    )\n    n_layers: int = Field(\n        default=3,\n        ge=0,\n        description=\"Number of layers in encoder/decoder, without latent layer. If 0, is only the latent layer.\",\n    )\n    enc_factor: float = Field(\n        default=4, gt=0, description=\"Scaling factor for encoder dimensions\"\n    )\n    maskix_hidden_dim: int = Field(\n        default=256,\n        ge=8,\n        description=\"The Maskix implementation follows https://doi.org/10.1093/bioinformatics/btae020. The authors use a hidden dimension 0f 256 for their neural network, so we set this as default\",\n    )\n    maskix_swap_prob: float = Field(\n        default=0.4,\n        ge=0,\n        description=\"For the Maskix input_data masinkg, we sample a probablity if samples within one gene should be swapt. This is done with a Bernoulli distribution, maskix_swap_prob is the probablity passed to the bernoulli distribution \",\n    )\n    drop_p: float = Field(\n        default=0.1, ge=0.0, le=1.0, description=\"Dropout probability\"\n    )\n\n    # Training configuration --------------------------------------------------\n    save_memory: bool = Field(\n        default=False, description=\"If set to True we don't store TrainingDynamics\"\n    )\n    learning_rate: float = Field(\n        default=0.001, gt=0, description=\"Learning rate for optimization\"\n    )\n    batch_size: int = Field(\n        default=32,\n        ge=2,\n        description=\"Number of samples per batch, has to be &gt; 1, because we use BatchNorm() Layer\",\n    )\n    epochs: int = Field(default=3, ge=1, description=\"Number of training epochs\")\n    weight_decay: float = Field(\n        default=0.01, ge=0, description=\"L2 regularization factor\"\n    )\n    reconstruction_loss: Literal[\"mse\", \"bce\"] = Field(\n        default=\"mse\", description=\"Type of reconstruction loss\"\n    )\n    default_vae_loss: Literal[\"kl\", \"mmd\"] = Field(\n        default=\"kl\", description=\"Type of VAE loss\"\n    )\n    loss_reduction: Literal[\"sum\", \"mean\"] = Field(\n        default=\"sum\",\n        description=\"Loss reduction in PyTorch i.e in torch.nn.functional.binary_cross_entropy_with_logits(reduction=loss_reduction)\",\n    )\n    beta: float = Field(\n        default=1, ge=0, description=\"Beta weighting factor for VAE loss\"\n    )\n    beta_mi: float = Field(\n        default=1,\n        ge=0,\n        description=\"Beta weighting factor for mutual information term in disentangled VAE loss\",\n    )\n    beta_tc: float = Field(\n        default=1,\n        ge=0,\n        description=\"Beta weighting factor for total correlation term in disentangled VAE loss\",\n    )\n    beta_dimKL: float = Field(\n        default=1,\n        ge=0,\n        description=\"Beta weighting factor for dimension-wise KL in disentangled VAE loss\",\n    )\n    use_mss: bool = Field(\n        default=True,\n        description=\"Using minibatch stratified sampling for disentangled VAE loss calculation (faster estimation)\",\n    )\n    gamma: float = Field(\n        default=10.0,\n        ge=0,\n        description=\"Gamma weighting factor for Adversial Loss Term i.e. for XModalix Classfier training\",\n    )\n    delta_pair: float = Field(\n        default=5.0,\n        ge=0,\n        description=\"Delta weighting factor for paired loss term in XModalix Training\",\n    )\n    delta_class: float = Field(\n        default=5.0,\n        ge=0,\n        description=\"Delta weighting factor for class loss term in XModalix Training\",\n    )\n    delta_mask_predictor: float = Field(\n        default=0.7,\n        ge=0.0,\n        description=\"Delt weighting factor of the mask predictin loss term for the Maskix\",\n    )\n    delta_mask_corrupted: float = Field(\n        default=0.75,\n        ge=0.0,\n        description=\"For the Maskix: if &gt;0.5 this gives more weight for the correct reconstruction of corrupted input\",\n    )\n    min_samples_per_split: int = Field(\n        default=1, ge=1, description=\"Minimum number of samples per split\"\n    )\n    anneal_function: Literal[\n        \"5phase-constant\",\n        \"3phase-linear\",\n        \"3phase-log\",\n        \"logistic-mid\",\n        \"logistic-early\",\n        \"logistic-late\",\n        \"no-annealing\",\n    ] = Field(\n        default=\"logistic-mid\",\n        description=\"Annealing function strategy for VAE loss scheduling\",\n    )\n    pretrain_epochs: int = Field(\n        default=0,\n        ge=0,\n        description=\"Number of pretraining epochs, can be overwritten in DataInfo to have different number of pretraining epochs for each data modality\",\n    )\n\n    # Hardware configuration --------------------------------------------------\n    device: Literal[\"cpu\", \"cuda\", \"gpu\", \"tpu\", \"mps\", \"auto\"] = Field(\n        default=\"auto\", description=\"Device to use\"\n    )\n    # 0 uses cpu and not gpu\n    n_gpus: int = Field(default=1, ge=1, description=\"Number of GPUs to use\")\n    checkpoint_interval: int = Field(\n        default=10, ge=1, description=\"Interval for saving checkpoints\"\n    )\n    float_precision: Literal[\n        \"transformer-engine\",\n        \"transformer-engine-float16\",\n        \"16-true\",\n        \"16-mixed\",\n        \"bf16-true\",\n        \"bf16-mixed\",\n        \"32-true\",\n        \"64-true\",\n        \"64\",\n        \"32\",\n        \"16\",\n        \"bf16\",\n    ] = Field(default=\"32\", description=\"Floating point precision\")\n    gpu_strategy: Literal[\n        \"auto\",\n        \"dp\",\n        \"ddp\",\n        \"ddp_spawn\",\n        \"ddp_find_unused_parameters_true\",\n        \"xla\",\n        \"deepspeed\",\n        \"fsdp\",\n    ] = Field(default=\"auto\", description=\"GPU parallelization strategy\")\n\n    # Data handling configuration ---------------------------------------------\n    train_ratio: float = Field(\n        default=0.7, ge=0, lt=1, description=\"Ratio of data for training\"\n    )\n    test_ratio: float = Field(\n        default=0.2, ge=0, lt=1, description=\"Ratio of data for testing\"\n    )\n    valid_ratio: float = Field(\n        default=0.1, ge=0, lt=1, description=\"Ratio of data for validation\"\n    )\n\n    # General configuration ---------------------------------------------------\n    reproducible: bool = Field(\n        default=False, description=\"Whether to ensure reproducibility\"\n    )\n    global_seed: int = Field(default=1, ge=0, description=\"Global random seed\")\n\n    ##### VALIDATION ##### -----------------------------------------------------\n    ##### ----------------- -----------------------------------------------------\n    @field_validator(\"data_config\")\n    @classmethod\n    def validate_data_config(cls, data_config: DataConfig):\n        \"\"\"Main validation logic for dataset consistency and translation.\"\"\"\n        data_info = data_config.data_info\n\n        numeric_count = sum(\n            1 for info in data_info.values() if info.data_type == \"NUMERIC\"\n        )\n        img_count = sum(1 for info in data_info.values() if info.data_type == \"IMG\")\n\n        if numeric_count == 0 and img_count == 0:\n            raise ConfigValidationError(\"At least one NUMERIC dataset is required.\")\n\n        numeric_datasets = [\n            info for info in data_info.values() if info.data_type == \"NUMERIC\"\n        ]\n        if numeric_datasets:\n            is_single_cell = numeric_datasets[0].is_single_cell\n            if any(info.is_single_cell != is_single_cell for info in numeric_datasets):\n                raise ConfigValidationError(\n                    \"All numeric datasets must be either single cell or bulk.\"\n                )\n\n        from_dataset = next(\n            (\n                (name, info)\n                for name, info in data_info.items()\n                if info.translate_direction == \"from\"\n            ),\n            None,\n        )\n        to_dataset = next(\n            (\n                (name, info)\n                for name, info in data_info.items()\n                if info.translate_direction == \"to\"\n            ),\n            None,\n        )\n\n        if bool(from_dataset) != bool(to_dataset):\n            raise ConfigValidationError(\n                \"Translation requires exactly one 'from' and one 'to' dataset.\"\n            )\n\n        if from_dataset and to_dataset:\n            from_info, to_info = from_dataset[1], to_dataset[1]\n            if from_info.data_type == \"NUMERIC\" and to_info.data_type == \"NUMERIC\":\n                if from_info.is_single_cell != to_info.is_single_cell:\n                    raise ConfigValidationError(\n                        \"Cannot translate between single cell and bulk data.\"\n                    )\n\n        return data_config\n\n    @model_validator(mode=\"after\")\n    def determine_case(self) -&gt; \"DefaultConfig\":\n        \"\"\"Assign the correct DataCase after model validation.\"\"\"\n        data_info = self.data_config.data_info\n\n        # Handle empty data_info case\n        if not data_info:\n            return self\n\n        # Find 'from' and 'to' datasets\n        from_dataset = next(\n            (\n                (name, info)\n                for name, info in data_info.items()\n                if info.translate_direction == \"from\"\n            ),\n            None,\n        )\n        to_dataset = next(\n            (\n                (name, info)\n                for name, info in data_info.items()\n                if info.translate_direction == \"to\"\n            ),\n            None,\n        )\n\n        if from_dataset and to_dataset:\n            from_info, to_info = from_dataset[1], to_dataset[1]\n            if from_info.data_type == \"NUMERIC\" and to_info.data_type == \"NUMERIC\":\n                self.data_case = (\n                    DataCase.SINGLE_CELL_TO_SINGLE_CELL\n                    if from_info.is_single_cell\n                    else DataCase.BULK_TO_BULK\n                )\n            elif \"IMG\" in {from_info.data_type, to_info.data_type}:\n                numeric_dataset = (\n                    from_info if from_info.data_type == \"NUMERIC\" else to_info\n                )\n                # check for IMG_IMG\n                if from_info.data_type == \"IMG\" and to_info.data_type == \"IMG\":\n                    self.data_case = DataCase.IMG_TO_IMG\n                else:\n                    self.data_case = (\n                        DataCase.SINGLE_CELL_TO_IMG\n                        if numeric_dataset.is_single_cell\n                        else DataCase.IMG_TO_BULK\n                    )\n        else:\n            img_ds = [info for info in data_info.values() if info.data_type == \"IMG\"]\n            if img_ds:\n                self.data_case = DataCase.IMG_TO_IMG\n\n            numeric_datasets = [\n                info for info in data_info.values() if info.data_type == \"NUMERIC\"\n            ]\n\n            if numeric_datasets:\n                numeric_dataset = numeric_datasets[0]\n                self.data_case = (\n                    DataCase.MULTI_SINGLE_CELL\n                    if numeric_dataset.is_single_cell\n                    else DataCase.MULTI_BULK\n                )\n            if self.data_case is None:\n                import warnings\n\n                warnings.warn(message=\"Could not determine data_case\")\n\n        return self\n\n    @field_validator(\"test_ratio\", \"valid_ratio\")\n    def validate_ratios(cls, v, values):\n        total = (\n            sum(\n                values.data.get(key, 0)\n                for key in [\"train_ratio\", \"test_ratio\", \"valid_ratio\"]\n            )\n            + v\n        )\n        if total &gt; 1.0:\n            raise ValueError(f\"Data split ratios must sum to 1.0 or less (got {total})\")\n        return v\n\n    # TODO test if other float precisions work with MPS\n    @field_validator(\"float_precision\")\n    def validate_float_precision(cls, v, values):\n        \"\"\"Validate float precision based on device type.\"\"\"\n        device = values.data[\"device\"]\n        if device == \"mps\" and v != \"32\":\n            raise ValueError(\"MPS backend only supports float precision '32'\")\n        return v\n\n    # gpu strategy needs to be auto for mps # TODO test if other strategies work\n    @field_validator(\"gpu_strategy\")\n    def validate_gpu_strategy(cls, v, values):\n        device = values.data.get(\"device\")\n        if device == \"mps\" and v != \"auto\":\n            raise ValueError(\"MPS backend only supports GPU strategy 'auto'\")\n\n    @model_validator(mode=\"after\")\n    def validate_k_filter_with_nonzero_var(self):\n        k_filter = self.k_filter\n\n        data_info = self.data_config.data_info\n\n        for info in data_info.values():\n            if info.filtering == \"NONZEROVAR\" and k_filter is not None:\n                raise ValueError(\n                    \"k_filter cannot be combined with DataInfo that has scaling set to 'NONZEROVAR'\"\n                )\n\n        return self\n\n    #### END VALIDATION #### --------------------------------------------------\n\n    #### READIBILITY #### ------------------------------------------------------\n    #### ------------ #### ------------------------------------------------------\n    @classmethod\n    def get_params(cls) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Get detailed information about all config fields including types and default values.\n\n        Returns:\n            Dictionary containing field name, type, default value, and description if available\n        \"\"\"\n        fields_info = {}\n        for name, field in cls.model_fields.items():\n            fields_info[name] = {\n                \"type\": str(field.annotation),\n                \"default\": field.default,\n                \"description\": field.description or \"No description available\",\n            }\n        return fields_info\n\n    @classmethod\n    def print_schema(cls, filter_params: Optional[None] = None) -&gt; None:  # type: ignore\n        \"\"\"\n        Print a human-readable schema of all config parameters.\n        \"\"\"\n        if filter_params:\n            filter_params = list(filter_params)\n            print(\"Valid Keyword Arguments:\")\n            print(\"-\" * 50)\n        else:\n            print(f\"\\n{cls.__name__} Configuration Parameters:\")\n            print(\"-\" * 50)\n\n        for name, info in cls.get_params().items():\n            if filter_params and name not in filter_params:\n                continue\n            print(f\"\\n{name}:\")\n            print(f\"  Type: {info['type']}\")\n            print(f\"  Default: {info['default']}\")  # type: ignore\n            print(f\"  Description: {info['description']}\")\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.DefaultConfig.determine_case","title":"<code>determine_case()</code>","text":"<p>Assign the correct DataCase after model validation.</p> Source code in <code>src/autoencodix/configs/default_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef determine_case(self) -&gt; \"DefaultConfig\":\n    \"\"\"Assign the correct DataCase after model validation.\"\"\"\n    data_info = self.data_config.data_info\n\n    # Handle empty data_info case\n    if not data_info:\n        return self\n\n    # Find 'from' and 'to' datasets\n    from_dataset = next(\n        (\n            (name, info)\n            for name, info in data_info.items()\n            if info.translate_direction == \"from\"\n        ),\n        None,\n    )\n    to_dataset = next(\n        (\n            (name, info)\n            for name, info in data_info.items()\n            if info.translate_direction == \"to\"\n        ),\n        None,\n    )\n\n    if from_dataset and to_dataset:\n        from_info, to_info = from_dataset[1], to_dataset[1]\n        if from_info.data_type == \"NUMERIC\" and to_info.data_type == \"NUMERIC\":\n            self.data_case = (\n                DataCase.SINGLE_CELL_TO_SINGLE_CELL\n                if from_info.is_single_cell\n                else DataCase.BULK_TO_BULK\n            )\n        elif \"IMG\" in {from_info.data_type, to_info.data_type}:\n            numeric_dataset = (\n                from_info if from_info.data_type == \"NUMERIC\" else to_info\n            )\n            # check for IMG_IMG\n            if from_info.data_type == \"IMG\" and to_info.data_type == \"IMG\":\n                self.data_case = DataCase.IMG_TO_IMG\n            else:\n                self.data_case = (\n                    DataCase.SINGLE_CELL_TO_IMG\n                    if numeric_dataset.is_single_cell\n                    else DataCase.IMG_TO_BULK\n                )\n    else:\n        img_ds = [info for info in data_info.values() if info.data_type == \"IMG\"]\n        if img_ds:\n            self.data_case = DataCase.IMG_TO_IMG\n\n        numeric_datasets = [\n            info for info in data_info.values() if info.data_type == \"NUMERIC\"\n        ]\n\n        if numeric_datasets:\n            numeric_dataset = numeric_datasets[0]\n            self.data_case = (\n                DataCase.MULTI_SINGLE_CELL\n                if numeric_dataset.is_single_cell\n                else DataCase.MULTI_BULK\n            )\n        if self.data_case is None:\n            import warnings\n\n            warnings.warn(message=\"Could not determine data_case\")\n\n    return self\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.DefaultConfig.get_params","title":"<code>get_params()</code>  <code>classmethod</code>","text":"<p>Get detailed information about all config fields including types and default values.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary containing field name, type, default value, and description if available</p> Source code in <code>src/autoencodix/configs/default_config.py</code> <pre><code>@classmethod\ndef get_params(cls) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Get detailed information about all config fields including types and default values.\n\n    Returns:\n        Dictionary containing field name, type, default value, and description if available\n    \"\"\"\n    fields_info = {}\n    for name, field in cls.model_fields.items():\n        fields_info[name] = {\n            \"type\": str(field.annotation),\n            \"default\": field.default,\n            \"description\": field.description or \"No description available\",\n        }\n    return fields_info\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.DefaultConfig.print_schema","title":"<code>print_schema(filter_params=None)</code>  <code>classmethod</code>","text":"<p>Print a human-readable schema of all config parameters.</p> Source code in <code>src/autoencodix/configs/default_config.py</code> <pre><code>@classmethod\ndef print_schema(cls, filter_params: Optional[None] = None) -&gt; None:  # type: ignore\n    \"\"\"\n    Print a human-readable schema of all config parameters.\n    \"\"\"\n    if filter_params:\n        filter_params = list(filter_params)\n        print(\"Valid Keyword Arguments:\")\n        print(\"-\" * 50)\n    else:\n        print(f\"\\n{cls.__name__} Configuration Parameters:\")\n        print(\"-\" * 50)\n\n    for name, info in cls.get_params().items():\n        if filter_params and name not in filter_params:\n            continue\n        print(f\"\\n{name}:\")\n        print(f\"  Type: {info['type']}\")\n        print(f\"  Default: {info['default']}\")  # type: ignore\n        print(f\"  Description: {info['description']}\")\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.DefaultConfig.validate_data_config","title":"<code>validate_data_config(data_config)</code>  <code>classmethod</code>","text":"<p>Main validation logic for dataset consistency and translation.</p> Source code in <code>src/autoencodix/configs/default_config.py</code> <pre><code>@field_validator(\"data_config\")\n@classmethod\ndef validate_data_config(cls, data_config: DataConfig):\n    \"\"\"Main validation logic for dataset consistency and translation.\"\"\"\n    data_info = data_config.data_info\n\n    numeric_count = sum(\n        1 for info in data_info.values() if info.data_type == \"NUMERIC\"\n    )\n    img_count = sum(1 for info in data_info.values() if info.data_type == \"IMG\")\n\n    if numeric_count == 0 and img_count == 0:\n        raise ConfigValidationError(\"At least one NUMERIC dataset is required.\")\n\n    numeric_datasets = [\n        info for info in data_info.values() if info.data_type == \"NUMERIC\"\n    ]\n    if numeric_datasets:\n        is_single_cell = numeric_datasets[0].is_single_cell\n        if any(info.is_single_cell != is_single_cell for info in numeric_datasets):\n            raise ConfigValidationError(\n                \"All numeric datasets must be either single cell or bulk.\"\n            )\n\n    from_dataset = next(\n        (\n            (name, info)\n            for name, info in data_info.items()\n            if info.translate_direction == \"from\"\n        ),\n        None,\n    )\n    to_dataset = next(\n        (\n            (name, info)\n            for name, info in data_info.items()\n            if info.translate_direction == \"to\"\n        ),\n        None,\n    )\n\n    if bool(from_dataset) != bool(to_dataset):\n        raise ConfigValidationError(\n            \"Translation requires exactly one 'from' and one 'to' dataset.\"\n        )\n\n    if from_dataset and to_dataset:\n        from_info, to_info = from_dataset[1], to_dataset[1]\n        if from_info.data_type == \"NUMERIC\" and to_info.data_type == \"NUMERIC\":\n            if from_info.is_single_cell != to_info.is_single_cell:\n                raise ConfigValidationError(\n                    \"Cannot translate between single cell and bulk data.\"\n                )\n\n    return data_config\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.DefaultConfig.validate_float_precision","title":"<code>validate_float_precision(v, values)</code>","text":"<p>Validate float precision based on device type.</p> Source code in <code>src/autoencodix/configs/default_config.py</code> <pre><code>@field_validator(\"float_precision\")\ndef validate_float_precision(cls, v, values):\n    \"\"\"Validate float precision based on device type.\"\"\"\n    device = values.data[\"device\"]\n    if device == \"mps\" and v != \"32\":\n        raise ValueError(\"MPS backend only supports float precision '32'\")\n    return v\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.DisentanglixConfig","title":"<code>DisentanglixConfig</code>","text":"<p>               Bases: <code>DefaultConfig</code></p> <p>A specialized configuration inheriting from DefaultConfig.</p> Source code in <code>src/autoencodix/configs/disentanglix_config.py</code> <pre><code>class DisentanglixConfig(DefaultConfig):\n    \"\"\"\n    A specialized configuration inheriting from DefaultConfig.\n    \"\"\"\n\n    beta: float = Field(\n        default=0.1,  # Overridden default (was 1.0)\n        ge=0,\n        description=\"Beta weighting factor for VAE loss\",\n    )\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.MaskixConfig","title":"<code>MaskixConfig</code>","text":"<p>               Bases: <code>DefaultConfig</code></p> <p>A specialized configuration inheriting from DefaultConfig.</p> Source code in <code>src/autoencodix/configs/maskix_config.py</code> <pre><code>class MaskixConfig(DefaultConfig):\n    \"\"\"\n    A specialized configuration inheriting from DefaultConfig.\n    \"\"\"\n\n    beta: float = Field(\n        default=0.1,  # Overridden default (was 1.0)\n        ge=0,\n        description=\"Beta weighting factor for VAE loss\",\n    )\n    epoch: int = Field(\n        default=30, ge=0, description=\"How many epochs should the model train for.\"\n    )\n    maskix_hidden_dim: int = Field(\n        default=128,\n        ge=8,\n        description=\"The Maskix implementation follows https://doi.org/10.1093/bioinformatics/btae020. The authors use a hidden dimension 0f 256 for their neural network, so we set this as default\",\n    )\n    maskix_swap_prob: float = Field(\n        default=0.2,\n        ge=0,\n        description=\"For the Maskix input_data masinkg, we sample a probablity if samples within one gene should be swapt. This is done with a Bernoulli distribution, maskix_swap_prob is the probablity passed to the bernoulli distribution \",\n    )\n    delta_mask_predictor: float = Field(\n        default=0.7,\n        ge=0.0,\n        description=\"Delt weighting factor of the mask predictin loss term for the Maskix\",\n    )\n    delta_mask_corrupted: float = Field(\n        default=0.75,\n        ge=0.0,\n        description=\"For the Maskix: if &gt;0.5 this gives more weight for the correct reconstruction of corrupted input\",\n    )\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.OntixConfig","title":"<code>OntixConfig</code>","text":"<p>               Bases: <code>DefaultConfig</code></p> <p>A specialized configuration for Ontix that only allows scaling methods guaranteeing non-negative outputs.</p> Source code in <code>src/autoencodix/configs/ontix_config.py</code> <pre><code>class OntixConfig(DefaultConfig):\n    \"\"\"\n    A specialized configuration for Ontix that only allows scaling methods\n    guaranteeing non-negative outputs.\n    \"\"\"\n\n    # 1. Override the top-level 'scaling' attribute\n    scaling: Literal[\"MINMAX\", \"NONE\", \"NOTSET\", \"LOG1P\"] = Field(\n        default=\"MINMAX\",\n        description=\"Global scaling method. For Ontix, only 'MINMAX' and 'NONE' are allowed, because we need positive values only\",\n    )\n\n    # 2. Add a validator for the nested 'scaling' attributes\n    @model_validator(mode=\"after\")\n    def validate_nested_scaling(self) -&gt; \"OntixConfig\":\n        \"\"\"\n        Ensures that any scaling method set within DataInfo is also a valid\n        positive-value scaler.\n        \"\"\"\n        # Define the set of allowed scaling methods\n        allowed_scalers = {\"MINMAX\", \"NONE\", \"NOTSET\"}\n\n        # Loop through each data modality defined in the data_config\n        for modality_name, data_info in self.data_config.data_info.items():\n            if data_info.scaling not in allowed_scalers:\n                raise ValueError(\n                    f\"Invalid scaling '{data_info.scaling}' for modality '{modality_name}'. \"\n                    f\"OntixConfig only permits {list(allowed_scalers)}.\"\n                )\n        return self\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.OntixConfig.validate_nested_scaling","title":"<code>validate_nested_scaling()</code>","text":"<p>Ensures that any scaling method set within DataInfo is also a valid positive-value scaler.</p> Source code in <code>src/autoencodix/configs/ontix_config.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_nested_scaling(self) -&gt; \"OntixConfig\":\n    \"\"\"\n    Ensures that any scaling method set within DataInfo is also a valid\n    positive-value scaler.\n    \"\"\"\n    # Define the set of allowed scaling methods\n    allowed_scalers = {\"MINMAX\", \"NONE\", \"NOTSET\"}\n\n    # Loop through each data modality defined in the data_config\n    for modality_name, data_info in self.data_config.data_info.items():\n        if data_info.scaling not in allowed_scalers:\n            raise ValueError(\n                f\"Invalid scaling '{data_info.scaling}' for modality '{modality_name}'. \"\n                f\"OntixConfig only permits {list(allowed_scalers)}.\"\n            )\n    return self\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.StackixConfig","title":"<code>StackixConfig</code>","text":"<p>               Bases: <code>DefaultConfig</code></p> <p>A specialized configuration inheriting from DefaultConfig. For Stackix, <code>save_memory</code> is always False (feature not supported).</p> Source code in <code>src/autoencodix/configs/stackix_config.py</code> <pre><code>class StackixConfig(DefaultConfig):\n    \"\"\"\n    A specialized configuration inheriting from DefaultConfig.\n    For Stackix, `save_memory` is always False (feature not supported).\n    \"\"\"\n\n    beta: float = Field(\n        default=0.1,\n        ge=0,\n        description=\"Beta weighting factor for VAE loss\",\n    )\n\n    save_memory: bool = Field(\n        default=False,\n        description=\"Always False \u2014 not supported for Stackix.\",\n    )\n\n    @model_validator(mode=\"before\")\n    def _force_save_memory_false(cls, values):\n        if values.get(\"save_memory\") is True:\n            warnings.warn(\n                \"`save_memory=True` is not supported for StackixConfig \u2014 forcing to False., Set the checkpoint_interval to number of epochs if you want to save memory\",\n                UserWarning,\n                stacklevel=2,\n            )\n            values[\"save_memory\"] = False\n        return values\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.VanillixConfig","title":"<code>VanillixConfig</code>","text":"<p>               Bases: <code>DefaultConfig</code></p> <p>A specialized configuration inheriting from DefaultConfig.</p> Source code in <code>src/autoencodix/configs/vanillix_config.py</code> <pre><code>class VanillixConfig(DefaultConfig):\n    \"\"\"\n    A specialized configuration inheriting from DefaultConfig.\n    \"\"\"\n\n    beta: float = Field(\n        default=0.1,  # Overridden default (was 1.0)\n        ge=0,\n        description=\"Beta weighting factor for VAE loss\",\n    )\n    epoch: int = Field(\n        default=30, ge=0, description=\"How many epochs should the model train for.\"\n    )\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.VarixConfig","title":"<code>VarixConfig</code>","text":"<p>               Bases: <code>DefaultConfig</code></p> <p>A specialized configuration inheriting from DefaultConfig.</p> Source code in <code>src/autoencodix/configs/varix_config.py</code> <pre><code>class VarixConfig(DefaultConfig):\n    \"\"\"\n    A specialized configuration inheriting from DefaultConfig.\n    \"\"\"\n\n    beta: float = Field(\n        default=0.1,  # Overridden default (was 1.0)\n        ge=0,\n        description=\"Beta weighting factor for VAE loss\",\n    )\n</code></pre>"},{"location":"api/configs/#autoencodix.configs.XModalixConfig","title":"<code>XModalixConfig</code>","text":"<p>               Bases: <code>DefaultConfig</code></p> <p>A specialized configuration inheriting from DefaultConfig.</p> <p>This class overrides specific training parameters like pretrain_epochs and beta for the XModalix model, while inheriting all other settings.</p> Source code in <code>src/autoencodix/configs/xmodalix_config.py</code> <pre><code>class XModalixConfig(DefaultConfig):\n    \"\"\"\n    A specialized configuration inheriting from DefaultConfig.\n\n    This class overrides specific training parameters like pretrain_epochs and beta\n    for the XModalix model, while inheriting all other settings.\n    \"\"\"\n\n    pretrain_epochs: Optional[int] = Field(\n        default=None,  # Overridden default (was 0)\n        description=\"Number of pretraining epochs, can be overwritten in DataInfo to have different number of pretraining epochs for each data modality\",\n    )\n\n    beta: float = Field(\n        default=0.1,  # Overridden default (was 1.0)\n        ge=0,\n        description=\"Beta weighting factor for VAE loss\",\n    )\n    requires_paired: bool = Field(default=False)\n    save_memory: bool = Field(\n        default=False,\n        description=\"Always False \u2014 not supported for Stackix.\",\n    )\n\n    @model_validator(mode=\"before\")\n    def _force_save_memory_false(cls, values):\n        if values.get(\"save_memory\") is True:\n            warnings.warn(\n                \"`save_memory=True` is not supported for XModalixConfig \u2014 forcing to False., Set the checkpoint_interval to number of epochs if you want to save memory\",\n                UserWarning,\n                stacklevel=2,\n            )\n            values[\"save_memory\"] = False\n        return values\n</code></pre>"},{"location":"api/data/","title":"Data Module","text":""},{"location":"api/data/#autoencodix.data.BalancedBatchSampler","title":"<code>BalancedBatchSampler</code>","text":"<p>               Bases: <code>Sampler[List[int]]</code></p> <p>A custom PyTorch Sampler that avoids creating a final batch of size 1.</p> <p>This sampler behaves like a standard <code>BatchSampler</code> but with a key difference in handling the last batch. If the last batch would normally have a size of 1, this sampler redistributes the last two batches to be of roughly equal size. For example, if a dataset of 129 samples is used with a batch size of 128, instead of yielding batches of [128, 1], it will yield two balanced batches, such as [65, 64].</p> <p>This is particularly useful for avoiding issues with layers like BatchNorm, which require batch sizes greater than 1, without having to drop data (<code>drop_last=True</code>).</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>Sized</code> <p>The dataset to sample from.</p> required <code>batch_size</code> <code>int</code> <p>The target number of samples in each batch.</p> required <code>shuffle</code> <code>bool</code> <p>If True, the sampler will shuffle the indices at start of each epoch.</p> <code>True</code> Source code in <code>src/autoencodix/data/_sampler.py</code> <pre><code>class BalancedBatchSampler(Sampler[List[int]]):\n    \"\"\"\n    A custom PyTorch Sampler that avoids creating a final batch of size 1.\n\n    This sampler behaves like a standard `BatchSampler` but with a key\n    difference in handling the last batch. If the last batch would normally\n    have a size of 1, this sampler redistributes the last two batches to be\n    of roughly equal size. For example, if a dataset of 129 samples is used\n    with a batch size of 128, instead of yielding batches of [128, 1], it\n    will yield two balanced batches, such as [65, 64].\n\n    This is particularly useful for avoiding issues with layers like\n    BatchNorm, which require batch sizes greater than 1, without having to\n    drop data (`drop_last=True`).\n\n    Args:\n        data_source: The dataset to sample from.\n        batch_size: The target number of samples in each batch.\n        shuffle: If True, the sampler will shuffle the indices at start of each epoch.\n    \"\"\"\n\n    def __init__(self, data_source: Sized, batch_size: int, shuffle: bool = True):\n        \"\"\"Initializes the BalancedBatchSampler.\n        Args:\n            data_source: The dataset to sample from.\n            batch_size: The target number of samples in each batch.\n            shuffle: If True, the sampler will shuffle the indices at start of each epoch.\n        \"\"\"\n        if not isinstance(batch_size, int) or batch_size &lt;= 0:\n            raise ValueError(\n                f\"batch_size should be a positive integer, but got {batch_size}\"\n            )\n\n        self.data_source = data_source\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n\n    def __iter__(self) -&gt; Iterator[List[int]]:\n        \"\"\"\n        Returns an iterator over batches of indices.\n        \"\"\"\n        n_samples = len(self.data_source)\n        if n_samples == 0:\n            return\n\n        # Generate a list of indices\n        indices = torch.arange(n_samples)\n        if self.shuffle:\n            # Use a random permutation for shuffling\n            indices = torch.randperm(n_samples)\n\n        # Check for the special case where the last batch would be of size 1.\n        # This logic only applies if there is more than one batch to begin with.\n        if n_samples &gt; self.batch_size and n_samples % self.batch_size == 1:\n            # Calculate the number of full batches to yield before special handling\n            num_full_batches = n_samples // self.batch_size - 1\n\n            # Yield the full-sized batches\n            for i in range(num_full_batches):\n                start_idx = i * self.batch_size\n                end_idx = start_idx + self.batch_size\n                yield indices[start_idx:end_idx].tolist()\n\n            # Handle the last two batches by redistributing them\n            remaining_indices_start = num_full_batches * self.batch_size\n            remaining_indices = indices[remaining_indices_start:]\n\n            # Split the remaining indices (batch_size + 1) into two roughly equal halves\n            split_point = (self.batch_size + 1) // 2\n            yield remaining_indices[:split_point].tolist()\n            yield remaining_indices[split_point:].tolist()\n\n        else:\n            # Standard behavior: yield batches of size `batch_size`\n            # The last batch will have size &gt; 1 or there will be no remainder.\n            for i in range(0, n_samples, self.batch_size):\n                end_idx = min(i + self.batch_size, n_samples)\n                yield indices[i:end_idx].tolist()\n\n    def __len__(self) -&gt; int:\n        \"\"\"\n        Returns the total number of batches in an epoch.\n        \"\"\"\n        n_samples = len(self.data_source)\n        if n_samples == 0:\n            return 0\n\n        # If we are redistributing, we create one extra batch compared to floor division\n        if n_samples &gt; self.batch_size and n_samples % self.batch_size == 1:\n            return n_samples // self.batch_size + 1\n        else:\n            # Standard ceiling division to calculate number of batches\n            return (n_samples + self.batch_size - 1) // self.batch_size\n</code></pre>"},{"location":"api/data/#autoencodix.data.BalancedBatchSampler.__init__","title":"<code>__init__(data_source, batch_size, shuffle=True)</code>","text":"<p>Initializes the BalancedBatchSampler. Args:     data_source: The dataset to sample from.     batch_size: The target number of samples in each batch.     shuffle: If True, the sampler will shuffle the indices at start of each epoch.</p> Source code in <code>src/autoencodix/data/_sampler.py</code> <pre><code>def __init__(self, data_source: Sized, batch_size: int, shuffle: bool = True):\n    \"\"\"Initializes the BalancedBatchSampler.\n    Args:\n        data_source: The dataset to sample from.\n        batch_size: The target number of samples in each batch.\n        shuffle: If True, the sampler will shuffle the indices at start of each epoch.\n    \"\"\"\n    if not isinstance(batch_size, int) or batch_size &lt;= 0:\n        raise ValueError(\n            f\"batch_size should be a positive integer, but got {batch_size}\"\n        )\n\n    self.data_source = data_source\n    self.batch_size = batch_size\n    self.shuffle = shuffle\n</code></pre>"},{"location":"api/data/#autoencodix.data.BalancedBatchSampler.__iter__","title":"<code>__iter__()</code>","text":"<p>Returns an iterator over batches of indices.</p> Source code in <code>src/autoencodix/data/_sampler.py</code> <pre><code>def __iter__(self) -&gt; Iterator[List[int]]:\n    \"\"\"\n    Returns an iterator over batches of indices.\n    \"\"\"\n    n_samples = len(self.data_source)\n    if n_samples == 0:\n        return\n\n    # Generate a list of indices\n    indices = torch.arange(n_samples)\n    if self.shuffle:\n        # Use a random permutation for shuffling\n        indices = torch.randperm(n_samples)\n\n    # Check for the special case where the last batch would be of size 1.\n    # This logic only applies if there is more than one batch to begin with.\n    if n_samples &gt; self.batch_size and n_samples % self.batch_size == 1:\n        # Calculate the number of full batches to yield before special handling\n        num_full_batches = n_samples // self.batch_size - 1\n\n        # Yield the full-sized batches\n        for i in range(num_full_batches):\n            start_idx = i * self.batch_size\n            end_idx = start_idx + self.batch_size\n            yield indices[start_idx:end_idx].tolist()\n\n        # Handle the last two batches by redistributing them\n        remaining_indices_start = num_full_batches * self.batch_size\n        remaining_indices = indices[remaining_indices_start:]\n\n        # Split the remaining indices (batch_size + 1) into two roughly equal halves\n        split_point = (self.batch_size + 1) // 2\n        yield remaining_indices[:split_point].tolist()\n        yield remaining_indices[split_point:].tolist()\n\n    else:\n        # Standard behavior: yield batches of size `batch_size`\n        # The last batch will have size &gt; 1 or there will be no remainder.\n        for i in range(0, n_samples, self.batch_size):\n            end_idx = min(i + self.batch_size, n_samples)\n            yield indices[i:end_idx].tolist()\n</code></pre>"},{"location":"api/data/#autoencodix.data.BalancedBatchSampler.__len__","title":"<code>__len__()</code>","text":"<p>Returns the total number of batches in an epoch.</p> Source code in <code>src/autoencodix/data/_sampler.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"\n    Returns the total number of batches in an epoch.\n    \"\"\"\n    n_samples = len(self.data_source)\n    if n_samples == 0:\n        return 0\n\n    # If we are redistributing, we create one extra batch compared to floor division\n    if n_samples &gt; self.batch_size and n_samples % self.batch_size == 1:\n        return n_samples // self.batch_size + 1\n    else:\n        # Standard ceiling division to calculate number of batches\n        return (n_samples + self.batch_size - 1) // self.batch_size\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataFilter","title":"<code>DataFilter</code>","text":"<p>Preprocesses dataframes, including filtering and scaling.</p> <p>This class separates the filtering logic that needs to be applied consistently across train, validation, and test sets from the scaling logic that is typically fitted on the training data and then applied to the other sets.</p> <p>Attributes:</p> Name Type Description <code>data_info</code> <p>Configuration object containing preprocessing parameters.</p> <code>filtered_features</code> <code>Optional[Set[str]]</code> <p>Set of features to keep after filtering on the training data. None initially.</p> <code>_scaler</code> <p>The fitted scaler object. None initially.</p> <code>ontologies</code> <p>Ontology information, if provided for Ontix.</p> <code>config</code> <p>Configuration object containing default parameters.</p> Source code in <code>src/autoencodix/data/_filter.py</code> <pre><code>class DataFilter:\n    \"\"\"Preprocesses dataframes, including filtering and scaling.\n\n    This class separates the filtering logic that needs to be applied consistently\n    across train, validation, and test sets from the scaling logic that is\n    typically fitted on the training data and then applied to the other sets.\n\n    Attributes:\n        data_info: Configuration object containing preprocessing parameters.\n        filtered_features: Set of features to keep after filtering on the training data. None initially.\n        _scaler: The fitted scaler object. None initially.\n        ontologies: Ontology information, if provided for Ontix.\n        config: Configuration object containing default parameters.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_info: DataInfo,\n        config: DefaultConfig,\n        ontologies: Optional[tuple] = None,\n    ):  # Addition to Varix, mandotory for Ontix\n        \"\"\"Initializes the DataFilter with a configuration.\n\n        Args:\n            data_info: Configuration object containing preprocessing parameters.\n            config: Configuration object containing default parameters.\n            ontologies: Ontology information, if provided for Ontix.\n        \"\"\"\n        self.data_info = data_info\n        self.config = config\n        self.filtered_features: Optional[Set[str]] = None\n        self._scaler = None\n        self.ontologies = ontologies  # Addition to Varix, mandotory for Ontix\n        self._init_scaler()\n\n    def _filter_nonzero_variance(self, df: pd.DataFrame) -&gt; pd.Series:\n        \"\"\"Removes features with zero variance.\n\n        Args:\n            df: Input dataframe.\n\n        Returns:\n            Filtered dataframe containing only columns with non-zero variance.\n        \"\"\"\n        var = pd.Series(np.var(df, axis=0), index=df.columns)\n        return df[var[var &gt; 0].index]\n\n    def _filter_by_variance(\n        self, df: pd.DataFrame, k: Optional[int]\n    ) -&gt; Union[pd.Series, pd.DataFrame]:\n        \"\"\"Keeps top k features by variance.\n\n        Args:\n            df: Input dataframe.\n            k: Number of top variance features to keep. If None or greater\n               than number of columns, all features are kept.\n\n        Returns:\n            Filtered dataframe with top k variance features.\n        \"\"\"\n        if k is None or k &gt; df.shape[1]:\n            warnings.warn(\n                \"WARNING: k is None or greater than number of columns, keeping all features.\"\n            )\n            return df\n        var = pd.Series(np.var(df, axis=0), index=df.columns)\n        return df[var.sort_values(ascending=False).index[:k]]\n\n    def _filter_by_mad(\n        self, df: pd.DataFrame, k: Optional[int]\n    ) -&gt; Union[pd.Series, pd.DataFrame]:\n        \"\"\"Keeps top k features by median absolute deviation.\n\n        Args:\n            df: Input dataframe.\n            k: Number of top MAD features to keep. If None or greater\n               than number of columns, all features are kept.\n\n        Returns:\n            Filtered dataframe with top k MAD features.\n        \"\"\"\n        if k is None or k &gt; df.shape[1]:\n            return df\n        mads = pd.Series(median_abs_deviation(df, axis=0), index=df.columns)\n        return df[mads.sort_values(ascending=False).index[:k]]\n\n    def _filter_by_correlation(\n        self, df: pd.DataFrame, k: Optional[int]\n    ) -&gt; Union[pd.Series, pd.DataFrame]:\n        \"\"\"Filters features using correlation-based clustering.\n\n        This method clusters features based on their correlation distance and\n        selects a representative feature (medoid) from each cluster.\n\n        Args:\n            df: Input dataframe.\n            k: Number of clusters to create. If None or greater\n               than number of columns, all features are kept.\n\n        Returns:\n            Filtered dataframe with one representative feature (medoid) per cluster.\n        \"\"\"\n        if k is None or k &gt; df.shape[1]:\n            warnings.warn(\n                \"WARNING: k is None or greater than number of columns, keeping all features.\"\n            )\n            return df\n        else:\n            X = df.transpose().values\n\n            dist_matrix = squareform(pdist(X, metric=\"correlation\"))\n\n            clustering = AgglomerativeClustering(\n                n_clusters=k,\n            ).fit(dist_matrix)\n\n            medoid_indices = []\n            for i in range(k):\n                cluster_points = np.where(clustering.labels_ == i)[0]\n                if len(cluster_points) &gt; 0:\n                    # The medoid is the point with minimum sum of distances to other points in the cluster\n                    cluster_dist_matrix = dist_matrix[\n                        np.ix_(cluster_points, cluster_points)\n                    ]\n                    sum_distances = np.sum(cluster_dist_matrix, axis=1)\n                    medoid_idx = cluster_points[np.argmin(sum_distances)]\n                    medoid_indices.append(medoid_idx)\n\n            df_filt: Union[pd.DataFrame, pd.Series] = df.iloc[:, medoid_indices]\n            return df_filt\n\n    def filter(\n        self, df: pd.DataFrame, genes_to_keep: Optional[List] = None\n    ) -&gt; Tuple[Union[pd.Series, pd.DataFrame], List[str]]:\n        \"\"\"Applies the configured filtering method to the dataframe.\n\n        This method is intended to be called on the training data to determine\n        which features to keep. The `filtered_features` attribute will be set\n        based on the result.\n\n        Args:\n            df: Input dataframe to be filtered (typically the training set).\n            genes_to_keep: A list of gene names to explicitly keep.\n                If provided, other filtering methods will be ignored.\n\n        Returns:\n            A tuple containing:\n                - The filtered dataframe.\n                - A list of column names (features) that were kept.\n\n        Raises:\n            KeyError: If some genes in `genes_to_keep` are not present in the dataframe.\n        \"\"\"\n        if genes_to_keep is not None:\n            try:\n                df: Union[pd.Series, pd.DataFrame] = df[genes_to_keep]\n                return df, genes_to_keep\n            except KeyError as e:\n                raise KeyError(\n                    f\"Some genes in genes_to_keep are not present in the dataframe: {e}\"\n                )\n\n        MIN_FILTER = 2\n        filtering_method = FilterMethod(self.data_info.filtering)\n\n        if df.shape[0] &lt; MIN_FILTER or df.empty:\n            warnings.warn(\n                f\"WARNING: df is too small for filtering, needs to have at least {MIN_FILTER}\"\n            )\n            return df, df.columns.tolist()\n\n        filtered_df = df.copy()\n\n        ## Remove features which are not in the ontology for Ontix architecture\n        ## must be done before other filtering is applied\n        if hasattr(self, \"ontologies\") and self.ontologies is not None:\n            all_feature_names: Union[Set, List] = set()\n            for key, values in self.ontologies[-1].items():\n                all_feature_names.update(values)\n            all_feature_names = list(all_feature_names)\n            feature_order = filtered_df.columns.tolist()\n            missing_features = [f for f in feature_order if f not in all_feature_names]\n            ## Filter out features not in the ontology\n            feature_order = [f for f in feature_order if f in all_feature_names]\n            if missing_features:\n                print(\n                    f\"Features in feature_order not found in all_feature_names: {missing_features}\"\n                )\n\n            filtered_df = filtered_df.loc[:, feature_order]\n\n        ####\n\n        if filtering_method == FilterMethod.NOFILT:\n            return filtered_df, df.columns.tolist()\n        if self.data_info.k_filter is None:\n            return filtered_df, df.columns.tolist()\n\n        if filtering_method == FilterMethod.NONZEROVAR:\n            filtered_df = self._filter_nonzero_variance(filtered_df)\n        elif filtering_method == FilterMethod.VAR:\n            filtered_df = self._filter_nonzero_variance(filtered_df)\n            filtered_df = self._filter_by_variance(filtered_df, self.data_info.k_filter)\n        elif filtering_method == FilterMethod.MAD:\n            filtered_df = self._filter_nonzero_variance(filtered_df)\n            filtered_df = self._filter_by_mad(filtered_df, self.data_info.k_filter)\n        elif filtering_method == FilterMethod.CORR:\n            filtered_df = self._filter_nonzero_variance(filtered_df)\n            filtered_df = self._filter_by_correlation(\n                filtered_df, self.data_info.k_filter\n            )\n        elif filtering_method == FilterMethod.VARCORR:\n            filtered_df = self._filter_nonzero_variance(filtered_df)\n            filtered_df = self._filter_by_variance(\n                filtered_df,\n                self.data_info.k_filter * 10 if self.data_info.k_filter else None,\n            )\n            if self.data_info.k_filter is not None:\n                # Apply correlation filter on the already variance-filtered data\n                num_features_after_var = filtered_df.shape[1]\n                k_corr = min(self.data_info.k_filter, num_features_after_var)\n                filtered_df = self._filter_by_correlation(filtered_df, k_corr)\n\n        return filtered_df, filtered_df.columns.tolist()\n\n    def _init_scaler(self) -&gt; None:\n        \"\"\"Initializes the scaler based on the configured scaling method.\"\"\"\n        self.method = self.data_info.scaling\n\n        if self.method == \"NOTSET\":\n            # if not set in data config, we use the global scaling config\n            self.method = self.config.scaling\n        if self.method == \"MINMAX\":\n            self._scaler = MinMaxScaler(clip=True)\n        elif self.method == \"STANDARD\":\n            self._scaler = StandardScaler()\n        elif self.method == \"ROBUST\":\n            self._scaler = RobustScaler()\n        elif self.method == \"MAXABS\":\n            self._scaler = MaxAbsScaler()\n        else:\n            self._scaler = None\n\n    def fit_scaler(self, df: Union[pd.Series, pd.DataFrame]) -&gt; Any:\n        \"\"\"Fits the scaler to the input dataframe (typically the training set).\n\n        Args:\n            df: Input dataframe to fit the scaler on.\n\n        Returns:\n            The fitted scaler object.\n        \"\"\"\n        self._init_scaler()\n        if self._scaler is not None:\n            self._scaler.fit(df)\n        else:\n            warnings.warn(\"No scaling applied.\")\n        return self._scaler\n\n    def scale(\n        self, df: Union[pd.Series, pd.DataFrame], scaler: Any\n    ) -&gt; Union[pd.Series, pd.DataFrame]:\n        \"\"\"Applies the fitted scaler to the input dataframe.\n\n        Args:\n            df: Input dataframe to be scaled.\n            scaler: The fitted scaler object.\n\n        Returns:\n            Scaled dataframe.\n        \"\"\"\n        if self.method == \"LOG1P\":\n            X_log = np.log1p(df.values)\n            X_norm = X_log / np.log1p(np.max(X_log, axis=0))\n            df_scaled = pd.DataFrame(X_norm, columns=df.columns, index=df.index)\n            return df_scaled\n        if scaler is None:\n            warnings.warn(\"No scaler has been fitted yet or scaling is set to none.\")\n            return df\n        df_scaled = pd.DataFrame(\n            scaler.transform(df), columns=df.columns, index=df.index\n        )\n        return df_scaled\n\n    @property\n    def available_methods(self) -&gt; List[str]:\n        \"\"\"Lists all available filtering methods.\n\n        Returns:\n            List of available filtering method names.\n        \"\"\"\n        return [method.value for method in FilterMethod]\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataFilter.available_methods","title":"<code>available_methods</code>  <code>property</code>","text":"<p>Lists all available filtering methods.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of available filtering method names.</p>"},{"location":"api/data/#autoencodix.data.DataFilter.__init__","title":"<code>__init__(data_info, config, ontologies=None)</code>","text":"<p>Initializes the DataFilter with a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>data_info</code> <code>DataInfo</code> <p>Configuration object containing preprocessing parameters.</p> required <code>config</code> <code>DefaultConfig</code> <p>Configuration object containing default parameters.</p> required <code>ontologies</code> <code>Optional[tuple]</code> <p>Ontology information, if provided for Ontix.</p> <code>None</code> Source code in <code>src/autoencodix/data/_filter.py</code> <pre><code>def __init__(\n    self,\n    data_info: DataInfo,\n    config: DefaultConfig,\n    ontologies: Optional[tuple] = None,\n):  # Addition to Varix, mandotory for Ontix\n    \"\"\"Initializes the DataFilter with a configuration.\n\n    Args:\n        data_info: Configuration object containing preprocessing parameters.\n        config: Configuration object containing default parameters.\n        ontologies: Ontology information, if provided for Ontix.\n    \"\"\"\n    self.data_info = data_info\n    self.config = config\n    self.filtered_features: Optional[Set[str]] = None\n    self._scaler = None\n    self.ontologies = ontologies  # Addition to Varix, mandotory for Ontix\n    self._init_scaler()\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataFilter.filter","title":"<code>filter(df, genes_to_keep=None)</code>","text":"<p>Applies the configured filtering method to the dataframe.</p> <p>This method is intended to be called on the training data to determine which features to keep. The <code>filtered_features</code> attribute will be set based on the result.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input dataframe to be filtered (typically the training set).</p> required <code>genes_to_keep</code> <code>Optional[List]</code> <p>A list of gene names to explicitly keep. If provided, other filtering methods will be ignored.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[Union[Series, DataFrame], List[str]]</code> <p>A tuple containing: - The filtered dataframe. - A list of column names (features) that were kept.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If some genes in <code>genes_to_keep</code> are not present in the dataframe.</p> Source code in <code>src/autoencodix/data/_filter.py</code> <pre><code>def filter(\n    self, df: pd.DataFrame, genes_to_keep: Optional[List] = None\n) -&gt; Tuple[Union[pd.Series, pd.DataFrame], List[str]]:\n    \"\"\"Applies the configured filtering method to the dataframe.\n\n    This method is intended to be called on the training data to determine\n    which features to keep. The `filtered_features` attribute will be set\n    based on the result.\n\n    Args:\n        df: Input dataframe to be filtered (typically the training set).\n        genes_to_keep: A list of gene names to explicitly keep.\n            If provided, other filtering methods will be ignored.\n\n    Returns:\n        A tuple containing:\n            - The filtered dataframe.\n            - A list of column names (features) that were kept.\n\n    Raises:\n        KeyError: If some genes in `genes_to_keep` are not present in the dataframe.\n    \"\"\"\n    if genes_to_keep is not None:\n        try:\n            df: Union[pd.Series, pd.DataFrame] = df[genes_to_keep]\n            return df, genes_to_keep\n        except KeyError as e:\n            raise KeyError(\n                f\"Some genes in genes_to_keep are not present in the dataframe: {e}\"\n            )\n\n    MIN_FILTER = 2\n    filtering_method = FilterMethod(self.data_info.filtering)\n\n    if df.shape[0] &lt; MIN_FILTER or df.empty:\n        warnings.warn(\n            f\"WARNING: df is too small for filtering, needs to have at least {MIN_FILTER}\"\n        )\n        return df, df.columns.tolist()\n\n    filtered_df = df.copy()\n\n    ## Remove features which are not in the ontology for Ontix architecture\n    ## must be done before other filtering is applied\n    if hasattr(self, \"ontologies\") and self.ontologies is not None:\n        all_feature_names: Union[Set, List] = set()\n        for key, values in self.ontologies[-1].items():\n            all_feature_names.update(values)\n        all_feature_names = list(all_feature_names)\n        feature_order = filtered_df.columns.tolist()\n        missing_features = [f for f in feature_order if f not in all_feature_names]\n        ## Filter out features not in the ontology\n        feature_order = [f for f in feature_order if f in all_feature_names]\n        if missing_features:\n            print(\n                f\"Features in feature_order not found in all_feature_names: {missing_features}\"\n            )\n\n        filtered_df = filtered_df.loc[:, feature_order]\n\n    ####\n\n    if filtering_method == FilterMethod.NOFILT:\n        return filtered_df, df.columns.tolist()\n    if self.data_info.k_filter is None:\n        return filtered_df, df.columns.tolist()\n\n    if filtering_method == FilterMethod.NONZEROVAR:\n        filtered_df = self._filter_nonzero_variance(filtered_df)\n    elif filtering_method == FilterMethod.VAR:\n        filtered_df = self._filter_nonzero_variance(filtered_df)\n        filtered_df = self._filter_by_variance(filtered_df, self.data_info.k_filter)\n    elif filtering_method == FilterMethod.MAD:\n        filtered_df = self._filter_nonzero_variance(filtered_df)\n        filtered_df = self._filter_by_mad(filtered_df, self.data_info.k_filter)\n    elif filtering_method == FilterMethod.CORR:\n        filtered_df = self._filter_nonzero_variance(filtered_df)\n        filtered_df = self._filter_by_correlation(\n            filtered_df, self.data_info.k_filter\n        )\n    elif filtering_method == FilterMethod.VARCORR:\n        filtered_df = self._filter_nonzero_variance(filtered_df)\n        filtered_df = self._filter_by_variance(\n            filtered_df,\n            self.data_info.k_filter * 10 if self.data_info.k_filter else None,\n        )\n        if self.data_info.k_filter is not None:\n            # Apply correlation filter on the already variance-filtered data\n            num_features_after_var = filtered_df.shape[1]\n            k_corr = min(self.data_info.k_filter, num_features_after_var)\n            filtered_df = self._filter_by_correlation(filtered_df, k_corr)\n\n    return filtered_df, filtered_df.columns.tolist()\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataFilter.fit_scaler","title":"<code>fit_scaler(df)</code>","text":"<p>Fits the scaler to the input dataframe (typically the training set).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[Series, DataFrame]</code> <p>Input dataframe to fit the scaler on.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The fitted scaler object.</p> Source code in <code>src/autoencodix/data/_filter.py</code> <pre><code>def fit_scaler(self, df: Union[pd.Series, pd.DataFrame]) -&gt; Any:\n    \"\"\"Fits the scaler to the input dataframe (typically the training set).\n\n    Args:\n        df: Input dataframe to fit the scaler on.\n\n    Returns:\n        The fitted scaler object.\n    \"\"\"\n    self._init_scaler()\n    if self._scaler is not None:\n        self._scaler.fit(df)\n    else:\n        warnings.warn(\"No scaling applied.\")\n    return self._scaler\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataFilter.scale","title":"<code>scale(df, scaler)</code>","text":"<p>Applies the fitted scaler to the input dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>Union[Series, DataFrame]</code> <p>Input dataframe to be scaled.</p> required <code>scaler</code> <code>Any</code> <p>The fitted scaler object.</p> required <p>Returns:</p> Type Description <code>Union[Series, DataFrame]</code> <p>Scaled dataframe.</p> Source code in <code>src/autoencodix/data/_filter.py</code> <pre><code>def scale(\n    self, df: Union[pd.Series, pd.DataFrame], scaler: Any\n) -&gt; Union[pd.Series, pd.DataFrame]:\n    \"\"\"Applies the fitted scaler to the input dataframe.\n\n    Args:\n        df: Input dataframe to be scaled.\n        scaler: The fitted scaler object.\n\n    Returns:\n        Scaled dataframe.\n    \"\"\"\n    if self.method == \"LOG1P\":\n        X_log = np.log1p(df.values)\n        X_norm = X_log / np.log1p(np.max(X_log, axis=0))\n        df_scaled = pd.DataFrame(X_norm, columns=df.columns, index=df.index)\n        return df_scaled\n    if scaler is None:\n        warnings.warn(\"No scaler has been fitted yet or scaling is set to none.\")\n        return df\n    df_scaled = pd.DataFrame(\n        scaler.transform(df), columns=df.columns, index=df.index\n    )\n    return df_scaled\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage","title":"<code>DataPackage</code>  <code>dataclass</code>","text":"<p>Represents a data package containing multiple types of data.</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>@dataclass\nclass DataPackage:\n    \"\"\"Represents a data package containing multiple types of data.\"\"\"\n\n    multi_sc: Optional[Dict[str, MuData]] = None  # # ty: ignore[invalid-type-form]\n    multi_bulk: Optional[Dict[str, pd.DataFrame]] = None\n    annotation: Optional[Dict[str, Union[pd.DataFrame, None]]] = None\n    img: Optional[Dict[str, List[ImgData]]] = None\n\n    from_modality: Optional[\n        Dict[\n            str,\n            Union[\n                pd.DataFrame,\n                List[ImgData],\n                MuData,  # ty: ignore[invalid-type-form]\n                AnnData,  # ty: ignore[invalid-type-form]\n            ],\n        ]  # ty: ignore[invalid-type-form]\n    ] = field(default_factory=dict, repr=False)\n    to_modality: Optional[\n        Dict[\n            str,\n            Union[\n                pd.DataFrame,\n                List[ImgData],\n                MuData,  # ty: ignore[invalid-type-form]\n                AnnData,  # ty: ignore[invalid-type-form]\n            ],  # ty: ignore[invalid-type-form]\n        ]  # ty: ignore[invalid-type-form]\n    ] = field(default_factory=dict, repr=False)\n\n    def __getitem__(self, key: str) -&gt; Any:\n        \"\"\"Allow dictionary-like access to top-level attributes.\"\"\"\n        if hasattr(self, key):\n            return getattr(self, key)\n        raise KeyError(f\"{key} not found in DataPackage.\")\n\n    def __setitem__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Allow dictionary-like item assignment to top-level attributes.\"\"\"\n        if hasattr(self, key):\n            setattr(self, key, value)\n        else:\n            raise KeyError(f\"{key} not found in DataPackage.\")\n\n    def __iter__(self) -&gt; Iterator[Tuple[str, Any]]:\n        \"\"\"Make DataPackage iterable, yielding (key, value) pairs.\n\n        For dictionary attributes, yields nested items as (parent_key.child_key, value).\n        \"\"\"\n        for attr_name in self.__annotations__.keys():\n            attr_value = getattr(self, attr_name)\n\n            if attr_value is None:\n                continue\n            if isinstance(attr_value, dict):\n                for sub_key, sub_value in attr_value.items():\n                    yield f\"{attr_name}.{sub_key}\", sub_value\n            else:\n                yield attr_name, attr_value\n\n    def format_shapes(self) -&gt; str:\n        \"\"\"Format the shape dictionary in a clean, readable way.\"\"\"\n        shapes = self.shape()\n        lines = []\n\n        for data_type, data_info in shapes.items():\n            # Skip empty entries\n            if not data_info:\n                continue\n\n            sub_items = []\n            for subtype, shape in data_info.items():\n                if shape is not None:\n                    if isinstance(shape, tuple):\n                        sub_items.append(\n                            f\"{subtype}: {shape[0]} samples \u00d7 {shape[1]} features\"\n                        )\n                    else:\n                        sub_items.append(f\"{subtype}: {shape} items\")\n\n            if sub_items:\n                lines.append(f\"{data_type}:\")\n                lines.extend(f\"  {item}\" for item in sub_items)\n\n        if not lines:\n            return \"Empty DataPackage\"\n\n        return \"\\n\".join(lines)\n\n    def __str__(self) -&gt; str:\n        return self.format_shapes()\n\n    def __repr__(self) -&gt; str:\n        return self.__str__()\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check if the data package is empty.\"\"\"\n        return all(\n            [\n                self.multi_sc is None,\n                self.multi_bulk is None or len(self.multi_bulk) == 0,\n                self.annotation is None,\n                self.img is None,\n                not self.from_modality,\n                not self.to_modality,\n            ]\n        )\n\n    def get_n_samples(self) -&gt; Dict[str, Dict[str, int]]:\n        \"\"\"Get the number of samples for each data type in nested dictionary format.\n\n        Returns:\n            Dictionary with nested structure: {modality_type: {sub_key: count}}\n        \"\"\"\n        n_samples: Dict[str, Dict[str, int]] = {}\n\n        # Process each main attribute\n        for attr_name in self.__annotations__.keys():\n            attr_value = getattr(self, attr_name)\n\n            if isinstance(attr_value, dict):\n                # Handle dictionary attributes (multi_sc, multi_bulk, etc.)\n                sub_counts = {}\n                for sub_key, sub_value in attr_value.items():\n                    if sub_value is None or len(sub_value) == 0:\n                        continue\n                    sub_counts[sub_key] = self._get_n_samples(sub_value)\n                n_samples[attr_name] = sub_counts if sub_counts else {}\n            else:\n                # Handle non-dictionary attributes\n                count = self._get_n_samples(attr_value)\n                n_samples[attr_name] = {attr_name: count}\n\n        paired_count = self._calculate_paired_count()\n        n_samples[\"paired_count\"] = {\"paired_count\": paired_count}\n\n        return n_samples\n\n    def _calculate_paired_count(self) -&gt; int:\n        \"\"\"\n        Calculate the number of samples that are common across modalities that have data.\n\n        Returns:\n            Number of common samples across modalities with data\n        \"\"\"\n        all_counts = []\n\n        # Collect all sample counts from modalities that have data\n        for attr_name in self.__annotations__.keys():\n            attr_value = getattr(self, attr_name)\n            if attr_value is None:\n                continue\n\n            if isinstance(attr_value, dict):\n                if attr_value:  # Non-empty dictionary\n                    for sub_value in attr_value.values():\n                        if sub_value is not None:\n                            count = self._get_n_samples(sub_value)\n                            if count &gt; 0:\n                                all_counts.append(count)\n            else:\n                count = self._get_n_samples(attr_value)\n                if count &gt; 0:\n                    all_counts.append(count)\n\n        # Return minimum count (intersection) or 0 if no data\n        return min(all_counts) if all_counts else 0\n\n    def get_common_ids(self) -&gt; List[str]:\n        \"\"\"Get the common sample IDs across modalities that have data.\n\n        Returns:\n            List of sample IDs that are present in all modalities with data\n        \"\"\"\n        all_ids = []\n\n        # Collect sample IDs from each modality that has data\n        for attr_name in self.__annotations__.keys():\n            attr_value = getattr(self, attr_name)\n            if attr_value is None:\n                continue\n\n            if isinstance(attr_value, dict):\n                if attr_value:  # Non-empty dictionary\n                    for sub_value in attr_value.values():\n                        if sub_value is not None:\n                            ids = self._get_sample_ids(sub_value)\n                            if ids:\n                                all_ids.append(set(ids))\n            else:\n                ids = self._get_sample_ids(attr_value)\n                if ids:\n                    all_ids.append(set(ids))\n\n        # Find intersection of all ID sets\n        if not all_ids:\n            return []\n\n        common = all_ids[0]\n        for id_set in all_ids[1:]:\n            common = common.intersection(id_set)\n\n        return sorted(list(common))\n\n    def _get_sample_ids(\n        self,\n        dataobj: Union[\n            MuData,  # ty: ignore[invalid-type-form]\n            pd.DataFrame,\n            List[ImgData],\n            AnnData,\n        ],  # ty: ignore[invalid-type-form]\n    ) -&gt; List[str]:\n        \"\"\"\n        Extract sample IDs from a data object.\n\n        Args:\n            dataobj: Data object to extract IDs from\n\n        Returns:\n            List of sample IDs\n        \"\"\"\n        if dataobj is None:\n            return []\n\n        if isinstance(dataobj, pd.DataFrame):\n            return dataobj.index.astype(str).tolist()\n        elif isinstance(dataobj, list):\n            # For lists of ImgData, extract sample_id from each object\n            return [\n                img_data.sample_id\n                for img_data in dataobj\n                if hasattr(img_data, \"sample_id\")\n            ]\n        elif isinstance(dataobj, AnnData):\n            return dataobj.obs.index.astype(str).tolist()\n        elif isinstance(dataobj, MuData):\n            # For MuData, we can use the obs.index directly\n            return dataobj.obs.index.astype(str).tolist()  # ty: ignore\n        else:\n            return []\n\n    def _get_n_samples(\n        self,\n        dataobj: Union[\n            MuData,  # ty: ignore[invalid-type-form]\n            pd.DataFrame,\n            List[ImgData],\n            AnnData,\n            Dict,\n        ],\n    ) -&gt; int:\n        \"\"\"Get the number of samples for a specific attribute.\"\"\"\n        if dataobj is None:\n            return 0\n\n        if isinstance(dataobj, pd.DataFrame):\n            return dataobj.shape[0]\n        elif isinstance(dataobj, dict):\n            if not dataobj:  # Empty dict\n                return 0\n            first_value = next(iter(dataobj.values()))\n            return self._get_n_samples(first_value)\n        elif isinstance(dataobj, list):\n            return len(dataobj)\n        elif isinstance(dataobj, AnnData):\n            return dataobj.obs.shape[0]\n        elif isinstance(dataobj, MuData):\n            if not dataobj.mod:  # ty: ignore\n                return 0\n            return dataobj.n_obs  # ty: ignore\n        else:\n            raise ValueError(\n                f\"Unknown data type {type(dataobj)} for dataobj. Probably you've implemented a new attribute in the DataPackage class or changed the data type of an existing attribute.\"\n            )\n\n    def shape(self) -&gt; Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Get the shape of the data for each data type in nested dictionary format.\n\n        Returns:\n            Dictionary with nested structure: {modality_type: {sub_key: shape}}\n        \"\"\"\n        shapes: Dict[str, Dict[str, Any]] = {}\n\n        for attr_name in self.__annotations__.keys():\n            attr_value = getattr(self, attr_name)\n\n            if isinstance(attr_value, dict):\n                # Handle dictionary attributes\n                if attr_value is None or len(attr_value) == 0:\n                    # Empty or None dictionary\n                    shapes[attr_name] = {}\n                else:\n                    sub_dict = self._get_shape_from_dict(attr_value)\n                    shapes[attr_name] = sub_dict\n            else:\n                # Handle non-dictionary attributes\n                shape = self._get_single_shape(attr_value)\n                shapes[attr_name] = {attr_name: shape}\n\n        return shapes\n\n    def _get_single_shape(self, dataobj: Any) -&gt; Optional[Union[Tuple, int]]:\n        \"\"\"Get shape for a single data object.\"\"\"\n        if dataobj is None:\n            return None\n        elif isinstance(dataobj, list):\n            return len(dataobj)\n        elif isinstance(dataobj, pd.DataFrame):\n            return dataobj.shape\n        elif isinstance(dataobj, AnnData):\n            return dataobj.shape\n        elif isinstance(dataobj, MuData):\n            return (dataobj.n_obs, dataobj.n_vars)\n        else:\n            return None\n\n    def _get_shape_from_dict(self, data_dict: Dict) -&gt; Dict[str, Any]:\n        \"\"\"Recursively process dictionary to extract shapes of contained data objects.\n\n        Args::\n            data_dict: Dictionary containing data objects\n\n        Returns:\n            Dictionary with shapes information\n        \"\"\"\n        result: Dict[str, Any] = {}\n        for key, value in data_dict.items():\n            if isinstance(value, pd.DataFrame):\n                result[key] = value.shape\n            elif isinstance(value, list):\n                # For lists of objects, just store the length\n                result[key] = len(value)\n            elif isinstance(value, AnnData):\n                result[key] = value.shape\n            elif isinstance(value, MuData):\n                result[key] = (value.n_obs, value.n_vars)\n            elif isinstance(value, dict):\n                # Recursively process nested dictionaries\n                nested_result = self._get_shape_from_dict(value)\n                result[key] = nested_result\n            elif value is None:\n                result[key] = None\n            else:\n                # For unknown types, store a descriptive string instead of raising an error\n                # This is more robust as it won't crash the entire method\n                result[key] = f\"&lt;{type(value).__name__}&gt;\"\n\n        return result\n\n    def get_modality_key(self, direction: str) -&gt; Optional[str]:\n        \"\"\"Get the first key for a specific direction's modality.\n\n        Args:\n            direction: Either 'from' or 'to'\n\n        Returns:\n            First key of the modality dictionary or None if empty\n        \"\"\"\n        if direction not in [\"from\", \"to\"]:\n            raise ValueError(f\"Direction must be 'from' or 'to', got {direction}\")\n\n        modality_dict = self.from_modality if direction == \"from\" else self.to_modality\n        if not modality_dict:\n            return None\n\n        return next(iter(modality_dict.keys()), None)\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Allow dictionary-like access to top-level attributes.</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Any:\n    \"\"\"Allow dictionary-like access to top-level attributes.\"\"\"\n    if hasattr(self, key):\n        return getattr(self, key)\n    raise KeyError(f\"{key} not found in DataPackage.\")\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage.__iter__","title":"<code>__iter__()</code>","text":"<p>Make DataPackage iterable, yielding (key, value) pairs.</p> <p>For dictionary attributes, yields nested items as (parent_key.child_key, value).</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>def __iter__(self) -&gt; Iterator[Tuple[str, Any]]:\n    \"\"\"Make DataPackage iterable, yielding (key, value) pairs.\n\n    For dictionary attributes, yields nested items as (parent_key.child_key, value).\n    \"\"\"\n    for attr_name in self.__annotations__.keys():\n        attr_value = getattr(self, attr_name)\n\n        if attr_value is None:\n            continue\n        if isinstance(attr_value, dict):\n            for sub_key, sub_value in attr_value.items():\n                yield f\"{attr_name}.{sub_key}\", sub_value\n        else:\n            yield attr_name, attr_value\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Allow dictionary-like item assignment to top-level attributes.</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>def __setitem__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Allow dictionary-like item assignment to top-level attributes.\"\"\"\n    if hasattr(self, key):\n        setattr(self, key, value)\n    else:\n        raise KeyError(f\"{key} not found in DataPackage.\")\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage.format_shapes","title":"<code>format_shapes()</code>","text":"<p>Format the shape dictionary in a clean, readable way.</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>def format_shapes(self) -&gt; str:\n    \"\"\"Format the shape dictionary in a clean, readable way.\"\"\"\n    shapes = self.shape()\n    lines = []\n\n    for data_type, data_info in shapes.items():\n        # Skip empty entries\n        if not data_info:\n            continue\n\n        sub_items = []\n        for subtype, shape in data_info.items():\n            if shape is not None:\n                if isinstance(shape, tuple):\n                    sub_items.append(\n                        f\"{subtype}: {shape[0]} samples \u00d7 {shape[1]} features\"\n                    )\n                else:\n                    sub_items.append(f\"{subtype}: {shape} items\")\n\n        if sub_items:\n            lines.append(f\"{data_type}:\")\n            lines.extend(f\"  {item}\" for item in sub_items)\n\n    if not lines:\n        return \"Empty DataPackage\"\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage.get_common_ids","title":"<code>get_common_ids()</code>","text":"<p>Get the common sample IDs across modalities that have data.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of sample IDs that are present in all modalities with data</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>def get_common_ids(self) -&gt; List[str]:\n    \"\"\"Get the common sample IDs across modalities that have data.\n\n    Returns:\n        List of sample IDs that are present in all modalities with data\n    \"\"\"\n    all_ids = []\n\n    # Collect sample IDs from each modality that has data\n    for attr_name in self.__annotations__.keys():\n        attr_value = getattr(self, attr_name)\n        if attr_value is None:\n            continue\n\n        if isinstance(attr_value, dict):\n            if attr_value:  # Non-empty dictionary\n                for sub_value in attr_value.values():\n                    if sub_value is not None:\n                        ids = self._get_sample_ids(sub_value)\n                        if ids:\n                            all_ids.append(set(ids))\n        else:\n            ids = self._get_sample_ids(attr_value)\n            if ids:\n                all_ids.append(set(ids))\n\n    # Find intersection of all ID sets\n    if not all_ids:\n        return []\n\n    common = all_ids[0]\n    for id_set in all_ids[1:]:\n        common = common.intersection(id_set)\n\n    return sorted(list(common))\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage.get_modality_key","title":"<code>get_modality_key(direction)</code>","text":"<p>Get the first key for a specific direction's modality.</p> <p>Parameters:</p> Name Type Description Default <code>direction</code> <code>str</code> <p>Either 'from' or 'to'</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>First key of the modality dictionary or None if empty</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>def get_modality_key(self, direction: str) -&gt; Optional[str]:\n    \"\"\"Get the first key for a specific direction's modality.\n\n    Args:\n        direction: Either 'from' or 'to'\n\n    Returns:\n        First key of the modality dictionary or None if empty\n    \"\"\"\n    if direction not in [\"from\", \"to\"]:\n        raise ValueError(f\"Direction must be 'from' or 'to', got {direction}\")\n\n    modality_dict = self.from_modality if direction == \"from\" else self.to_modality\n    if not modality_dict:\n        return None\n\n    return next(iter(modality_dict.keys()), None)\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage.get_n_samples","title":"<code>get_n_samples()</code>","text":"<p>Get the number of samples for each data type in nested dictionary format.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, int]]</code> <p>Dictionary with nested structure: {modality_type: {sub_key: count}}</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>def get_n_samples(self) -&gt; Dict[str, Dict[str, int]]:\n    \"\"\"Get the number of samples for each data type in nested dictionary format.\n\n    Returns:\n        Dictionary with nested structure: {modality_type: {sub_key: count}}\n    \"\"\"\n    n_samples: Dict[str, Dict[str, int]] = {}\n\n    # Process each main attribute\n    for attr_name in self.__annotations__.keys():\n        attr_value = getattr(self, attr_name)\n\n        if isinstance(attr_value, dict):\n            # Handle dictionary attributes (multi_sc, multi_bulk, etc.)\n            sub_counts = {}\n            for sub_key, sub_value in attr_value.items():\n                if sub_value is None or len(sub_value) == 0:\n                    continue\n                sub_counts[sub_key] = self._get_n_samples(sub_value)\n            n_samples[attr_name] = sub_counts if sub_counts else {}\n        else:\n            # Handle non-dictionary attributes\n            count = self._get_n_samples(attr_value)\n            n_samples[attr_name] = {attr_name: count}\n\n    paired_count = self._calculate_paired_count()\n    n_samples[\"paired_count\"] = {\"paired_count\": paired_count}\n\n    return n_samples\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage.is_empty","title":"<code>is_empty()</code>","text":"<p>Check if the data package is empty.</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check if the data package is empty.\"\"\"\n    return all(\n        [\n            self.multi_sc is None,\n            self.multi_bulk is None or len(self.multi_bulk) == 0,\n            self.annotation is None,\n            self.img is None,\n            not self.from_modality,\n            not self.to_modality,\n        ]\n    )\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackage.shape","title":"<code>shape()</code>","text":"<p>Get the shape of the data for each data type in nested dictionary format.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary with nested structure: {modality_type: {sub_key: shape}}</p> Source code in <code>src/autoencodix/data/datapackage.py</code> <pre><code>def shape(self) -&gt; Dict[str, Dict[str, Any]]:\n    \"\"\"\n    Get the shape of the data for each data type in nested dictionary format.\n\n    Returns:\n        Dictionary with nested structure: {modality_type: {sub_key: shape}}\n    \"\"\"\n    shapes: Dict[str, Dict[str, Any]] = {}\n\n    for attr_name in self.__annotations__.keys():\n        attr_value = getattr(self, attr_name)\n\n        if isinstance(attr_value, dict):\n            # Handle dictionary attributes\n            if attr_value is None or len(attr_value) == 0:\n                # Empty or None dictionary\n                shapes[attr_name] = {}\n            else:\n                sub_dict = self._get_shape_from_dict(attr_value)\n                shapes[attr_name] = sub_dict\n        else:\n            # Handle non-dictionary attributes\n            shape = self._get_single_shape(attr_value)\n            shapes[attr_name] = {attr_name: shape}\n\n    return shapes\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackageSplitter","title":"<code>DataPackageSplitter</code>","text":"<p>Splits DataPackage objects into training, validation, and testing sets.</p> <p>Supports paired and unpaired (translation) splitting.</p> <p>Attributes:</p> Name Type Description <code>data_package</code> <p>The original DataPackage to split.</p> <code>config</code> <p>The configuration settings for the splitting process.</p> <code>indices</code> <p>The indices for each split (train/val/test).</p> Source code in <code>src/autoencodix/data/_datapackage_splitter.py</code> <pre><code>class DataPackageSplitter:\n    \"\"\"Splits DataPackage objects into training, validation, and testing sets.\n\n    Supports paired and unpaired (translation) splitting.\n\n    Attributes:\n        data_package: The original DataPackage to split.\n        config: The configuration settings for the splitting process.\n        indices: The indices for each split (train/val/test).\n    \"\"\"\n\n    def __init__(\n        self,\n        data_package: DataPackage,\n        config: DefaultConfig,\n        indices: Dict[str, Dict[str, Dict[str, np.ndarray]]],\n    ) -&gt; None:\n        self._data_package = data_package\n        self.indices = indices\n        self.config = config\n\n        if not isinstance(self._data_package, DataPackage):\n            raise TypeError(\n                f\"Expected data_package to be of type DataPackage, got {type(self._data_package)}\"\n            )\n\n    def _shallow_copy(self, value: Any) -&gt; Any:\n        try:\n            return copy.copy(value)\n        except AttributeError:\n            return value\n\n    def _indexing(self, obj: Any, indices: np.ndarray) -&gt; Any:\n        \"\"\"Indexes pd.DataFrame, list, AnnData, or MuData objects using the provided indices.\n\n        Args:\n            obj: The object to index (can be pd.DataFrame, list, AnnData, MuData, or None).\n            indices: The indices to use for indexing.\n        Returns:\n            The indexed object, or None if the input object is None.\n        Raises:\n            TypeError: If an unsupported type is encountered.\n        \"\"\"\n\n        if obj is None:\n            return None\n        if isinstance(obj, pd.DataFrame):\n            return obj.iloc[indices]\n        elif isinstance(obj, list):\n            return [obj[i] for i in indices]\n        elif isinstance(obj, (AnnData, MuData)):\n            # print(f\"shape of obj: {obj.shape}\")\n            # print(f\"obj: {obj}\")\n            # print(f\"len(ind): {len(indices)}\")\n            # print(f\"max of index{np.max(indices)}\")\n            # print(f\"ind: {indices}\")\n            return obj[indices]\n        else:\n            raise TypeError(\n                f\"Unsupported type for indexing: {type(obj)}. \"\n                \"Supported types are pd.DataFrame, list, AnnData, and MuData.\"\n            )\n\n    def _split_data_package(self, split: str) -&gt; Optional[DataPackage]:\n        \"\"\"Creates a new DataPackage where each attribute is indexed (if applicable)\n        by the given indices. Returns None if indices are empty.\n\n        Args:\n            indices: The indices to use for splitting the DataPackage.\n        Returns:\n            A new DataPackage with attributes indexed by the provided indices,\n            or None if indices are empty.\n        \"\"\"\n        if len(self.indices) == 0:\n            return None\n\n        split_data = {}\n        for key, value in self._data_package.__dict__.items():\n            if value is None:\n                continue\n            split_data[key] = {\n                modality: self._indexing(data, self.indices[key][modality][split])\n                for modality, data in value.items()\n            }\n        return DataPackage(**split_data)\n\n    def _split_mudata(\n        self,\n        mudata: MuData,  # ty: ignore[invalid-type-form]\n        indices_map: Dict[str, Dict[str, np.ndarray]],\n        split: str,\n    ) -&gt; MuData:  # ty: ignore[invalid-type-form]\n        \"\"\"Splits a MuData object based on the provided indices map.\n\n        Args:\n            mudata: The MuData object to split.\n            indices_map: A dictionary mapping modalities to their respective indices.\n            split: The split type (\"train\", \"valid\", or \"test\").\n        Returns:\n            A new MuData object with the specified splits applied.\n        \"\"\"\n        for modality, data in mudata.mod.items():\n            indices = indices_map[modality][split]\n            mudata.mod[modality] = self._indexing(data, indices)\n        return mudata\n\n    def _requires_paired(self) -&gt; bool:\n        return self.config.requires_paired is None or self.config.requires_paired\n\n    def split(self) -&gt; Dict[str, Optional[Dict[str, Any]]]:\n        \"\"\"Splits the underlying DataPackage into train, valid, and test subsets.\n        Returns:\n            A dictionary containing the split data packages for \"train\", \"valid\", and \"test\".\n            Each entry contains a \"data\" key with the DataPackage and an \"indices\" key with\n            the corresponding indices.\n        Raises:\n            ValueError: If no data package is available for splitting.\n            TypeError: If indices are not provided for unpaired translation case.\n        \"\"\"\n        if self._data_package is None:\n            raise ValueError(\"No data package available for splitting\")\n\n        splits = [\"train\", \"valid\", \"test\"]\n        result: Dict[str, Optional[Dict[str, Any]]] = {\n            \"train\": {},\n            \"valid\": {},\n            \"test\": {},\n        }\n\n        for split in splits:\n            if self.indices is None:  # or split not in self.indices:\n                result[split] = None\n                continue\n            result[split] = {\n                \"data\": self._split_data_package(split=split),\n                \"indices\": self.indices,\n            }\n\n        return result\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataPackageSplitter.split","title":"<code>split()</code>","text":"<p>Splits the underlying DataPackage into train, valid, and test subsets. Returns:     A dictionary containing the split data packages for \"train\", \"valid\", and \"test\".     Each entry contains a \"data\" key with the DataPackage and an \"indices\" key with     the corresponding indices. Raises:     ValueError: If no data package is available for splitting.     TypeError: If indices are not provided for unpaired translation case.</p> Source code in <code>src/autoencodix/data/_datapackage_splitter.py</code> <pre><code>def split(self) -&gt; Dict[str, Optional[Dict[str, Any]]]:\n    \"\"\"Splits the underlying DataPackage into train, valid, and test subsets.\n    Returns:\n        A dictionary containing the split data packages for \"train\", \"valid\", and \"test\".\n        Each entry contains a \"data\" key with the DataPackage and an \"indices\" key with\n        the corresponding indices.\n    Raises:\n        ValueError: If no data package is available for splitting.\n        TypeError: If indices are not provided for unpaired translation case.\n    \"\"\"\n    if self._data_package is None:\n        raise ValueError(\"No data package available for splitting\")\n\n    splits = [\"train\", \"valid\", \"test\"]\n    result: Dict[str, Optional[Dict[str, Any]]] = {\n        \"train\": {},\n        \"valid\": {},\n        \"test\": {},\n    }\n\n    for split in splits:\n        if self.indices is None:  # or split not in self.indices:\n            result[split] = None\n            continue\n        result[split] = {\n            \"data\": self._split_data_package(split=split),\n            \"indices\": self.indices,\n        }\n\n    return result\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataSplitter","title":"<code>DataSplitter</code>","text":"<p>Splits data into train, validation, and test sets. And validates the splits.</p> <p>Also allows for custom splits to be provided. Here we allow empty splits (e.g. test_ratio=0), this might raise an error later in the pipeline, when this split is expected to be non-empty. However, this allows are more flexible usage of the pipeline (e.g. when the user only wants to run the fit step).</p> <p>Constraints: 1. Split ratios must sum to 1 2. Each non-empty split must have at least min_samples_per_split samples 3. Any split ratio must be &lt;= 1.0 4. Custom splits must contain 'train', 'valid', and 'test' keys and non-overlapping indices</p> <p>Attributes:</p> Name Type Description <code>_config</code> <p>Configuration object containing split ratios</p> <code>_custom_splits</code> <p>Optional pre-defined split indices</p> Source code in <code>src/autoencodix/data/_datasplitter.py</code> <pre><code>class DataSplitter:\n    \"\"\"\n    Splits data into train, validation, and test sets. And validates the splits.\n\n    Also allows for custom splits to be provided.\n    Here we allow empty splits (e.g. test_ratio=0), this might raise an error later\n    in the pipeline, when this split is expected to be non-empty. However, this allows\n    are more flexible usage of the pipeline (e.g. when the user only wants to run the fit step).\n\n    Constraints:\n    1. Split ratios must sum to 1\n    2. Each non-empty split must have at least min_samples_per_split samples\n    3. Any split ratio must be &lt;= 1.0\n    4. Custom splits must contain 'train', 'valid', and 'test' keys and non-overlapping indices\n\n    Attributes:\n        _config: Configuration object containing split ratios\n\n        _custom_splits: Optional pre-defined split indices\n        _test_ratio\n        _valid_ratio\n\n    \"\"\"\n\n    def __init__(\n        self,\n        config: DefaultConfig,\n        custom_splits: Optional[Dict[str, np.ndarray]] = None,\n    ):\n        \"\"\"\n        Initialize DataSplitter with configuration and optional custom splits.\n\n        Args:\n            config (DefaultConfig): Configuration object containing split ratios\n            custom_splits (Optional[Dict[str, np.ndarray]]): Pre-defined split indices\n        \"\"\"\n        self._config = config\n        self._test_ratio = self._config.test_ratio\n        self._valid_ratio = self._config.valid_ratio\n        self._train_ratio = self._config.train_ratio\n        self._min_samples = self._config.min_samples_per_split\n        self._custom_splits = custom_splits\n\n        self._validate_ratios()\n        if self._custom_splits:\n            self._validate_custom_splits(self._custom_splits)\n\n    def _validate_ratios(self) -&gt; None:\n        \"\"\"\n        Validate that the splitting ratios meet required constraints.\n        Returns:\n            None\n        Raises:\n            ValueError: If ratios violate constraints\n\n        \"\"\"\n        if not 0 &lt;= self._test_ratio &lt;= 1:\n            raise ValueError(\n                f\"Test ratio must be between 0 and 1, got {self._test_ratio}\"\n            )\n        if not 0 &lt;= self._valid_ratio &lt;= 1:\n            raise ValueError(\n                f\"Validation ratio must be between 0 and 1, got {self._valid_ratio}\"\n            )\n        if not 0 &lt;= self._train_ratio &lt;= 1:\n            raise ValueError(\n                f\"Train ratio must be between 0 and 1, got {self._train_ratio}\"\n            )\n\n        if np.sum([self._test_ratio, self._valid_ratio, self._train_ratio]) != 1:\n            raise ValueError(\"Split ratios must sum to 1\")\n\n    def _validate_split_sizes(self, n_samples: int) -&gt; None:\n        \"\"\"\n        Validate that each non-empty split will have sufficient samples.\n\n        Args:\n            n_samples: Total number of samples in dataset\n        Returns:\n            None\n        Raises:\n            ValueError: If any non-empty split would have too few samples\n\n        \"\"\"\n        # Calculate expected sizes\n        n_train = int(n_samples * (1 - self._test_ratio - self._valid_ratio))\n        n_valid = int(n_samples * self._valid_ratio) if self._valid_ratio &gt; 0 else 0\n        n_test = int(n_samples * self._test_ratio) if self._test_ratio &gt; 0 else 0\n\n        if self._train_ratio &gt; 0 and n_train &lt; self._min_samples:\n            raise ValueError(\n                f\"Training set would have {n_train} samples, \"\n                f\"which is less than minimum required ({self._min_samples})\"\n            )\n\n        if self._valid_ratio &gt; 0 and n_valid &lt; self._min_samples:\n            raise ValueError(\n                f\"Validation set would have {n_valid} samples, \"\n                f\"which is less than minimum required ({self._min_samples})\"\n            )\n\n        if self._test_ratio &gt; 0 and n_test &lt; self._min_samples:\n            raise ValueError(\n                f\"Test set would have {n_test} samples, \"\n                f\"which is less than minimum required ({self._min_samples})\"\n            )\n\n    def _validate_custom_splits(self, splits: Dict[str, np.ndarray]) -&gt; None:\n        \"\"\"\n        Validate custom splits for correctness.\n\n        Args:\n            splits: Custom split indices\n        Returns:\n            None\n        Raises:\n            ValueError: If custom splits violate constraints\n\n        \"\"\"\n        required_keys = {\"train\", \"valid\", \"test\"}\n        if not all(key in splits for key in required_keys):\n            raise ValueError(\n                f\"Custom splits must contain all of: {required_keys} \\ Got: {splits.keys()} \\ if you want to pass empty splits, pass an empty array\"\n            )\n\n        # check for index out of bounds\n        if len(splits[\"train\"]) &lt; self._min_samples:\n            raise ValueError(\n                f\"Custom training split has {len(splits['train'])} samples, \"\n                f\"which is less than minimum required ({self._min_samples})\"\n            )\n\n        # For non-empty validation and test splits, check minimum size\n        if len(splits[\"valid\"]) &gt; 0 and len(splits[\"valid\"]) &lt; self._min_samples:\n            raise ValueError(\n                f\"Custom validation split has {len(splits['valid'])} samples, \"\n                f\"which is less than minimum required ({self._min_samples})\"\n            )\n\n        if len(splits[\"test\"]) &gt; 0 and len(splits[\"test\"]) &lt; self._min_samples:\n            raise ValueError(\n                f\"Custom test split has {len(splits['test'])} samples, \"\n                f\"which is less than minimum required ({self._min_samples})\"\n            )\n\n        # Check for overlap between splits\n        for k1, k2 in itertools.combinations(required_keys, 2):\n            intersection = set(splits[k1]) &amp; set(splits[k2])\n            if intersection:\n                raise ValueError(\n                    f\"Overlapping indices found between splits '{k1}' and '{k2}': {intersection}\"\n                )\n\n    def split(\n        self,\n        n_samples: int,\n    ) -&gt; Dict[str, np.ndarray]:\n        \"\"\"\n        Split data into train, validation, and test sets.\n\n        Args:\n            n_samples: Total number of samples in the dataset\n\n        Returns:\n            Dictionary containing indices for each split, with empty arrays for splits with ratio=0\n\n        Raises:\n            ValueError: If resulting splits would violate size constraints\n        \"\"\"\n        self._validate_split_sizes(n_samples)\n        indices = np.arange(n_samples)\n\n        if self._custom_splits:\n            max_index = n_samples - 1\n            for split in self._custom_splits.values():\n                if len(split) &gt; 0:\n                    if np.max(split) &gt; max_index:\n                        raise AssertionError(\n                            f\"Custom split indices must be within range [0, {max_index}]\"\n                        )\n                    elif np.min(split) &lt; 0:\n                        raise AssertionError(\n                            f\"Custom split indices must be within range [0, {max_index}]\"\n                        )\n            return self._custom_splits\n\n        # all three 0 case already handled in _validate_ratios (sum to 1)\n        if self._test_ratio == 0 and self._valid_ratio == 0:\n            return {\n                \"train\": indices,\n                \"valid\": np.array([], dtype=int),\n                \"test\": np.array([], dtype=int),\n            }\n        if self._train_ratio == 0 and self._valid_ratio == 0:\n            return {\n                \"train\": np.array([], dtype=int),\n                \"valid\": np.array([], dtype=int),\n                \"test\": indices,\n            }\n        if self._train_ratio == 0 and self._test_ratio == 0:\n            return {\n                \"train\": np.array([], dtype=int),\n                \"valid\": indices,\n                \"test\": np.array([], dtype=int),\n            }\n\n        if self._train_ratio == 0:\n            valid_indices, test_indices = train_test_split(\n                indices,\n                test_size=self._test_ratio,\n                random_state=self._config.global_seed,\n            )\n            return {\n                \"train\": np.array([], dtype=int),\n                \"valid\": valid_indices,\n                \"test\": test_indices,\n            }\n\n        if self._test_ratio == 0:\n            train_indices, valid_indices = train_test_split(\n                indices,\n                test_size=self._valid_ratio,\n                random_state=self._config.global_seed,\n            )\n            return {\n                \"train\": train_indices,\n                \"valid\": valid_indices,\n                \"test\": np.array([], dtype=int),\n            }\n\n        if self._valid_ratio == 0:\n            train_indices, test_indices = train_test_split(\n                indices,\n                test_size=self._test_ratio,\n                random_state=self._config.global_seed,\n            )\n            return {\n                \"train\": train_indices,\n                \"valid\": np.array([], dtype=int),\n                \"test\": test_indices,\n            }\n\n        # Normal case: split into all three sets\n        train_valid_indices, test_indices = train_test_split(\n            indices, test_size=self._test_ratio, random_state=self._config.global_seed\n        )\n\n        train_indices, valid_indices = train_test_split(\n            train_valid_indices,\n            test_size=self._valid_ratio / (1 - self._test_ratio),\n            random_state=self._config.global_seed,\n        )\n\n        return {\"train\": train_indices, \"valid\": valid_indices, \"test\": test_indices}\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataSplitter.__init__","title":"<code>__init__(config, custom_splits=None)</code>","text":"<p>Initialize DataSplitter with configuration and optional custom splits.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DefaultConfig</code> <p>Configuration object containing split ratios</p> required <code>custom_splits</code> <code>Optional[Dict[str, ndarray]]</code> <p>Pre-defined split indices</p> <code>None</code> Source code in <code>src/autoencodix/data/_datasplitter.py</code> <pre><code>def __init__(\n    self,\n    config: DefaultConfig,\n    custom_splits: Optional[Dict[str, np.ndarray]] = None,\n):\n    \"\"\"\n    Initialize DataSplitter with configuration and optional custom splits.\n\n    Args:\n        config (DefaultConfig): Configuration object containing split ratios\n        custom_splits (Optional[Dict[str, np.ndarray]]): Pre-defined split indices\n    \"\"\"\n    self._config = config\n    self._test_ratio = self._config.test_ratio\n    self._valid_ratio = self._config.valid_ratio\n    self._train_ratio = self._config.train_ratio\n    self._min_samples = self._config.min_samples_per_split\n    self._custom_splits = custom_splits\n\n    self._validate_ratios()\n    if self._custom_splits:\n        self._validate_custom_splits(self._custom_splits)\n</code></pre>"},{"location":"api/data/#autoencodix.data.DataSplitter.split","title":"<code>split(n_samples)</code>","text":"<p>Split data into train, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>Total number of samples in the dataset</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>Dictionary containing indices for each split, with empty arrays for splits with ratio=0</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If resulting splits would violate size constraints</p> Source code in <code>src/autoencodix/data/_datasplitter.py</code> <pre><code>def split(\n    self,\n    n_samples: int,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Split data into train, validation, and test sets.\n\n    Args:\n        n_samples: Total number of samples in the dataset\n\n    Returns:\n        Dictionary containing indices for each split, with empty arrays for splits with ratio=0\n\n    Raises:\n        ValueError: If resulting splits would violate size constraints\n    \"\"\"\n    self._validate_split_sizes(n_samples)\n    indices = np.arange(n_samples)\n\n    if self._custom_splits:\n        max_index = n_samples - 1\n        for split in self._custom_splits.values():\n            if len(split) &gt; 0:\n                if np.max(split) &gt; max_index:\n                    raise AssertionError(\n                        f\"Custom split indices must be within range [0, {max_index}]\"\n                    )\n                elif np.min(split) &lt; 0:\n                    raise AssertionError(\n                        f\"Custom split indices must be within range [0, {max_index}]\"\n                    )\n        return self._custom_splits\n\n    # all three 0 case already handled in _validate_ratios (sum to 1)\n    if self._test_ratio == 0 and self._valid_ratio == 0:\n        return {\n            \"train\": indices,\n            \"valid\": np.array([], dtype=int),\n            \"test\": np.array([], dtype=int),\n        }\n    if self._train_ratio == 0 and self._valid_ratio == 0:\n        return {\n            \"train\": np.array([], dtype=int),\n            \"valid\": np.array([], dtype=int),\n            \"test\": indices,\n        }\n    if self._train_ratio == 0 and self._test_ratio == 0:\n        return {\n            \"train\": np.array([], dtype=int),\n            \"valid\": indices,\n            \"test\": np.array([], dtype=int),\n        }\n\n    if self._train_ratio == 0:\n        valid_indices, test_indices = train_test_split(\n            indices,\n            test_size=self._test_ratio,\n            random_state=self._config.global_seed,\n        )\n        return {\n            \"train\": np.array([], dtype=int),\n            \"valid\": valid_indices,\n            \"test\": test_indices,\n        }\n\n    if self._test_ratio == 0:\n        train_indices, valid_indices = train_test_split(\n            indices,\n            test_size=self._valid_ratio,\n            random_state=self._config.global_seed,\n        )\n        return {\n            \"train\": train_indices,\n            \"valid\": valid_indices,\n            \"test\": np.array([], dtype=int),\n        }\n\n    if self._valid_ratio == 0:\n        train_indices, test_indices = train_test_split(\n            indices,\n            test_size=self._test_ratio,\n            random_state=self._config.global_seed,\n        )\n        return {\n            \"train\": train_indices,\n            \"valid\": np.array([], dtype=int),\n            \"test\": test_indices,\n        }\n\n    # Normal case: split into all three sets\n    train_valid_indices, test_indices = train_test_split(\n        indices, test_size=self._test_ratio, random_state=self._config.global_seed\n    )\n\n    train_indices, valid_indices = train_test_split(\n        train_valid_indices,\n        test_size=self._valid_ratio / (1 - self._test_ratio),\n        random_state=self._config.global_seed,\n    )\n\n    return {\"train\": train_indices, \"valid\": valid_indices, \"test\": test_indices}\n</code></pre>"},{"location":"api/data/#autoencodix.data.DatasetContainer","title":"<code>DatasetContainer</code>  <code>dataclass</code>","text":"<p>A container for datasets used in training, validation, and testing.</p> <p>train : Dataset     The training dataset. valid : Dataset     The validation dataset. test : Dataset     The testing dataset.</p> Source code in <code>src/autoencodix/data/_datasetcontainer.py</code> <pre><code>@dataclass\nclass DatasetContainer:\n    \"\"\"A container for datasets used in training, validation, and testing.\n\n    Attributes:\n    train : Dataset\n        The training dataset.\n    valid : Dataset\n        The validation dataset.\n    test : Dataset\n        The testing dataset.\n    \"\"\"\n\n    train: Optional[Union[BaseDataset, None]] = None\n    valid: Optional[Union[BaseDataset, None]] = None\n    test: Optional[Union[BaseDataset, None]] = None\n\n    def __getitem__(self, key: str) -&gt; BaseDataset:\n        \"\"\"Allows dictionary-like access to datasets.\"\"\"\n        if key not in {\"train\", \"valid\", \"test\"}:\n            raise KeyError(f\"Invalid key: {key}. Must be 'train', 'valid', or 'test'.\")\n        return getattr(self, key)\n\n    def __setitem__(self, key: str, value: BaseDataset):\n        \"\"\"Allows dictionary-like assignment of datasets.\"\"\"\n        if key not in {\"train\", \"valid\", \"test\"}:\n            raise KeyError(f\"Invalid key: {key}. Must be 'train', 'valid', or 'test'.\")\n        setattr(self, key, value)\n</code></pre>"},{"location":"api/data/#autoencodix.data.DatasetContainer.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Allows dictionary-like access to datasets.</p> Source code in <code>src/autoencodix/data/_datasetcontainer.py</code> <pre><code>def __getitem__(self, key: str) -&gt; BaseDataset:\n    \"\"\"Allows dictionary-like access to datasets.\"\"\"\n    if key not in {\"train\", \"valid\", \"test\"}:\n        raise KeyError(f\"Invalid key: {key}. Must be 'train', 'valid', or 'test'.\")\n    return getattr(self, key)\n</code></pre>"},{"location":"api/data/#autoencodix.data.DatasetContainer.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Allows dictionary-like assignment of datasets.</p> Source code in <code>src/autoencodix/data/_datasetcontainer.py</code> <pre><code>def __setitem__(self, key: str, value: BaseDataset):\n    \"\"\"Allows dictionary-like assignment of datasets.\"\"\"\n    if key not in {\"train\", \"valid\", \"test\"}:\n        raise KeyError(f\"Invalid key: {key}. Must be 'train', 'valid', or 'test'.\")\n    setattr(self, key, value)\n</code></pre>"},{"location":"api/data/#autoencodix.data.GeneralPreprocessor","title":"<code>GeneralPreprocessor</code>","text":"<p>               Bases: <code>BasePreprocessor</code></p> <p>Preprocessor for handling multi-modal data.</p> <p>Attributes:</p> Name Type Description <code>_datapackage_dict</code> <code>Optional[Dict[str, Any]]</code> <p>Dictionary holding DataPackage objects for each data split.</p> <code>_dataset_container</code> <code>Optional[DatasetContainer]</code> <p>Container holding processed datasets for each split.</p> <code>_reverse_mapping_multi_bulk</code> <code>Dict[str, Dict[str, Tuple[List[int], List[str]]]]</code> <p>Reverse mapping for multi-bulk data reconstruction.</p> <code>_reverse_mapping_multi_sc</code> <code>Dict[str, Dict[str, Tuple[List[int], List[str]]]]</code> <p>Reverse mapping for multi-single-cell data reconstruction.</p> Source code in <code>src/autoencodix/data/general_preprocessor.py</code> <pre><code>class GeneralPreprocessor(BasePreprocessor):\n    \"\"\"Preprocessor for handling multi-modal data.\n\n    Attributes:\n        _datapackage_dict: Dictionary holding DataPackage objects for each data split.\n        _dataset_container: Container holding processed datasets for each split.\n        _reverse_mapping_multi_bulk: Reverse mapping for multi-bulk data reconstruction.\n        _reverse_mapping_multi_sc: Reverse mapping for multi-single-cell data reconstruction.\n\n    \"\"\"\n\n    def __init__(\n        self, config: DefaultConfig, ontologies: Optional[Union[Tuple, Dict]] = None\n    ) -&gt; None:\n        super().__init__(config=config, ontologies=ontologies)\n        self._datapackage_dict: Optional[Dict[str, Any]] = None\n        self._dataset_container: Optional[DatasetContainer] = None\n        # Reverse mappings for reconstruction\n        self._reverse_mapping_multi_bulk: Dict[\n            str, Dict[str, Tuple[List[int], List[str]]]\n        ] = {\"train\": {}, \"test\": {}, \"valid\": {}}\n        self._reverse_mapping_multi_sc: Dict[\n            str, Dict[str, Tuple[List[int], List[str]]]\n        ] = {\"train\": {}, \"test\": {}, \"valid\": {}}\n\n    def _combine_layers(\n        self, modality_name: str, modality_data: Any\n    ) -&gt; List[np.ndarray]:\n        layer_list: List[np.ndarray] = []\n        selected_layers = self.config.data_config.data_info[\n            modality_name\n        ].selected_layers\n        for layer_name in selected_layers:\n            if layer_name == \"X\":\n                layer_list.append(modality_data.X)\n            elif layer_name in modality_data.layers:\n                layer_list.append(modality_data.layers[layer_name])\n        return layer_list\n\n    def _combine_modality_data(\n        self,\n        mudata: md.MuData,  # ty: ignore[invalid-type-form]\n    ) -&gt; Union[np.ndarray, sp.sparse.spmatrix]:  # ty: ignore[invalid-type-form]\n        # Reset single-cell reverse mapping\n        modality_data_list: List[np.ndarray] = []\n        start_idx = 0\n\n        for modality_name, modality_data in mudata.mod.items():\n            self._reverse_mapping_multi_sc[self._split][modality_name] = {}\n            layers = self.config.data_config.data_info[modality_name].selected_layers\n            for layer_name in layers:\n                if layer_name == \"X\":\n                    n_feats = modality_data.shape[1]\n                else:\n                    n_feats = modality_data.layers[layer_name].shape[1]\n\n                end_idx = start_idx + n_feats\n                feature_ids = modality_data.var_names.tolist()\n                self._reverse_mapping_multi_sc[self._split][modality_name][\n                    layer_name\n                ] = (\n                    list(range(start_idx, end_idx)),\n                    feature_ids,\n                )\n                start_idx = end_idx\n\n            combined_layers = self._combine_layers(\n                modality_name=modality_name, modality_data=modality_data\n            )\n            modality_data_list.extend(combined_layers)\n        all_sparse = all(issparse(arr) for arr in modality_data_list)\n        if all_sparse:\n            combined = sp.sparse.hstack(modality_data_list, format=\"csr\")\n        else:\n            dense_layers = [\n                arr.toarray() if issparse(arr) else arr  # ty: ignore\n                for arr in modality_data_list\n            ]\n            combined = np.concatenate(dense_layers, axis=1)\n\n        return combined\n\n    def _create_numeric_dataset(\n        self,\n        data: Union[np.ndarray, sp.sparse.spmatrix],\n        config: DefaultConfig,\n        split_ids: np.ndarray,\n        metadata: pd.DataFrame,\n        ids: List[str],\n        feature_ids: List[str],\n    ) -&gt; NumericDataset:\n        # keep sparse data sparse until batch level in training for memory efficency\n        ds = NumericDataset(\n            data=data,\n            config=config,\n            split_indices=split_ids,\n            metadata=metadata,\n            sample_ids=ids,\n            feature_ids=feature_ids,\n        )\n        return ds\n\n    def _process_data_package(self, data_dict: Dict[str, Any]) -&gt; BaseDataset:\n        data, split_ids = data_dict[\"data\"], data_dict[\"indices\"]\n        # MULTI-BULK\n        if data.multi_bulk is not None:\n            # reset bulk mapping\n            metadata = data.annotation\n            bulk_dict: Dict[str, pd.DataFrame] = data.multi_bulk\n\n            # Check if all DataFrames have the same number of samples\n            sample_counts = {}\n            for subkey, df in bulk_dict.items():\n                if not isinstance(df, pd.DataFrame):\n                    raise ValueError(\n                        f\"Expected a DataFrame for '{subkey}', got {type(df)}\"\n                    )\n                sample_counts[subkey] = df.shape[0]\n                # print(f\"cur shape: {subkey}: {df.shape}\")\n\n            # Validate all modalities have the same number of samples\n            unique_sample_counts = set(sample_counts.values())\n            if len(unique_sample_counts) &gt; 1:\n                sample_count_str = \", \".join(\n                    [f\"{k}: {v} samples\" for k, v in sample_counts.items()]\n                )\n                raise NotImplementedError(\n                    f\"Different sample counts across modalities are not currently supported for Varix and Vanillix\"\n                    \"Set requires_pared=True in config.\"\n                    f\"Found: {sample_count_str}. All modalities must have the same number of samples.\"\n                )\n\n            combined_cols: List[str] = []\n            start_idx = 0\n            for subkey, df in bulk_dict.items():\n                n_feats = df.shape[1]\n                end_idx = start_idx + n_feats\n                self._reverse_mapping_multi_bulk[self._split][subkey] = (\n                    list(range(start_idx, end_idx)),\n                    df.columns.tolist(),\n                )\n                combined_cols.extend(df.columns.tolist())\n                start_idx = end_idx\n\n            combined_df = pd.concat(bulk_dict.values(), axis=1)\n            return self._create_numeric_dataset(\n                data=combined_df.values,\n                config=self.config,\n                split_ids=split_ids,\n                metadata=metadata,\n                ids=combined_df.index.tolist(),\n                feature_ids=combined_cols,\n            )\n        # MULTI-SINGLE-CELL\n        elif data.multi_sc is not None:\n            # reset single-cell mapping\n            mudata: md.MuData = data.multi_sc.get(  # ty: ignore[invalid-type-form]\n                \"multi_sc\", None\n            )  # ty: ignore[invalid-type-form]\n            if mudata is None:\n                raise NotImplementedError(\n                    \"Unpaired multi Single Cell case not implemented vor Varix and Vanillix, set requires_paired=True in config\"\n                )\n            combined_data = self._combine_modality_data(mudata)\n\n            # collect feature IDs in concatenation order\n            feature_ids: List[str] = []\n            for layers in self._reverse_mapping_multi_sc[self._split].values():\n                for _, fids in layers.values():\n                    feature_ids.extend(fids)\n            return self._create_numeric_dataset(\n                data=combined_data,\n                config=self.config,\n                split_ids=split_ids,\n                metadata=mudata.obs,\n                ids=mudata.obs_names.tolist(),\n                feature_ids=feature_ids,\n            )\n        else:\n            raise NotImplementedError(\n                \"GeneralPreprocessor only handles multi_bulk or multi_sc.\"\n            )\n\n    def preprocess(\n        self,\n        raw_user_data: Optional[DataPackage] = None,\n        predict_new_data: bool = False,\n    ) -&gt; DatasetContainer:\n        # run common preprocessing\n\n        # self._reverse_mapping_multi_bulk.clear()\n        # self._reverse_mapping_multi_sc.clear()\n\n        self._datapackage_dict = self._general_preprocess(\n            raw_user_data=raw_user_data, predict_new_data=predict_new_data\n        )\n        if self._datapackage_dict is None:\n            raise TypeError(\"Datapackage cannot be None\")\n\n        # prepare container\n        ds_container: DatasetContainer = DatasetContainer()\n\n        for split in [\"train\", \"test\", \"valid\"]:\n            split_data = self._datapackage_dict.get(split)\n            self._split = split\n            if not split_data or split_data[\"data\"] is None:\n                ds_container[split] = None  # type: ignore\n                continue\n            ds = self._process_data_package(split_data)\n            ds_container[split] = ds\n        self._dataset_container = ds_container\n        return ds_container\n\n    def format_reconstruction(\n        self, reconstruction: torch.Tensor, result: Optional[Result] = None\n    ) -&gt; DataPackage:\n        self._split = self._match_split(n_samples=reconstruction.shape[0])\n        if self.config.data_case == DataCase.MULTI_BULK:\n            return self._reverse_multi_bulk(reconstruction)\n        elif self.config.data_case == DataCase.MULTI_SINGLE_CELL:\n            return self._reverse_multi_sc(reconstruction)\n        else:\n            raise NotImplementedError(\n                f\"Reconstruction not implemented for {self.config.data_case}\"\n            )\n\n    def _match_split(self, n_samples: int) -&gt; str:\n        \"\"\"Match the split based on the number of samples.\"\"\"\n        print(f\"n_samples in format recon: {n_samples}\")\n        for split, split_data in self._datapackage_dict.items():\n            print(split)\n            data = split_data.get(\"data\")\n            if data is None:\n                continue\n            ref_n = data.get_n_samples()[\"paired_count\"]\n            print(f\"n_samples from datatpackge: {ref_n}\")\n            if n_samples == data.get_n_samples()[\"paired_count\"][\"paired_count\"]:\n                return split\n        raise ValueError(\n            f\"Cannot find matching split for {n_samples} samples in the dataset.\"\n        )\n\n    def _reverse_multi_bulk(\n        self, reconstruction: Union[np.ndarray, torch.Tensor]\n    ) -&gt; DataPackage:\n        data_package = DataPackage(\n            multi_bulk={},\n            multi_sc=None,\n            annotation=None,\n            img=None,\n            from_modality=None,\n            to_modality=None,\n        )\n        # reconstruct each bulk subkey\n        dfs: Dict[str, pd.DataFrame] = {}\n        for subkey, (indices, fids) in self._reverse_mapping_multi_bulk[\n            self._split\n        ].items():\n            arr = self._slice_tensor(\n                reconstruction=reconstruction,\n                indices=indices,\n            )\n            dfs[subkey] = pd.DataFrame(\n                arr,\n                columns=fids,\n                index=self._dataset_container[self._split].sample_ids,\n            )\n        data_package.annotation = self._dataset_container[self._split].metadata\n\n        data_package.multi_bulk = dfs\n        return data_package\n\n    def _slice_tensor(\n        self, reconstruction: Union[np.ndarray, torch.Tensor], indices: List[int]\n    ) -&gt; np.ndarray:\n        if isinstance(reconstruction, torch.Tensor):\n            arr = reconstruction[:, indices].detach().cpu().numpy()\n        elif isinstance(reconstruction, np.ndarray):\n            arr = reconstruction[:, indices]\n        else:\n            raise TypeError(\n                f\"Expected reconstruction to be a torch.Tensor or np.ndarray, got {type(reconstruction)}\"\n            )\n        return arr\n\n    def _reverse_multi_sc(self, reconstruction: torch.Tensor) -&gt; DataPackage:\n        data_package = DataPackage(\n            multi_bulk=None,\n            multi_sc=None,\n            annotation=None,\n            img=None,\n            from_modality=None,\n            to_modality=None,\n        )\n        modalities: Dict[str, AnnData] = {}\n\n        for modality_name, layers in self._reverse_mapping_multi_sc[\n            self._split\n        ].items():\n            # rebuild each layer as DataFrame\n            layers_dict: Dict[str, pd.DataFrame] = {}\n            for layer_name, (indices, fids) in layers.items():\n                arr = self._slice_tensor(reconstruction=reconstruction, indices=indices)\n                layers_dict[layer_name] = pd.DataFrame(\n                    arr,\n                    columns=fids,\n                    index=self._dataset_container[self._split].sample_ids,\n                )\n\n            # extract X layer for AnnData var\n            feature_ids = layers.get(\"X\", (None, []))[1]\n            var = pd.DataFrame(index=feature_ids)\n            X_df = layers_dict.pop(\"X\", None)\n            adata = AnnData(\n                X=X_df.values if X_df is not None else None,\n                obs=self._dataset_container[self._split].metadata,\n                var=var,\n                layers={k: v.values for k, v in layers_dict.items()},\n            )\n            modalities[modality_name] = adata\n\n        data_package.multi_sc = {\"multi_sc\": md.MuData(modalities)}\n        data_package.annotation = self._dataset_container[self._split].metadata\n        return data_package\n</code></pre>"},{"location":"api/data/#autoencodix.data.ImageDataset","title":"<code>ImageDataset</code>","text":"<p>               Bases: <code>TensorAwareDataset</code></p> <p>A custom PyTorch dataset that handles image data with proper dtype conversion.</p> <p>Attributes:</p> Name Type Description <code>raw_data</code> <p>List of ImgData objects containing original image data and metadata.</p> <code>config</code> <p>Configuration object for dataset settings.</p> <code>mytype</code> <p>Enum indicating the dataset type (set to DataSetTypes.IMG).</p> <code>data</code> <p>List of image tensors converted to the appropriate dtype.</p> <code>sample_ids</code> <p>List of identifiers for each sample.</p> <code>split_indices</code> <p>Optional numpy array of indices for splitting the dataset.</p> <code>feature_ids</code> <p>Optional list of identifiers for each feature (set to None for images).</p> <code>metadata</code> <p>Optional pandas DataFrame containing additional metadata.</p> Source code in <code>src/autoencodix/data/_image_dataset.py</code> <pre><code>class ImageDataset(TensorAwareDataset):\n    \"\"\"\n    A custom PyTorch dataset that handles image data with proper dtype conversion.\n\n\n    Attributes:\n        raw_data: List of ImgData objects containing original image data and metadata.\n        config: Configuration object for dataset settings.\n        mytype: Enum indicating the dataset type (set to DataSetTypes.IMG).\n        data: List of image tensors converted to the appropriate dtype.\n        sample_ids: List of identifiers for each sample.\n        split_indices: Optional numpy array of indices for splitting the dataset.\n        feature_ids: Optional list of identifiers for each feature (set to None for images).\n        metadata: Optional pandas DataFrame containing additional metadata.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: List[ImgData],\n        config: DefaultConfig,\n        split_indices: Optional[Dict[str, Any]] = None,\n        metadata: Optional[pd.DataFrame] = None,\n    ):\n        \"\"\"\n        Initialize the dataset\n        Args:\n            data: List of image data objects\n            config: Configuration object\n        \"\"\"\n        self.raw_data = data  # image data before conversion to keep original infos\n        self.config = config\n        self.mytype = DataSetTypes.IMG\n\n        if self.config is None:\n            raise ValueError(\"config cannot be None\")\n\n        # Convert all images to tensors with proper dtype once during initialization\n        target_dtype = self._get_target_dtype()\n        self.data = self._convert_all_images_to_tensors(target_dtype)\n\n        # Extract sample_ids for consistency\n        self.sample_ids = [img_data.sample_id for img_data in data]\n\n        self.split_indices = split_indices\n        self.feature_ids = None\n        self.metadata = metadata\n\n    def _convert_all_images_to_tensors(self, dtype: torch.dtype) -&gt; List[torch.Tensor]:\n        \"\"\"\n        Convert all images to tensors with specified dtype during initialization.\n\n        Args:\n            dtype: Target dtype for the tensors\n\n        Returns:\n            List of converted image tensors\n        \"\"\"\n        print(f\"Converting {len(self.raw_data)} images to {dtype} tensors...\")\n        converted_data = []\n\n        for img_data in self.raw_data:\n            tensor = self._to_tensor(img_data.img, dtype)\n            converted_data.append(tensor)\n\n        return converted_data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \"\"\"Get item at index - data is already converted to proper dtype\n        Returns:\n            Tuple of (index, image tensor, sample_id)\n        \"\"\"\n        return idx, self.data[idx], self.sample_ids[idx]\n\n    def get_input_dim(self) -&gt; Tuple[int, ...]:\n        \"\"\"\n        Gets the input dimension of the dataset's feature space.\n\n        Returns:\n            The input dimension of the dataset's feature space\n        \"\"\"\n        return self.data[0].shape  # All images should have the same shape\n</code></pre>"},{"location":"api/data/#autoencodix.data.ImageDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Get item at index - data is already converted to proper dtype Returns:     Tuple of (index, image tensor, sample_id)</p> Source code in <code>src/autoencodix/data/_image_dataset.py</code> <pre><code>def __getitem__(self, idx):\n    \"\"\"Get item at index - data is already converted to proper dtype\n    Returns:\n        Tuple of (index, image tensor, sample_id)\n    \"\"\"\n    return idx, self.data[idx], self.sample_ids[idx]\n</code></pre>"},{"location":"api/data/#autoencodix.data.ImageDataset.__init__","title":"<code>__init__(data, config, split_indices=None, metadata=None)</code>","text":"<p>Initialize the dataset Args:     data: List of image data objects     config: Configuration object</p> Source code in <code>src/autoencodix/data/_image_dataset.py</code> <pre><code>def __init__(\n    self,\n    data: List[ImgData],\n    config: DefaultConfig,\n    split_indices: Optional[Dict[str, Any]] = None,\n    metadata: Optional[pd.DataFrame] = None,\n):\n    \"\"\"\n    Initialize the dataset\n    Args:\n        data: List of image data objects\n        config: Configuration object\n    \"\"\"\n    self.raw_data = data  # image data before conversion to keep original infos\n    self.config = config\n    self.mytype = DataSetTypes.IMG\n\n    if self.config is None:\n        raise ValueError(\"config cannot be None\")\n\n    # Convert all images to tensors with proper dtype once during initialization\n    target_dtype = self._get_target_dtype()\n    self.data = self._convert_all_images_to_tensors(target_dtype)\n\n    # Extract sample_ids for consistency\n    self.sample_ids = [img_data.sample_id for img_data in data]\n\n    self.split_indices = split_indices\n    self.feature_ids = None\n    self.metadata = metadata\n</code></pre>"},{"location":"api/data/#autoencodix.data.ImageDataset.get_input_dim","title":"<code>get_input_dim()</code>","text":"<p>Gets the input dimension of the dataset's feature space.</p> <p>Returns:</p> Type Description <code>Tuple[int, ...]</code> <p>The input dimension of the dataset's feature space</p> Source code in <code>src/autoencodix/data/_image_dataset.py</code> <pre><code>def get_input_dim(self) -&gt; Tuple[int, ...]:\n    \"\"\"\n    Gets the input dimension of the dataset's feature space.\n\n    Returns:\n        The input dimension of the dataset's feature space\n    \"\"\"\n    return self.data[0].shape  # All images should have the same shape\n</code></pre>"},{"location":"api/data/#autoencodix.data.ImagePreprocessor","title":"<code>ImagePreprocessor</code>","text":"<p>               Bases: <code>GeneralPreprocessor</code></p> <p>Preprocessor for cross-modal data, handling multiple data types and their transformations.</p> <p>Attributes:</p> Name Type Description <code>data_config</code> <p>Configuration specific to data handling and preprocessing.</p> <code>dataset_dicts</code> <p>Dictionary holding datasets for different splits (train/test/valid).</p> Source code in <code>src/autoencodix/data/_image_processor.py</code> <pre><code>class ImagePreprocessor(GeneralPreprocessor):\n    \"\"\"\n    Preprocessor for cross-modal data, handling multiple data types and their transformations.\n\n\n    Attributes:\n        data_config: Configuration specific to data handling and preprocessing.\n        dataset_dicts: Dictionary holding datasets for different splits (train/test/valid).\n    \"\"\"\n\n    def __init__(\n        self, config: DefaultConfig, ontologies: Optional[Union[Tuple, Dict]] = None\n    ):\n        super().__init__(config=config, ontologies=ontologies)\n        self.data_config = config.data_config\n\n    def preprocess(\n        self,\n        raw_user_data: Optional[DataPackage] = None,\n        predict_new_data: bool = False,\n    ) -&gt; DatasetContainer:\n        \"\"\"\n        Preprocess the data according to the configuration.\n\n        Args:\n            raw_user_data: The raw data package provided by the user.\n            predict_new_data: Flag indicating if new data is being predicted.\n        Returns:\n            A DatasetContainer with processed training, validation, and test datasets.\n        \"\"\"\n        self.dataset_dicts = self._general_preprocess(\n            raw_user_data=raw_user_data, predict_new_data=predict_new_data\n        )\n        datasets = {}\n        for split in [\"train\", \"test\", \"valid\"]:\n            cur_split = self.dataset_dicts.get(split)\n            if cur_split is None:\n                print(f\"split is None: {split}\")\n                continue\n            cur_data = cur_split.get(\"data\")\n            if not isinstance(cur_data, DataPackage):\n                raise TypeError(\n                    f\"expected type of cur_data to be DataPackage, got {type(cur_data)}\"\n                )\n            cur_indices = cur_split.get(\"indices\")\n            datasets[split] = self._process_dp(dp=cur_data, indices=cur_indices)\n\n        return DatasetContainer(\n            train=datasets[\"train\"], test=datasets[\"test\"], valid=datasets[\"valid\"]\n        )\n\n    def _process_dp(self, dp: DataPackage, indices: Dict[str, Any]) -&gt; ImageDataset:\n        if dp.img is None:\n            raise ValueError(\"no img attribute found in datapackage\")\n        first_key = next(iter(list(dp.img.keys())))\n        if not isinstance(dp.img, dict):\n            raise TypeError(\n                f\"Expected `img` attribute of DataPackage to be `dict`, got {type(dp.img)}\"\n            )\n        if len(dp.img.keys()) &gt; 1:\n            import warnings\n\n            warnings.warn(\n                f\"got multiple image datasets for Imagix: {dp.img.keys()},\\\n                          we only support a single image dataset in this case, using: {first_key}\"\n            )\n        if dp.annotation is None:\n            metadata = None\n        else:\n            metadata = dp.annotation.get(first_key)\n            if metadata is None:\n                metadata = dp.annotation.get(\"paired\")\n        data = dp.img[first_key]\n        return ImageDataset(\n            data=data,\n            config=self.config,\n            split_indices=indices,\n            metadata=metadata,\n        )\n</code></pre>"},{"location":"api/data/#autoencodix.data.ImagePreprocessor.preprocess","title":"<code>preprocess(raw_user_data=None, predict_new_data=False)</code>","text":"<p>Preprocess the data according to the configuration.</p> <p>Parameters:</p> Name Type Description Default <code>raw_user_data</code> <code>Optional[DataPackage]</code> <p>The raw data package provided by the user.</p> <code>None</code> <code>predict_new_data</code> <code>bool</code> <p>Flag indicating if new data is being predicted.</p> <code>False</code> <p>Returns:     A DatasetContainer with processed training, validation, and test datasets.</p> Source code in <code>src/autoencodix/data/_image_processor.py</code> <pre><code>def preprocess(\n    self,\n    raw_user_data: Optional[DataPackage] = None,\n    predict_new_data: bool = False,\n) -&gt; DatasetContainer:\n    \"\"\"\n    Preprocess the data according to the configuration.\n\n    Args:\n        raw_user_data: The raw data package provided by the user.\n        predict_new_data: Flag indicating if new data is being predicted.\n    Returns:\n        A DatasetContainer with processed training, validation, and test datasets.\n    \"\"\"\n    self.dataset_dicts = self._general_preprocess(\n        raw_user_data=raw_user_data, predict_new_data=predict_new_data\n    )\n    datasets = {}\n    for split in [\"train\", \"test\", \"valid\"]:\n        cur_split = self.dataset_dicts.get(split)\n        if cur_split is None:\n            print(f\"split is None: {split}\")\n            continue\n        cur_data = cur_split.get(\"data\")\n        if not isinstance(cur_data, DataPackage):\n            raise TypeError(\n                f\"expected type of cur_data to be DataPackage, got {type(cur_data)}\"\n            )\n        cur_indices = cur_split.get(\"indices\")\n        datasets[split] = self._process_dp(dp=cur_data, indices=cur_indices)\n\n    return DatasetContainer(\n        train=datasets[\"train\"], test=datasets[\"test\"], valid=datasets[\"valid\"]\n    )\n</code></pre>"},{"location":"api/data/#autoencodix.data.ImgData","title":"<code>ImgData</code>  <code>dataclass</code>","text":"<p>Stores image data along with its associated metadata.</p> <p>Attributes:</p> Name Type Description <code>img</code> <p>The image data as a NumPy array.</p> <code>sample_id</code> <code>str</code> <p>A unique identifier for the image sample.</p> <code>annotation</code> <code>Union[Series, DataFrame]</code> <p>A DataFrame containing annotations or metadata related to the image.</p> Source code in <code>src/autoencodix/data/_imgdataclass.py</code> <pre><code>@dataclass\nclass ImgData:\n    \"\"\"Stores image data along with its associated metadata.\n\n    Attributes:\n        img : The image data as a NumPy array.\n        sample_id: A unique identifier for the image sample.\n        annotation: A DataFrame containing annotations or metadata related to the image.\n    \"\"\"\n\n    img: np.ndarray\n    sample_id: str\n    annotation: Union[pd.Series, pd.DataFrame]\n\n    def __repr__(self):\n        return (\n            f\"ImgData(\\n\"\n            f\"    sample_id={self.sample_id!r},\\n\"\n            f\"    img_shape={self.img.shape},\\n\"\n            f\"    annotation_shape={self.annotation.shape}\\n\"\n            f\" .  img: actual image data is not shown for brevity, use img attribute to access it\"\n            f\")\"\n        )\n</code></pre>"},{"location":"api/data/#autoencodix.data.MultiModalDataset","title":"<code>MultiModalDataset</code>","text":"<p>               Bases: <code>BaseDataset</code>, <code>Dataset</code></p> <p>Handles multiple datasets of different modalities.</p> <p>Attributes:</p> Name Type Description <code>datasets</code> <p>Dictionary of datasets for each modality.</p> <code>n_modalities</code> <p>Number of modalities.</p> <code>sample_to_modalities</code> <p>Mapping from sample IDs to available modalities.</p> <code>sample_ids</code> <code>List[Any]</code> <p>List of all unique sample IDs across modalities.</p> <code>config</code> <p>Configuration object.</p> <code>data</code> <p>Data from the first modality (for compatibility).</p> <code>feature_ids</code> <p>Feature IDs (currently None, to be implemented).</p> <code>_id_to_idx</code> <p>Reverse lookup tables for sample IDs to indices per modality.</p> <code>paired_sample_ids</code> <p>List of sample IDs that have data in all modalities.</p> <code>unpaired_sample_ids</code> <p>List of sample IDs that do not have data in all modalities.</p> Source code in <code>src/autoencodix/data/_multimodal_dataset.py</code> <pre><code>class MultiModalDataset(BaseDataset, torch.utils.data.Dataset):  # type: ignore\n    \"\"\"Handles multiple datasets of different modalities.\n\n    Attributes:\n        datasets: Dictionary of datasets for each modality.\n        n_modalities: Number of modalities.\n        sample_to_modalities: Mapping from sample IDs to available modalities.\n        sample_ids: List of all unique sample IDs across modalities.\n        config: Configuration object.\n        data: Data from the first modality (for compatibility).\n        feature_ids: Feature IDs (currently None, to be implemented).\n        _id_to_idx: Reverse lookup tables for sample IDs to indices per modality.\n        paired_sample_ids: List of sample IDs that have data in all modalities.\n        unpaired_sample_ids: List of sample IDs that do not have data in all modalities.\n    \"\"\"\n\n    def __init__(self, datasets: Dict[str, BaseDataset], config: DefaultConfig):\n        \"\"\"\n        Initialize the MultiModalDataset.\n\n        Args:\n            datasets: Dictionary of datasets for each modality.\n            config: Configuration object.\n        \"\"\"\n        self.datasets = datasets\n        self.modalities = list(datasets.keys())\n        self.n_modalities = len(self.datasets.keys())\n        self.sample_to_modalities = self._build_sample_map()\n        self.sample_ids: List[Any] = list(self.sample_to_modalities.keys())\n        self.config = config\n        self.data = next(iter(self.datasets.values())).data\n        self.feature_ids = None  # TODO\n\n        # Build reverse lookup tables once\n        for ds_name, ds in self.datasets.items():\n            if ds.sample_ids is None:\n                raise ValueError(f\"There are no sample_ids for {ds_name}\")\n        self._id_to_idx = {\n            mod: {sid: idx for idx, sid in enumerate(ds.sample_ids)}  # type: ignore\n            for mod, ds in self.datasets.items()\n        }\n        self.paired_sample_ids = self._get_paired_sample_ids()\n        self.unpaired_sample_ids = list(\n            set(self.sample_ids) - set(self.paired_sample_ids)\n        )\n\n    def _to_df(self, modality: Optional[str] = None) -&gt; pd.DataFrame:\n        \"\"\"Convert the dataset to a pandas DataFrame.\n\n        Returns:\n            DataFrame representation of the dataset\n        \"\"\"\n        if modality is None:\n            all_modality = list(self.datasets.keys())\n        else:\n            all_modality = [modality]\n\n        df_all = pd.DataFrame()\n        for modality in all_modality:\n            if modality not in self.datasets:\n                raise ValueError(f\"Unknown modality: {modality}\")\n\n            ds = self.datasets[modality]\n            if isinstance(ds.data, torch.Tensor):\n                df = pd.DataFrame(\n                    ds.data.numpy(), columns=ds.feature_ids, index=ds.sample_ids\n                )\n            elif isinstance(ds.data, list):\n                # Handle image modality\n                # Get the list of tensors\n                tensor_list = self.datasets[modality].data\n                if not isinstance(tensor_list[0], torch.Tensor):\n                    raise TypeError(\n                        f\" Image List is not a List[torch.Tensor], but a {type(tensor_list[0])} and cannot be converted to DataFrame.\"\n                    )\n\n                rows = [\n                    (\n                        t.flatten().cpu().numpy()\n                        if isinstance(t, torch.Tensor)\n                        else t.flatten()\n                    )\n                    for t in tensor_list\n                ]\n\n                df = pd.DataFrame(\n                    rows,\n                    index=ds.sample_ids,\n                    columns=[\"Pixel_\" + str(i) for i in range(len(rows[0]))],\n                )\n            else:\n                raise TypeError(\n                    f\"Data is not a torch.Tensor or image data, but a {type(ds.data)} and cannot be converted to DataFrame.\"\n                )\n\n            df = df.add_prefix(f\"{modality}_\")\n            if df_all.empty:\n                df_all = df\n            else:\n                df_all = pd.concat([df_all, df], axis=1, join=\"inner\")\n\n        return df_all\n\n    def _build_sample_map(self):\n        sample_to_mods = {}\n        for modality, dataset in self.datasets.items():\n            for sid in dataset.sample_ids:\n                sample_to_mods.setdefault(sid, set()).add(modality)\n        return sample_to_mods\n\n    def _get_paired_sample_ids(self):\n        return [\n            sid\n            for sid, mods in self.sample_to_modalities.items()\n            if all(mod in mods for mod in self.datasets.keys())\n        ]\n\n    def __len__(self):\n        return len(self.paired_sample_ids)\n\n    def __getitem__(self, idx: Union[int, str]):\n        sid = self.paired_sample_ids[idx] if isinstance(idx, int) else idx\n        out = {\"sample_id\": sid}\n        for mod in self.modalities:\n            if sid not in self._id_to_idx[mod]:  # missing modality\n                out[mod] = None\n                continue\n            _, data, _ = self.datasets[mod][self._id_to_idx[mod][sid]]\n            out[mod] = data\n        return out\n\n    @property\n    def is_fully_paired(self) -&gt; bool:\n        \"\"\"Returns True if all samples are fully paired across all modalities (no unpaired samples).\"\"\"\n\n        return len(self.unpaired_sample_ids) == 0\n</code></pre>"},{"location":"api/data/#autoencodix.data.MultiModalDataset.is_fully_paired","title":"<code>is_fully_paired</code>  <code>property</code>","text":"<p>Returns True if all samples are fully paired across all modalities (no unpaired samples).</p>"},{"location":"api/data/#autoencodix.data.MultiModalDataset.__init__","title":"<code>__init__(datasets, config)</code>","text":"<p>Initialize the MultiModalDataset.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>Dict[str, BaseDataset]</code> <p>Dictionary of datasets for each modality.</p> required <code>config</code> <code>DefaultConfig</code> <p>Configuration object.</p> required Source code in <code>src/autoencodix/data/_multimodal_dataset.py</code> <pre><code>def __init__(self, datasets: Dict[str, BaseDataset], config: DefaultConfig):\n    \"\"\"\n    Initialize the MultiModalDataset.\n\n    Args:\n        datasets: Dictionary of datasets for each modality.\n        config: Configuration object.\n    \"\"\"\n    self.datasets = datasets\n    self.modalities = list(datasets.keys())\n    self.n_modalities = len(self.datasets.keys())\n    self.sample_to_modalities = self._build_sample_map()\n    self.sample_ids: List[Any] = list(self.sample_to_modalities.keys())\n    self.config = config\n    self.data = next(iter(self.datasets.values())).data\n    self.feature_ids = None  # TODO\n\n    # Build reverse lookup tables once\n    for ds_name, ds in self.datasets.items():\n        if ds.sample_ids is None:\n            raise ValueError(f\"There are no sample_ids for {ds_name}\")\n    self._id_to_idx = {\n        mod: {sid: idx for idx, sid in enumerate(ds.sample_ids)}  # type: ignore\n        for mod, ds in self.datasets.items()\n    }\n    self.paired_sample_ids = self._get_paired_sample_ids()\n    self.unpaired_sample_ids = list(\n        set(self.sample_ids) - set(self.paired_sample_ids)\n    )\n</code></pre>"},{"location":"api/data/#autoencodix.data.NaNRemover","title":"<code>NaNRemover</code>","text":"<p>Removes NaN values from multi-modal datasets.</p> <p>This object identifies and removes NaN values from various data structures commonly used in single-cell and multi-modal omics, including AnnData, MuData, and Pandas DataFrames. It supports processing of X matrices, layers, and observation annotations within AnnData objects, as well as handling bulk and annotation data within a DataPackage.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration object containing settings for data processing.</p> <code>relevant_cols</code> <p>List of columns in metadata to check for NaNs.</p> Source code in <code>src/autoencodix/data/_nanremover.py</code> <pre><code>class NaNRemover:\n    \"\"\"Removes NaN values from multi-modal datasets.\n\n    This object identifies and removes NaN values from various data structures\n    commonly used in single-cell and multi-modal omics, including AnnData, MuData,\n    and Pandas DataFrames. It supports processing of X matrices, layers, and\n    observation annotations within AnnData objects, as well as handling bulk and\n    annotation data within a DataPackage.\n\n    Attributes:\n        config: Configuration object containing settings for data processing.\n        relevant_cols: List of columns in metadata to check for NaNs.\n    \"\"\"\n\n    def __init__(\n        self,\n        config: DefaultConfig,\n    ):\n        \"\"\"Initialize the NaNRemover with configuration settings.\n        Args:\n            config: Configuration object containing settings for data processing.\n\n        \"\"\"\n        self.config = config\n        self.relevant_cols = self.config.data_config.annotation_columns\n\n    def _process_modality(self, adata: ad.AnnData) -&gt; ad.AnnData:\n        \"\"\"Converts NaN values in AnnData object to zero and metadata NaNs to 'missing'.\n        Args:\n            adata: The AnnData object to process.\n        Returns:\n            The processed AnnData object with NaN values replaced.\n        \"\"\"\n        adata = adata.copy()\n\n        # Handle X matrix\n        if sparse.issparse(adata.X):\n            if hasattr(adata.X, \"data\"):\n                adata.X.data = np.nan_to_num(  # ty:  ignore\n                    adata.X.data, nan=0.0\n                )  # ty: ignore[invalid-assignment]\n                adata.X.eliminate_zeros()  # ty: ignore\n        else:\n            adata.X = np.nan_to_num(adata.X, nan=0.0)\n\n        # Handle all layers\n        for layer_name, layer_data in adata.layers.items():\n            if sparse.issparse(layer_data):\n                if hasattr(layer_data, \"data\"):\n                    layer_data.data = np.nan_to_num(layer_data.data, nan=0.0)\n                    layer_data.eliminate_zeros()\n            else:\n                adata.layers[layer_name] = np.nan_to_num(layer_data, nan=0.0)\n\n        # Handle obs metadata\n        if self.relevant_cols is not None:\n            print(adata.obs.columns)\n            for col in self.relevant_cols:\n                if col in adata.obs.columns:\n                    # Fill NaNs with \"missing\" for non-numeric columns\n                    if not pd.api.types.is_numeric_dtype(adata.obs[col]):\n                        # Add \"missing\" to categories first, then fill\n                        adata.obs[col] = adata.obs[col].cat.add_categories([\"missing\"])\n                        adata.obs[col] = (\n                            adata.obs[col].fillna(\"missing\").astype(\"category\")\n                        )\n        return adata\n\n    def remove_nan(self, data: DataPackage) -&gt; DataPackage:\n        \"\"\"Removes NaN values from all applicable DataPackage components.\n\n        Iterates through the bulk data, annotation data, and multi-modal\n        single-cell data (MuData and AnnData objects) within the provided\n        DataPackage and removes rows/columns/entries containing NaN values.\n\n        Args:\n            data: The DataPackage object containing multi-modal data.\n\n        Returns:\n            The DataPackage object with NaN values removed from its components.\n        \"\"\"\n        # Handle bulk data\n        if data.multi_bulk:\n            for key, df in data.multi_bulk.items():\n                data.multi_bulk[key] = df.dropna(axis=1)\n\n        # Handle annotation data\n        if data.annotation is not None:\n            non_na = {}\n            for k, v in data.annotation.items():\n                if v is None:\n                    continue\n                if self.relevant_cols is not None:\n                    for col in self.relevant_cols:\n                        # Fill with \"missing\" if column is not integer or float\n                        if col in v.columns and not pd.api.types.is_numeric_dtype(\n                            v[col]\n                        ):\n                            v.fillna(value={col: \"missing\"}, inplace=True)\n\n                non_na[k] = v\n            data.annotation = non_na  # type: ignore\n\n        # Handle MuData in multi_sc\n        if data.multi_sc is not None and self.config.requires_paired:\n            mudata = data.multi_sc[\"multi_sc\"]\n            # Process each modality\n            for mod_name, mod_data in mudata.mod.items():\n                processed_mod = self._process_modality(adata=mod_data)\n                data.multi_sc[\"multi_sc\"].mod[mod_name] = processed_mod\n\n        elif data.multi_sc is not None:\n            print(f\"data in multi_sc: {data.multi_sc}\")\n            processed = {k: None for k, _ in data.multi_sc.items()}\n\n            for k, v in data.multi_sc.items():\n                # we know from screader that there is only one modality\n                for modkey, adata in v.mod.items():\n                    processed_mod = self._process_modality(adata=adata)\n                    processed_mod = md.MuData({modkey: processed_mod})\n                processed[k] = processed_mod\n            data.multi_sc = processed\n\n        # Handle from_modality and to_modality (for translation cases)\n        for direction in [\"from_modality\", \"to_modality\"]:\n            modality_dict = getattr(data, direction)\n            if not modality_dict:\n                continue\n\n            for mod_key, mod_value in modality_dict.items():\n                # Handle MuData objects - use the proper import\n                if isinstance(mod_value, md.MuData):\n                    # Process each modality in the MuData\n                    for inner_mod_name, inner_mod_data in mod_value.mod.items():\n                        processed_mod = self._process_modality(inner_mod_data)\n                        mod_value.mod[inner_mod_name] = processed_mod\n\n                    # Ensure cell alignment if there are multiple modalities\n                    if len(mod_value.mod) &gt; 1:\n                        common_cells = list(\n                            set.intersection(\n                                *(set(mod.obs_names) for mod in mod_value.mod.values())\n                            )\n                        )\n                        mod_value = mod_value[common_cells]\n\n                    modality_dict[mod_key] = mod_value\n\n                # Handle AnnData objects directly\n                elif isinstance(mod_value, ad.AnnData):\n                    processed_mod = self._process_modality(mod_value)\n                    modality_dict[mod_key] = processed_mod\n\n                # Handle other types of data (e.g., dictionaries of AnnData objects)\n                elif isinstance(mod_value, dict):\n                    for sub_key, sub_value in mod_value.items():\n                        if isinstance(sub_value, ad.AnnData):\n                            processed_mod = self._process_modality(sub_value)\n                            mod_value[sub_key] = processed_mod\n\n                elif isinstance(mod_value, pd.DataFrame):\n                    mod_value.dropna(axis=1, inplace=True)\n                    modality_dict[mod_key] = mod_value\n\n                else:\n                    warnings.warn(\n                        f\"Skipping unknown type in {direction}.{mod_key}: {type(mod_value)}\"\n                    )\n\n        return data\n</code></pre>"},{"location":"api/data/#autoencodix.data.NaNRemover.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the NaNRemover with configuration settings. Args:     config: Configuration object containing settings for data processing.</p> Source code in <code>src/autoencodix/data/_nanremover.py</code> <pre><code>def __init__(\n    self,\n    config: DefaultConfig,\n):\n    \"\"\"Initialize the NaNRemover with configuration settings.\n    Args:\n        config: Configuration object containing settings for data processing.\n\n    \"\"\"\n    self.config = config\n    self.relevant_cols = self.config.data_config.annotation_columns\n</code></pre>"},{"location":"api/data/#autoencodix.data.NaNRemover.remove_nan","title":"<code>remove_nan(data)</code>","text":"<p>Removes NaN values from all applicable DataPackage components.</p> <p>Iterates through the bulk data, annotation data, and multi-modal single-cell data (MuData and AnnData objects) within the provided DataPackage and removes rows/columns/entries containing NaN values.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataPackage</code> <p>The DataPackage object containing multi-modal data.</p> required <p>Returns:</p> Type Description <code>DataPackage</code> <p>The DataPackage object with NaN values removed from its components.</p> Source code in <code>src/autoencodix/data/_nanremover.py</code> <pre><code>def remove_nan(self, data: DataPackage) -&gt; DataPackage:\n    \"\"\"Removes NaN values from all applicable DataPackage components.\n\n    Iterates through the bulk data, annotation data, and multi-modal\n    single-cell data (MuData and AnnData objects) within the provided\n    DataPackage and removes rows/columns/entries containing NaN values.\n\n    Args:\n        data: The DataPackage object containing multi-modal data.\n\n    Returns:\n        The DataPackage object with NaN values removed from its components.\n    \"\"\"\n    # Handle bulk data\n    if data.multi_bulk:\n        for key, df in data.multi_bulk.items():\n            data.multi_bulk[key] = df.dropna(axis=1)\n\n    # Handle annotation data\n    if data.annotation is not None:\n        non_na = {}\n        for k, v in data.annotation.items():\n            if v is None:\n                continue\n            if self.relevant_cols is not None:\n                for col in self.relevant_cols:\n                    # Fill with \"missing\" if column is not integer or float\n                    if col in v.columns and not pd.api.types.is_numeric_dtype(\n                        v[col]\n                    ):\n                        v.fillna(value={col: \"missing\"}, inplace=True)\n\n            non_na[k] = v\n        data.annotation = non_na  # type: ignore\n\n    # Handle MuData in multi_sc\n    if data.multi_sc is not None and self.config.requires_paired:\n        mudata = data.multi_sc[\"multi_sc\"]\n        # Process each modality\n        for mod_name, mod_data in mudata.mod.items():\n            processed_mod = self._process_modality(adata=mod_data)\n            data.multi_sc[\"multi_sc\"].mod[mod_name] = processed_mod\n\n    elif data.multi_sc is not None:\n        print(f\"data in multi_sc: {data.multi_sc}\")\n        processed = {k: None for k, _ in data.multi_sc.items()}\n\n        for k, v in data.multi_sc.items():\n            # we know from screader that there is only one modality\n            for modkey, adata in v.mod.items():\n                processed_mod = self._process_modality(adata=adata)\n                processed_mod = md.MuData({modkey: processed_mod})\n            processed[k] = processed_mod\n        data.multi_sc = processed\n\n    # Handle from_modality and to_modality (for translation cases)\n    for direction in [\"from_modality\", \"to_modality\"]:\n        modality_dict = getattr(data, direction)\n        if not modality_dict:\n            continue\n\n        for mod_key, mod_value in modality_dict.items():\n            # Handle MuData objects - use the proper import\n            if isinstance(mod_value, md.MuData):\n                # Process each modality in the MuData\n                for inner_mod_name, inner_mod_data in mod_value.mod.items():\n                    processed_mod = self._process_modality(inner_mod_data)\n                    mod_value.mod[inner_mod_name] = processed_mod\n\n                # Ensure cell alignment if there are multiple modalities\n                if len(mod_value.mod) &gt; 1:\n                    common_cells = list(\n                        set.intersection(\n                            *(set(mod.obs_names) for mod in mod_value.mod.values())\n                        )\n                    )\n                    mod_value = mod_value[common_cells]\n\n                modality_dict[mod_key] = mod_value\n\n            # Handle AnnData objects directly\n            elif isinstance(mod_value, ad.AnnData):\n                processed_mod = self._process_modality(mod_value)\n                modality_dict[mod_key] = processed_mod\n\n            # Handle other types of data (e.g., dictionaries of AnnData objects)\n            elif isinstance(mod_value, dict):\n                for sub_key, sub_value in mod_value.items():\n                    if isinstance(sub_value, ad.AnnData):\n                        processed_mod = self._process_modality(sub_value)\n                        mod_value[sub_key] = processed_mod\n\n            elif isinstance(mod_value, pd.DataFrame):\n                mod_value.dropna(axis=1, inplace=True)\n                modality_dict[mod_key] = mod_value\n\n            else:\n                warnings.warn(\n                    f\"Skipping unknown type in {direction}.{mod_key}: {type(mod_value)}\"\n                )\n\n    return data\n</code></pre>"},{"location":"api/data/#autoencodix.data.NumericDataset","title":"<code>NumericDataset</code>","text":"<p>               Bases: <code>TensorAwareDataset</code></p> <p>A custom PyTorch dataset that handles tensors.</p> <p>Attributes:</p> Name Type Description <code>data</code> <p>The input features as a torch.Tensor.</p> <code>config</code> <p>Configuration object containing settings for data processing.</p> <code>sample_ids</code> <p>Optional list of sample identifiers.</p> <code>feature_ids</code> <p>Optional list of feature identifiers.</p> <code>metadata</code> <p>Optional pandas DataFrame containing metadata.</p> <code>split_indices</code> <p>Optional numpy array for data splitting.</p> <code>mytype</code> <p>Enum indicating the dataset type (set to DataSetTypes.NUM).</p> Source code in <code>src/autoencodix/data/_numeric_dataset.py</code> <pre><code>class NumericDataset(TensorAwareDataset):\n    \"\"\"A custom PyTorch dataset that handles tensors.\n\n\n    Attributes:\n        data: The input features as a torch.Tensor.\n        config: Configuration object containing settings for data processing.\n        sample_ids: Optional list of sample identifiers.\n        feature_ids: Optional list of feature identifiers.\n        metadata: Optional pandas DataFrame containing metadata.\n        split_indices: Optional numpy array for data splitting.\n        mytype: Enum indicating the dataset type (set to DataSetTypes.NUM).\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Union[torch.Tensor, np.ndarray, sp.sparse.spmatrix],\n        config: DefaultConfig,\n        sample_ids: Union[None, List[Any]] = None,\n        feature_ids: Union[None, List[Any]] = None,\n        metadata: Optional[Union[pd.Series, pd.DataFrame]] = None,\n        split_indices: Optional[Union[Dict[str, Any], List[Any], np.ndarray]] = None,\n    ):\n        \"\"\"\n        Initialize the dataset\n\n        Args:\n            data: Input features\n            config: Configuration object\n            sample_ids: Optional sample identifiers\n            feature_ids: Optional feature identifiers\n            metadata: Optional metadata\n            split_indices: Optional split indices\n            Optional split indices\n        \"\"\"\n        super().__init__(\n            data=data, sample_ids=sample_ids, config=config, feature_ids=feature_ids\n        )\n\n        if self.config is None:\n            raise ValueError(\"config cannot be None\")\n\n        # Convert data to appropriate dtype once during initialization\n        self.target_dtype = self._get_target_dtype()\n        # keep data sparce if it is a scipy sparse matrix to be memory\n        # efficient for large single cell data, convert at batch level to dense tensor\n        if isinstance(self.data, (np.ndarray, torch.Tensor)):\n            self.data = self._to_tensor(data, self.target_dtype)\n\n        self.metadata = metadata\n        self.split_indices = split_indices\n        self.mytype = DataSetTypes.NUM\n\n    @no_type_check\n    def __getitem__(self, index: int) -&gt; Union[\n        Tuple[\n            Union[torch.Tensor, int],\n            Union[torch.Tensor, \"ImgData\"],  # ty: ignore  # noqa: F821\n            Any,\n        ],\n        Dict[str, Tuple[Any, torch.Tensor, Any]],\n    ]:\n        \"\"\"Retrieves a single sample and its corresponding label.\n\n        Args:\n            index: Index of the sample to retrieve.\n\n        Returns:\n            A tuple containing the index, the data sample and its label, or a dictionary\n            mapping keys to such tuples in case we have multiple uncombined data at this step.\n        \"\"\"\n\n        row = self.data[index]  # idx: int, slice, or list\n        if self.sample_ids is not None:\n            label = self.sample_ids[index]\n        else:\n            label = index\n        if issparse(row):\n            # print(\"calling to array\")\n\n            # print(f\"Size of data sparse: {asizeof.asizeof(row)}\")\n            row = torch.tensor(row.toarray(), dtype=self.target_dtype).squeeze(0)\n\n            # print(f\"Size of data dense: {asizeof.asizeof(row)}\")\n\n        return index, row, label\n\n    def __len__(self) -&gt; int:\n        \"\"\"Returns the number of samples (rows) in the dataset\"\"\"\n        return self.data.shape[0]\n</code></pre>"},{"location":"api/data/#autoencodix.data.NumericDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Retrieves a single sample and its corresponding label.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the sample to retrieve.</p> required <p>Returns:</p> Type Description <code>Union[Tuple[Union[Tensor, int], Union[Tensor, 'ImgData'], Any], Dict[str, Tuple[Any, Tensor, Any]]]</code> <p>A tuple containing the index, the data sample and its label, or a dictionary</p> <code>Union[Tuple[Union[Tensor, int], Union[Tensor, 'ImgData'], Any], Dict[str, Tuple[Any, Tensor, Any]]]</code> <p>mapping keys to such tuples in case we have multiple uncombined data at this step.</p> Source code in <code>src/autoencodix/data/_numeric_dataset.py</code> <pre><code>@no_type_check\ndef __getitem__(self, index: int) -&gt; Union[\n    Tuple[\n        Union[torch.Tensor, int],\n        Union[torch.Tensor, \"ImgData\"],  # ty: ignore  # noqa: F821\n        Any,\n    ],\n    Dict[str, Tuple[Any, torch.Tensor, Any]],\n]:\n    \"\"\"Retrieves a single sample and its corresponding label.\n\n    Args:\n        index: Index of the sample to retrieve.\n\n    Returns:\n        A tuple containing the index, the data sample and its label, or a dictionary\n        mapping keys to such tuples in case we have multiple uncombined data at this step.\n    \"\"\"\n\n    row = self.data[index]  # idx: int, slice, or list\n    if self.sample_ids is not None:\n        label = self.sample_ids[index]\n    else:\n        label = index\n    if issparse(row):\n        # print(\"calling to array\")\n\n        # print(f\"Size of data sparse: {asizeof.asizeof(row)}\")\n        row = torch.tensor(row.toarray(), dtype=self.target_dtype).squeeze(0)\n\n        # print(f\"Size of data dense: {asizeof.asizeof(row)}\")\n\n    return index, row, label\n</code></pre>"},{"location":"api/data/#autoencodix.data.NumericDataset.__init__","title":"<code>__init__(data, config, sample_ids=None, feature_ids=None, metadata=None, split_indices=None)</code>","text":"<p>Initialize the dataset</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Tensor, ndarray, spmatrix]</code> <p>Input features</p> required <code>config</code> <code>DefaultConfig</code> <p>Configuration object</p> required <code>sample_ids</code> <code>Union[None, List[Any]]</code> <p>Optional sample identifiers</p> <code>None</code> <code>feature_ids</code> <code>Union[None, List[Any]]</code> <p>Optional feature identifiers</p> <code>None</code> <code>metadata</code> <code>Optional[Union[Series, DataFrame]]</code> <p>Optional metadata</p> <code>None</code> <code>split_indices</code> <code>Optional[Union[Dict[str, Any], List[Any], ndarray]]</code> <p>Optional split indices</p> <code>None</code> Source code in <code>src/autoencodix/data/_numeric_dataset.py</code> <pre><code>def __init__(\n    self,\n    data: Union[torch.Tensor, np.ndarray, sp.sparse.spmatrix],\n    config: DefaultConfig,\n    sample_ids: Union[None, List[Any]] = None,\n    feature_ids: Union[None, List[Any]] = None,\n    metadata: Optional[Union[pd.Series, pd.DataFrame]] = None,\n    split_indices: Optional[Union[Dict[str, Any], List[Any], np.ndarray]] = None,\n):\n    \"\"\"\n    Initialize the dataset\n\n    Args:\n        data: Input features\n        config: Configuration object\n        sample_ids: Optional sample identifiers\n        feature_ids: Optional feature identifiers\n        metadata: Optional metadata\n        split_indices: Optional split indices\n        Optional split indices\n    \"\"\"\n    super().__init__(\n        data=data, sample_ids=sample_ids, config=config, feature_ids=feature_ids\n    )\n\n    if self.config is None:\n        raise ValueError(\"config cannot be None\")\n\n    # Convert data to appropriate dtype once during initialization\n    self.target_dtype = self._get_target_dtype()\n    # keep data sparce if it is a scipy sparse matrix to be memory\n    # efficient for large single cell data, convert at batch level to dense tensor\n    if isinstance(self.data, (np.ndarray, torch.Tensor)):\n        self.data = self._to_tensor(data, self.target_dtype)\n\n    self.metadata = metadata\n    self.split_indices = split_indices\n    self.mytype = DataSetTypes.NUM\n</code></pre>"},{"location":"api/data/#autoencodix.data.NumericDataset.__len__","title":"<code>__len__()</code>","text":"<p>Returns the number of samples (rows) in the dataset</p> Source code in <code>src/autoencodix/data/_numeric_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Returns the number of samples (rows) in the dataset\"\"\"\n    return self.data.shape[0]\n</code></pre>"},{"location":"api/data/#autoencodix.data.SingleCellFilter","title":"<code>SingleCellFilter</code>","text":"<p>Filter and scale single-cell data, returning a MuData object with synchronized metadata.AnnData</p> <p>Attributes:</p> Name Type Description <code>data_info</code> <p>Configuration for filtering and scaling (can be a single DataInfo or a dict of DataInfo per modality).</p> <code>total_features</code> <p>Total number of features to keep across all modalities.</p> <code>config</code> <p>Configuration object containing settings for data processing.</p> <code>_is_data_info_dict</code> <p>Internal flag indicating if data_info is a dictionary.</p> Source code in <code>src/autoencodix/data/_sc_filter.py</code> <pre><code>class SingleCellFilter:\n    \"\"\"Filter and scale single-cell data, returning a MuData object with synchronized metadata.AnnData\n\n    Attributes:\n        data_info: Configuration for filtering and scaling (can be a single DataInfo or a dict of DataInfo per modality).\n        total_features: Total number of features to keep across all modalities.\n        config: Configuration object containing settings for data processing.\n        _is_data_info_dict: Internal flag indicating if data_info is a dictionary.\n    \"\"\"\n\n    def __init__(\n        self, data_info: Union[Dict[str, DataInfo], DataInfo], config: DefaultConfig\n    ):\n        \"\"\"\n        Initialize single-cell filter.\n        Args:\n            data_info: Either a single data_info object for all modalities or a dictionary of data_info objects for each modality.\n            config: Configuration object containing settings for data processing.\n        \"\"\"\n        self.data_info = data_info\n        self.total_features = config.k_filter\n        self._is_data_info_dict = isinstance(data_info, dict)\n        self.config = config\n\n    def _get_data_info_for_modality(self, mod_key: str) -&gt; DataInfo:\n        \"\"\"\n        Get the data_info configuration for a specific modality.\n        Args:\n            mod_key: The modality key (e.g., \"RNA\", \"METH\")\n        Returns\n            The data_info configuration for the modality\n        \"\"\"\n        if self._is_data_info_dict:\n            info = self.data_info.get(mod_key)  # type: ignore\n            if info is None:\n                raise ValueError(f\"No data info found for modality {mod_key}\")\n            return info\n        return self.data_info  # type: ignore\n\n    def _get_layers_for_modality(self, mod_key: str, mod_data) -&gt; List[str]:\n        \"\"\"\n        Get the layers to process for a specific modality.\n        Args\n            mod_key: The modality key (e.g., \"RNA\", \"METH\")\n            mod_data: The AnnData object for the modality\n        Returns\n            List of layer names to process. If None or empty, returns ['X'] for default layer.\n        \"\"\"\n        data_info = self._get_data_info_for_modality(mod_key)\n        selected_layers = data_info.selected_layers\n\n        # Validate that the specified layers exist\n        available_layers = list(mod_data.layers.keys())\n        valid_layers = []\n\n        for layer in selected_layers:\n            if layer == \"X\":\n                valid_layers.append(\"X\")\n            elif layer in available_layers:\n                valid_layers.append(layer)\n            else:\n                print(\n                    f\"Warning: Layer '{layer}' not found in modality '{mod_key}'. Skipping.\"\n                )\n        if not valid_layers:\n            valid_layers = [\"X\"]\n\n        return valid_layers\n\n    def _presplit_processing(\n        self,\n        mudata: MuData,  # type: ignore[invalid-type-form]\n    ) -&gt; MuData:  # type: ignore[invalid-type-form]\n        \"\"\"\n        Preprocess the data using modality-specific configurations.\n        Returns:\n            Preprocessed data\n        \"\"\"\n        print(f\"mudata: {mudata}\")\n        for mod_key, mod_data in mudata.mod.items():\n            data_info = self._get_data_info_for_modality(mod_key)\n            if data_info is not None:\n                sc.pp.filter_cells(mod_data, min_genes=data_info.min_genes)\n                layers_to_process = self._get_layers_for_modality(mod_key, mod_data)\n\n                for layer in layers_to_process:\n                    if layer == \"X\":\n                        if data_info.log_transform:\n                            sc.pp.log1p(mod_data)\n                    else:\n                        temp_view = mod_data.copy()\n                        temp_view.X = mod_data.layers[layer].copy()\n                        if data_info.log_transform:\n                            sc.pp.log1p(temp_view)\n                        mod_data.layers[layer] = temp_view.X.copy()\n\n                mudata.mod[mod_key] = mod_data\n\n        return mudata\n\n    def presplit_processing(\n        self,\n        multi_sc: Union[MuData, Dict[str, MuData]],  # ty: ignore[invalid-type-form]\n    ) -&gt; Dict[str, MuData]:  # ty: ignore[invalid-type-form]\n        \"\"\"\n        Process each modality independently to filter cells based on min_genes.\n\n        Args:\n            multi_sc: Either a single MuData object or a dictionary of MuData objects.\n        Returns:\n            A dictionary mapping modality keys to processed MuData objects.\n        \"\"\"\n        from mudata import MuData\n\n        if isinstance(multi_sc, MuData):\n            return self._presplit_processing(mudata=multi_sc)\n        res = {k: None for k in multi_sc.keys()}\n        for k, v in multi_sc.items():\n            processed = self._presplit_processing(mudata=v)\n            res[k] = processed\n        return res\n\n    def _to_dataframe(self, mod_data, layer=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Transform a modality's AnnData object to a pandas DataFrame.\n        Args:\n            mod_data: Modality data to be transformed\n            layer: Layer to convert to DataFrame. If None, uses X.\n        Returns:\n            Transformed DataFrame\n        \"\"\"\n        if layer is None or layer == \"X\":\n            data = mod_data.X\n        else:\n            data = mod_data.layers[layer]\n\n        # Convert to dense array if sparse\n        if isinstance(data, np.ndarray):\n            matrix = data\n        else:  # Assuming it's a sparse matrix\n            matrix = data.toarray()\n\n        return pd.DataFrame(\n            matrix, columns=mod_data.var_names, index=mod_data.obs_names\n        )\n\n    def _from_dataframe(self, df: pd.DataFrame, mod_data, layer=None):\n        \"\"\"\n        Update a modality's AnnData object with the values from a DataFrame.\n        This also synchronizes the `obs` and `var` metadata to match the filtered data.\n        Args:\n            df: DataFrame containing the updated values\n            mod_data: Modality data to be updated\n            layer: Layer to update with DataFrame values. If None, updates X.\n        Returns:\n            Updated AnnData object\n        \"\"\"\n        # Filter the AnnData object to match the rows and columns of the DataFrame\n        filtered_mod_data = mod_data[df.index, df.columns].copy()\n\n        # Update the data matrix with the filtered and scaled values\n        if layer is None or layer == \"X\":\n            filtered_mod_data.X = df.values\n        else:\n            if layer not in filtered_mod_data.layers:\n                filtered_mod_data.layers[layer] = df.values\n            else:\n                filtered_mod_data.layers[layer] = df.values\n\n        return filtered_mod_data\n\n    def sc_postsplit_processing(\n        self,\n        mudata: MuData,  # ty: ignore[invalid-type-form]\n        gene_map: Optional[\n            Dict[str, List[str]]\n        ] = None,  # ty: ignore[invalid-type-form]\n    ) -&gt; Tuple[MuData, Dict[str, List[str]]]:  # ty: ignore[invalid-type-form]\n        \"\"\"\n        Process each modality independently to filter genes based on X layer, then\n        consistently apply the same filtering to all layers.\n\n        Args:\n        mudata : Input multi-modal data container\n        gene_map : Optional override of genes to keep per modality\n\n        Returns:\n            - Processed MuData with filtered modalities\n            - Mapping of modality to kept gene names\n        \"\"\"\n        kept_genes = {}\n        processed_mods = {}\n\n        for mod_key, adata in mudata.mod.items():\n            # Get configuration for this modality\n            info = self._get_data_info_for_modality(mod_key)\n            if info is None:\n                raise ValueError(f\"No data info for modality '{mod_key}'\")\n\n            # Determine which genes to keep\n            if gene_map and mod_key in gene_map:\n                # Use provided gene list if available\n                genes_to_keep = gene_map[mod_key]\n                var_mask = adata.var_names.isin(genes_to_keep)\n            else:\n                # Filter genes based on minimum cells expressing each gene\n                var_mask = sc.pp.filter_genes(\n                    adata.copy(), min_cells=info.min_cells, inplace=False\n                )[0]\n                genes_to_keep = adata.var_names[var_mask].tolist()\n\n            kept_genes[mod_key] = genes_to_keep\n\n            # Create new AnnData with filtered X layer\n            filtered_adata = AnnData(\n                X=adata.X[:, var_mask],\n                obs=adata.obs.copy(),\n                var=adata.var[var_mask].copy(),\n                uns=adata.uns.copy(),\n                obsm=adata.obsm.copy(),\n            )\n\n            # Normalize if configured\n            if info.normalize_counts:\n                sc.pp.normalize_total(filtered_adata)\n\n            # Copy filtered layers\n            for layer in self._get_layers_for_modality(mod_key, adata):\n                if layer == \"X\":\n                    continue\n\n                if layer not in adata.layers:\n                    raise ValueError(\n                        f\"Layer '{layer}' not found in modality '{mod_key}'\"\n                    )\n\n                filtered_adata.layers[layer] = adata.layers[layer][:, var_mask].copy()\n\n            processed_mods[mod_key] = filtered_adata\n\n        # Construct new MuData from filtered modalities\n        return md.MuData(processed_mods), kept_genes\n\n    def _apply_general_filtering(\n        self, df: pd.DataFrame, data_info: DataInfo, gene_list: Optional[List]\n    ) -&gt; Tuple[Union[pd.Series, pd.DataFrame], List]:\n        data_processor = DataFilter(data_info=data_info, config=self.config)\n        return data_processor.filter(df=df, genes_to_keep=gene_list)\n\n    def _apply_scaling(\n        self, df: pd.DataFrame, data_info: DataInfo, scaler: Any\n    ) -&gt; Tuple[Union[pd.Series, pd.DataFrame], Any]:\n        data_processor = DataFilter(data_info=data_info, config=self.config)\n        if scaler is None:\n            scaler = data_processor.fit_scaler(df=df)\n        scaled_df = data_processor.scale(df=df, scaler=scaler)\n        return scaled_df, scaler\n\n    def general_postsplit_processing(\n        self,\n        mudata: MuData,  # ty: ignore[invalid-type-form]\n        gene_map: Optional[Dict[str, List]],\n        scaler_map: Optional[Dict[str, Dict[str, Any]]] = None,\n    ) -&gt; Tuple[\n        MuData,  # ty: ignore[invalid-type-form]\n        Dict[str, List],\n        Dict[str, Dict[str, Any]],  # ty: ignore[invalid-type-form]\n    ]:  # ty: ignore[invalid-type-form]\n        \"\"\"Process single-cell data with proper MuData handling\n        Args:\n            mudata: Input multi-modal data container\n            gene_map: Optional override of genes to keep per modality\n            scaler_map: Optional pre-fitted scalers per modality and layer\n        Returns:\n            Processed MuData with filtered and scaled modalities,\n        \"\"\"\n        feature_distribution = self.distribute_features_across_modalities(\n            mudata, self.total_features\n        )\n        out_gene_map = {}\n        out_scaler_map = {mod_key: {} for mod_key in mudata.mod.keys()}\n\n        # Dictionary to store processed modalities\n        processed_modalities = {}\n\n        for mod_key, original_mod in mudata.mod.items():\n            data_info = self._get_data_info_for_modality(mod_key)\n            data_info.k_filter = feature_distribution[mod_key]\n\n            if data_info is None:\n                raise ValueError(f\"No data info found for modality {mod_key}\")\n\n            # Create working copy of the modality data\n            mod_data = original_mod.copy()\n\n            # Process X matrix\n            x_df = self._to_dataframe(mod_data, layer=None)\n            filtered_x, gene_list = self._apply_general_filtering(\n                df=x_df,\n                gene_list=gene_map.get(mod_key) if gene_map else None,\n                data_info=data_info,\n            )\n            out_gene_map[mod_key] = gene_list\n\n            # Apply scaling to X\n            scaled_x, x_scaler = self._apply_scaling(\n                df=filtered_x,\n                data_info=data_info,\n                scaler=scaler_map[mod_key].get(\"X\") if scaler_map else None,\n            )\n            out_scaler_map[mod_key][\"X\"] = x_scaler\n\n            # Create new AnnData for this modality\n            processed_adata = self._create_new_adata(\n                scaled_x,\n                original_adata=mod_data,\n                obs_names=mod_data.obs_names.tolist(),\n                var_names=filtered_x.columns.tolist(),\n            )\n\n            # Process layers\n            layers_to_process = self._get_layers_for_modality(mod_key, mod_data)\n            for layer in layers_to_process:\n                if layer == \"X\":\n                    continue\n\n                # Process layer data\n                layer_df = self._to_dataframe(mod_data, layer=layer)\n                filtered_layer = layer_df[filtered_x.columns]  # Match X's columns\n\n                # Apply scaling with same genes as X\n                scaled_layer, layer_scaler = self._apply_scaling(\n                    df=filtered_layer,\n                    data_info=data_info,\n                    scaler=scaler_map[mod_key].get(layer) if scaler_map else None,\n                )\n                out_scaler_map[mod_key][layer] = layer_scaler\n\n                # Store in new AnnData\n                processed_adata.layers[layer] = scaled_layer.values\n\n            # Store processed modality\n            processed_modalities[mod_key] = processed_adata\n\n        # Create new MuData from processed modalities\n        new_mudata = md.MuData(processed_modalities)\n\n        return new_mudata, out_gene_map, out_scaler_map\n\n    def _create_new_adata(self, df, original_adata, obs_names, var_names):\n        \"\"\"Helper to create properly structured AnnData\"\"\"\n        return AnnData(\n            X=df.values,\n            obs=original_adata.obs.loc[obs_names],\n            var=pd.DataFrame(index=var_names),\n            layers={},\n            uns=original_adata.uns.copy(),\n            obsm=original_adata.obsm.copy(),\n            varm=original_adata.varm.copy(),\n        )\n\n    def distribute_features_across_modalities(\n        self,\n        mudata: MuData,  # ty: ignore[invalid-type-form]\n        total_features: Optional[int],  # ty: ignore[invalid-type-form]\n    ) -&gt; Dict[str, int]:\n        \"\"\"\n        Distributes a total number of features across modalities evenly.\n\n        Args:\n            mudata: Multi-modal data object\n            total_features: Total number of features to distribute across all modalities\n\n        Returns:\n            Dictionary mapping modality keys to number of features to keep\n        \"\"\"\n\n        valid_modalities = [key for key in mudata.mod.keys()]\n        if total_features is None:\n            return {k: None for k in valid_modalities}\n        n_modalities = len(valid_modalities)\n\n        if n_modalities == 0:\n            return {}\n\n        base_features = total_features // n_modalities\n        remainder = total_features % n_modalities\n\n        # Distribute features\n        feature_distribution = {}\n        for i, mod_key in enumerate(valid_modalities):\n            # Add one extra feature to early modalities if there's remainder\n            extra = 1 if i &lt; remainder else 0\n            feature_distribution[mod_key] = base_features + extra\n\n            # Set k_filter in data_info if available\n            data_info = self._get_data_info_for_modality(mod_key)\n            if data_info is not None:\n                if not hasattr(data_info, \"k_filter\"):\n                    setattr(data_info, \"k_filter\", feature_distribution[mod_key])\n                else:\n                    data_info.k_filter = feature_distribution[mod_key]\n\n        return feature_distribution\n</code></pre>"},{"location":"api/data/#autoencodix.data.SingleCellFilter.__init__","title":"<code>__init__(data_info, config)</code>","text":"<p>Initialize single-cell filter. Args:     data_info: Either a single data_info object for all modalities or a dictionary of data_info objects for each modality.     config: Configuration object containing settings for data processing.</p> Source code in <code>src/autoencodix/data/_sc_filter.py</code> <pre><code>def __init__(\n    self, data_info: Union[Dict[str, DataInfo], DataInfo], config: DefaultConfig\n):\n    \"\"\"\n    Initialize single-cell filter.\n    Args:\n        data_info: Either a single data_info object for all modalities or a dictionary of data_info objects for each modality.\n        config: Configuration object containing settings for data processing.\n    \"\"\"\n    self.data_info = data_info\n    self.total_features = config.k_filter\n    self._is_data_info_dict = isinstance(data_info, dict)\n    self.config = config\n</code></pre>"},{"location":"api/data/#autoencodix.data.SingleCellFilter.distribute_features_across_modalities","title":"<code>distribute_features_across_modalities(mudata, total_features)</code>","text":"<p>Distributes a total number of features across modalities evenly.</p> <p>Parameters:</p> Name Type Description Default <code>mudata</code> <code>MuData</code> <p>Multi-modal data object</p> required <code>total_features</code> <code>Optional[int]</code> <p>Total number of features to distribute across all modalities</p> required <p>Returns:</p> Type Description <code>Dict[str, int]</code> <p>Dictionary mapping modality keys to number of features to keep</p> Source code in <code>src/autoencodix/data/_sc_filter.py</code> <pre><code>def distribute_features_across_modalities(\n    self,\n    mudata: MuData,  # ty: ignore[invalid-type-form]\n    total_features: Optional[int],  # ty: ignore[invalid-type-form]\n) -&gt; Dict[str, int]:\n    \"\"\"\n    Distributes a total number of features across modalities evenly.\n\n    Args:\n        mudata: Multi-modal data object\n        total_features: Total number of features to distribute across all modalities\n\n    Returns:\n        Dictionary mapping modality keys to number of features to keep\n    \"\"\"\n\n    valid_modalities = [key for key in mudata.mod.keys()]\n    if total_features is None:\n        return {k: None for k in valid_modalities}\n    n_modalities = len(valid_modalities)\n\n    if n_modalities == 0:\n        return {}\n\n    base_features = total_features // n_modalities\n    remainder = total_features % n_modalities\n\n    # Distribute features\n    feature_distribution = {}\n    for i, mod_key in enumerate(valid_modalities):\n        # Add one extra feature to early modalities if there's remainder\n        extra = 1 if i &lt; remainder else 0\n        feature_distribution[mod_key] = base_features + extra\n\n        # Set k_filter in data_info if available\n        data_info = self._get_data_info_for_modality(mod_key)\n        if data_info is not None:\n            if not hasattr(data_info, \"k_filter\"):\n                setattr(data_info, \"k_filter\", feature_distribution[mod_key])\n            else:\n                data_info.k_filter = feature_distribution[mod_key]\n\n    return feature_distribution\n</code></pre>"},{"location":"api/data/#autoencodix.data.SingleCellFilter.general_postsplit_processing","title":"<code>general_postsplit_processing(mudata, gene_map, scaler_map=None)</code>","text":"<p>Process single-cell data with proper MuData handling Args:     mudata: Input multi-modal data container     gene_map: Optional override of genes to keep per modality     scaler_map: Optional pre-fitted scalers per modality and layer Returns:     Processed MuData with filtered and scaled modalities,</p> Source code in <code>src/autoencodix/data/_sc_filter.py</code> <pre><code>def general_postsplit_processing(\n    self,\n    mudata: MuData,  # ty: ignore[invalid-type-form]\n    gene_map: Optional[Dict[str, List]],\n    scaler_map: Optional[Dict[str, Dict[str, Any]]] = None,\n) -&gt; Tuple[\n    MuData,  # ty: ignore[invalid-type-form]\n    Dict[str, List],\n    Dict[str, Dict[str, Any]],  # ty: ignore[invalid-type-form]\n]:  # ty: ignore[invalid-type-form]\n    \"\"\"Process single-cell data with proper MuData handling\n    Args:\n        mudata: Input multi-modal data container\n        gene_map: Optional override of genes to keep per modality\n        scaler_map: Optional pre-fitted scalers per modality and layer\n    Returns:\n        Processed MuData with filtered and scaled modalities,\n    \"\"\"\n    feature_distribution = self.distribute_features_across_modalities(\n        mudata, self.total_features\n    )\n    out_gene_map = {}\n    out_scaler_map = {mod_key: {} for mod_key in mudata.mod.keys()}\n\n    # Dictionary to store processed modalities\n    processed_modalities = {}\n\n    for mod_key, original_mod in mudata.mod.items():\n        data_info = self._get_data_info_for_modality(mod_key)\n        data_info.k_filter = feature_distribution[mod_key]\n\n        if data_info is None:\n            raise ValueError(f\"No data info found for modality {mod_key}\")\n\n        # Create working copy of the modality data\n        mod_data = original_mod.copy()\n\n        # Process X matrix\n        x_df = self._to_dataframe(mod_data, layer=None)\n        filtered_x, gene_list = self._apply_general_filtering(\n            df=x_df,\n            gene_list=gene_map.get(mod_key) if gene_map else None,\n            data_info=data_info,\n        )\n        out_gene_map[mod_key] = gene_list\n\n        # Apply scaling to X\n        scaled_x, x_scaler = self._apply_scaling(\n            df=filtered_x,\n            data_info=data_info,\n            scaler=scaler_map[mod_key].get(\"X\") if scaler_map else None,\n        )\n        out_scaler_map[mod_key][\"X\"] = x_scaler\n\n        # Create new AnnData for this modality\n        processed_adata = self._create_new_adata(\n            scaled_x,\n            original_adata=mod_data,\n            obs_names=mod_data.obs_names.tolist(),\n            var_names=filtered_x.columns.tolist(),\n        )\n\n        # Process layers\n        layers_to_process = self._get_layers_for_modality(mod_key, mod_data)\n        for layer in layers_to_process:\n            if layer == \"X\":\n                continue\n\n            # Process layer data\n            layer_df = self._to_dataframe(mod_data, layer=layer)\n            filtered_layer = layer_df[filtered_x.columns]  # Match X's columns\n\n            # Apply scaling with same genes as X\n            scaled_layer, layer_scaler = self._apply_scaling(\n                df=filtered_layer,\n                data_info=data_info,\n                scaler=scaler_map[mod_key].get(layer) if scaler_map else None,\n            )\n            out_scaler_map[mod_key][layer] = layer_scaler\n\n            # Store in new AnnData\n            processed_adata.layers[layer] = scaled_layer.values\n\n        # Store processed modality\n        processed_modalities[mod_key] = processed_adata\n\n    # Create new MuData from processed modalities\n    new_mudata = md.MuData(processed_modalities)\n\n    return new_mudata, out_gene_map, out_scaler_map\n</code></pre>"},{"location":"api/data/#autoencodix.data.SingleCellFilter.presplit_processing","title":"<code>presplit_processing(multi_sc)</code>","text":"<p>Process each modality independently to filter cells based on min_genes.</p> <p>Parameters:</p> Name Type Description Default <code>multi_sc</code> <code>Union[MuData, Dict[str, MuData]]</code> <p>Either a single MuData object or a dictionary of MuData objects.</p> required <p>Returns:     A dictionary mapping modality keys to processed MuData objects.</p> Source code in <code>src/autoencodix/data/_sc_filter.py</code> <pre><code>def presplit_processing(\n    self,\n    multi_sc: Union[MuData, Dict[str, MuData]],  # ty: ignore[invalid-type-form]\n) -&gt; Dict[str, MuData]:  # ty: ignore[invalid-type-form]\n    \"\"\"\n    Process each modality independently to filter cells based on min_genes.\n\n    Args:\n        multi_sc: Either a single MuData object or a dictionary of MuData objects.\n    Returns:\n        A dictionary mapping modality keys to processed MuData objects.\n    \"\"\"\n    from mudata import MuData\n\n    if isinstance(multi_sc, MuData):\n        return self._presplit_processing(mudata=multi_sc)\n    res = {k: None for k in multi_sc.keys()}\n    for k, v in multi_sc.items():\n        processed = self._presplit_processing(mudata=v)\n        res[k] = processed\n    return res\n</code></pre>"},{"location":"api/data/#autoencodix.data.SingleCellFilter.sc_postsplit_processing","title":"<code>sc_postsplit_processing(mudata, gene_map=None)</code>","text":"<p>Process each modality independently to filter genes based on X layer, then consistently apply the same filtering to all layers.</p> <p>Args: mudata : Input multi-modal data container gene_map : Optional override of genes to keep per modality</p> <p>Returns:</p> Type Description <code>MuData</code> <ul> <li>Processed MuData with filtered modalities</li> </ul> <code>Dict[str, List[str]]</code> <ul> <li>Mapping of modality to kept gene names</li> </ul> Source code in <code>src/autoencodix/data/_sc_filter.py</code> <pre><code>def sc_postsplit_processing(\n    self,\n    mudata: MuData,  # ty: ignore[invalid-type-form]\n    gene_map: Optional[\n        Dict[str, List[str]]\n    ] = None,  # ty: ignore[invalid-type-form]\n) -&gt; Tuple[MuData, Dict[str, List[str]]]:  # ty: ignore[invalid-type-form]\n    \"\"\"\n    Process each modality independently to filter genes based on X layer, then\n    consistently apply the same filtering to all layers.\n\n    Args:\n    mudata : Input multi-modal data container\n    gene_map : Optional override of genes to keep per modality\n\n    Returns:\n        - Processed MuData with filtered modalities\n        - Mapping of modality to kept gene names\n    \"\"\"\n    kept_genes = {}\n    processed_mods = {}\n\n    for mod_key, adata in mudata.mod.items():\n        # Get configuration for this modality\n        info = self._get_data_info_for_modality(mod_key)\n        if info is None:\n            raise ValueError(f\"No data info for modality '{mod_key}'\")\n\n        # Determine which genes to keep\n        if gene_map and mod_key in gene_map:\n            # Use provided gene list if available\n            genes_to_keep = gene_map[mod_key]\n            var_mask = adata.var_names.isin(genes_to_keep)\n        else:\n            # Filter genes based on minimum cells expressing each gene\n            var_mask = sc.pp.filter_genes(\n                adata.copy(), min_cells=info.min_cells, inplace=False\n            )[0]\n            genes_to_keep = adata.var_names[var_mask].tolist()\n\n        kept_genes[mod_key] = genes_to_keep\n\n        # Create new AnnData with filtered X layer\n        filtered_adata = AnnData(\n            X=adata.X[:, var_mask],\n            obs=adata.obs.copy(),\n            var=adata.var[var_mask].copy(),\n            uns=adata.uns.copy(),\n            obsm=adata.obsm.copy(),\n        )\n\n        # Normalize if configured\n        if info.normalize_counts:\n            sc.pp.normalize_total(filtered_adata)\n\n        # Copy filtered layers\n        for layer in self._get_layers_for_modality(mod_key, adata):\n            if layer == \"X\":\n                continue\n\n            if layer not in adata.layers:\n                raise ValueError(\n                    f\"Layer '{layer}' not found in modality '{mod_key}'\"\n                )\n\n            filtered_adata.layers[layer] = adata.layers[layer][:, var_mask].copy()\n\n        processed_mods[mod_key] = filtered_adata\n\n    # Construct new MuData from filtered modalities\n    return md.MuData(processed_mods), kept_genes\n</code></pre>"},{"location":"api/data/#autoencodix.data.StackixDataset","title":"<code>StackixDataset</code>","text":"<p>               Bases: <code>NumericDataset</code></p> <p>Dataset for handling multiple modalities in Stackix models.</p> <p>This dataset holds individual BaseDataset objects for multiple data modalities and provides a consistent interface for accessing them during training. It's designed to work specifically with StackixTrainer.</p> <p>Attributes:</p> Name Type Description <code>dataset_dict</code> <p>Dictionary mapping modality names to dataset objects</p> <code>modality_keys</code> <p>List of modality names</p> Source code in <code>src/autoencodix/data/_stackix_dataset.py</code> <pre><code>class StackixDataset(NumericDataset):\n    \"\"\"\n    Dataset for handling multiple modalities in Stackix models.\n\n    This dataset holds individual BaseDataset objects for multiple data modalities\n    and provides a consistent interface for accessing them during training.\n    It's designed to work specifically with StackixTrainer.\n\n    Attributes:\n        dataset_dict: Dictionary mapping modality names to dataset objects\n        modality_keys: List of modality names\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_dict: Dict[str, BaseDataset],\n        config: DefaultConfig,\n    ):\n        \"\"\"\n        Initialize a StackixDataset instance.\n\n        Args:\n            dataset_dict: Dictionary mapping modality names to dataset objects\n            config: Configuration object\n\n        Raises:\n            ValueError: If the datasets dictionary is empty or if modality datasets have different numbers of samples\n            NotImplementedError: If the datasets have incompatible shapes for concatenation\n        \"\"\"\n        if not dataset_dict:\n            raise ValueError(\"dataset_dict cannot be empty\")\n\n        # Use first modality for base class initialization\n        first_modality_key = next(iter(dataset_dict.keys()))\n        first_modality = dataset_dict[first_modality_key]\n        try:\n            data = torch.cat(\n                [v.data for _, v in dataset_dict.items() if hasattr(v, \"data\")], dim=1\n            )\n        except Exception:\n            raise NotImplementedError(\n                \"Data modalities have different shapes, set requires_paired=True in config\"\n            )\n        super().__init__(\n            data=data,\n            sample_ids=first_modality.sample_ids,\n            config=config,\n            split_indices=first_modality.split_indices,\n            metadata=first_modality.metadata,\n            feature_ids=[\n                v.feature_ids\n                for v in dataset_dict.values()\n                if hasattr(v, \"feature_ids\")\n            ],\n        )\n\n        self.dataset_dict = dataset_dict\n        self.modality_keys = list(dataset_dict.keys())\n\n        # Ensure all datasets have the same number of samples\n        sample_counts = [len(dataset) for dataset in dataset_dict.values()]\n        if not all(count == sample_counts[0] for count in sample_counts):\n            raise ValueError(\n                \"All modality datasets must have the same number of samples\"\n            )\n\n    def __len__(self) -&gt; int:\n        \"\"\"Return the number of samples in the dataset.\"\"\"\n        return len(next(iter(self.dataset_dict.values())))\n\n    def __getitem__(\n        self, index: int\n    ) -&gt; Union[Tuple[torch.Tensor, Any], Dict[str, Tuple[torch.Tensor, Any]]]:\n        \"\"\"\n        Get a single sample and its label from the dataset.\n\n        Returns the data from the first modality to maintain compatibility\n        with the BaseDataset interface, while still supporting multi-modality\n        access through dataset_dict.\n        Args:\n            index: Index of the sample to retrieve\n\n        Returns:\n            Dictionary of (data tensor, label) pairs for each modality\n\n        \"\"\"\n        return {\n            k: self.dataset_dict[k].__getitem__(index) for k in self.dataset_dict.keys()\n        }\n\n    def get_modality_item(self, modality: str, index: int) -&gt; Tuple[torch.Tensor, Any]:\n        \"\"\"\n        Get a sample for a specific modality.\n        Args:\n            modality: The modality name to retrieve data from\n            index: Index of the sample to retrieve\n\n        Returns:\n            Tuple of (data tensor, label) for the specified modality and sample index\n\n        Raises:\n            KeyError: If the requested modality doesn't exist in the dataset\n        \"\"\"\n        if modality not in self.dataset_dict:\n            raise KeyError(f\"Modality '{modality}' not found in dataset\")\n\n        return self.dataset_dict[modality][index]\n</code></pre>"},{"location":"api/data/#autoencodix.data.StackixDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Get a single sample and its label from the dataset.</p> <p>Returns the data from the first modality to maintain compatibility with the BaseDataset interface, while still supporting multi-modality access through dataset_dict. Args:     index: Index of the sample to retrieve</p> <p>Returns:</p> Type Description <code>Union[Tuple[Tensor, Any], Dict[str, Tuple[Tensor, Any]]]</code> <p>Dictionary of (data tensor, label) pairs for each modality</p> Source code in <code>src/autoencodix/data/_stackix_dataset.py</code> <pre><code>def __getitem__(\n    self, index: int\n) -&gt; Union[Tuple[torch.Tensor, Any], Dict[str, Tuple[torch.Tensor, Any]]]:\n    \"\"\"\n    Get a single sample and its label from the dataset.\n\n    Returns the data from the first modality to maintain compatibility\n    with the BaseDataset interface, while still supporting multi-modality\n    access through dataset_dict.\n    Args:\n        index: Index of the sample to retrieve\n\n    Returns:\n        Dictionary of (data tensor, label) pairs for each modality\n\n    \"\"\"\n    return {\n        k: self.dataset_dict[k].__getitem__(index) for k in self.dataset_dict.keys()\n    }\n</code></pre>"},{"location":"api/data/#autoencodix.data.StackixDataset.__init__","title":"<code>__init__(dataset_dict, config)</code>","text":"<p>Initialize a StackixDataset instance.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <code>Dict[str, BaseDataset]</code> <p>Dictionary mapping modality names to dataset objects</p> required <code>config</code> <code>DefaultConfig</code> <p>Configuration object</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the datasets dictionary is empty or if modality datasets have different numbers of samples</p> <code>NotImplementedError</code> <p>If the datasets have incompatible shapes for concatenation</p> Source code in <code>src/autoencodix/data/_stackix_dataset.py</code> <pre><code>def __init__(\n    self,\n    dataset_dict: Dict[str, BaseDataset],\n    config: DefaultConfig,\n):\n    \"\"\"\n    Initialize a StackixDataset instance.\n\n    Args:\n        dataset_dict: Dictionary mapping modality names to dataset objects\n        config: Configuration object\n\n    Raises:\n        ValueError: If the datasets dictionary is empty or if modality datasets have different numbers of samples\n        NotImplementedError: If the datasets have incompatible shapes for concatenation\n    \"\"\"\n    if not dataset_dict:\n        raise ValueError(\"dataset_dict cannot be empty\")\n\n    # Use first modality for base class initialization\n    first_modality_key = next(iter(dataset_dict.keys()))\n    first_modality = dataset_dict[first_modality_key]\n    try:\n        data = torch.cat(\n            [v.data for _, v in dataset_dict.items() if hasattr(v, \"data\")], dim=1\n        )\n    except Exception:\n        raise NotImplementedError(\n            \"Data modalities have different shapes, set requires_paired=True in config\"\n        )\n    super().__init__(\n        data=data,\n        sample_ids=first_modality.sample_ids,\n        config=config,\n        split_indices=first_modality.split_indices,\n        metadata=first_modality.metadata,\n        feature_ids=[\n            v.feature_ids\n            for v in dataset_dict.values()\n            if hasattr(v, \"feature_ids\")\n        ],\n    )\n\n    self.dataset_dict = dataset_dict\n    self.modality_keys = list(dataset_dict.keys())\n\n    # Ensure all datasets have the same number of samples\n    sample_counts = [len(dataset) for dataset in dataset_dict.values()]\n    if not all(count == sample_counts[0] for count in sample_counts):\n        raise ValueError(\n            \"All modality datasets must have the same number of samples\"\n        )\n</code></pre>"},{"location":"api/data/#autoencodix.data.StackixDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of samples in the dataset.</p> Source code in <code>src/autoencodix/data/_stackix_dataset.py</code> <pre><code>def __len__(self) -&gt; int:\n    \"\"\"Return the number of samples in the dataset.\"\"\"\n    return len(next(iter(self.dataset_dict.values())))\n</code></pre>"},{"location":"api/data/#autoencodix.data.StackixDataset.get_modality_item","title":"<code>get_modality_item(modality, index)</code>","text":"<p>Get a sample for a specific modality. Args:     modality: The modality name to retrieve data from     index: Index of the sample to retrieve</p> <p>Returns:</p> Type Description <code>Tuple[Tensor, Any]</code> <p>Tuple of (data tensor, label) for the specified modality and sample index</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the requested modality doesn't exist in the dataset</p> Source code in <code>src/autoencodix/data/_stackix_dataset.py</code> <pre><code>def get_modality_item(self, modality: str, index: int) -&gt; Tuple[torch.Tensor, Any]:\n    \"\"\"\n    Get a sample for a specific modality.\n    Args:\n        modality: The modality name to retrieve data from\n        index: Index of the sample to retrieve\n\n    Returns:\n        Tuple of (data tensor, label) for the specified modality and sample index\n\n    Raises:\n        KeyError: If the requested modality doesn't exist in the dataset\n    \"\"\"\n    if modality not in self.dataset_dict:\n        raise KeyError(f\"Modality '{modality}' not found in dataset\")\n\n    return self.dataset_dict[modality][index]\n</code></pre>"},{"location":"api/data/#autoencodix.data.StackixPreprocessor","title":"<code>StackixPreprocessor</code>","text":"<p>               Bases: <code>BasePreprocessor</code></p> <p>Preprocessor for Stackix architecture, which handles multiple modalities separately.</p> <p>Unlike GeneralPreprocessor which combines all modalities, StackixPreprocessor keeps modalities separate for individual VAE training in the Stackix architecture.</p> <p>Attributes: config: Configuration parameters for preprocessing and model architecture _datapackage: Dictionary storing processed data splits _dataset_container:Container for processed datasets by split</p> Source code in <code>src/autoencodix/data/_stackix_preprocessor.py</code> <pre><code>class StackixPreprocessor(BasePreprocessor):\n    \"\"\"Preprocessor for Stackix architecture, which handles multiple modalities separately.\n\n    Unlike GeneralPreprocessor which combines all modalities, StackixPreprocessor\n    keeps modalities separate for individual VAE training in the Stackix architecture.\n\n    Attributes:\n    config: Configuration parameters for preprocessing and model architecture\n    _datapackage: Dictionary storing processed data splits\n    _dataset_container:Container for processed datasets by split\n    \"\"\"\n\n    def __init__(\n        self, config: DefaultConfig, ontologies: Optional[Union[Tuple, Dict]] = None\n    ) -&gt; None:\n        \"\"\"Initialize the StackixPreprocessor with the given configuration.\n        Args:\n            config: Configuration parameters for preprocessing\n        \"\"\"\n        super().__init__(config=config)\n        self._datapackage: Optional[Dict[str, Any]] = None\n        self._dataset_container: Optional[DatasetContainer] = None\n\n    def preprocess(\n        self, raw_user_data: Optional[DataPackage] = None, predict_new_data=False\n    ) -&gt; DatasetContainer:\n        \"\"\"Execute preprocessing steps for Stackix architecture.\n\n        Args\n        raw_user_data: Raw user data to preprocess, or None to use self._datapackage\n\n        Returns:\n            Container with MultiModalDataset for each split\n\n        Raises:\n            TypeError: If datapackage is None after preprocessing\n        \"\"\"\n        self._datapackage = self._general_preprocess(\n            raw_user_data, predict_new_data=predict_new_data\n        )\n        self._dataset_container = DatasetContainer()\n\n        for split in [\"train\", \"valid\", \"test\"]:\n            if (\n                split not in self._datapackage\n                or self._datapackage[split].get(\"data\") is None\n            ):\n                self._dataset_container[split] = None\n                continue\n            dataset_dict = self._build_dataset_dict(\n                datapackage=self._datapackage[split][\"data\"],\n                split_indices=self._datapackage[split][\"indices\"],\n            )\n            stackix_ds = MultiModalDataset(\n                datasets=dataset_dict,\n                config=self.config,\n            )\n            self._dataset_container[split] = stackix_ds\n        return self._dataset_container\n\n    def _extract_primary_data(self, modality_data: Any) -&gt; np.ndarray:\n        primary_data = modality_data.X\n        if issparse(primary_data):\n            primary_data = primary_data.toarray()\n        return primary_data\n\n    @no_type_check\n    def _combine_layers(\n        self, modality_name: str, modality_data: Any\n    ) -&gt; Tuple[np.ndarray, Dict[str, tuple[int]]]:\n        \"\"\"Combine layers from a modality and return the combined data and indices.\n\n        Args:\n            modality_name: Name of the modality\n            modality_data: Data for the modality\n\n        Returns:\n            Combined data and list of (layer_name, start_idx, end_idx) tuples\n        \"\"\"\n        layer_list: List[np.ndarray] = []\n        layer_indices: Dict[str, Tuple[int]] = {}\n\n        selected_layers: List[str] = self.config.data_config.data_info[\n            modality_name\n        ].selected_layers\n\n        start_idx = 0\n        for layer_name in selected_layers:\n            if layer_name == \"X\":\n                data = self._extract_primary_data(modality_data)\n                layer_list.append(data)\n                end_idx = start_idx + data.shape[1]\n                layer_indices[layer_name] = [start_idx, end_idx]  # type: ignore\n                start_idx += data.shape[1]\n                continue\n            elif layer_name in modality_data.layers:\n                layer_data = modality_data.layers[layer_name]\n                if issparse(layer_data):\n                    layer_data = layer_data.toarray()\n                layer_list.append(layer_data)\n                end_idx = start_idx + layer_data.shape[1]\n                layer_indices[layer_name] = [start_idx, end_idx]  # type: ignore\n                start_idx += layer_data.shape[1]\n\n        combined_data: np.ndarray = (\n            np.concatenate(layer_list, axis=1) if layer_list else np.array([])\n        )\n        return combined_data, layer_indices\n\n    def _build_dataset_dict(\n        self, datapackage: DataPackage, split_indices: np.ndarray\n    ) -&gt; Dict[str, NumericDataset]:\n        \"\"\"For each seperate entry in our datapackge we build a NumericDataset\n        and store it in a dictionary with the modality as key.\n\n        Args:\n            datapackage:DataPackage containing the data to be processed\n            split_indices: List of indices for splitting the data\n        Returns:\n            Dictionary mapping modality names to NumericDataset objects\n\n        \"\"\"\n        dataset_dict: Dict[str, NumericDataset] = {}\n        layer_id_dict: Dict[str, Dict[str, List]] = {}\n        for k, _ in datapackage:\n            attr_name, dict_key = k.split(\n                \".\"\n            )  # see DataPackage __iter__ method for why this makes sense\n            metadata = None\n            if datapackage.annotation is not None:  # prevents error in Single Cell case\n                # case where each numeric data has it's own annotation/metadata\n                metadata = datapackage.annotation.get(dict_key)\n                if metadata is None:\n                    # case where there is one \"paired\" metadata for all numeric data\n                    metadata = datapackage.annotation.get(\"paired\")\n                # case where we have the unpaired case, but we have one metadata that included all samples across all numeric data\n                if metadata is None:\n                    if not len(datapackage.annotation.keys()) == 1:\n                        raise ValueError(\n                            f\"annotation key needs to be either 'paired' match a key of the numeric data or only one key exists that holds all unpaired data, please adjust config, got: {datapackage.annotation.keys()}\"\n                        )\n                    metadata_key = next(iter(datapackage.annotation.keys()))\n                    metadata = datapackage.annotation.get(metadata_key)\n\n            if attr_name == \"multi_bulk\":\n                df = datapackage[attr_name][dict_key]\n                ds = NumericDataset(\n                    data=df.values,\n                    config=self.config,\n                    sample_ids=df.index,\n                    feature_ids=df.columns,\n                    metadata=metadata,\n                    split_indices=split_indices,\n                )\n                dataset_dict[dict_key] = ds\n            elif attr_name == \"multi_sc\":\n                mudata = datapackage[\"multi_sc\"][\"multi_sc\"]\n                if isinstance(mudata, ad.AnnData):\n                    raise TypeError(\n                        \"Expected a MuData object, but got an AnnData object.\"\n                    )\n\n                layer_list: List[Any] = []\n                print(\"building dataset_dict\")\n                for mod_name, mod_data in mudata.mod.items():\n                    layers, indices = self._combine_layers(\n                        modality_name=mod_name, modality_data=mod_data\n                    )\n                    layer_id_dict[mod_name] = indices\n                    layer_list.append(layers)\n                    mod_concat = np.concatenate(layer_list, axis=1)\n                    ds = NumericDataset(\n                        data=mod_concat,\n                        config=self.config,\n                        sample_ids=mudata.obs_names,\n                        feature_ids=mod_data.var_names * len(layer_list),\n                        metadata=mod_data.obs,\n                        split_indices=split_indices,\n                    )\n                    dataset_dict[mod_name] = ds\n            else:\n                continue\n        self._layer_indices = layer_id_dict\n        return dataset_dict\n\n    def format_reconstruction(\n        self, reconstruction: Any, result: Optional[Result] = None\n    ) -&gt; DataPackage:\n        \"\"\"Takes the reconstructed tensor and from which modality it comes and uses the dataset_dict\n        to obtain the format of the original datapackage, but instead of the .data attribute\n        we populate this attribute with the reconstructed tensor (as pd.DataFrame or MuData object)\n\n        Args:\n            reconstruction: The reconstructed tensor\n            result: Optional[Result] containing additional information\n        Returns:\n            DataPackage with reconstructed data in original format\n\n        \"\"\"\n\n        if result is None:\n            raise ValueError(\n                \"Result object is not provided. This is needed for the StackixPreprocessor.\"\n            )\n        reconstruction = result.sub_reconstructions\n        if not isinstance(reconstruction, dict):\n            raise TypeError(\n                f\"Expected value to be of type dict for Stackix, got {type(reconstruction)}.\"\n            )\n\n        if self.config.data_case == DataCase.MULTI_BULK:\n            return self._format_multi_bulk(reconstructions=reconstruction)\n\n        elif self.config.data_case == DataCase.MULTI_SINGLE_CELL:\n            return self._format_multi_sc(reconstructions=reconstruction)\n        else:\n            raise ValueError(\n                f\"Unsupported data_case {self.config.data_case} for StackixPreprocessor.\"\n            )\n\n    def _format_multi_bulk(\n        self, reconstructions: Dict[str, torch.Tensor]\n    ) -&gt; DataPackage:\n        multi_bulk_dict = {}\n        annotation_dict = {}\n        dp = DataPackage()\n        for name, reconstruction in reconstructions.items():\n            if not isinstance(reconstruction, torch.Tensor):\n                raise TypeError(\n                    f\"Expected value to be of type torch.Tensor, got {type(reconstruction)}.\"\n                )\n            if self._dataset_container is None:\n                raise ValueError(\"Dataset container is not initialized.\")\n            stackix_ds = self._dataset_container[\"test\"]\n            if stackix_ds is None:\n                raise ValueError(\"No dataset found for split: test\")\n            dataset_dict = stackix_ds.datasets\n            df = pd.DataFrame(\n                reconstruction.numpy(),\n                index=dataset_dict[name].sample_ids,\n                columns=dataset_dict[name].feature_ids,\n            )\n            multi_bulk_dict[name] = df\n            annotation_dict[name] = dataset_dict[name].metadata\n\n        dp[\"multi_bulk\"] = multi_bulk_dict\n        dp[\"annotation\"] = annotation_dict\n        return dp\n\n    def _format_multi_sc(self, reconstructions: Dict[str, torch.Tensor]) -&gt; DataPackage:\n        \"\"\"Formats reconstructed tensors back into a MuData object for single-cell data.\n\n        This uses the stored layer indices to accurately split the reconstructed tensor\n        back into the original layers.\n\n        Args:\n        reconstruction: Dictionary of reconstructed tensors for each modality\n\n        Returns:\n            DataPackage containing the reconstructed MuData object\n        \"\"\"\n        import mudata as md\n\n        dp = DataPackage()\n        modalities = {}\n\n        if self._dataset_container is None:\n            raise ValueError(\"Dataset container is not initialized.\")\n        if not hasattr(self, \"_layer_indices\"):\n            raise ValueError(\n                \"Layer indices not found. Make sure _build_dataset_dict was called.\"\n            )\n\n        stackix_ds = self._dataset_container[\"test\"]\n        if stackix_ds is None:\n            raise ValueError(\"No dataset found for split: test\")\n\n        dataset_dict = stackix_ds.datasets\n\n        # Process each modality in the reconstruction\n        for mod_name, recon_tensor in reconstructions.items():\n            if not isinstance(recon_tensor, torch.Tensor):\n                raise TypeError(\n                    f\"Expected value to be of type torch.Tensor, got {type(recon_tensor)}.\"\n                )\n            if mod_name not in dataset_dict:\n                raise ValueError(f\"Modality {mod_name} not found in dataset dictionary\")\n            original_dataset = dataset_dict[mod_name]\n\n            layer_indices = self._layer_indices[mod_name]\n\n            start_idx, end_idx = layer_indices[\"X\"]\n            x_data = recon_tensor.numpy()[:, start_idx:end_idx]\n\n            var_names = original_dataset.feature_ids\n\n            mod_data = ad.AnnData(\n                X=x_data,\n                obs=original_dataset.metadata,\n                var=pd.DataFrame(index=var_names[0 : x_data.shape[1]]),\n            )\n\n            # Add additional layers based on stored indices\n            for layer_name, ids in layer_indices.items():\n                if layer_name == \"X\":\n                    continue  # X is already set\n\n                layer_data = recon_tensor.numpy()[:, ids[0] : ids[1]]\n                mod_data.layers[layer_name] = layer_data\n\n            modalities[mod_name] = mod_data\n\n        # Create MuData object from all modalities\n        mdata = md.MuData(modalities)\n\n        # Create and return DataPackage\n        dp[\"multi_sc\"] = {\"multi_sc\": mdata}\n        return dp\n</code></pre>"},{"location":"api/data/#autoencodix.data.StackixPreprocessor.__init__","title":"<code>__init__(config, ontologies=None)</code>","text":"<p>Initialize the StackixPreprocessor with the given configuration. Args:     config: Configuration parameters for preprocessing</p> Source code in <code>src/autoencodix/data/_stackix_preprocessor.py</code> <pre><code>def __init__(\n    self, config: DefaultConfig, ontologies: Optional[Union[Tuple, Dict]] = None\n) -&gt; None:\n    \"\"\"Initialize the StackixPreprocessor with the given configuration.\n    Args:\n        config: Configuration parameters for preprocessing\n    \"\"\"\n    super().__init__(config=config)\n    self._datapackage: Optional[Dict[str, Any]] = None\n    self._dataset_container: Optional[DatasetContainer] = None\n</code></pre>"},{"location":"api/data/#autoencodix.data.StackixPreprocessor.format_reconstruction","title":"<code>format_reconstruction(reconstruction, result=None)</code>","text":"<p>Takes the reconstructed tensor and from which modality it comes and uses the dataset_dict to obtain the format of the original datapackage, but instead of the .data attribute we populate this attribute with the reconstructed tensor (as pd.DataFrame or MuData object)</p> <p>Parameters:</p> Name Type Description Default <code>reconstruction</code> <code>Any</code> <p>The reconstructed tensor</p> required <code>result</code> <code>Optional[Result]</code> <p>Optional[Result] containing additional information</p> <code>None</code> <p>Returns:     DataPackage with reconstructed data in original format</p> Source code in <code>src/autoencodix/data/_stackix_preprocessor.py</code> <pre><code>def format_reconstruction(\n    self, reconstruction: Any, result: Optional[Result] = None\n) -&gt; DataPackage:\n    \"\"\"Takes the reconstructed tensor and from which modality it comes and uses the dataset_dict\n    to obtain the format of the original datapackage, but instead of the .data attribute\n    we populate this attribute with the reconstructed tensor (as pd.DataFrame or MuData object)\n\n    Args:\n        reconstruction: The reconstructed tensor\n        result: Optional[Result] containing additional information\n    Returns:\n        DataPackage with reconstructed data in original format\n\n    \"\"\"\n\n    if result is None:\n        raise ValueError(\n            \"Result object is not provided. This is needed for the StackixPreprocessor.\"\n        )\n    reconstruction = result.sub_reconstructions\n    if not isinstance(reconstruction, dict):\n        raise TypeError(\n            f\"Expected value to be of type dict for Stackix, got {type(reconstruction)}.\"\n        )\n\n    if self.config.data_case == DataCase.MULTI_BULK:\n        return self._format_multi_bulk(reconstructions=reconstruction)\n\n    elif self.config.data_case == DataCase.MULTI_SINGLE_CELL:\n        return self._format_multi_sc(reconstructions=reconstruction)\n    else:\n        raise ValueError(\n            f\"Unsupported data_case {self.config.data_case} for StackixPreprocessor.\"\n        )\n</code></pre>"},{"location":"api/data/#autoencodix.data.StackixPreprocessor.preprocess","title":"<code>preprocess(raw_user_data=None, predict_new_data=False)</code>","text":"<p>Execute preprocessing steps for Stackix architecture.</p> <p>Args raw_user_data: Raw user data to preprocess, or None to use self._datapackage</p> <p>Returns:</p> Type Description <code>DatasetContainer</code> <p>Container with MultiModalDataset for each split</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If datapackage is None after preprocessing</p> Source code in <code>src/autoencodix/data/_stackix_preprocessor.py</code> <pre><code>def preprocess(\n    self, raw_user_data: Optional[DataPackage] = None, predict_new_data=False\n) -&gt; DatasetContainer:\n    \"\"\"Execute preprocessing steps for Stackix architecture.\n\n    Args\n    raw_user_data: Raw user data to preprocess, or None to use self._datapackage\n\n    Returns:\n        Container with MultiModalDataset for each split\n\n    Raises:\n        TypeError: If datapackage is None after preprocessing\n    \"\"\"\n    self._datapackage = self._general_preprocess(\n        raw_user_data, predict_new_data=predict_new_data\n    )\n    self._dataset_container = DatasetContainer()\n\n    for split in [\"train\", \"valid\", \"test\"]:\n        if (\n            split not in self._datapackage\n            or self._datapackage[split].get(\"data\") is None\n        ):\n            self._dataset_container[split] = None\n            continue\n        dataset_dict = self._build_dataset_dict(\n            datapackage=self._datapackage[split][\"data\"],\n            split_indices=self._datapackage[split][\"indices\"],\n        )\n        stackix_ds = MultiModalDataset(\n            datasets=dataset_dict,\n            config=self.config,\n        )\n        self._dataset_container[split] = stackix_ds\n    return self._dataset_container\n</code></pre>"},{"location":"api/data/#autoencodix.data.TensorAwareDataset","title":"<code>TensorAwareDataset</code>","text":"<p>               Bases: <code>BaseDataset</code></p> <p>Handles dtype mapping and tensor conversion logic.</p> Source code in <code>src/autoencodix/data/_numeric_dataset.py</code> <pre><code>class TensorAwareDataset(BaseDataset):\n    \"\"\"\n    Handles dtype mapping and tensor conversion logic.\n    \"\"\"\n\n    @staticmethod\n    def _to_tensor(\n        data: Union[torch.Tensor, np.ndarray, Any], dtype: torch.dtype\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Convert data to tensor with specified dtype.\n\n        Args:\n            data: Input data to convert\n            dtype: Desired data type\n\n        Returns:\n            Tensor with the specified dtype\n        \"\"\"\n        if isinstance(data, torch.Tensor):\n            return data.clone().detach().to(dtype)\n        else:\n            return torch.tensor(data, dtype=dtype)\n\n    @staticmethod\n    def _map_float_precision_to_dtype(float_precision: str) -&gt; torch.dtype:\n        \"\"\"\n        Map fabric precision types to torch tensor dtypes.\n\n        Args:\n            float_precision: Precision type (e.g., 'bf16-mixed', '16-mixed')\n\n        Returns:\n            Corresponding torch dtype\n        \"\"\"\n        precision_mapping = {\n            \"transformer-engine\": torch.float32,  # Default for transformer-engine\n            \"transformer-engine-float16\": torch.float16,\n            \"16-true\": torch.float16,\n            \"16-mixed\": torch.float16,\n            \"bf16-true\": torch.bfloat16,\n            \"bf16-mixed\": torch.bfloat16,\n            \"32-true\": torch.float32,\n            \"64-true\": torch.float64,\n            \"64\": torch.float64,\n            \"32\": torch.float32,\n            \"16\": torch.float16,\n            \"bf16\": torch.bfloat16,\n        }\n        # Default to torch.float32 if the precision is not recognized\n        return precision_mapping.get(float_precision, torch.float32)\n\n    def _to_df(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Convert the dataset to a pandas DataFrame.\n\n        Returns:\n            DataFrame representation of the dataset\n        \"\"\"\n        if isinstance(self.data, torch.Tensor):\n            return pd.DataFrame(\n                self.data.numpy(), columns=self.feature_ids, index=self.sample_ids\n            )\n        elif isinstance(self.data, list) and all(\n            isinstance(item, torch.Tensor) for item in self.data\n        ):\n            # Handle image modality\n            # Get the list of tensors\n            tensor_list = self.data\n\n            # Flatten each tensor and collect as rows\n            rows = [\n                (\n                    t.flatten().cpu().numpy()\n                    if isinstance(t, torch.Tensor)\n                    else t.flatten()\n                )\n                for t in tensor_list\n            ]\n\n            df_flat = pd.DataFrame(\n                rows,\n                index=self.sample_ids,\n                columns=[\"Pixel_\" + str(i) for i in range(len(rows[0]))],\n            )\n            return df_flat\n        else:\n            raise TypeError(\n                \"Data is not a torch.Tensor and cannot be converted to DataFrame.\"\n            )\n\n    def _get_target_dtype(self) -&gt; torch.dtype:\n        \"\"\"Get the target dtype based on config, with MPS compatibility check.\"\"\"\n        target_dtype = self._map_float_precision_to_dtype(self.config.float_precision)\n\n        # MPS doesn't support float64, so fallback to float32\n        if target_dtype == torch.float64 and self.config.device == \"mps\":\n            print(\"Warning: MPS doesn't support float64, using float32 instead\")\n            target_dtype = torch.float32\n\n        return target_dtype\n</code></pre>"},{"location":"api/data/#autoencodix.data.XModalPreprocessor","title":"<code>XModalPreprocessor</code>","text":"<p>               Bases: <code>GeneralPreprocessor</code></p> <p>Preprocessor for cross-modal data, handling multiple data types and their transformations.</p> <p>Attributes:</p> Name Type Description <code>data_config</code> <p>Configuration specific to data handling.</p> <code>dataset_dicts</code> <p>Dictionary holding datasets for different splits (train, test, valid).</p> Source code in <code>src/autoencodix/data/_xmodal_preprocessor.py</code> <pre><code>class XModalPreprocessor(GeneralPreprocessor):\n    \"\"\"Preprocessor for cross-modal data, handling multiple data types and their transformations.\n\n\n    Attributes:\n        data_config: Configuration specific to data handling.\n        dataset_dicts: Dictionary holding datasets for different splits (train, test, valid).\n    \"\"\"\n\n    def __init__(\n        self, config: DefaultConfig, ontologies: Optional[Union[Tuple, Dict]] = None\n    ):\n        \"\"\"Initializes the XModalPreprocessor\n        Args:\n            config: Configuration object for the preprocessor.\n            ontologies: Optional ontologies for data processing.\n        \"\"\"\n        super().__init__(config=config, ontologies=ontologies)\n        self.data_config = config.data_config\n\n    def preprocess(\n        self,\n        raw_user_data: Optional[DataPackage] = None,\n        predict_new_data: bool = False,\n    ) -&gt; DatasetContainer:\n        \"\"\"Preprocess the data according to the configuration.\n        Args:\n            raw_user_data: Optional raw data provided by the user.\n            predict_new_data: Flag indicating if new data is being predicted.\n        \"\"\"\n        self.dataset_dicts = self._general_preprocess(\n            raw_user_data=raw_user_data, predict_new_data=predict_new_data\n        )\n        datasets = {}\n        for split in [\"train\", \"test\", \"valid\"]:\n            cur_split = self.dataset_dicts.get(split)\n            if cur_split is None:\n                print(f\"split is None: {split}\")\n                continue\n            cur_data = cur_split.get(\"data\")\n            if not isinstance(cur_data, DataPackage):\n                raise TypeError(\n                    f\"expected type of cur_data to be DataPackage, got {type(cur_data)}\"\n                )\n            cur_indices = cur_split.get(\"indices\")\n            datasets[split] = MultiModalDataset(\n                datasets=self._process_dp(dp=cur_data, indices=cur_indices),\n                config=self.config,\n            )\n\n        for k, v in self.dataset_dicts.items():\n            print(f\"key: {k}, type: {type(v)}\")\n\n        return DatasetContainer(\n            train=datasets[\"train\"], test=datasets[\"test\"], valid=datasets[\"valid\"]\n        )\n\n    def format_reconstruction(self, reconstruction, result=None):\n        pass\n\n    def _process_dp(self, dp: DataPackage, indices: Dict[str, Any]):\n        \"\"\"Processes a DataPackage into a dictionary of BaseDataset objects.\n\n        Args:\n            dp: The DataPackage to process.\n            indices: The indices for splitting the data.\n        Returns:\n            A dictionary mapping modality names to BaseDataset objects.\n        \"\"\"\n\n        dataset_dict: Dict[str, BaseDataset] = {}\n        for k, v in dp:\n            dp_key, sub_key = k.split(\".\")\n            data = v\n            metadata = None\n            if dp.annotation is not None:  # prevents error in SingleCell case\n                metadata = dp.annotation.get(sub_key)\n                if metadata is None:\n                    metadata = dp.annotation.get(\"paired\")\n                # case where we have the unpaired case, but we have one metadata that included all samples across all numeric data\n                if metadata is None:\n                    if not len(dp.annotation.keys()) == 1:\n                        raise ValueError(\n                            f\"annotation key needs to be either 'paired' match a key of the numeric data or only one key exists that holds all unpaired data, please adjust config, got: {dp.annotation.keys()}\"\n                        )\n                    metadata_key = next(iter(dp.annotation.keys()))\n                    metadata = dp.annotation.get(metadata_key)\n\n            if dp_key == \"multi_bulk\":\n                if not isinstance(data, pd.DataFrame):\n                    raise ValueError(\n                        f\"Expected data for multi_bulk: {k}, {v} to be pd.DataFrame, got {type(data)}\"\n                    )\n                if metadata is None:\n                    raise ValueError(\"metadata cannot be None\")\n                metadata_num = metadata.loc[\n                    data.index\n                ]  # needed when we have only one annotation df containing metadata for all modalities\n                dataset_dict[k] = NumericDataset(\n                    data=data.values,\n                    config=self.config,\n                    sample_ids=data.index,\n                    feature_ids=data.columns,\n                    split_indices=indices,\n                    metadata=metadata_num,\n                )\n            elif dp_key == \"img\":\n                if not isinstance(data, list):\n                    raise ValueError()\n                if not isinstance(data[0], ImgData):\n                    raise ValueError()\n                dataset_dict[k] = ImageDataset(\n                    data=data,\n                    config=self.config,\n                    split_indices=indices,\n                    metadata=metadata,\n                )\n            elif dp_key == \"multi_sc\":\n                if not isinstance(data, md.MuData):\n                    raise ValueError()\n                for mod_key, mod_data in data.mod.items():\n                    selected_layers = self.config.data_config.data_info[\n                        mod_key\n                    ].selected_layers\n                    if not selected_layers[0] == \"X\" and len(selected_layers) != 1:\n                        raise NotImplementedError(\n                            \"Xmodalix works only with X layer of single cell data as of now\"\n                        )\n                    dataset_dict[k] = NumericDataset(\n                        data=mod_data.X,\n                        config=self.config,\n                        sample_ids=mod_data.obs_names,\n                        feature_ids=mod_data.var_names,\n                        split_indices=indices,\n                        metadata=mod_data.obs,\n                    )\n\n            elif dp_key == \"annotation\":\n                pass\n\n            else:\n                raise NotImplementedError(\n                    f\"Got datapackage attribute: {k}, probably you have added an attribute to the Datapackage class without adjusting this method. Only supports: ['multi_bulk', 'multi_sc', 'img' and 'annotation']\"\n                )\n        return dataset_dict\n</code></pre>"},{"location":"api/data/#autoencodix.data.XModalPreprocessor.__init__","title":"<code>__init__(config, ontologies=None)</code>","text":"<p>Initializes the XModalPreprocessor Args:     config: Configuration object for the preprocessor.     ontologies: Optional ontologies for data processing.</p> Source code in <code>src/autoencodix/data/_xmodal_preprocessor.py</code> <pre><code>def __init__(\n    self, config: DefaultConfig, ontologies: Optional[Union[Tuple, Dict]] = None\n):\n    \"\"\"Initializes the XModalPreprocessor\n    Args:\n        config: Configuration object for the preprocessor.\n        ontologies: Optional ontologies for data processing.\n    \"\"\"\n    super().__init__(config=config, ontologies=ontologies)\n    self.data_config = config.data_config\n</code></pre>"},{"location":"api/data/#autoencodix.data.XModalPreprocessor.preprocess","title":"<code>preprocess(raw_user_data=None, predict_new_data=False)</code>","text":"<p>Preprocess the data according to the configuration. Args:     raw_user_data: Optional raw data provided by the user.     predict_new_data: Flag indicating if new data is being predicted.</p> Source code in <code>src/autoencodix/data/_xmodal_preprocessor.py</code> <pre><code>def preprocess(\n    self,\n    raw_user_data: Optional[DataPackage] = None,\n    predict_new_data: bool = False,\n) -&gt; DatasetContainer:\n    \"\"\"Preprocess the data according to the configuration.\n    Args:\n        raw_user_data: Optional raw data provided by the user.\n        predict_new_data: Flag indicating if new data is being predicted.\n    \"\"\"\n    self.dataset_dicts = self._general_preprocess(\n        raw_user_data=raw_user_data, predict_new_data=predict_new_data\n    )\n    datasets = {}\n    for split in [\"train\", \"test\", \"valid\"]:\n        cur_split = self.dataset_dicts.get(split)\n        if cur_split is None:\n            print(f\"split is None: {split}\")\n            continue\n        cur_data = cur_split.get(\"data\")\n        if not isinstance(cur_data, DataPackage):\n            raise TypeError(\n                f\"expected type of cur_data to be DataPackage, got {type(cur_data)}\"\n            )\n        cur_indices = cur_split.get(\"indices\")\n        datasets[split] = MultiModalDataset(\n            datasets=self._process_dp(dp=cur_data, indices=cur_indices),\n            config=self.config,\n        )\n\n    for k, v in self.dataset_dicts.items():\n        print(f\"key: {k}, type: {type(v)}\")\n\n    return DatasetContainer(\n        train=datasets[\"train\"], test=datasets[\"test\"], valid=datasets[\"valid\"]\n    )\n</code></pre>"},{"location":"api/disentanglix/","title":"Disentanglix Module","text":""},{"location":"api/disentanglix/#autoencodix.disentanglix.Disentanglix","title":"<code>Disentanglix</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>Disentanglix-specific version of the BasePipeline.</p> <p>This class extends BasePipeline. See the parent class for a full list of attributes and methods.</p> Additional Attributes <p>_default_config: Is set to DisentanglixConfig here.</p> Source code in <code>src/autoencodix/disentanglix.py</code> <pre><code>class Disentanglix(BasePipeline):\n    \"\"\"Disentanglix-specific version of the BasePipeline.\n\n    This class extends BasePipeline. See the parent class for a full list\n    of attributes and methods.\n\n    Additional Attributes:\n        _default_config: Is set to DisentanglixConfig here.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Optional[Union[DataPackage, DatasetContainer]] = None,\n        trainer_type: Type[BaseTrainer] = GeneralTrainer,\n        dataset_type: Type[BaseDataset] = NumericDataset,\n        model_type: Type[BaseAutoencoder] = VarixArchitecture,\n        loss_type: Type[BaseLoss] = DisentanglixLoss,\n        preprocessor_type: Type[BasePreprocessor] = GeneralPreprocessor,\n        visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n        evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n        result: Optional[Result] = None,\n        datasplitter_type: Type[DataSplitter] = DataSplitter,\n        custom_splits: Optional[Dict[str, np.ndarray]] = None,\n        config: Optional[DefaultConfig] = None,\n    ) -&gt; None:\n        \"\"\"Initialize Varix pipeline with customizable components.\n\n        See the init method of parent class for a full list of Args.\n        \"\"\"\n        self._default_config = DisentanglixConfig()\n        super().__init__(\n            data=data,\n            dataset_type=dataset_type,\n            trainer_type=trainer_type,\n            model_type=model_type,\n            loss_type=loss_type,\n            preprocessor_type=preprocessor_type,\n            visualizer=visualizer,\n            evaluator=evaluator,\n            result=result,\n            datasplitter_type=datasplitter_type,\n            config=config,\n            custom_split=custom_splits,\n        )\n</code></pre>"},{"location":"api/disentanglix/#autoencodix.disentanglix.Disentanglix.__init__","title":"<code>__init__(data=None, trainer_type=GeneralTrainer, dataset_type=NumericDataset, model_type=VarixArchitecture, loss_type=DisentanglixLoss, preprocessor_type=GeneralPreprocessor, visualizer=GeneralVisualizer, evaluator=GeneralEvaluator, result=None, datasplitter_type=DataSplitter, custom_splits=None, config=None)</code>","text":"<p>Initialize Varix pipeline with customizable components.</p> <p>See the init method of parent class for a full list of Args.</p> Source code in <code>src/autoencodix/disentanglix.py</code> <pre><code>def __init__(\n    self,\n    data: Optional[Union[DataPackage, DatasetContainer]] = None,\n    trainer_type: Type[BaseTrainer] = GeneralTrainer,\n    dataset_type: Type[BaseDataset] = NumericDataset,\n    model_type: Type[BaseAutoencoder] = VarixArchitecture,\n    loss_type: Type[BaseLoss] = DisentanglixLoss,\n    preprocessor_type: Type[BasePreprocessor] = GeneralPreprocessor,\n    visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n    evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n    result: Optional[Result] = None,\n    datasplitter_type: Type[DataSplitter] = DataSplitter,\n    custom_splits: Optional[Dict[str, np.ndarray]] = None,\n    config: Optional[DefaultConfig] = None,\n) -&gt; None:\n    \"\"\"Initialize Varix pipeline with customizable components.\n\n    See the init method of parent class for a full list of Args.\n    \"\"\"\n    self._default_config = DisentanglixConfig()\n    super().__init__(\n        data=data,\n        dataset_type=dataset_type,\n        trainer_type=trainer_type,\n        model_type=model_type,\n        loss_type=loss_type,\n        preprocessor_type=preprocessor_type,\n        visualizer=visualizer,\n        evaluator=evaluator,\n        result=result,\n        datasplitter_type=datasplitter_type,\n        config=config,\n        custom_split=custom_splits,\n    )\n</code></pre>"},{"location":"api/evaluate/","title":"Evaluate Module","text":""},{"location":"api/evaluate/#autoencodix.evaluate.GeneralEvaluator","title":"<code>GeneralEvaluator</code>","text":"<p>               Bases: <code>BaseEvaluator</code></p> Source code in <code>src/autoencodix/evaluate/_general_evaluator.py</code> <pre><code>class GeneralEvaluator(BaseEvaluator):\n    def __init__(self):\n        # super().__init__()\n        pass\n\n    @no_type_check\n    def evaluate(\n        self,\n        datasets: DatasetContainer,\n        result: Result,\n        ml_model_class: ClassifierMixin = linear_model.LogisticRegression(\n            max_iter=1000\n        ),  # Default is sklearn LogisticRegression\n        ml_model_regression: RegressorMixin = linear_model.LinearRegression(),  # Default is sklearn LinearRegression\n        params: Union[\n            list, str\n        ] = \"all\",  # No default? ... or all params in annotation?\n        metric_class: str = \"roc_auc_ovo\",  # Default is 'roc_auc_ovo' via https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-string-names\n        metric_regression: str = \"r2\",  # Default is 'r2'\n        reference_methods: list = [],  # Default [], Options are \"PCA\", \"UMAP\", \"TSNE\", \"RandomFeature\"\n        split_type: str = \"use-split\",  # Default is \"use-split\", other options: \"CV-5\", ... \"LOOCV\"?\n        n_downsample: Union[\n            int, None\n        ] = 10000,  # Default is 10000, if provided downsample to this number of samples for faster evaluation. Set to None to disable downsampling.\n    ) -&gt; Result:\n        \"\"\"Evaluates the performance of machine learning models on various feature representations and clinical parameters.\n\n        This method performs classification or regression tasks using specified machine learning models on different feature sets (e.g., latent space, PCA, UMAP, TSNE, RandomFeature) and clinical annotation parameters. It supports multiple evaluation strategies, including pre-defined train/valid/test splits, k-fold cross-validation, and leave-one-out cross-validation. The results are aggregated and stored in the provided `result` object.\n        - Samples with missing annotation values for a given parameter are excluded from the corresponding evaluation.\n        - For \"RandomFeature\", five random feature sets are evaluated.\n        - The method appends results to any existing `embedding_evaluation` in the result object.\n\n        Args:\n            datasets: A DatasetContainer containing train, valid, and test datasets, each with `sample_ids` and `metadata` (either a DataFrame or a dictionary with a 'paired' key for clinical annotations).\n            result: An Result object to store the evaluation results. Should have an `embedding_evaluation` attribute which updated (typically a DataFrame).\n            ml_model_class: The scikit-learn classifier to use for classification tasks (default: `sklearn.linear_model.LogisticRegression()`).\n            ml_model_regression: The scikit-learn regressor to use for regression tasks (default: `sklearn.linear_model.LinearRegression()`).\n            params:List of clinical annotation columns to evaluate, or \"all\" to use all columns (default: \"all\").\n            metric_class: Scoring metric for classification tasks (default: \"roc_auc_ovo\").\n            metric_regression: Scoring metric for regression tasks (default: \"r2\").\n            reference_methods:List of feature representations to evaluate (e.g., \"PCA\", \"UMAP\", \"TSNE\", \"RandomFeature\"). \"Latent\" is always included (default: []).\n            split_type: which split to use\n                use-split\" for pre-defined splits, \"CV-N\" for N-fold cross-validation, or \"LOOCV\" for leave-one-out cross-validation (default: \"use-split\").\n            n_downsample: If provided, downsample the data to this number of samples for faster evaluation. Default is 10000. Set to None to disable downsampling.\n        Returns:\n            The updated result object with evaluation results stored in `embedding_evaluation`.\n        Raises\n            ValueError: If required annotation data is missing or improperly formatted, or if an unsupported split type is specified.\n\n        \"\"\"\n\n        already_warned = False\n\n        df_results = pd.DataFrame()\n\n        reference_methods.append(\"Latent\")\n\n        reference_methods = self._expand_reference_methods(\n            reference_methods=reference_methods, result=result\n        )\n\n        ## Overwrite original datasets with new_datasets if available after predict with other data\n        if datasets is None:\n            datasets = DatasetContainer()\n\n        if bool(result.new_datasets.test):\n            datasets.test = result.new_datasets.test\n\n        if not bool(datasets.train or datasets.valid or datasets.test):\n            raise ValueError(\n                \"No datasets found in result object. Please run predict with new data or save/load with all datasets by using save_all=True.\"\n            )\n        elif split_type == \"use-split\" and not bool(datasets.train):\n            warnings.warn(\n                \"Warning: No train split found in result datasets for 'use-split' evaluation. ML model cannot be trained without a train split. Switch to cross-validation (CV-5) instead.\"\n            )\n            split_type = \"CV-5\"\n\n        for task in reference_methods:\n            print(f\"Perform ML task with feature df: {task}\")\n\n            # clin_data = self._get_clin_data(datasets)\n            clin_data = BaseVisualizer._collect_all_metadata(result=result)\n\n            if split_type == \"use-split\":\n                # Pandas dataframe with sample_ids and split information\n                sample_split = pd.DataFrame(columns=[\"SAMPLE_ID\", \"SPLIT\"])\n\n                if datasets.train is not None:\n                    if hasattr(datasets.train, \"paired_sample_ids\"):\n                        if datasets.train.paired_sample_ids is not None:\n                            sample_ids = datasets.train.paired_sample_ids\n                    else:\n                        sample_ids = datasets.train.sample_ids\n                    sample_split_temp = dict(\n                        sample_split,\n                        **{\n                            \"SAMPLE_ID\": sample_ids,\n                            \"SPLIT\": [\"train\"] * len(sample_ids),\n                        },\n                    )\n                    sample_split = pd.concat(\n                        [sample_split, pd.DataFrame(sample_split_temp)],\n                        axis=0,\n                        ignore_index=True,\n                    )\n                # else:\n                #     raise ValueError(\n                #         \"No training data found. Please provide a valid training dataset.\"\n                #     )\n                if datasets.valid is not None:\n                    if hasattr(datasets.valid, \"paired_sample_ids\"):\n                        if datasets.valid.paired_sample_ids is not None:\n                            sample_ids = datasets.valid.paired_sample_ids\n                    else:\n                        sample_ids = datasets.valid.sample_ids\n                    sample_split_temp = dict(\n                        sample_split,\n                        **{\n                            \"SAMPLE_ID\": sample_ids,\n                            \"SPLIT\": [\"valid\"] * len(sample_ids),\n                        },\n                    )\n                    sample_split = pd.concat(\n                        [sample_split, pd.DataFrame(sample_split_temp)],\n                        axis=0,\n                        ignore_index=True,\n                    )\n                if datasets.test is not None:\n                    if hasattr(datasets.test, \"paired_sample_ids\"):\n                        if datasets.test.paired_sample_ids is not None:\n                            sample_ids = datasets.test.paired_sample_ids\n                    else:\n                        sample_ids = datasets.test.sample_ids\n                    sample_split_temp = dict(\n                        sample_split,\n                        **{\n                            \"SAMPLE_ID\": sample_ids,\n                            \"SPLIT\": [\"test\"] * len(sample_ids),\n                        },\n                    )\n                    sample_split = pd.concat(\n                        [sample_split, pd.DataFrame(sample_split_temp)],\n                        axis=0,\n                        ignore_index=True,\n                    )\n\n                sample_split = sample_split.set_index(\"SAMPLE_ID\", drop=False)\n\n            ## df -&gt; task\n            subtask = [task]\n            if \"RandomFeature\" in task:\n                subtask = [task + \"_R\" + str(x) for x in range(1, 6)]\n            for sub in subtask:\n                print(sub)\n                # if is_modalix:\n                #     modality = task.split(\"_$_\")[1]\n                #     task_xmodal = task.split(\"_$_\")[0]\n\n                #     df = self._load_input_for_ml_xmodal(task_xmodal, datasets, result, modality=modality)\n                # else:\n                df = self._load_input_for_ml(task, datasets, result)\n\n                if params == \"all\":\n                    params = clin_data.columns.tolist()\n\n                for task_param in params:\n                    if \"Latent\" in task:\n                        print(f\"Perform ML task for target parameter: {task_param}\")\n                    ## Check if classification or regression task\n                    ml_type = self._get_ml_type(clin_data, task_param)\n\n                    if pd.isna(clin_data[task_param]).sum() &gt; 0:\n                        # if pd.isna(clin_data[task_param]).values.any():\n                        if not already_warned:\n                            print(\n                                \"There are NA values in the annotation file. Samples with missing data will be removed for ML task evaluation.\"\n                            )\n                        already_warned = True\n                        # logger.warning(clin_data.loc[pd.isna(clin_data[task_param]), task_param])\n\n                        samples_nonna = clin_data.loc[\n                            pd.notna(clin_data[task_param]), task_param\n                        ].index\n                        # print(df)\n                        df = df.loc[samples_nonna.intersection(df.index), :]\n                        if split_type == \"use-split\":\n                            sample_split = sample_split.loc[\n                                samples_nonna.intersection(sample_split.index), :\n                            ]\n                        # print(sample_split)\n\n                    if n_downsample is not None:\n                        if df.shape[0] &gt; n_downsample:\n                            sample_idx = np.random.choice(\n                                df.shape[0], n_downsample, replace=False\n                            )\n                            df = df.iloc[sample_idx]\n                            if split_type == \"use-split\":\n                                sample_split = sample_split.loc[df.index, :]\n\n                    if ml_type == \"classification\":\n                        metric = metric_class\n                        sklearn_ml = ml_model_class\n\n                    if ml_type == \"regression\":\n                        metric = metric_regression\n                        sklearn_ml = ml_model_regression\n\n                    if split_type == \"use-split\":\n                        # print(\"Sample Split:\")\n                        # print(sample_split)\n                        # print(\"Latent:\")\n                        # print(df)\n                        results = self._single_ml_presplit(\n                            sample_split=sample_split,\n                            df=df,\n                            clin_data=clin_data,\n                            task_param=task_param,\n                            sklearn_ml=sklearn_ml,\n                            metric=metric,\n                            ml_type=ml_type,\n                        )\n                    elif split_type.startswith(\"CV-\"):\n                        cv_folds = int(split_type.split(\"-\")[1])\n\n                        results = self._single_ml(\n                            df=df,\n                            clin_data=clin_data,\n                            task_param=task_param,\n                            sklearn_ml=sklearn_ml,\n                            metric=metric,\n                            cv_folds=cv_folds,\n                        )\n                    elif split_type == \"LOOCV\":\n                        # Leave One Out Cross Validation\n                        results = self._single_ml(\n                            df=df,\n                            clin_data=clin_data,\n                            task_param=task_param,\n                            sklearn_ml=sklearn_ml,\n                            metric=metric,\n                            cv_folds=len(df),\n                        )\n                    else:\n                        raise ValueError(\n                            f\"Your split type {split_type} is not supported. Please use 'use-split', 'CV-5', 'LOOCV' or 'CV-N'.\"\n                        )\n                    results = self._enrich_results(\n                        results=results,\n                        sklearn_ml=sklearn_ml,\n                        ml_type=ml_type,\n                        task=task,\n                        sub=sub,\n                    )\n\n                    df_results = pd.concat([df_results, results])\n\n        ## Check if embedding_evaluation is empty\n        if (\n            hasattr(result, \"embedding_evaluation\")\n            and len(result.embedding_evaluation) == 0\n        ):\n            result.embedding_evaluation = df_results\n        else:\n            # merge with existing results\n            result.embedding_evaluation = pd.concat(\n                [result.embedding_evaluation, df_results], axis=0\n            )\n\n        return result\n\n    @staticmethod\n    def _single_ml(\n        df: pd.DataFrame,\n        clin_data: pd.DataFrame,\n        task_param: str,\n        sklearn_ml: Union[ClassifierMixin, RegressorMixin],\n        metric: str,\n        cv_folds: int = 5,\n    ):\n        \"\"\"Function learns on the given data frame df and label data the provided sklearn model.\n\n        Cross validation is performed according to the config and scores are returned as output as specified by metrics\n\n        Args:\n            df: Dataframe with input data\n            clin_data: Dataframe with label data\n            task_param: Column name with label data\n            sklearn_ml: Sklearn ML module specifying the ML algorithm\n            metric: string specifying the metric to be calculated by cross validation\n            cv_folds:\n        Returns:\n            score_df: data frame containing metrics (scores) for all CV runs (long format)\n\n        \"\"\"\n\n        # X -&gt; df\n        # Y -&gt; task_param\n        y: Union[pd.Series, pd.DataFrame] = clin_data.loc[df.index, task_param]\n        score_df = dict()\n\n        ## Cross Validation\n        if len(y.unique()) &gt; 1:  # ty: ignore\n            scores = cross_validate(\n                sklearn_ml, df, y, cv=cv_folds, scoring=metric, return_train_score=True\n            )\n\n            # Output\n\n            # Output Format\n            # CV_RUN | SCORE_SPLIT | TASK_PARAM | METRIC | VALUE\n\n            score_df[\"cv_run\"] = list()\n            score_df[\"score_split\"] = list()\n            score_df[\"CLINIC_PARAM\"] = list()\n            score_df[\"metric\"] = list()\n            score_df[\"value\"] = list()\n\n            cv_runs = [\"CV_\" + str(x) for x in range(1, cv_folds + 1)]\n            task_param_cv = [task_param for x in range(1, cv_folds + 1)]\n\n            for m in scores:\n                if m.split(\"_\")[0] == \"test\" or m.split(\"_\")[0] == \"train\":\n                    split_cv = [m.split(\"_\")[0] for x in range(1, cv_folds + 1)]\n                    metric_cv = [metric for x in range(1, cv_folds + 1)]\n\n                    score_df[\"cv_run\"].extend(cv_runs)\n                    score_df[\"score_split\"].extend(split_cv)\n                    score_df[\"CLINIC_PARAM\"].extend(task_param_cv)\n                    score_df[\"metric\"].extend(metric_cv)\n                    score_df[\"value\"].extend(scores[m])\n\n        return pd.DataFrame(score_df)\n\n    def _enrich_results(\n        self,\n        results: pd.DataFrame,\n        sklearn_ml: Union[ClassifierMixin, RegressorMixin],\n        ml_type: str,\n        task: str,\n        sub: str,\n    ) -&gt; pd.DataFrame:\n        res_ml_alg = [str(sklearn_ml) for x in range(0, results.shape[0])]\n        res_ml_type = [ml_type for x in range(0, results.shape[0])]\n        res_ml_task = [task for x in range(0, results.shape[0])]\n        res_ml_subtask = [sub for x in range(0, results.shape[0])]\n\n        results[\"ML_ALG\"] = res_ml_alg\n        results[\"ML_TYPE\"] = res_ml_type\n        # if is_modalix:\n        #     results[\"MODALITY\"] = [modality for x in range(0, results.shape[0])]\n        #     results[\"ML_TASK\"] = [task_xmodal for x in range(0, results.shape[0])]\n        # else:\n        results[\"ML_TASK\"] = res_ml_task\n        results[\"ML_SUBTASK\"] = res_ml_subtask\n\n        return results\n\n    @staticmethod\n    def _single_ml_presplit(\n        sample_split: pd.DataFrame,\n        df: pd.DataFrame,\n        clin_data: pd.DataFrame,\n        task_param: str,\n        sklearn_ml: Union[ClassifierMixin, RegressorMixin],\n        metric: str,\n        ml_type: str,\n    ):\n        \"\"\"Trains the provided sklearn model on the training split and evaluates it on train, valid, and test splits using the specified metric.\n\n        Args:\n            sample_split: DataFrame with sample IDs and their corresponding split (\"train\", \"valid\", \"test\").\n            df: DataFrame with input features, indexed by sample IDs.\n            clin_data: DataFrame with label/annotation data, indexed by sample IDs.\n            task_param: Column name in clin_data specifying the target variable.\n            sklearn_ml: Instantiated sklearn model to use for training and evaluation.\n            metric: Scoring metric compatible with sklearn's get_scorer.\n            ml_type: Type of machine learning task (\"classification\" or \"regression\").\n\n        Returns:\n            DataFrame containing evaluation scores for each split (train, valid, test) and the specified metric.\n\n        Raises\n            ValueError: If the provided metric is not supported by sklearn.\n        \"\"\"\n        split_list = [\"train\", \"valid\", \"test\"]\n\n        score_df = dict()\n        score_df[\"score_split\"] = list()\n        score_df[\"CLINIC_PARAM\"] = list()\n        score_df[\"metric\"] = list()\n        score_df[\"value\"] = list()\n\n        X_train = df.loc[\n            sample_split.loc[sample_split.SPLIT == \"train\", \"SAMPLE_ID\"], :\n        ]\n        train_samples = [s for s in X_train.index]\n        Y_train = clin_data.loc[train_samples, task_param]\n        # train model once on training data\n        if len(Y_train.unique()) &gt; 1:  # ty: ignore\n            sklearn_ml.fit(X_train, Y_train)  # ty: ignore\n\n            # eval on all splits\n            for split in split_list:\n                X = df.loc[\n                    sample_split.loc[sample_split.SPLIT == split, \"SAMPLE_ID\"], :\n                ]\n                if X.shape[0] == 0:\n                    # No samples in this split, skip\n                    continue\n                samples = [s for s in X.index]\n                Y = clin_data.loc[samples, task_param]\n\n                # Performace on train, valid and test data split\n\n                score_df[\"score_split\"].append(split)\n                score_df[\"CLINIC_PARAM\"].append(task_param)\n                score_df[\"metric\"].append(metric)\n                sklearn_scorer = get_scorer(metric)\n\n                if sklearn_scorer is None:\n                    raise ValueError(\n                        f\"Your metric {metric} is not supported by sklearn. Please use a valid metric.\"\n                    )\n\n                if ml_type == \"classification\":\n                    # Check that Y has only classes which are present in Y_train\n                    if (\n                        len(\n                            set(Y.unique()).difference(  # ty: ignore\n                                set(Y_train.unique())  # ty: ignore\n                            )  # ty: ignore\n                        )  # ty: ignore\n                        &gt; 0  # ty: ignore\n                    ):  # ty: ignore\n                        print(\n                            f\"Classes in split {split} are not present in training data\"\n                        )\n                        # Adjust Y to only contain classes present in Y_train\n                        Y = Y[Y.isin(Y_train.unique())]  # ty: ignore\n                        # Adjust X as well\n                        X = X.loc[Y.index, :]\n\n                if ml_type == \"classification\":\n                    score_temp = sklearn_scorer(\n                        sklearn_ml, X, Y, labels=np.sort(Y_train.unique())\n                    )\n                elif ml_type == \"regression\":\n                    score_temp = sklearn_scorer(sklearn_ml, X, Y)\n                else:\n                    raise ValueError(\n                        f\"Your ML type {ml_type} is not supported. Please use 'classification' or 'regression'.\"\n                    )\n                score_df[\"value\"].append(score_temp)\n        else:\n            ## Warning that there is only one class in the training data\n            warnings.warn(\n                f\"Warning: There is only one class in the training data for task parameter {task_param}. Skipping evaluation for this task.\"\n            )\n\n        return pd.DataFrame(score_df)\n\n    @staticmethod\n    def _get_ml_type(clin_data: pd.DataFrame, task_param: str) -&gt; str:\n        \"\"\"Determines the machine learning task type (classification or regression) based on the data type of a specified column in clinical data.\n\n        Args:\n            clin_data: The clinical data as a pandas DataFrame.\n            task_param: The column name in clin_data to inspect for determining the task type.\n\n        Returns:\n            \"classification\" if the first value in the specified column is a string, otherwise \"regression\".\n        \"\"\"\n        ## Auto-Detection\n        if type(list(clin_data[task_param])[0]) is str:\n            ml_type = \"classification\"\n        elif clin_data[task_param].unique().shape[0] &lt; 3:\n            ml_type = \"classification\"\n        else:\n            ml_type = \"regression\"\n\n        return ml_type\n\n    @staticmethod\n    def _load_input_for_ml(\n        task: str, dataset: DatasetContainer, result: Result\n    ) -&gt; pd.DataFrame:\n        \"\"\"Loads and processes input data for various machine learning tasks based on the specified task type.\n\n\n        Task Details:\n            - \"Latent\": Concatenates latent representations from train, validation, and test splits at the final epoch.\n            - \"UMAP\": Applies UMAP dimensionality reduction to the concatenated dataset splits.\n            - \"PCA\": Applies PCA dimensionality reduction to the concatenated dataset splits.\n            - \"TSNE\": Applies t-SNE dimensionality reduction to the concatenated dataset splits.\n            - \"RandomFeature\": Randomly samples columns (features) from the concatenated dataset splits.\n\n        Args:\n            task: The type of ML task. Supported values are \"Latent\", \"UMAP\", \"PCA\", \"TSNE\", and \"RandomFeature\".\n            dataset: The dataset container object holding train, validation, and test splits.\n            result: The result object containing model configuration and methods to retrieve latent representations.\n        Returns:\n            A DataFrame containing the processed input data suitable for the specified ML task.\n        Raises:\n            ValueError: If the provided task is not supported.\n        \"\"\"\n\n        final_epoch = result.model.config.epochs - 1\n\n        # if task == \"Latent\":\n        #     df = pd.concat(\n        #         [\n        #             result.get_latent_df(epoch=final_epoch, split=\"train\"),\n        #             result.get_latent_df(epoch=final_epoch, split=\"valid\"),\n        #             result.get_latent_df(epoch=-1, split=\"test\"),\n        #         ]\n        #     )\n\n        if task == \"Latent\":\n            dfs = []\n            for split in [\"train\", \"valid\", \"test\"]:\n                df_split = result.get_latent_df(\n                    epoch=final_epoch if split != \"test\" else -1, split=split\n                )\n                if df_split is not None and not df_split.empty:\n                    dfs.append(df_split)\n\n            df = pd.concat(dfs) if dfs else pd.DataFrame()\n\n        elif task in [\"UMAP\", \"PCA\", \"TSNE\", \"RandomFeature\"]:\n            dfs = []\n            for split_name in [\"train\", \"valid\", \"test\"]:\n                split_data = getattr(dataset, split_name, None)\n                if split_data is not None:\n                    dfs.append(split_data._to_df())\n\n            if not dfs:\n                raise ValueError(\n                    \"No available dataset splits (train, valid, test) to process.\"\n                )\n\n            df_processed = pd.concat(dfs)\n\n            # elif task in [\"UMAP\", \"PCA\", \"TSNE\", \"RandomFeature\"]:\n            #     if dataset.train is None:\n            #         raise ValueError(\"train attribute of dataset cannot be None\")\n            #     if dataset.valid is None:\n            #         raise ValueError(\"valid attribute of dataset cannot be None\")\n            #     if dataset.test is None:\n            #         raise ValueError(\"test attribute of dataset cannot be None\")\n\n            #     df_processed = pd.concat(\n            #         [\n            #             dataset.train._to_df(),\n            #             dataset.test._to_df(),\n            #             dataset.valid._to_df(),\n            #         ]\n            #     )\n            if task == \"UMAP\":\n                reducer = UMAP(n_components=result.model.config.latent_dim)\n                df = pd.DataFrame(\n                    reducer.fit_transform(df_processed), index=df_processed.index\n                )\n            elif task == \"PCA\":\n                reducer = PCA(n_components=result.model.config.latent_dim)\n                df = pd.DataFrame(\n                    reducer.fit_transform(df_processed), index=df_processed.index\n                )\n            elif task == \"TSNE\":\n                reducer = TSNE(n_components=result.model.config.latent_dim)\n                df = pd.DataFrame(\n                    reducer.fit_transform(df_processed), index=df_processed.index\n                )\n            elif task == \"RandomFeature\":\n                df = df_processed.sample(n=result.model.config.latent_dim, axis=1)\n        else:\n            raise ValueError(\n                f\"Your ML task {task} is not supported. Please use Latent, UMAP, PCA or RandomFeature.\"\n            )\n\n        return df\n</code></pre>"},{"location":"api/evaluate/#autoencodix.evaluate.GeneralEvaluator.evaluate","title":"<code>evaluate(datasets, result, ml_model_class=linear_model.LogisticRegression(max_iter=1000), ml_model_regression=linear_model.LinearRegression(), params='all', metric_class='roc_auc_ovo', metric_regression='r2', reference_methods=[], split_type='use-split', n_downsample=10000)</code>","text":"<p>Evaluates the performance of machine learning models on various feature representations and clinical parameters.</p> <p>This method performs classification or regression tasks using specified machine learning models on different feature sets (e.g., latent space, PCA, UMAP, TSNE, RandomFeature) and clinical annotation parameters. It supports multiple evaluation strategies, including pre-defined train/valid/test splits, k-fold cross-validation, and leave-one-out cross-validation. The results are aggregated and stored in the provided <code>result</code> object. - Samples with missing annotation values for a given parameter are excluded from the corresponding evaluation. - For \"RandomFeature\", five random feature sets are evaluated. - The method appends results to any existing <code>embedding_evaluation</code> in the result object.</p> <p>Parameters:</p> Name Type Description Default <code>datasets</code> <code>DatasetContainer</code> <p>A DatasetContainer containing train, valid, and test datasets, each with <code>sample_ids</code> and <code>metadata</code> (either a DataFrame or a dictionary with a 'paired' key for clinical annotations).</p> required <code>result</code> <code>Result</code> <p>An Result object to store the evaluation results. Should have an <code>embedding_evaluation</code> attribute which updated (typically a DataFrame).</p> required <code>ml_model_class</code> <code>ClassifierMixin</code> <p>The scikit-learn classifier to use for classification tasks (default: <code>sklearn.linear_model.LogisticRegression()</code>).</p> <code>LogisticRegression(max_iter=1000)</code> <code>ml_model_regression</code> <code>RegressorMixin</code> <p>The scikit-learn regressor to use for regression tasks (default: <code>sklearn.linear_model.LinearRegression()</code>).</p> <code>LinearRegression()</code> <code>params</code> <code>Union[list, str]</code> <p>List of clinical annotation columns to evaluate, or \"all\" to use all columns (default: \"all\").</p> <code>'all'</code> <code>metric_class</code> <code>str</code> <p>Scoring metric for classification tasks (default: \"roc_auc_ovo\").</p> <code>'roc_auc_ovo'</code> <code>metric_regression</code> <code>str</code> <p>Scoring metric for regression tasks (default: \"r2\").</p> <code>'r2'</code> <code>reference_methods</code> <code>list</code> <p>List of feature representations to evaluate (e.g., \"PCA\", \"UMAP\", \"TSNE\", \"RandomFeature\"). \"Latent\" is always included (default: []).</p> <code>[]</code> <code>split_type</code> <code>str</code> <p>which split to use use-split\" for pre-defined splits, \"CV-N\" for N-fold cross-validation, or \"LOOCV\" for leave-one-out cross-validation (default: \"use-split\").</p> <code>'use-split'</code> <code>n_downsample</code> <code>Union[int, None]</code> <p>If provided, downsample the data to this number of samples for faster evaluation. Default is 10000. Set to None to disable downsampling.</p> <code>10000</code> <p>Returns:     The updated result object with evaluation results stored in <code>embedding_evaluation</code>. Raises     ValueError: If required annotation data is missing or improperly formatted, or if an unsupported split type is specified.</p> Source code in <code>src/autoencodix/evaluate/_general_evaluator.py</code> <pre><code>@no_type_check\ndef evaluate(\n    self,\n    datasets: DatasetContainer,\n    result: Result,\n    ml_model_class: ClassifierMixin = linear_model.LogisticRegression(\n        max_iter=1000\n    ),  # Default is sklearn LogisticRegression\n    ml_model_regression: RegressorMixin = linear_model.LinearRegression(),  # Default is sklearn LinearRegression\n    params: Union[\n        list, str\n    ] = \"all\",  # No default? ... or all params in annotation?\n    metric_class: str = \"roc_auc_ovo\",  # Default is 'roc_auc_ovo' via https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-string-names\n    metric_regression: str = \"r2\",  # Default is 'r2'\n    reference_methods: list = [],  # Default [], Options are \"PCA\", \"UMAP\", \"TSNE\", \"RandomFeature\"\n    split_type: str = \"use-split\",  # Default is \"use-split\", other options: \"CV-5\", ... \"LOOCV\"?\n    n_downsample: Union[\n        int, None\n    ] = 10000,  # Default is 10000, if provided downsample to this number of samples for faster evaluation. Set to None to disable downsampling.\n) -&gt; Result:\n    \"\"\"Evaluates the performance of machine learning models on various feature representations and clinical parameters.\n\n    This method performs classification or regression tasks using specified machine learning models on different feature sets (e.g., latent space, PCA, UMAP, TSNE, RandomFeature) and clinical annotation parameters. It supports multiple evaluation strategies, including pre-defined train/valid/test splits, k-fold cross-validation, and leave-one-out cross-validation. The results are aggregated and stored in the provided `result` object.\n    - Samples with missing annotation values for a given parameter are excluded from the corresponding evaluation.\n    - For \"RandomFeature\", five random feature sets are evaluated.\n    - The method appends results to any existing `embedding_evaluation` in the result object.\n\n    Args:\n        datasets: A DatasetContainer containing train, valid, and test datasets, each with `sample_ids` and `metadata` (either a DataFrame or a dictionary with a 'paired' key for clinical annotations).\n        result: An Result object to store the evaluation results. Should have an `embedding_evaluation` attribute which updated (typically a DataFrame).\n        ml_model_class: The scikit-learn classifier to use for classification tasks (default: `sklearn.linear_model.LogisticRegression()`).\n        ml_model_regression: The scikit-learn regressor to use for regression tasks (default: `sklearn.linear_model.LinearRegression()`).\n        params:List of clinical annotation columns to evaluate, or \"all\" to use all columns (default: \"all\").\n        metric_class: Scoring metric for classification tasks (default: \"roc_auc_ovo\").\n        metric_regression: Scoring metric for regression tasks (default: \"r2\").\n        reference_methods:List of feature representations to evaluate (e.g., \"PCA\", \"UMAP\", \"TSNE\", \"RandomFeature\"). \"Latent\" is always included (default: []).\n        split_type: which split to use\n            use-split\" for pre-defined splits, \"CV-N\" for N-fold cross-validation, or \"LOOCV\" for leave-one-out cross-validation (default: \"use-split\").\n        n_downsample: If provided, downsample the data to this number of samples for faster evaluation. Default is 10000. Set to None to disable downsampling.\n    Returns:\n        The updated result object with evaluation results stored in `embedding_evaluation`.\n    Raises\n        ValueError: If required annotation data is missing or improperly formatted, or if an unsupported split type is specified.\n\n    \"\"\"\n\n    already_warned = False\n\n    df_results = pd.DataFrame()\n\n    reference_methods.append(\"Latent\")\n\n    reference_methods = self._expand_reference_methods(\n        reference_methods=reference_methods, result=result\n    )\n\n    ## Overwrite original datasets with new_datasets if available after predict with other data\n    if datasets is None:\n        datasets = DatasetContainer()\n\n    if bool(result.new_datasets.test):\n        datasets.test = result.new_datasets.test\n\n    if not bool(datasets.train or datasets.valid or datasets.test):\n        raise ValueError(\n            \"No datasets found in result object. Please run predict with new data or save/load with all datasets by using save_all=True.\"\n        )\n    elif split_type == \"use-split\" and not bool(datasets.train):\n        warnings.warn(\n            \"Warning: No train split found in result datasets for 'use-split' evaluation. ML model cannot be trained without a train split. Switch to cross-validation (CV-5) instead.\"\n        )\n        split_type = \"CV-5\"\n\n    for task in reference_methods:\n        print(f\"Perform ML task with feature df: {task}\")\n\n        # clin_data = self._get_clin_data(datasets)\n        clin_data = BaseVisualizer._collect_all_metadata(result=result)\n\n        if split_type == \"use-split\":\n            # Pandas dataframe with sample_ids and split information\n            sample_split = pd.DataFrame(columns=[\"SAMPLE_ID\", \"SPLIT\"])\n\n            if datasets.train is not None:\n                if hasattr(datasets.train, \"paired_sample_ids\"):\n                    if datasets.train.paired_sample_ids is not None:\n                        sample_ids = datasets.train.paired_sample_ids\n                else:\n                    sample_ids = datasets.train.sample_ids\n                sample_split_temp = dict(\n                    sample_split,\n                    **{\n                        \"SAMPLE_ID\": sample_ids,\n                        \"SPLIT\": [\"train\"] * len(sample_ids),\n                    },\n                )\n                sample_split = pd.concat(\n                    [sample_split, pd.DataFrame(sample_split_temp)],\n                    axis=0,\n                    ignore_index=True,\n                )\n            # else:\n            #     raise ValueError(\n            #         \"No training data found. Please provide a valid training dataset.\"\n            #     )\n            if datasets.valid is not None:\n                if hasattr(datasets.valid, \"paired_sample_ids\"):\n                    if datasets.valid.paired_sample_ids is not None:\n                        sample_ids = datasets.valid.paired_sample_ids\n                else:\n                    sample_ids = datasets.valid.sample_ids\n                sample_split_temp = dict(\n                    sample_split,\n                    **{\n                        \"SAMPLE_ID\": sample_ids,\n                        \"SPLIT\": [\"valid\"] * len(sample_ids),\n                    },\n                )\n                sample_split = pd.concat(\n                    [sample_split, pd.DataFrame(sample_split_temp)],\n                    axis=0,\n                    ignore_index=True,\n                )\n            if datasets.test is not None:\n                if hasattr(datasets.test, \"paired_sample_ids\"):\n                    if datasets.test.paired_sample_ids is not None:\n                        sample_ids = datasets.test.paired_sample_ids\n                else:\n                    sample_ids = datasets.test.sample_ids\n                sample_split_temp = dict(\n                    sample_split,\n                    **{\n                        \"SAMPLE_ID\": sample_ids,\n                        \"SPLIT\": [\"test\"] * len(sample_ids),\n                    },\n                )\n                sample_split = pd.concat(\n                    [sample_split, pd.DataFrame(sample_split_temp)],\n                    axis=0,\n                    ignore_index=True,\n                )\n\n            sample_split = sample_split.set_index(\"SAMPLE_ID\", drop=False)\n\n        ## df -&gt; task\n        subtask = [task]\n        if \"RandomFeature\" in task:\n            subtask = [task + \"_R\" + str(x) for x in range(1, 6)]\n        for sub in subtask:\n            print(sub)\n            # if is_modalix:\n            #     modality = task.split(\"_$_\")[1]\n            #     task_xmodal = task.split(\"_$_\")[0]\n\n            #     df = self._load_input_for_ml_xmodal(task_xmodal, datasets, result, modality=modality)\n            # else:\n            df = self._load_input_for_ml(task, datasets, result)\n\n            if params == \"all\":\n                params = clin_data.columns.tolist()\n\n            for task_param in params:\n                if \"Latent\" in task:\n                    print(f\"Perform ML task for target parameter: {task_param}\")\n                ## Check if classification or regression task\n                ml_type = self._get_ml_type(clin_data, task_param)\n\n                if pd.isna(clin_data[task_param]).sum() &gt; 0:\n                    # if pd.isna(clin_data[task_param]).values.any():\n                    if not already_warned:\n                        print(\n                            \"There are NA values in the annotation file. Samples with missing data will be removed for ML task evaluation.\"\n                        )\n                    already_warned = True\n                    # logger.warning(clin_data.loc[pd.isna(clin_data[task_param]), task_param])\n\n                    samples_nonna = clin_data.loc[\n                        pd.notna(clin_data[task_param]), task_param\n                    ].index\n                    # print(df)\n                    df = df.loc[samples_nonna.intersection(df.index), :]\n                    if split_type == \"use-split\":\n                        sample_split = sample_split.loc[\n                            samples_nonna.intersection(sample_split.index), :\n                        ]\n                    # print(sample_split)\n\n                if n_downsample is not None:\n                    if df.shape[0] &gt; n_downsample:\n                        sample_idx = np.random.choice(\n                            df.shape[0], n_downsample, replace=False\n                        )\n                        df = df.iloc[sample_idx]\n                        if split_type == \"use-split\":\n                            sample_split = sample_split.loc[df.index, :]\n\n                if ml_type == \"classification\":\n                    metric = metric_class\n                    sklearn_ml = ml_model_class\n\n                if ml_type == \"regression\":\n                    metric = metric_regression\n                    sklearn_ml = ml_model_regression\n\n                if split_type == \"use-split\":\n                    # print(\"Sample Split:\")\n                    # print(sample_split)\n                    # print(\"Latent:\")\n                    # print(df)\n                    results = self._single_ml_presplit(\n                        sample_split=sample_split,\n                        df=df,\n                        clin_data=clin_data,\n                        task_param=task_param,\n                        sklearn_ml=sklearn_ml,\n                        metric=metric,\n                        ml_type=ml_type,\n                    )\n                elif split_type.startswith(\"CV-\"):\n                    cv_folds = int(split_type.split(\"-\")[1])\n\n                    results = self._single_ml(\n                        df=df,\n                        clin_data=clin_data,\n                        task_param=task_param,\n                        sklearn_ml=sklearn_ml,\n                        metric=metric,\n                        cv_folds=cv_folds,\n                    )\n                elif split_type == \"LOOCV\":\n                    # Leave One Out Cross Validation\n                    results = self._single_ml(\n                        df=df,\n                        clin_data=clin_data,\n                        task_param=task_param,\n                        sklearn_ml=sklearn_ml,\n                        metric=metric,\n                        cv_folds=len(df),\n                    )\n                else:\n                    raise ValueError(\n                        f\"Your split type {split_type} is not supported. Please use 'use-split', 'CV-5', 'LOOCV' or 'CV-N'.\"\n                    )\n                results = self._enrich_results(\n                    results=results,\n                    sklearn_ml=sklearn_ml,\n                    ml_type=ml_type,\n                    task=task,\n                    sub=sub,\n                )\n\n                df_results = pd.concat([df_results, results])\n\n    ## Check if embedding_evaluation is empty\n    if (\n        hasattr(result, \"embedding_evaluation\")\n        and len(result.embedding_evaluation) == 0\n    ):\n        result.embedding_evaluation = df_results\n    else:\n        # merge with existing results\n        result.embedding_evaluation = pd.concat(\n            [result.embedding_evaluation, df_results], axis=0\n        )\n\n    return result\n</code></pre>"},{"location":"api/evaluate/#autoencodix.evaluate.XModalixEvaluator","title":"<code>XModalixEvaluator</code>","text":"<p>               Bases: <code>GeneralEvaluator</code></p> Source code in <code>src/autoencodix/evaluate/_xmodalix_evaluator.py</code> <pre><code>class XModalixEvaluator(GeneralEvaluator):\n    def __init__(self):\n        # super().__init__()\n        pass\n\n    @staticmethod\n    @no_type_check\n    def pure_vae_comparison(\n        xmodalix_result: Result,\n        pure_vae_result: Result,\n        to_key: str,\n        param: Optional[str] = None,\n    ) -&gt; Tuple[Figure, pd.DataFrame]:\n        \"\"\"Compares the reconstruction performance of a pure VAE model and a cross-modal VAE (xmodalix) model using Mean Squared Error (MSE) on test samples.\n\n        For each sample in the test set, computes the MSE between the original and reconstructed images for:\n            - Pure VAE reconstructions (\"imagix\")\n            - xmodalix reference reconstructions (\"xmodalix_reference\")\n            - xmodalix translated reconstructions (\"xmodalix_translated\")\n        The results are merged with sample metadata and returned in a long-format DataFrame suitable for plotting. Optionally, boxplots are generated grouped by a specified metadata parameter.\n\n        Args:\n            xmodalix_result: The result object containing xmodalix model outputs and test datasets.\n            pure_vae_result: The result object containing pure VAE model outputs and test datasets.\n            to_key: The key specifying the target modality in the xmodalix dataset.\n            param: Metadata column name to group boxplots by. If None, plots are grouped by model only.\n\n        Returns:\n                - The matplotlib/seaborn boxplot figure comparing MSE distributions.\n                - DataFrame: Long-format DataFrame containing MSE values and associated metadata for each sample and model.\n        \"\"\"\n\n        if \"img\" not in to_key:\n            raise NotImplementedError(\n                \"Comparison is currently only implemented for the image case.\"\n            )\n\n        ## Pure VAE MSE calculation\n        meta_imagix = pure_vae_result.datasets.test.metadata\n        if meta_imagix is None:\n            raise ValueError(\"metadata cannot be None\")\n        sample_ids = list(meta_imagix.index)\n\n        all_sample_order = sample_ids  ## TODO check code, seems unnecessary\n        indices = [\n            all_sample_order.index(sid) for sid in sample_ids if sid in all_sample_order\n        ]\n\n        mse_records = []\n\n        for c in range(len(indices)):\n            # print(f\"Sample {c+1}/{len(indices)}: {sample_ids[c]}\")\n\n            # Original image\n            orig = torch.Tensor(\n                pure_vae_result.datasets.test.raw_data[indices[c]].img.squeeze()\n            )\n\n            # Reconstructed image\n            recon = torch.Tensor(\n                pure_vae_result.reconstructions.get(split=\"test\", epoch=-1)[\n                    indices[c]\n                ].squeeze()\n            )\n\n            # Calculate MSE via torch\n            mse_sample = F.mse_loss(orig, recon, reduction=\"mean\")\n            # print(f\"Mean Squared Error (MSE) for sample {c+1}: {mse_sample.item()}\")\n\n            # Collect results\n            mse_records.append(\n                {\"sample_id\": sample_ids[c], \"mse_imagix\": mse_sample.item()}\n            )\n\n        df_imagix_mse = pd.DataFrame(mse_records)\n        df_imagix_mse.set_index(\"sample_id\", inplace=True)\n        # Merge with meta_imagix\n        df_imagix_mse = df_imagix_mse.join(meta_imagix, on=\"sample_id\")\n\n        meta_xmodalix = xmodalix_result.datasets.test.datasets[to_key].metadata\n        sample_ids = list(meta_xmodalix.index)\n\n        all_sample_order = sample_ids\n        indices = [\n            all_sample_order.index(sid) for sid in sample_ids if sid in all_sample_order\n        ]\n\n        mse_records = []\n\n        for c in range(len(indices)):\n            # print(f\"Sample {c+1}/{len(indices)}: {sample_ids[c]}\")\n\n            # Original image\n            orig = torch.Tensor(\n                xmodalix_result.datasets.test.datasets[to_key][indices[c]][1].squeeze()\n            )\n            # print(orig.shape)\n\n            # Reference Reconstructed image\n            reference = torch.Tensor(\n                xmodalix_result.reconstructions.get(epoch=-1, split=\"test\")[\n                    f\"reference_{to_key}_to_{to_key}\"\n                ][indices[c]].squeeze()\n            )\n            # print(reference.shape)\n\n            # Translated Reconstructed image\n            translation = torch.Tensor(\n                xmodalix_result.reconstructions.get(epoch=-1, split=\"test\")[\n                    \"translation\"\n                ][indices[c]].squeeze()\n            )\n            # print(translation.shape)\n\n            # Calculate MSE via torch\n            mse_sample_translated = F.mse_loss(orig, translation, reduction=\"mean\")\n            # print(f\"Mean Squared Error (MSE) for sample {c+1}: {mse_sample_translated.item()}\")\n            mse_sample_reference = F.mse_loss(orig, reference, reduction=\"mean\")\n            # print(f\"Mean Squared Error (MSE) for sample {c+1}: {mse_sample_reference.item()}\")\n\n            # Collect results\n            mse_records.append(\n                {\n                    \"sample_id\": sample_ids[c],\n                    \"mse_xmodalix_translated\": mse_sample_translated.item(),\n                    \"mse_xmodalix_reference\": mse_sample_reference.item(),\n                }\n            )\n\n        df_xmodalix_mse = pd.DataFrame(mse_records)\n        df_xmodalix_mse.set_index(\"sample_id\", inplace=True)\n\n        # Merge with meta_xmodalix\n        df_xmodalix_mse = df_xmodalix_mse.join(meta_xmodalix, on=\"sample_id\")\n\n        # Merge via sample_id and keep non overlapping entries\n        df_both_mse = df_imagix_mse.merge(\n            df_xmodalix_mse, on=list(meta_imagix.columns), how=\"outer\"\n        )\n\n        # Make long format for plotting\n        df_long = df_both_mse.melt(\n            id_vars=[\n                col\n                for col in df_both_mse.columns\n                if col\n                not in [\n                    \"mse_imagix\",\n                    \"mse_xmodalix_translated\",\n                    \"mse_xmodalix_reference\",\n                ]\n            ],\n            value_vars=[\n                \"mse_imagix\",\n                \"mse_xmodalix_translated\",\n                \"mse_xmodalix_reference\",\n            ],\n            var_name=\"model\",\n            value_name=\"mse_value\",\n        )\n\n        df_long[\"model\"] = df_long[\"model\"].map(\n            {\n                \"mse_imagix\": \"imagix\",\n                \"mse_xmodalix_translated\": \"xmodalix_translated\",\n                \"mse_xmodalix_reference\": \"xmodalix_reference\",\n            }\n        )\n\n        if param:\n            plt.figure(figsize=(2 * len(df_long[param].unique()), 8))\n\n            fig = sns.boxplot(data=df_long, x=param, y=\"mse_value\", hue=\"model\")\n            sns.move_legend(\n                fig,\n                \"lower center\",\n                bbox_to_anchor=(0.5, 1),\n                ncol=3,\n                title=None,\n                frameon=False,\n            )\n        else:\n            plt.figure(figsize=(5, 8))\n\n            fig = sns.boxplot(data=df_long, x=\"model\", y=\"mse_value\")\n            # Rotate tick labels\n            plt.xticks(rotation=-45)\n            plt.xlabel(\"\")\n\n        return fig, df_long\n\n    @staticmethod\n    def _get_clin_data(datasets) -&gt; Union[pd.Series, pd.DataFrame]:\n        \"\"\"Retrieves the clinical annotation DataFrame (clin_data) from the provided datasets.\n\n        Handles both standard and XModalix dataset structures.\n        \"\"\"\n        # XModalix-Case\n        if hasattr(datasets.train, \"datasets\"):\n            clin_data = pd.DataFrame()\n            splits = [datasets.train, datasets.valid, datasets.test]\n\n            for s in splits:\n                for k in s.datasets.keys():\n                    print(f\"Processing dataset: {k}\")\n                    # Merge metadata by overlapping columns\n                    overlap = clin_data.columns.intersection(\n                        s.datasets[k].metadata.columns\n                    )\n                    if overlap.empty:\n                        overlap = s.datasets[k].metadata.columns\n                    clin_data = pd.concat(\n                        [clin_data, s.datasets[k].metadata[overlap]], axis=0\n                    )\n\n            # Remove duplicate rows\n            clin_data = clin_data[~clin_data.index.duplicated(keep=\"first\")]\n        else:\n            # Raise error no annotation given\n            raise ValueError(\n                \"No annotation data found. Please provide a valid annotation data type.\"\n            )\n        return clin_data\n\n    def _enrich_results(\n        self,\n        results: pd.DataFrame,\n        sklearn_ml: Union[ClassifierMixin, RegressorMixin],\n        ml_type: str,\n        task: str,\n        sub: str,\n    ) -&gt; pd.DataFrame:\n        res_ml_alg = [str(sklearn_ml) for x in range(0, results.shape[0])]\n        res_ml_type = [ml_type for x in range(0, results.shape[0])]\n        res_ml_subtask = [sub for x in range(0, results.shape[0])]\n\n        results[\"ML_ALG\"] = res_ml_alg\n        results[\"ML_TYPE\"] = res_ml_type\n\n        modality = task.split(\"_$_\")[1]\n        task_xmodal = task.split(\"_$_\")[0]\n\n        results[\"MODALITY\"] = [modality for x in range(0, results.shape[0])]\n        results[\"ML_TASK\"] = [task_xmodal for x in range(0, results.shape[0])]\n\n        results[\"ML_SUBTASK\"] = res_ml_subtask\n\n        return results\n\n    @staticmethod\n    @no_type_check\n    def _expand_reference_methods(reference_methods: list, result: Result) -&gt; list:\n        \"\"\"\n        Expands each reference method by appending a suffix for every key of used data modalities.\n        For each method in `reference_methods`, this function generates new method names by concatenating\n        the method name with each key for the data modalities of the xmodalix.\n        Args:\n            reference_methods (list): A list of reference method names to be expanded.\n            result (Result): An object containing latent space information.\n        Returns:\n            list: A list of expanded reference method names, each suffixed with a key from the latent space.\n        \"\"\"\n        if not isinstance(result.latentspaces.get(epoch=-1, split=\"train\"), dict):\n            raise NotImplementedError(\n                \"This evaluate feature does not support .save(save_all=False) results.\"\n            )\n        reference_methods = [\n            f\"{method}_$_{key}\"\n            for method in reference_methods\n            for key in result.latentspaces.get(epoch=-1, split=\"train\").keys()\n        ]\n\n        return reference_methods\n\n    ## New for x-modalix\n    @staticmethod\n    def _load_input_for_ml(\n        task: str, dataset: DatasetContainer, result: Result\n    ) -&gt; pd.DataFrame:\n        \"\"\"Loads and processes input data for various machine learning tasks based on the specified task type.\n\n        Task Details:\n            - \"Latent\": Concatenates latent representations from train, validation, and test splits at the final epoch.\n            - \"UMAP\": Applies UMAP dimensionality reduction to the concatenated dataset splits.\n            - \"PCA\": Applies PCA dimensionality reduction to the concatenated dataset splits.\n            - \"TSNE\": Applies t-SNE dimensionality reduction to the concatenated dataset splits.\n            - \"RandomFeature\": Randomly samples columns (features) from the concatenated dataset splits.\n\n\n        Args:\n            task: The type of ML task. Supported values are \"Latent\", \"UMAP\", \"PCA\", \"TSNE\", and \"RandomFeature\".\n            dataset: The dataset container object holding train, validation, and test splits.\n            result: The result object containing model configuration and methods to retrieve latent representations.\n        Returns:\n            A DataFrame containing the processed input data suitable for the specified ML task.\n        Raises:\n            ValueError: If the provided task is not supported.\n        \"\"\"\n\n        # final_epoch = result.model.config.epochs - 1\n        modality = task.split(\"_$_\")[1]\n        task = task.split(\"_$_\")[0]\n\n        if task == \"Latent\":\n            df = pd.concat(\n                [\n                    result.get_latent_df(epoch=-1, split=\"train\", modality=modality),\n                    result.get_latent_df(epoch=-1, split=\"valid\", modality=modality),\n                    result.get_latent_df(epoch=-1, split=\"test\", modality=modality),\n                ]\n            )\n        elif task in [\"UMAP\", \"PCA\", \"TSNE\", \"RandomFeature\"]:\n            latent_dim = result.get_latent_df(\n                epoch=-1, split=\"train\", modality=modality\n            ).shape[1]\n            if dataset.train is None:\n                raise ValueError(\"train attribute of dataset cannot be None\")\n            if dataset.valid is None:\n                raise ValueError(\"valid attribute of dataset cannot be None\")\n            if dataset.test is None:\n                raise ValueError(\"test attribute of dataset cannot be None\")\n\n            df_processed = pd.concat(\n                [\n                    dataset.train._to_df(modality=modality),\n                    dataset.test._to_df(modality=modality),\n                    dataset.valid._to_df(modality=modality),\n                ]\n            )\n            if task == \"UMAP\":\n                reducer = UMAP(n_components=latent_dim)\n                df = pd.DataFrame(\n                    reducer.fit_transform(df_processed), index=df_processed.index\n                )\n            elif task == \"PCA\":\n                reducer = PCA(n_components=latent_dim)\n                df = pd.DataFrame(\n                    reducer.fit_transform(df_processed), index=df_processed.index\n                )\n            elif task == \"TSNE\":\n                reducer = TSNE(n_components=latent_dim)\n                df = pd.DataFrame(\n                    reducer.fit_transform(df_processed), index=df_processed.index\n                )\n            elif task == \"RandomFeature\":\n                df = df_processed.sample(n=latent_dim, axis=1)\n        else:\n            raise ValueError(\n                f\"Your ML task {task} is not supported. Please use Latent, UMAP, PCA or RandomFeature.\"\n            )\n\n        return df\n</code></pre>"},{"location":"api/evaluate/#autoencodix.evaluate.XModalixEvaluator.pure_vae_comparison","title":"<code>pure_vae_comparison(xmodalix_result, pure_vae_result, to_key, param=None)</code>  <code>staticmethod</code>","text":"<p>Compares the reconstruction performance of a pure VAE model and a cross-modal VAE (xmodalix) model using Mean Squared Error (MSE) on test samples.</p> <p>For each sample in the test set, computes the MSE between the original and reconstructed images for:     - Pure VAE reconstructions (\"imagix\")     - xmodalix reference reconstructions (\"xmodalix_reference\")     - xmodalix translated reconstructions (\"xmodalix_translated\") The results are merged with sample metadata and returned in a long-format DataFrame suitable for plotting. Optionally, boxplots are generated grouped by a specified metadata parameter.</p> <p>Parameters:</p> Name Type Description Default <code>xmodalix_result</code> <code>Result</code> <p>The result object containing xmodalix model outputs and test datasets.</p> required <code>pure_vae_result</code> <code>Result</code> <p>The result object containing pure VAE model outputs and test datasets.</p> required <code>to_key</code> <code>str</code> <p>The key specifying the target modality in the xmodalix dataset.</p> required <code>param</code> <code>Optional[str]</code> <p>Metadata column name to group boxplots by. If None, plots are grouped by model only.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <ul> <li>The matplotlib/seaborn boxplot figure comparing MSE distributions.</li> </ul> <code>DataFrame</code> <ul> <li>DataFrame: Long-format DataFrame containing MSE values and associated metadata for each sample and model.</li> </ul> Source code in <code>src/autoencodix/evaluate/_xmodalix_evaluator.py</code> <pre><code>@staticmethod\n@no_type_check\ndef pure_vae_comparison(\n    xmodalix_result: Result,\n    pure_vae_result: Result,\n    to_key: str,\n    param: Optional[str] = None,\n) -&gt; Tuple[Figure, pd.DataFrame]:\n    \"\"\"Compares the reconstruction performance of a pure VAE model and a cross-modal VAE (xmodalix) model using Mean Squared Error (MSE) on test samples.\n\n    For each sample in the test set, computes the MSE between the original and reconstructed images for:\n        - Pure VAE reconstructions (\"imagix\")\n        - xmodalix reference reconstructions (\"xmodalix_reference\")\n        - xmodalix translated reconstructions (\"xmodalix_translated\")\n    The results are merged with sample metadata and returned in a long-format DataFrame suitable for plotting. Optionally, boxplots are generated grouped by a specified metadata parameter.\n\n    Args:\n        xmodalix_result: The result object containing xmodalix model outputs and test datasets.\n        pure_vae_result: The result object containing pure VAE model outputs and test datasets.\n        to_key: The key specifying the target modality in the xmodalix dataset.\n        param: Metadata column name to group boxplots by. If None, plots are grouped by model only.\n\n    Returns:\n            - The matplotlib/seaborn boxplot figure comparing MSE distributions.\n            - DataFrame: Long-format DataFrame containing MSE values and associated metadata for each sample and model.\n    \"\"\"\n\n    if \"img\" not in to_key:\n        raise NotImplementedError(\n            \"Comparison is currently only implemented for the image case.\"\n        )\n\n    ## Pure VAE MSE calculation\n    meta_imagix = pure_vae_result.datasets.test.metadata\n    if meta_imagix is None:\n        raise ValueError(\"metadata cannot be None\")\n    sample_ids = list(meta_imagix.index)\n\n    all_sample_order = sample_ids  ## TODO check code, seems unnecessary\n    indices = [\n        all_sample_order.index(sid) for sid in sample_ids if sid in all_sample_order\n    ]\n\n    mse_records = []\n\n    for c in range(len(indices)):\n        # print(f\"Sample {c+1}/{len(indices)}: {sample_ids[c]}\")\n\n        # Original image\n        orig = torch.Tensor(\n            pure_vae_result.datasets.test.raw_data[indices[c]].img.squeeze()\n        )\n\n        # Reconstructed image\n        recon = torch.Tensor(\n            pure_vae_result.reconstructions.get(split=\"test\", epoch=-1)[\n                indices[c]\n            ].squeeze()\n        )\n\n        # Calculate MSE via torch\n        mse_sample = F.mse_loss(orig, recon, reduction=\"mean\")\n        # print(f\"Mean Squared Error (MSE) for sample {c+1}: {mse_sample.item()}\")\n\n        # Collect results\n        mse_records.append(\n            {\"sample_id\": sample_ids[c], \"mse_imagix\": mse_sample.item()}\n        )\n\n    df_imagix_mse = pd.DataFrame(mse_records)\n    df_imagix_mse.set_index(\"sample_id\", inplace=True)\n    # Merge with meta_imagix\n    df_imagix_mse = df_imagix_mse.join(meta_imagix, on=\"sample_id\")\n\n    meta_xmodalix = xmodalix_result.datasets.test.datasets[to_key].metadata\n    sample_ids = list(meta_xmodalix.index)\n\n    all_sample_order = sample_ids\n    indices = [\n        all_sample_order.index(sid) for sid in sample_ids if sid in all_sample_order\n    ]\n\n    mse_records = []\n\n    for c in range(len(indices)):\n        # print(f\"Sample {c+1}/{len(indices)}: {sample_ids[c]}\")\n\n        # Original image\n        orig = torch.Tensor(\n            xmodalix_result.datasets.test.datasets[to_key][indices[c]][1].squeeze()\n        )\n        # print(orig.shape)\n\n        # Reference Reconstructed image\n        reference = torch.Tensor(\n            xmodalix_result.reconstructions.get(epoch=-1, split=\"test\")[\n                f\"reference_{to_key}_to_{to_key}\"\n            ][indices[c]].squeeze()\n        )\n        # print(reference.shape)\n\n        # Translated Reconstructed image\n        translation = torch.Tensor(\n            xmodalix_result.reconstructions.get(epoch=-1, split=\"test\")[\n                \"translation\"\n            ][indices[c]].squeeze()\n        )\n        # print(translation.shape)\n\n        # Calculate MSE via torch\n        mse_sample_translated = F.mse_loss(orig, translation, reduction=\"mean\")\n        # print(f\"Mean Squared Error (MSE) for sample {c+1}: {mse_sample_translated.item()}\")\n        mse_sample_reference = F.mse_loss(orig, reference, reduction=\"mean\")\n        # print(f\"Mean Squared Error (MSE) for sample {c+1}: {mse_sample_reference.item()}\")\n\n        # Collect results\n        mse_records.append(\n            {\n                \"sample_id\": sample_ids[c],\n                \"mse_xmodalix_translated\": mse_sample_translated.item(),\n                \"mse_xmodalix_reference\": mse_sample_reference.item(),\n            }\n        )\n\n    df_xmodalix_mse = pd.DataFrame(mse_records)\n    df_xmodalix_mse.set_index(\"sample_id\", inplace=True)\n\n    # Merge with meta_xmodalix\n    df_xmodalix_mse = df_xmodalix_mse.join(meta_xmodalix, on=\"sample_id\")\n\n    # Merge via sample_id and keep non overlapping entries\n    df_both_mse = df_imagix_mse.merge(\n        df_xmodalix_mse, on=list(meta_imagix.columns), how=\"outer\"\n    )\n\n    # Make long format for plotting\n    df_long = df_both_mse.melt(\n        id_vars=[\n            col\n            for col in df_both_mse.columns\n            if col\n            not in [\n                \"mse_imagix\",\n                \"mse_xmodalix_translated\",\n                \"mse_xmodalix_reference\",\n            ]\n        ],\n        value_vars=[\n            \"mse_imagix\",\n            \"mse_xmodalix_translated\",\n            \"mse_xmodalix_reference\",\n        ],\n        var_name=\"model\",\n        value_name=\"mse_value\",\n    )\n\n    df_long[\"model\"] = df_long[\"model\"].map(\n        {\n            \"mse_imagix\": \"imagix\",\n            \"mse_xmodalix_translated\": \"xmodalix_translated\",\n            \"mse_xmodalix_reference\": \"xmodalix_reference\",\n        }\n    )\n\n    if param:\n        plt.figure(figsize=(2 * len(df_long[param].unique()), 8))\n\n        fig = sns.boxplot(data=df_long, x=param, y=\"mse_value\", hue=\"model\")\n        sns.move_legend(\n            fig,\n            \"lower center\",\n            bbox_to_anchor=(0.5, 1),\n            ncol=3,\n            title=None,\n            frameon=False,\n        )\n    else:\n        plt.figure(figsize=(5, 8))\n\n        fig = sns.boxplot(data=df_long, x=\"model\", y=\"mse_value\")\n        # Rotate tick labels\n        plt.xticks(rotation=-45)\n        plt.xlabel(\"\")\n\n    return fig, df_long\n</code></pre>"},{"location":"api/imagix/","title":"Imagix Module","text":""},{"location":"api/imagix/#autoencodix.imagix.Imagix","title":"<code>Imagix</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>Imagix specific version of the BasePipeline class.</p> <p>This class extends BasePipeline. See the parent class for a full list of attributes and methods.</p> Additional Attributes <p>_default_config: Is set to DefaultConfig here.</p> Source code in <code>src/autoencodix/imagix.py</code> <pre><code>class Imagix(BasePipeline):\n    \"\"\"Imagix specific version of the BasePipeline class.\n\n    This class extends BasePipeline. See the parent class for a full list\n    of attributes and methods.\n\n    Additional Attributes:\n        _default_config: Is set to DefaultConfig here.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Optional[Union[DataPackage, DatasetContainer]] = None,\n        trainer_type: Type[BaseTrainer] = GeneralTrainer,\n        dataset_type: Type[BaseDataset] = ImageDataset,\n        model_type: Type[BaseAutoencoder] = ImageVAEArchitecture,\n        loss_type: Type[BaseLoss] = VarixLoss,\n        preprocessor_type: Type[BasePreprocessor] = ImagePreprocessor,\n        visualizer: Optional[Type[BaseVisualizer]] = ImagixVisualizer,\n        evaluator: Optional[Type[GeneralEvaluator]] = GeneralEvaluator,\n        result: Optional[Result] = None,\n        datasplitter_type: Type[DataSplitter] = DataSplitter,\n        custom_splits: Optional[Dict[str, np.ndarray]] = None,\n        config: Optional[DefaultConfig] = None,\n    ) -&gt; None:\n        \"\"\"Initialize Imagix pipeline with customizable components.\n\n        Some components are passed as types rather than instances because they require\n        data that is only available after preprocessing.\n\n        See Parentclass for full list of Args.\n\n        \"\"\"\n        self._default_config = DefaultConfig()\n        super().__init__(\n            data=data,\n            dataset_type=dataset_type,\n            trainer_type=trainer_type,\n            model_type=model_type,\n            loss_type=loss_type,\n            preprocessor_type=preprocessor_type,\n            visualizer=visualizer,\n            evaluator=evaluator,\n            result=result,\n            datasplitter_type=datasplitter_type,\n            config=config,\n            custom_split=custom_splits,\n        )\n</code></pre>"},{"location":"api/imagix/#autoencodix.imagix.Imagix.__init__","title":"<code>__init__(data=None, trainer_type=GeneralTrainer, dataset_type=ImageDataset, model_type=ImageVAEArchitecture, loss_type=VarixLoss, preprocessor_type=ImagePreprocessor, visualizer=ImagixVisualizer, evaluator=GeneralEvaluator, result=None, datasplitter_type=DataSplitter, custom_splits=None, config=None)</code>","text":"<p>Initialize Imagix pipeline with customizable components.</p> <p>Some components are passed as types rather than instances because they require data that is only available after preprocessing.</p> <p>See Parentclass for full list of Args.</p> Source code in <code>src/autoencodix/imagix.py</code> <pre><code>def __init__(\n    self,\n    data: Optional[Union[DataPackage, DatasetContainer]] = None,\n    trainer_type: Type[BaseTrainer] = GeneralTrainer,\n    dataset_type: Type[BaseDataset] = ImageDataset,\n    model_type: Type[BaseAutoencoder] = ImageVAEArchitecture,\n    loss_type: Type[BaseLoss] = VarixLoss,\n    preprocessor_type: Type[BasePreprocessor] = ImagePreprocessor,\n    visualizer: Optional[Type[BaseVisualizer]] = ImagixVisualizer,\n    evaluator: Optional[Type[GeneralEvaluator]] = GeneralEvaluator,\n    result: Optional[Result] = None,\n    datasplitter_type: Type[DataSplitter] = DataSplitter,\n    custom_splits: Optional[Dict[str, np.ndarray]] = None,\n    config: Optional[DefaultConfig] = None,\n) -&gt; None:\n    \"\"\"Initialize Imagix pipeline with customizable components.\n\n    Some components are passed as types rather than instances because they require\n    data that is only available after preprocessing.\n\n    See Parentclass for full list of Args.\n\n    \"\"\"\n    self._default_config = DefaultConfig()\n    super().__init__(\n        data=data,\n        dataset_type=dataset_type,\n        trainer_type=trainer_type,\n        model_type=model_type,\n        loss_type=loss_type,\n        preprocessor_type=preprocessor_type,\n        visualizer=visualizer,\n        evaluator=evaluator,\n        result=result,\n        datasplitter_type=datasplitter_type,\n        config=config,\n        custom_split=custom_splits,\n    )\n</code></pre>"},{"location":"api/modeling/","title":"Modeling Module","text":""},{"location":"api/modeling/#autoencodix.modeling.Classifier","title":"<code>Classifier</code>","text":"<p>               Bases: <code>Module</code></p> <p>Multi-class classifier for adversarial training in n-modal latent space alignment.</p> <p>Attributes:</p> Name Type Description <code>input_dim</code> <p>Dimension of the input features.</p> <code>n_modalities</code> <p>Number of modalities (classes) to classify.</p> <code>n_hidden</code> <p>Number of hidden units in the classifier network.</p> Source code in <code>src/autoencodix/modeling/_classifier.py</code> <pre><code>class Classifier(nn.Module):\n    \"\"\"Multi-class classifier for adversarial training in n-modal latent space alignment.\n\n\n    Attributes:\n        input_dim: Dimension of the input features.\n        n_modalities: Number of modalities (classes) to classify.\n        n_hidden: Number of hidden units in the classifier network.\n\n    \"\"\"\n\n    def __init__(self, input_dim: int, n_modalities: int, n_hidden: int = 64) -&gt; None:\n        super().__init__()\n        self.input_dim = input_dim\n        self.n_modalities = n_modalities\n        self.classifier = nn.Sequential(\n            nn.Linear(input_dim, n_hidden),\n            nn.ReLU(inplace=False),\n            nn.Dropout(0.1),\n            nn.Linear(n_hidden, n_hidden // 2),\n            nn.ReLU(inplace=False),\n            nn.Dropout(0.1),\n            nn.Linear(n_hidden // 2, n_modalities),\n        )\n        self.apply(self._init_weights)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Forward pass through the classifier.\n\n        Args:\n            x: Input tensor of shape (batch_size, input_dim).\n        Returns:\n            Output tensor of shape (batch_size, n_modalities) representing class scores.\n        \"\"\"\n        return self.classifier(x)\n\n    def _init_weights(self, m: nn.Module) -&gt; None:\n        if isinstance(m, nn.Linear):\n            nn.init.xavier_uniform_(m.weight)\n            if m.bias is not None:\n                m.bias.data.fill_(0.01)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.Classifier.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the classifier.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor of shape (batch_size, input_dim).</p> required <p>Returns:     Output tensor of shape (batch_size, n_modalities) representing class scores.</p> Source code in <code>src/autoencodix/modeling/_classifier.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass through the classifier.\n\n    Args:\n        x: Input tensor of shape (batch_size, input_dim).\n    Returns:\n        Output tensor of shape (batch_size, n_modalities) representing class scores.\n    \"\"\"\n    return self.classifier(x)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.ImageVAEArchitecture","title":"<code>ImageVAEArchitecture</code>","text":"<p>               Bases: <code>BaseAutoencoder</code></p> <p>This class defines a VAE, based on a CNN for images</p> <p>It takes as input an image and of shape (C,W,H) and reconstructs it. We ensure to have a latent space of shape  and img_in.shape = img_out.shape We have a fixed kernel_size=4, padding=1 and stride=2 (given from https://github.com/uhlerlab/cross-modal-auto_encoders/tree/master) <p>So we need to calculate how the image dimension changes after each Convolution (we assume W=H) Applying the formular:     W_out = (((W - kernel_size + 2padding)/stride) + 1) We get:     W_out = (((W-4+2*1)/2)+1) =     = (W-2/2)+1 =     = (2(0.5W-1)/2) +1 # factor 2 out     = 0.5W - 1 + 1     W_out = 0.5W So in this configuration the output shape halfs after every convolutional step (assuming W=H)</p> <p>Attributes:</p> Name Type Description <code>input_dim</code> <code>int</code> <p>(C,W,H) the input image shape</p> <code>config</code> <p>Configuration object containing model architecture parameters</p> <code>_encoder</code> <code>Optional[Module]</code> <p>Encoder network of the autoencoder</p> <code>_decoder</code> <code>Optional[Module]</code> <p>Decoder network of the autoencoder</p> <code>latent_dim</code> <code>int</code> <p>Dimension of the latent space</p> <code>nc</code> <code>int</code> <p>number of channels in the input image</p> <code>h</code> <code>int</code> <p>height of the input image</p> <code>w</code> <code>int</code> <p>width of the input image</p> <code>img_shape</code> <code>Tuple[int, int, int]</code> <p>(C,W,H) the input image shape</p> <code>hidden_dim</code> <code>int</code> <p>number of filters in the first convolutional layer</p> Source code in <code>src/autoencodix/modeling/_imagevae_architecture.py</code> <pre><code>class ImageVAEArchitecture(BaseAutoencoder):\n    \"\"\"This class defines a VAE, based on a CNN for images\n\n    It takes as input an image and of shape (C,W,H) and reconstructs it.\n    We ensure to have a latent space of shape &lt;batchsize,1,LatentDim&gt; and img_in.shape = img_out.shape\n    We have a fixed kernel_size=4, padding=1 and stride=2 (given from https://github.com/uhlerlab/cross-modal-auto_encoders/tree/master)\n\n    So we need to calculate how the image dimension changes after each Convolution (we assume W=H)\n    Applying the formular:\n        W_out = (((W - kernel_size + 2padding)/stride) + 1)\n    We get:\n        W_out = (((W-4+2*1)/2)+1) =\n        = (W-2/2)+1 =\n        = (2(0.5W-1)/2) +1 # factor 2 out\n        = 0.5W - 1 + 1\n        W_out = 0.5W\n    So in this configuration the output shape halfs after every convolutional step (assuming W=H)\n\n\n    Attributes:\n        input_dim: (C,W,H) the input image shape\n        config: Configuration object containing model architecture parameters\n        _encoder: Encoder network of the autoencoder\n        _decoder: Decoder network of the autoencoder\n        latent_dim: Dimension of the latent space\n        nc: number of channels in the input image\n        h: height of the input image\n        w: width of the input image\n        img_shape: (C,W,H) the input image shape\n        hidden_dim: number of filters in the first convolutional layer\n    \"\"\"\n\n    def __init__(\n        self,\n        input_dim: Tuple[int, int, int],  # (C,W,H) the input image shape\n        config: Optional[DefaultConfig],\n        ontologies: Optional[Union[Tuple, Dict]] = None,\n        feature_order: Optional[Union[Tuple, Dict]] = None,\n        # the input_dim is the number of channels in the image, e.g. 3\n    ):\n        \"\"\"Initialize the ImageVAEArchitecture with the given configuration.\n\n        Args:\n            input_dim: (C,W,H) the input image shape\n            config: Configuration object containing model parameters.\n            hidden_dim: number of filters in the first convolutional layer\n        \"\"\"\n        if config is None:\n            config = DefaultConfig()\n        self._config: DefaultConfig = config\n        super().__init__(config=config, input_dim=input_dim)\n        self.input_dim: int = input_dim\n        self.latent_dim: int = self._config.latent_dim\n        self.nc, self.h, self.w = input_dim\n        self.img_shape: Tuple[int, int, int] = input_dim\n        self.hidden_dim: int = self._config.hidden_dim\n        self._build_network()\n        self.apply(self._init_weights)\n\n    def _build_network(self):\n        \"\"\"Construct the encoder and decoder networks.\"\"\"\n        self._encoder = nn.Sequential(\n            nn.Conv2d(\n                in_channels=self.nc,\n                out_channels=self.hidden_dim,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.LeakyReLU(0.2, inplace=False),\n            nn.Conv2d(\n                in_channels=self.hidden_dim,\n                out_channels=self.hidden_dim * 2,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(self.hidden_dim * 2),\n            nn.LeakyReLU(0.2, inplace=False),\n            nn.Conv2d(\n                in_channels=self.hidden_dim * 2,\n                out_channels=self.hidden_dim * 4,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(self.hidden_dim * 4),\n            nn.LeakyReLU(0.2, inplace=False),\n            nn.Conv2d(\n                in_channels=self.hidden_dim * 4,\n                out_channels=self.hidden_dim * 8,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(self.hidden_dim * 8),\n            nn.LeakyReLU(0.2, inplace=False),\n            nn.Conv2d(\n                in_channels=self.hidden_dim * 8,\n                out_channels=self.hidden_dim * 8,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(self.hidden_dim * 8),\n            nn.LeakyReLU(0.2, inplace=False),\n        )\n\n        # to Calculate the image shape after the _encoder, we need to know the number of layers\n        # because the shape halfs after every Conv2D layer\n        self.num__encoder_layers = sum(\n            1 for _ in self._encoder.children() if isinstance(_, nn.Conv2d)\n        )\n        # So the output shape after all layers is in_shape / 2**N_layers\n        # We showed above in the DocString why the shape halfs\n        self.spatial_dim = self.h // (2**self.num__encoder_layers)\n        # In the Linear mu and logvar layer we need to flatten the 3D output to a 2D matrix\n        # Therefore we need to multiply the size of every out diemension of the input layer to the Linear layers\n        # This is hidden_dim * 8 (the number of filter/channel layer) * spatial dim (the widht of the image) * spatial diem (the height of the image)\n        # assuimg width = height\n        # The original paper had a fixed spatial dimension of 2, which only worked for images with 64x64 shape\n        self.mu = nn.Linear(\n            self.hidden_dim * 8 * self.spatial_dim * self.spatial_dim, self.latent_dim\n        )\n        self.logvar = nn.Linear(\n            self.hidden_dim * 8 * self.spatial_dim * self.spatial_dim, self.latent_dim\n        )\n\n        # the same logic goes for the first _decoder layer, which takes the latent_dim as inshape\n        # which is the outshape of the previous mu/logvar layer\n        # and the shape of the first ConvTranspose2D layer is the last outpus shape of the _encoder layer\n        # This the same multiplication as above\n        self.d1 = nn.Sequential(\n            nn.Linear(\n                self.latent_dim,\n                self.hidden_dim * 8 * self.spatial_dim * self.spatial_dim,\n            ),\n            nn.ReLU(inplace=False),\n        )\n        self._decoder = nn.Sequential(\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_dim * 8,\n                out_channels=self.hidden_dim * 8,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(self.hidden_dim * 8),\n            nn.LeakyReLU(0.2, inplace=False),\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_dim * 8,\n                out_channels=self.hidden_dim * 4,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(self.hidden_dim * 4),\n            nn.LeakyReLU(0.2, inplace=False),\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_dim * 4,\n                out_channels=self.hidden_dim * 2,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(self.hidden_dim * 2),\n            nn.LeakyReLU(0.2, inplace=False),\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_dim * 2,\n                out_channels=self.hidden_dim,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n            nn.BatchNorm2d(self.hidden_dim),\n            nn.LeakyReLU(0.2, inplace=False),\n            nn.ConvTranspose2d(\n                in_channels=self.hidden_dim,\n                out_channels=self.nc,\n                kernel_size=4,\n                stride=2,\n                padding=1,\n                bias=False,\n            ),\n        )\n\n    def _get_spatial_dim(self) -&gt; int:\n        return self.spatial_dim\n\n    def encode(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Encodes the input tensor x.\n\n        Args:\n            x: Input tensor\n        Returns:\n            The encoded latent space representation, or mu and logvar for VAEs.\n\n        \"\"\"\n        h = self._encoder(x)\n        # this makes sure we get the &lt;batchsize, 1, latent_dim&gt; shape for our latent space in the next step\n        # because we put all dimensionaltiy in the second dimension of the output shape.\n        # By covering all dimensionality here, we are sure that the rest is\n        h = h.view(-1, self.hidden_dim * 8 * self.spatial_dim * self.spatial_dim)\n        logvar = self.logvar(h)\n        mu = self.mu(h)\n        # prevent  mu and logvar from being too close to zero, this increased\n        # numerical stability\n        logvar = torch.clamp(logvar, 0.1, 20)\n        # replace mu when mu &lt; 0.00000001 with 0.1\n        mu = torch.where(mu &lt; 0.000001, torch.zeros_like(mu), mu)\n        return mu, logvar\n\n    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Reparameterization trick for VAE.\n\n        Args:\n             mu: mean of the latent distribution\n             logvar: log-variance of the latent distribution\n        Returns:\n                z: sampled latent vector\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Returns the latent space representation of the input.\n\n        Args:\n            x: Input tensor\n        Returns:\n            Latent space representation\n\n        \"\"\"\n        mu, logvar = self.encode(x)\n        return self.reparameterize(mu, logvar)\n\n    def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decode the latent tensor x\n        Args:\n            x: Latent tensor\n        Returns:\n            Decoded tensor, reconstructed from the latent space\n        \"\"\"\n        h = self.d1(x)\n        # here we do a similar thing as in the _encoder,\n        # but instead of ensuring the correct dimension for the latent space,\n        # we ensure the correct dimension for the first Conv2DTranspose layer\n        # so we make sure that the last 3 dimension are (n_filters, reduced_img_dim, reduced_img_dim)\n        h = h.view(-1, self.hidden_dim * 8, self.spatial_dim, self.spatial_dim)\n        return self._decoder(h)\n\n    def translate(self, z: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Reshapes the output to get actual images\n\n        Args:\n            z: Latent tensor\n        Returns:\n            Reconstructed image of shape (C,W,H)\n        \"\"\"\n        out = self.decode(z)\n        return out.view(-1, *self.img_shape)\n\n    def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n        \"\"\"Forward pass of the model.\n        Args:\n            x: Input tensor\n        Returns:\n            ModelOutput object containing the reconstructed tensor and latent tensor\n        \"\"\"\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        return ModelOutput(\n            reconstruction=self.translate(z),\n            latentspace=z,\n            latent_mean=mu,\n            latent_logvar=logvar,\n            additional_info=None,\n        )\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.ImageVAEArchitecture.__init__","title":"<code>__init__(input_dim, config, ontologies=None, feature_order=None)</code>","text":"<p>Initialize the ImageVAEArchitecture with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>Tuple[int, int, int]</code> <p>(C,W,H) the input image shape</p> required <code>config</code> <code>Optional[DefaultConfig]</code> <p>Configuration object containing model parameters.</p> required <code>hidden_dim</code> <p>number of filters in the first convolutional layer</p> required Source code in <code>src/autoencodix/modeling/_imagevae_architecture.py</code> <pre><code>def __init__(\n    self,\n    input_dim: Tuple[int, int, int],  # (C,W,H) the input image shape\n    config: Optional[DefaultConfig],\n    ontologies: Optional[Union[Tuple, Dict]] = None,\n    feature_order: Optional[Union[Tuple, Dict]] = None,\n    # the input_dim is the number of channels in the image, e.g. 3\n):\n    \"\"\"Initialize the ImageVAEArchitecture with the given configuration.\n\n    Args:\n        input_dim: (C,W,H) the input image shape\n        config: Configuration object containing model parameters.\n        hidden_dim: number of filters in the first convolutional layer\n    \"\"\"\n    if config is None:\n        config = DefaultConfig()\n    self._config: DefaultConfig = config\n    super().__init__(config=config, input_dim=input_dim)\n    self.input_dim: int = input_dim\n    self.latent_dim: int = self._config.latent_dim\n    self.nc, self.h, self.w = input_dim\n    self.img_shape: Tuple[int, int, int] = input_dim\n    self.hidden_dim: int = self._config.hidden_dim\n    self._build_network()\n    self.apply(self._init_weights)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.ImageVAEArchitecture.decode","title":"<code>decode(x)</code>","text":"<p>Decode the latent tensor x Args:     x: Latent tensor Returns:     Decoded tensor, reconstructed from the latent space</p> Source code in <code>src/autoencodix/modeling/_imagevae_architecture.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decode the latent tensor x\n    Args:\n        x: Latent tensor\n    Returns:\n        Decoded tensor, reconstructed from the latent space\n    \"\"\"\n    h = self.d1(x)\n    # here we do a similar thing as in the _encoder,\n    # but instead of ensuring the correct dimension for the latent space,\n    # we ensure the correct dimension for the first Conv2DTranspose layer\n    # so we make sure that the last 3 dimension are (n_filters, reduced_img_dim, reduced_img_dim)\n    h = h.view(-1, self.hidden_dim * 8, self.spatial_dim, self.spatial_dim)\n    return self._decoder(h)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.ImageVAEArchitecture.encode","title":"<code>encode(x)</code>","text":"<p>Encodes the input tensor x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:     The encoded latent space representation, or mu and logvar for VAEs.</p> Source code in <code>src/autoencodix/modeling/_imagevae_architecture.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Encodes the input tensor x.\n\n    Args:\n        x: Input tensor\n    Returns:\n        The encoded latent space representation, or mu and logvar for VAEs.\n\n    \"\"\"\n    h = self._encoder(x)\n    # this makes sure we get the &lt;batchsize, 1, latent_dim&gt; shape for our latent space in the next step\n    # because we put all dimensionaltiy in the second dimension of the output shape.\n    # By covering all dimensionality here, we are sure that the rest is\n    h = h.view(-1, self.hidden_dim * 8 * self.spatial_dim * self.spatial_dim)\n    logvar = self.logvar(h)\n    mu = self.mu(h)\n    # prevent  mu and logvar from being too close to zero, this increased\n    # numerical stability\n    logvar = torch.clamp(logvar, 0.1, 20)\n    # replace mu when mu &lt; 0.00000001 with 0.1\n    mu = torch.where(mu &lt; 0.000001, torch.zeros_like(mu), mu)\n    return mu, logvar\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.ImageVAEArchitecture.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model. Args:     x: Input tensor Returns:     ModelOutput object containing the reconstructed tensor and latent tensor</p> Source code in <code>src/autoencodix/modeling/_imagevae_architecture.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n    \"\"\"Forward pass of the model.\n    Args:\n        x: Input tensor\n    Returns:\n        ModelOutput object containing the reconstructed tensor and latent tensor\n    \"\"\"\n    mu, logvar = self.encode(x)\n    z = self.reparameterize(mu, logvar)\n    return ModelOutput(\n        reconstruction=self.translate(z),\n        latentspace=z,\n        latent_mean=mu,\n        latent_logvar=logvar,\n        additional_info=None,\n    )\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.ImageVAEArchitecture.get_latent_space","title":"<code>get_latent_space(x)</code>","text":"<p>Returns the latent space representation of the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:     Latent space representation</p> Source code in <code>src/autoencodix/modeling/_imagevae_architecture.py</code> <pre><code>def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Returns the latent space representation of the input.\n\n    Args:\n        x: Input tensor\n    Returns:\n        Latent space representation\n\n    \"\"\"\n    mu, logvar = self.encode(x)\n    return self.reparameterize(mu, logvar)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.ImageVAEArchitecture.reparameterize","title":"<code>reparameterize(mu, logvar)</code>","text":"<p>Reparameterization trick for VAE.</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>mean of the latent distribution</p> required <code>logvar</code> <code>Tensor</code> <p>log-variance of the latent distribution</p> required <p>Returns:         z: sampled latent vector</p> Source code in <code>src/autoencodix/modeling/_imagevae_architecture.py</code> <pre><code>def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Reparameterization trick for VAE.\n\n    Args:\n         mu: mean of the latent distribution\n         logvar: log-variance of the latent distribution\n    Returns:\n            z: sampled latent vector\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.ImageVAEArchitecture.translate","title":"<code>translate(z)</code>","text":"<p>Reshapes the output to get actual images</p> <p>Parameters:</p> Name Type Description Default <code>z</code> <code>Tensor</code> <p>Latent tensor</p> required <p>Returns:     Reconstructed image of shape (C,W,H)</p> Source code in <code>src/autoencodix/modeling/_imagevae_architecture.py</code> <pre><code>def translate(self, z: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Reshapes the output to get actual images\n\n    Args:\n        z: Latent tensor\n    Returns:\n        Reconstructed image of shape (C,W,H)\n    \"\"\"\n    out = self.decode(z)\n    return out.view(-1, *self.img_shape)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.LayerFactory","title":"<code>LayerFactory</code>","text":"<p>Factory for creating configurable neural network layers.</p> Source code in <code>src/autoencodix/modeling/_layer_factory.py</code> <pre><code>class LayerFactory:\n    \"\"\"Factory for creating configurable neural network layers.\"\"\"\n\n    @staticmethod\n    def get_layer_dimensions(\n        feature_dim: int, latent_dim: int, n_layers: int, enc_factor: float\n    ) -&gt; List[int]:\n        \"\"\"Calculate progressive layer dimensions.\n\n        Args:\n        feature_dim: Input feature dimension\n        latent_dim: Target latent dimension\n        n_layers: Number of layers\n        enc_factor: Reduction factor for layer sizes\n\n        Returns:\n            Calculated layer dimensions\n        \"\"\"\n        if n_layers == 0:\n            return [feature_dim, latent_dim]  # Direct projection from input to latent\n\n        layer_dimensions = [feature_dim]\n        for _ in range(n_layers):\n            prev_layer_size = layer_dimensions[-1]\n            next_layer_size = max(int(prev_layer_size / enc_factor), latent_dim)\n            layer_dimensions.append(next_layer_size)\n        layer_dimensions.append(latent_dim)\n\n        return layer_dimensions\n\n    @staticmethod\n    def create_layer(\n        in_features: int,\n        out_features: int,\n        dropout_p: float = 0.1,\n        last_layer: bool = False,\n    ) -&gt; List[nn.Module]:\n        \"\"\"Create a configurable layer with optional components.\n\n        Args:\n            in_features: Input feature dimension\n            out_features: Output feature dimension\n            dropout_p: Dropout probability, by default 0.1\n            last_layer: Flag to skip activation/dropout for final layer, by default False\n\n        Returns:\n            List of layer components\n        \"\"\"\n        if last_layer:\n            return [nn.Linear(in_features, out_features)]\n\n        return [\n            nn.Linear(in_features, out_features),\n            nn.BatchNorm1d(out_features),\n            nn.Dropout(dropout_p),\n            nn.ReLU(),\n        ]\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.LayerFactory.create_layer","title":"<code>create_layer(in_features, out_features, dropout_p=0.1, last_layer=False)</code>  <code>staticmethod</code>","text":"<p>Create a configurable layer with optional components.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <code>int</code> <p>Input feature dimension</p> required <code>out_features</code> <code>int</code> <p>Output feature dimension</p> required <code>dropout_p</code> <code>float</code> <p>Dropout probability, by default 0.1</p> <code>0.1</code> <code>last_layer</code> <code>bool</code> <p>Flag to skip activation/dropout for final layer, by default False</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Module]</code> <p>List of layer components</p> Source code in <code>src/autoencodix/modeling/_layer_factory.py</code> <pre><code>@staticmethod\ndef create_layer(\n    in_features: int,\n    out_features: int,\n    dropout_p: float = 0.1,\n    last_layer: bool = False,\n) -&gt; List[nn.Module]:\n    \"\"\"Create a configurable layer with optional components.\n\n    Args:\n        in_features: Input feature dimension\n        out_features: Output feature dimension\n        dropout_p: Dropout probability, by default 0.1\n        last_layer: Flag to skip activation/dropout for final layer, by default False\n\n    Returns:\n        List of layer components\n    \"\"\"\n    if last_layer:\n        return [nn.Linear(in_features, out_features)]\n\n    return [\n        nn.Linear(in_features, out_features),\n        nn.BatchNorm1d(out_features),\n        nn.Dropout(dropout_p),\n        nn.ReLU(),\n    ]\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.LayerFactory.get_layer_dimensions","title":"<code>get_layer_dimensions(feature_dim, latent_dim, n_layers, enc_factor)</code>  <code>staticmethod</code>","text":"<p>Calculate progressive layer dimensions.</p> <p>Args: feature_dim: Input feature dimension latent_dim: Target latent dimension n_layers: Number of layers enc_factor: Reduction factor for layer sizes</p> <p>Returns:</p> Type Description <code>List[int]</code> <p>Calculated layer dimensions</p> Source code in <code>src/autoencodix/modeling/_layer_factory.py</code> <pre><code>@staticmethod\ndef get_layer_dimensions(\n    feature_dim: int, latent_dim: int, n_layers: int, enc_factor: float\n) -&gt; List[int]:\n    \"\"\"Calculate progressive layer dimensions.\n\n    Args:\n    feature_dim: Input feature dimension\n    latent_dim: Target latent dimension\n    n_layers: Number of layers\n    enc_factor: Reduction factor for layer sizes\n\n    Returns:\n        Calculated layer dimensions\n    \"\"\"\n    if n_layers == 0:\n        return [feature_dim, latent_dim]  # Direct projection from input to latent\n\n    layer_dimensions = [feature_dim]\n    for _ in range(n_layers):\n        prev_layer_size = layer_dimensions[-1]\n        next_layer_size = max(int(prev_layer_size / enc_factor), latent_dim)\n        layer_dimensions.append(next_layer_size)\n    layer_dimensions.append(latent_dim)\n\n    return layer_dimensions\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.MaskixArchitectureVanilla","title":"<code>MaskixArchitectureVanilla</code>","text":"<p>               Bases: <code>BaseAutoencoder</code></p> <p>Masked Autoencoder Architecture that follows https://doi.org/10.1093/bioinformatics/btae020</p> <p>To closely mimic the publication, the network is not build with our LayerFactory as in other architectures.</p> <p>Attributes:</p> Name Type Description <code>input_dim</code> <code>Union[int, Tuple[int, ...]]</code> <p>number of input features</p> <code>config</code> <p>Configuration object containing model architecture parameters</p> <code>encoder</code> <p>Encoder network of the autoencoder</p> <code>decoder</code> <p>Decoder network of the autoencoder</p> Source code in <code>src/autoencodix/modeling/_maskix_architecture.py</code> <pre><code>class MaskixArchitectureVanilla(BaseAutoencoder):\n    \"\"\"Masked Autoencoder Architecture that follows https://doi.org/10.1093/bioinformatics/btae020\n\n    To closely mimic the publication, the network is not build with our LayerFactory as in\n    other architectures.\n\n    Attributes:\n        input_dim: number of input features\n        config: Configuration object containing model architecture parameters\n        encoder: Encoder network of the autoencoder\n        decoder: Decoder network of the autoencoder\n\n\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[DefaultConfig],\n        input_dim: Union[int, Tuple[int, ...]],\n        ontologies: Optional[Union[Tuple, Dict]] = None,\n        feature_order: Optional[Union[Tuple, Dict]] = None,\n    ):\n        if config is None:\n            config = DefaultConfig()\n        self._config: DefaultConfig = config\n        super().__init__(config, input_dim)\n        self.input_dim: Union[int, Tuple[int, ...]] = input_dim\n        if not isinstance(self.input_dim, int):\n            raise TypeError(\n                f\"input dim needs to be int for MaskixArchitecture, got {type(self.input_dim)}\"\n            )\n        self.latent_dim: int = self._config.latent_dim\n\n        # populate self.encoder and self.decoder\n        self._build_network()\n        self.apply(self._init_weights)\n\n    def _build_network(self):\n        self._encoder = nn.Sequential(\n            nn.Dropout(p=self.config.drop_p),\n            nn.Linear(self.input_dim, self._config.maskix_hidden_dim),\n            nn.LayerNorm(self._config.maskix_hidden_dim),\n            nn.Mish(inplace=True),\n            nn.Linear(self._config.maskix_hidden_dim, self.latent_dim),\n            nn.LayerNorm(self.latent_dim),\n            nn.Mish(inplace=True),\n            nn.Linear(self.latent_dim, self.latent_dim),\n        )\n\n        self._mask_predictor = nn.Linear(self.latent_dim, self.input_dim)\n        self._decoder = nn.Linear(\n            in_features=self.latent_dim + self.input_dim, out_features=self.input_dim\n        )\n\n    def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Encodes the input data.\n\n        Args:\n            x: input Tensor\n        Returns:\n            torch.Tensor\n\n        \"\"\"\n        return self._encoder(x)\n\n    def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Returns the latent space representation of the input data.\n\n        Args:\n            x: input Tensor\n        Returns:\n            torch.Tensor\n\n        \"\"\"\n        return self.encode(x)\n\n    def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decodes the latent representation.\n\n        Args:\n            x: input Tensor\n        Returns:\n            torch.Tensor\n\n        \"\"\"\n        return self._decoder(x)\n\n    def forward_mask(self, x):\n        latent = self.encoder(x)\n        predicted_mask = self.mask_predictor(latent)\n        reconstruction = self.decoder(torch.cat([latent, predicted_mask], dim=1))\n\n        return latent, predicted_mask, reconstruction\n\n    def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n        latent: torch.Tensor = self.encode(x=x)\n        predicted_mask: torch.Tensor = self._mask_predictor(latent)\n        return ModelOutput(\n            reconstruction=self.decode(torch.cat([latent, predicted_mask], dim=1)),\n            latentspace=latent,\n            latent_mean=None,\n            latent_logvar=None,\n            additional_info={\"predicted_mask\": predicted_mask},\n        )\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.MaskixArchitectureVanilla.decode","title":"<code>decode(x)</code>","text":"<p>Decodes the latent representation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input Tensor</p> required <p>Returns:     torch.Tensor</p> Source code in <code>src/autoencodix/modeling/_maskix_architecture.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decodes the latent representation.\n\n    Args:\n        x: input Tensor\n    Returns:\n        torch.Tensor\n\n    \"\"\"\n    return self._decoder(x)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.MaskixArchitectureVanilla.encode","title":"<code>encode(x)</code>","text":"<p>Encodes the input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input Tensor</p> required <p>Returns:     torch.Tensor</p> Source code in <code>src/autoencodix/modeling/_maskix_architecture.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Encodes the input data.\n\n    Args:\n        x: input Tensor\n    Returns:\n        torch.Tensor\n\n    \"\"\"\n    return self._encoder(x)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.MaskixArchitectureVanilla.get_latent_space","title":"<code>get_latent_space(x)</code>","text":"<p>Returns the latent space representation of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input Tensor</p> required <p>Returns:     torch.Tensor</p> Source code in <code>src/autoencodix/modeling/_maskix_architecture.py</code> <pre><code>def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Returns the latent space representation of the input data.\n\n    Args:\n        x: input Tensor\n    Returns:\n        torch.Tensor\n\n    \"\"\"\n    return self.encode(x)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.OntixArchitecture","title":"<code>OntixArchitecture</code>","text":"<p>               Bases: <code>BaseAutoencoder</code></p> <p>Ontology Autoencoder implementation with separate encoder and decoder construction.</p> <p>Attributes: input_dim: number of input features config: Configuration object containing model architecture parameters _encoder: Encoder network of the autoencoder _decoder: Decoder network of the autoencoder mu: Linear layer to compute the mean of the latent distribution logvar: Linear layer to compute the log-variance of the latent distribution masks: Tuple of weight masks for the decoder layers based on ontology latent_dim: Dimension of the latent space, inferred from the first mask ontologies: Ontology information. feature_order: Order of features for input data.</p> Source code in <code>src/autoencodix/modeling/_ontix_architecture.py</code> <pre><code>class OntixArchitecture(BaseAutoencoder):\n    \"\"\"Ontology Autoencoder implementation with separate encoder and decoder construction.\n\n    Attributes:\n    input_dim: number of input features\n    config: Configuration object containing model architecture parameters\n    _encoder: Encoder network of the autoencoder\n    _decoder: Decoder network of the autoencoder\n    mu: Linear layer to compute the mean of the latent distribution\n    logvar: Linear layer to compute the log-variance of the latent distribution\n    masks: Tuple of weight masks for the decoder layers based on ontology\n    latent_dim: Dimension of the latent space, inferred from the first mask\n    ontologies: Ontology information.\n    feature_order: Order of features for input data.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[Union[None, DefaultConfig]],\n        input_dim: int,\n        ontologies: tuple,\n        feature_order: list,\n    ) -&gt; None:\n        \"\"\"Initialize the Vanilla Autoencoder with the given configuration.\n\n        Args:\n            config: Configuration object containing model parameters.\n            input_dim: Number of input features.\n            ontologies: Ontology information.\n            feature_order: Order of features for input data.\n        \"\"\"\n        if config is None:\n            config = DefaultConfig()\n        self._config = config\n        super().__init__(config, input_dim)\n        self.input_dim = input_dim\n        self._mu: nn.Module\n        self._logvar: nn.Module\n        # create masks for sparse decoder\n        self.ontologies = ontologies\n        self.feature_order = feature_order\n        self.masks = self._make_masks(config=self._config, feature_order=feature_order)\n        self.latent_dim = self.masks[0].shape[1]\n        print(\"Latent Dim: \" + str(self.latent_dim))\n        # populate self.encoder and self.decoder\n        self._build_network()\n        self.apply(self._init_weights)\n        self._decoder.apply(\n            self._positive_dec\n        )  # Sparse decoder only has positive weights\n\n        # Apply weight mask to create ontology-based decoder\n        with torch.no_grad():\n            # Check that the decoder has the same number of layers as masks\n            if len(self.masks) != len(self._decoder):\n                print(len(self.masks), len(self._decoder))\n                print(self._encoder)\n                print(self._decoder)\n                raise ValueError(\n                    \"Number of masks does not match number of decoder layers\"\n                )\n            else:\n                for i, mask in enumerate(self.masks):\n                    self._decoder[i].weight.mul_(mask)\n\n    def _make_masks(\n        self, config: DefaultConfig, feature_order: list\n    ) -&gt; Tuple[torch.Tensor, ...]:\n        \"\"\"Create masks for sparse decoder based on ontology via config\n\n        Args:\n            config: Configuration object containing model parameters\n            feature_order: Order of features for input data\n\n        Returns:\n            Tuple containing the masks for the decoder network\n\n        \"\"\"\n        # Read ontology from config\n\n        masks = tuple()\n        # feature_names are all values in the last ontology layer\n        all_feature_names = set()\n        for key, values in self.ontologies[-1].items():\n            all_feature_names.update(values)\n        all_feature_names = list(all_feature_names)\n        print(\"Ontix checks:\")\n        print(f\"All possible feature names length: {len(all_feature_names)}\")\n        print(f\"Feature order length: {len(feature_order)}\")\n        # Check if all features in feature_order are present in all_feature_names\n        feature_names = [f for f in feature_order]\n        missing_features = [f for f in feature_order if f not in all_feature_names]\n        if missing_features:\n            print(\n                f\"Features in feature_order not found in all_feature_names: {missing_features}\"\n            )\n        print(f\"Feature names without filtering: {len(feature_names)}\")\n\n        # Enumerate through the ontologies\n        for x, ont_dic in enumerate(self.ontologies):\n            prev_lay_dim = len(ont_dic.keys())\n\n            if x == len(self.ontologies) - 1:\n                # fixed sort of feature list\n                node_list = feature_names\n            else:\n                node_list = list(self.ontologies[x + 1].keys())\n            next_lay_dim = len(node_list)\n            # create masks for sparse decoder\n            mask = torch.zeros(next_lay_dim, prev_lay_dim)\n            p_int = 0\n            if len(node_list) == next_lay_dim:\n                if len(ont_dic.keys()) == prev_lay_dim:\n                    for p_id in ont_dic:\n                        feature_list = ont_dic[p_id]\n                        for f_id in feature_list:\n                            if f_id in node_list:\n                                f_int = node_list.index(f_id)\n                                mask[f_int, p_int] = 1\n\n                        p_int += 1\n                else:\n                    print(\n                        \"Mask layer cannot be calculated. Ontology key list does not match previous layer dimension\"\n                    )\n                    print(\"Returning zero mask\")\n            else:\n                print(f\"node list: {len(node_list)} vs next_lay_dim:{next_lay_dim}\")\n                print(\n                    \"Mask layer cannot be calculated. Output layer list does not match next layer dimension\"\n                )\n                print(\"Returning zero mask\")\n            print(\n                f\"Mask layer {x} with shape {mask.shape} and {torch.sum(mask)} connections\"\n            )\n            masks += (mask,)\n\n        if torch.max(mask) &lt; 1:\n            print(\n                \"You provided an ontology with no connections between layers in the decoder. Please check your ontology definition.\"\n            )\n\n        return masks\n\n    def _build_network(self) -&gt; None:\n        \"\"\"Construct the encoder and decoder networks.\n\n        Handles cases where `n_layers=0` by skipping the encoder and using only mu/logvar.\n        \"\"\"\n        #### Encoder copied from varix architecture ####\n        enc_dim = LayerFactory.get_layer_dimensions(\n            feature_dim=self.input_dim,\n            latent_dim=self.latent_dim,\n            n_layers=self._config.n_layers,\n            enc_factor=self._config.enc_factor,\n        )\n        #\n\n        # Case 1: No Hidden Layers (Direct Mapping)\n        self._encoder = nn.Sequential()\n        self._mu = nn.Linear(self.input_dim, self.latent_dim)\n        self._logvar = nn.Linear(self.input_dim, self.latent_dim)\n\n        # Case 2: At Least One Hidden Layer\n        if self._config.n_layers &gt; 0:\n            encoder_layers = []\n            # print(enc_dim)\n            for i, (in_features, out_features) in enumerate(\n                zip(enc_dim[:-1], enc_dim[1:])\n            ):\n                # since we add mu and logvar, we will remove the last layer\n                if i == len(enc_dim) - 2:\n                    break\n                encoder_layers.extend(\n                    LayerFactory.create_layer(\n                        in_features=in_features,\n                        out_features=out_features,\n                        dropout_p=self._config.drop_p,\n                        last_layer=False,  # only for decoder relevant\n                    )\n                )\n\n            self._encoder = nn.Sequential(*encoder_layers)\n            self._mu = nn.Linear(enc_dim[-2], self.latent_dim)\n            self._logvar = nn.Linear(enc_dim[-2], self.latent_dim)\n        #### Encoder copied from varix architecture ####\n\n        # Construct Decoder with Sparse Connections via masks\n        # Decoder dimension is determined by the masks\n        dec_dim = [self.latent_dim] + [\n            mask.shape[0] for mask in self.masks\n        ]  # + [self.input_dim]\n        decoder_layers = []\n        for i, (in_features, out_features) in enumerate(zip(dec_dim[:-1], dec_dim[1:])):\n            # last_layer = i == len(dec_dim) - 2\n            last_layer = True  ## Only linear layers in sparse decoder\n            decoder_layers.extend(\n                LayerFactory.create_layer(\n                    in_features=in_features,\n                    out_features=out_features,\n                    dropout_p=0,  ## No dropout in sparse decoder\n                    last_layer=last_layer,\n                    # only_linear=True,\n                )\n            )\n\n        self._decoder = nn.Sequential(*decoder_layers)\n\n    def _positive_dec(self, m):\n        if isinstance(m, nn.Linear):\n            m.weight.data = m.weight.data.clamp(min=0)\n\n    def encode(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Encode the input tensor x\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Encoded tensor\n        \"\"\"\n        latent = x  # for case where n_layers=0\n        if len(self._encoder) &gt; 0:\n            latent = self._encoder(x)\n        mu = self._mu(latent)\n        logvar = self._logvar(latent)\n        # numeric stability\n        logvar = torch.clamp(logvar, 0.01, 20)\n        mu = torch.where(mu &lt; 0.0000001, torch.zeros_like(mu), mu)\n        return mu, logvar\n\n    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Reparameterization trick for VAE\n\n        Args:\n            mu: Mean tensor\n            logvar: Log variance tensor\n\n        Returns:\n            Reparameterized latent tensor\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decode the latent tensor x\n\n        Args:\n            x: Latent tensor\n\n        Returns:\n            torch.Tensor\n            Decoded tensor\n\n        \"\"\"\n        return self._decoder(x)\n\n    def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n        \"\"\"Forward pass of the model, fill\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            ModelOutput object containing the reconstructed tensor and latent tensor\n\n        \"\"\"\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        x_hat = self.decode(z)\n        return ModelOutput(\n            reconstruction=x_hat,\n            latentspace=z,\n            latent_mean=mu,\n            latent_logvar=logvar,\n            additional_info=None,\n        )\n\n    def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Returns the latent space representation of the input.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Latent space representation\n\n        \"\"\"\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        return z\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.OntixArchitecture.__init__","title":"<code>__init__(config, input_dim, ontologies, feature_order)</code>","text":"<p>Initialize the Vanilla Autoencoder with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[None, DefaultConfig]]</code> <p>Configuration object containing model parameters.</p> required <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required <code>ontologies</code> <code>tuple</code> <p>Ontology information.</p> required <code>feature_order</code> <code>list</code> <p>Order of features for input data.</p> required Source code in <code>src/autoencodix/modeling/_ontix_architecture.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[Union[None, DefaultConfig]],\n    input_dim: int,\n    ontologies: tuple,\n    feature_order: list,\n) -&gt; None:\n    \"\"\"Initialize the Vanilla Autoencoder with the given configuration.\n\n    Args:\n        config: Configuration object containing model parameters.\n        input_dim: Number of input features.\n        ontologies: Ontology information.\n        feature_order: Order of features for input data.\n    \"\"\"\n    if config is None:\n        config = DefaultConfig()\n    self._config = config\n    super().__init__(config, input_dim)\n    self.input_dim = input_dim\n    self._mu: nn.Module\n    self._logvar: nn.Module\n    # create masks for sparse decoder\n    self.ontologies = ontologies\n    self.feature_order = feature_order\n    self.masks = self._make_masks(config=self._config, feature_order=feature_order)\n    self.latent_dim = self.masks[0].shape[1]\n    print(\"Latent Dim: \" + str(self.latent_dim))\n    # populate self.encoder and self.decoder\n    self._build_network()\n    self.apply(self._init_weights)\n    self._decoder.apply(\n        self._positive_dec\n    )  # Sparse decoder only has positive weights\n\n    # Apply weight mask to create ontology-based decoder\n    with torch.no_grad():\n        # Check that the decoder has the same number of layers as masks\n        if len(self.masks) != len(self._decoder):\n            print(len(self.masks), len(self._decoder))\n            print(self._encoder)\n            print(self._decoder)\n            raise ValueError(\n                \"Number of masks does not match number of decoder layers\"\n            )\n        else:\n            for i, mask in enumerate(self.masks):\n                self._decoder[i].weight.mul_(mask)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.OntixArchitecture.decode","title":"<code>decode(x)</code>","text":"<p>Decode the latent tensor x</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Latent tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor</p> <code>Tensor</code> <p>Decoded tensor</p> Source code in <code>src/autoencodix/modeling/_ontix_architecture.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decode the latent tensor x\n\n    Args:\n        x: Latent tensor\n\n    Returns:\n        torch.Tensor\n        Decoded tensor\n\n    \"\"\"\n    return self._decoder(x)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.OntixArchitecture.encode","title":"<code>encode(x)</code>","text":"<p>Encode the input tensor x</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Encoded tensor</p> Source code in <code>src/autoencodix/modeling/_ontix_architecture.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Encode the input tensor x\n\n    Args:\n        x: Input tensor\n\n    Returns:\n        Encoded tensor\n    \"\"\"\n    latent = x  # for case where n_layers=0\n    if len(self._encoder) &gt; 0:\n        latent = self._encoder(x)\n    mu = self._mu(latent)\n    logvar = self._logvar(latent)\n    # numeric stability\n    logvar = torch.clamp(logvar, 0.01, 20)\n    mu = torch.where(mu &lt; 0.0000001, torch.zeros_like(mu), mu)\n    return mu, logvar\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.OntixArchitecture.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model, fill</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>ModelOutput object containing the reconstructed tensor and latent tensor</p> Source code in <code>src/autoencodix/modeling/_ontix_architecture.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n    \"\"\"Forward pass of the model, fill\n\n    Args:\n        x: Input tensor\n\n    Returns:\n        ModelOutput object containing the reconstructed tensor and latent tensor\n\n    \"\"\"\n    mu, logvar = self.encode(x)\n    z = self.reparameterize(mu, logvar)\n    x_hat = self.decode(z)\n    return ModelOutput(\n        reconstruction=x_hat,\n        latentspace=z,\n        latent_mean=mu,\n        latent_logvar=logvar,\n        additional_info=None,\n    )\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.OntixArchitecture.get_latent_space","title":"<code>get_latent_space(x)</code>","text":"<p>Returns the latent space representation of the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Latent space representation</p> Source code in <code>src/autoencodix/modeling/_ontix_architecture.py</code> <pre><code>def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Returns the latent space representation of the input.\n\n    Args:\n        x: Input tensor\n\n    Returns:\n        Latent space representation\n\n    \"\"\"\n    mu, logvar = self.encode(x)\n    z = self.reparameterize(mu, logvar)\n    return z\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.OntixArchitecture.reparameterize","title":"<code>reparameterize(mu, logvar)</code>","text":"<p>Reparameterization trick for VAE</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>Mean tensor</p> required <code>logvar</code> <code>Tensor</code> <p>Log variance tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Reparameterized latent tensor</p> Source code in <code>src/autoencodix/modeling/_ontix_architecture.py</code> <pre><code>def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Reparameterization trick for VAE\n\n    Args:\n        mu: Mean tensor\n        logvar: Log variance tensor\n\n    Returns:\n        Reparameterized latent tensor\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VanillixArchitecture","title":"<code>VanillixArchitecture</code>","text":"<p>               Bases: <code>BaseAutoencoder</code></p> <p>Vanilla Autoencoder implementation with separate encoder and decoder construction.</p> <p>Attributes:</p> Name Type Description <code>input_dim</code> <p>number of input features</p> <code>config</code> <p>Configuration object containing model architecture parameters</p> <code>encoder</code> <p>Encoder network of the autoencoder</p> <code>decoder</code> <p>Decoder network of the autoencoder</p> Source code in <code>src/autoencodix/modeling/_vanillix_architecture.py</code> <pre><code>class VanillixArchitecture(BaseAutoencoder):\n    \"\"\"Vanilla Autoencoder implementation with separate encoder and decoder construction.\n\n    Attributes:\n        input_dim: number of input features\n        config: Configuration object containing model architecture parameters\n        encoder: Encoder network of the autoencoder\n        decoder: Decoder network of the autoencoder\n\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[Union[None, DefaultConfig]],\n        input_dim: int,\n        ontologies: Optional[Union[Tuple, Dict]] = None,\n        feature_order: Optional[Union[Tuple, Dict]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Vanilla Autoencoder with the given configuration.\n\n        Args:\n            config: Configuration object containing model parameters.\n            input_dim: Number of input features.\n        \"\"\"\n\n        if config is None:\n            config = DefaultConfig()\n        self._config = config\n        super().__init__(config, input_dim)\n        self.input_dim = input_dim\n\n        # populate self.encoder and self.decoder\n        self._build_network()\n        self.apply(self._init_weights)\n\n    def _build_network(self) -&gt; None:\n        \"\"\"Construct the encoder with linear layers.\"\"\"\n        # Calculate layer dimensions\n        enc_dim = LayerFactory.get_layer_dimensions(\n            feature_dim=self.input_dim,\n            latent_dim=self._config.latent_dim,\n            n_layers=self._config.n_layers,\n            enc_factor=self._config.enc_factor,\n        )\n\n        encoder_layers = []\n        for i, (in_features, out_features) in enumerate(zip(enc_dim[:-1], enc_dim[1:])):\n            last_layer = i == len(enc_dim) - 2\n            encoder_layers.extend(\n                LayerFactory.create_layer(\n                    in_features=in_features,\n                    out_features=out_features,\n                    dropout_p=self._config.drop_p,\n                    last_layer=last_layer,\n                )\n            )\n\n        dec_dim = enc_dim[::-1]  # Reverse the dimensions and copy\n        decoder_layers = []\n        for i, (in_features, out_features) in enumerate(zip(dec_dim[:-1], dec_dim[1:])):\n            last_layer = i == len(dec_dim) - 2\n            decoder_layers.extend(\n                LayerFactory.create_layer(\n                    in_features=in_features,\n                    out_features=out_features,\n                    dropout_p=self._config.drop_p,\n                    last_layer=last_layer,\n                )\n            )\n        self._encoder = nn.Sequential(*encoder_layers)\n        self._decoder = nn.Sequential(*decoder_layers)\n\n    def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Encodes the input data.\n\n        Args:\n            x: input Tensor\n        Returns:\n            torch.Tensor\n\n        \"\"\"\n        return self._encoder(x)\n\n    def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Returns the latent space representation of the input data.\n\n        Args:\n            x: input Tensor\n        Returns:\n            torch.Tensor\n\n        \"\"\"\n        return self.encode(x)\n\n    def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decodes the latent representation.\n\n        Args:\n            x: input Tensor\n        Returns:\n            torch.Tensor\n\n        \"\"\"\n        return self._decoder(x)\n\n    def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n        \"\"\"Forward pass of the model.\n\n        Args:\n            x: input Tensor\n        Returns:\n            ModelOutput\n\n        \"\"\"\n        latent = self.encode(x)\n        return ModelOutput(\n            reconstruction=self.decode(latent),\n            latentspace=latent,\n            latent_mean=None,\n            latent_logvar=None,\n            additional_info=None,\n        )\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VanillixArchitecture.__init__","title":"<code>__init__(config, input_dim, ontologies=None, feature_order=None)</code>","text":"<p>Initialize the Vanilla Autoencoder with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[None, DefaultConfig]]</code> <p>Configuration object containing model parameters.</p> required <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required Source code in <code>src/autoencodix/modeling/_vanillix_architecture.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[Union[None, DefaultConfig]],\n    input_dim: int,\n    ontologies: Optional[Union[Tuple, Dict]] = None,\n    feature_order: Optional[Union[Tuple, Dict]] = None,\n) -&gt; None:\n    \"\"\"Initialize the Vanilla Autoencoder with the given configuration.\n\n    Args:\n        config: Configuration object containing model parameters.\n        input_dim: Number of input features.\n    \"\"\"\n\n    if config is None:\n        config = DefaultConfig()\n    self._config = config\n    super().__init__(config, input_dim)\n    self.input_dim = input_dim\n\n    # populate self.encoder and self.decoder\n    self._build_network()\n    self.apply(self._init_weights)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VanillixArchitecture.decode","title":"<code>decode(x)</code>","text":"<p>Decodes the latent representation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input Tensor</p> required <p>Returns:     torch.Tensor</p> Source code in <code>src/autoencodix/modeling/_vanillix_architecture.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decodes the latent representation.\n\n    Args:\n        x: input Tensor\n    Returns:\n        torch.Tensor\n\n    \"\"\"\n    return self._decoder(x)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VanillixArchitecture.encode","title":"<code>encode(x)</code>","text":"<p>Encodes the input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input Tensor</p> required <p>Returns:     torch.Tensor</p> Source code in <code>src/autoencodix/modeling/_vanillix_architecture.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Encodes the input data.\n\n    Args:\n        x: input Tensor\n    Returns:\n        torch.Tensor\n\n    \"\"\"\n    return self._encoder(x)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VanillixArchitecture.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input Tensor</p> required <p>Returns:     ModelOutput</p> Source code in <code>src/autoencodix/modeling/_vanillix_architecture.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n    \"\"\"Forward pass of the model.\n\n    Args:\n        x: input Tensor\n    Returns:\n        ModelOutput\n\n    \"\"\"\n    latent = self.encode(x)\n    return ModelOutput(\n        reconstruction=self.decode(latent),\n        latentspace=latent,\n        latent_mean=None,\n        latent_logvar=None,\n        additional_info=None,\n    )\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VanillixArchitecture.get_latent_space","title":"<code>get_latent_space(x)</code>","text":"<p>Returns the latent space representation of the input data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>input Tensor</p> required <p>Returns:     torch.Tensor</p> Source code in <code>src/autoencodix/modeling/_vanillix_architecture.py</code> <pre><code>def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Returns the latent space representation of the input data.\n\n    Args:\n        x: input Tensor\n    Returns:\n        torch.Tensor\n\n    \"\"\"\n    return self.encode(x)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VarixArchitecture","title":"<code>VarixArchitecture</code>","text":"<p>               Bases: <code>BaseAutoencoder</code></p> <p>Variational Autoencoder implementation with separate encoder and decoder construction.</p> <p>Attributes:</p> Name Type Description <code>input_dim</code> <code>int</code> <p>number of input features</p> <code>config</code> <p>Configuration object containing model architecture parameters</p> <code>encoder</code> <p>Encoder network of the autoencoder</p> <code>decoder</code> <p>Decoder network of the autoencoder</p> <code>mu</code> <p>Linear layer to compute the mean of the latent distribution</p> <p>logvar: Linear layer to compute the log-variance of the latent distribution</p> Source code in <code>src/autoencodix/modeling/_varix_architecture.py</code> <pre><code>class VarixArchitecture(BaseAutoencoder):\n    \"\"\"Variational Autoencoder implementation with separate encoder and decoder construction.\n\n    Attributes:\n        input_dim: number of input features\n        config: Configuration object containing model architecture parameters\n        encoder: Encoder network of the autoencoder\n        decoder: Decoder network of the autoencoder\n        mu: Linear layer to compute the mean of the latent distribution\n    logvar: Linear layer to compute the log-variance of the latent distribution\n\n    \"\"\"\n\n    def __init__(\n        self,\n        config: Optional[Union[None, DefaultConfig]],\n        input_dim: int,\n        ontologies: Optional[Union[Tuple, Dict]] = None,\n        feature_order: Optional[Union[Tuple, Dict]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Vanilla Autoencoder with the given configuration.\n\n        Args:\n            config: Configuration object containing model parameters.\n            input_dim: Number of input features.\n        \"\"\"\n\n        if config is None:\n            config = DefaultConfig()\n        self._config: DefaultConfig = config\n        super().__init__(config=config, input_dim=input_dim)\n        self.input_dim: int = input_dim\n        self._mu: nn.Module\n        self._logvar: nn.Module\n\n        # populate self.encoder and self.decoder\n        self._build_network()\n        self.apply(self._init_weights)\n\n    def _build_network(self) -&gt; None:\n        \"\"\"Construct the encoder and decoder networks.\n\n        Handles cases where `n_layers=0` by skipping the encoder and using only mu/logvar.\n        \"\"\"\n        enc_dim = LayerFactory.get_layer_dimensions(\n            feature_dim=self.input_dim,\n            latent_dim=self._config.latent_dim,\n            n_layers=self._config.n_layers,\n            enc_factor=self._config.enc_factor,\n        )\n        #\n\n        # Case 1: No Hidden Layers (Direct Mapping)\n        self._encoder = nn.Sequential()\n        self._mu = nn.Linear(self.input_dim, self._config.latent_dim)\n        self._logvar = nn.Linear(self.input_dim, self._config.latent_dim)\n\n        # Case 2: At Least One Hidden Layer\n        if self._config.n_layers &gt; 0:\n            encoder_layers = []\n            # print(enc_dim)\n            for i, (in_features, out_features) in enumerate(\n                zip(enc_dim[:-1], enc_dim[1:])\n            ):\n                # since we add mu and logvar, we will remove the last layer\n                if i == len(enc_dim) - 2:\n                    break\n                encoder_layers.extend(\n                    LayerFactory.create_layer(\n                        in_features=in_features,\n                        out_features=out_features,\n                        dropout_p=self._config.drop_p,\n                        last_layer=False,  # only for decoder relevant\n                    )\n                )\n\n            self._encoder = nn.Sequential(*encoder_layers)\n            self._mu = nn.Linear(enc_dim[-2], self._config.latent_dim)\n            self._logvar = nn.Linear(enc_dim[-2], self._config.latent_dim)\n\n        # Construct Decoder (Same for Both Cases)\n        dec_dim = enc_dim[::-1]  # Reverse the dimensions and copy\n        decoder_layers = []\n        for i, (in_features, out_features) in enumerate(zip(dec_dim[:-1], dec_dim[1:])):\n            last_layer = i == len(dec_dim) - 2\n            decoder_layers.extend(\n                LayerFactory.create_layer(\n                    in_features=in_features,\n                    out_features=out_features,\n                    dropout_p=self._config.drop_p,\n                    last_layer=last_layer,\n                )\n            )\n\n        self._decoder = nn.Sequential(*decoder_layers)\n\n    def encode(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Encode the input tensor x.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Encoded tensor\n\n        \"\"\"\n\n        latent = x  # for case where n_layers=0\n        if len(self._encoder) &gt; 0:\n            latent = self._encoder(x)\n        mu = self._mu(latent)\n        logvar = self._logvar(latent)\n        # numeric stability\n        logvar = torch.clamp(logvar, 0.01, 20)\n        mu = torch.where(mu &lt; 0.0000001, torch.zeros_like(mu), mu)\n        return mu, logvar\n\n    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Reparameterization trick for VAE\n\n        Args:\n            mu: torch.Tensor\n            logvar: torch.Tensor\n\n        Returns:\n            torch.Tensor\n\n        \"\"\"\n        std = torch.exp(0.5 * logvar)\n        eps = torch.randn_like(std)\n        return mu + eps * std\n\n    def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Returns the latent space representation of the input.\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            Latent space representation\n\n        \"\"\"\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        return z\n\n    def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"Decode the latent tensor x\n\n        Args:\n            x: Latent tensor\n\n        Returns:\n            Decoded tensor\n        \"\"\"\n\n        return self._decoder(x)\n\n    def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n        \"\"\"Forward pass of the model, fill\n\n        Args:\n            x: Input tensor\n\n        Returns:\n            ModelOutput object containing the reconstructed tensor and latent tensor\n\n        \"\"\"\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        x_hat = self.decode(z)\n        return ModelOutput(\n            reconstruction=x_hat,\n            latentspace=z,\n            latent_mean=mu,\n            latent_logvar=logvar,\n            additional_info=None,\n        )\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VarixArchitecture.__init__","title":"<code>__init__(config, input_dim, ontologies=None, feature_order=None)</code>","text":"<p>Initialize the Vanilla Autoencoder with the given configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Optional[Union[None, DefaultConfig]]</code> <p>Configuration object containing model parameters.</p> required <code>input_dim</code> <code>int</code> <p>Number of input features.</p> required Source code in <code>src/autoencodix/modeling/_varix_architecture.py</code> <pre><code>def __init__(\n    self,\n    config: Optional[Union[None, DefaultConfig]],\n    input_dim: int,\n    ontologies: Optional[Union[Tuple, Dict]] = None,\n    feature_order: Optional[Union[Tuple, Dict]] = None,\n) -&gt; None:\n    \"\"\"Initialize the Vanilla Autoencoder with the given configuration.\n\n    Args:\n        config: Configuration object containing model parameters.\n        input_dim: Number of input features.\n    \"\"\"\n\n    if config is None:\n        config = DefaultConfig()\n    self._config: DefaultConfig = config\n    super().__init__(config=config, input_dim=input_dim)\n    self.input_dim: int = input_dim\n    self._mu: nn.Module\n    self._logvar: nn.Module\n\n    # populate self.encoder and self.decoder\n    self._build_network()\n    self.apply(self._init_weights)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VarixArchitecture.decode","title":"<code>decode(x)</code>","text":"<p>Decode the latent tensor x</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Latent tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Decoded tensor</p> Source code in <code>src/autoencodix/modeling/_varix_architecture.py</code> <pre><code>def decode(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Decode the latent tensor x\n\n    Args:\n        x: Latent tensor\n\n    Returns:\n        Decoded tensor\n    \"\"\"\n\n    return self._decoder(x)\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VarixArchitecture.encode","title":"<code>encode(x)</code>","text":"<p>Encode the input tensor x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor]</code> <p>Encoded tensor</p> Source code in <code>src/autoencodix/modeling/_varix_architecture.py</code> <pre><code>def encode(self, x: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Encode the input tensor x.\n\n    Args:\n        x: Input tensor\n\n    Returns:\n        Encoded tensor\n\n    \"\"\"\n\n    latent = x  # for case where n_layers=0\n    if len(self._encoder) &gt; 0:\n        latent = self._encoder(x)\n    mu = self._mu(latent)\n    logvar = self._logvar(latent)\n    # numeric stability\n    logvar = torch.clamp(logvar, 0.01, 20)\n    mu = torch.where(mu &lt; 0.0000001, torch.zeros_like(mu), mu)\n    return mu, logvar\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VarixArchitecture.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass of the model, fill</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>ModelOutput object containing the reconstructed tensor and latent tensor</p> Source code in <code>src/autoencodix/modeling/_varix_architecture.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; ModelOutput:\n    \"\"\"Forward pass of the model, fill\n\n    Args:\n        x: Input tensor\n\n    Returns:\n        ModelOutput object containing the reconstructed tensor and latent tensor\n\n    \"\"\"\n    mu, logvar = self.encode(x)\n    z = self.reparameterize(mu, logvar)\n    x_hat = self.decode(z)\n    return ModelOutput(\n        reconstruction=x_hat,\n        latentspace=z,\n        latent_mean=mu,\n        latent_logvar=logvar,\n        additional_info=None,\n    )\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VarixArchitecture.get_latent_space","title":"<code>get_latent_space(x)</code>","text":"<p>Returns the latent space representation of the input.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>Latent space representation</p> Source code in <code>src/autoencodix/modeling/_varix_architecture.py</code> <pre><code>def get_latent_space(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Returns the latent space representation of the input.\n\n    Args:\n        x: Input tensor\n\n    Returns:\n        Latent space representation\n\n    \"\"\"\n    mu, logvar = self.encode(x)\n    z = self.reparameterize(mu, logvar)\n    return z\n</code></pre>"},{"location":"api/modeling/#autoencodix.modeling.VarixArchitecture.reparameterize","title":"<code>reparameterize(mu, logvar)</code>","text":"<p>Reparameterization trick for VAE</p> <p>Parameters:</p> Name Type Description Default <code>mu</code> <code>Tensor</code> <p>torch.Tensor</p> required <code>logvar</code> <code>Tensor</code> <p>torch.Tensor</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor</p> Source code in <code>src/autoencodix/modeling/_varix_architecture.py</code> <pre><code>def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Reparameterization trick for VAE\n\n    Args:\n        mu: torch.Tensor\n        logvar: torch.Tensor\n\n    Returns:\n        torch.Tensor\n\n    \"\"\"\n    std = torch.exp(0.5 * logvar)\n    eps = torch.randn_like(std)\n    return mu + eps * std\n</code></pre>"},{"location":"api/ontix/","title":"Ontix Module","text":""},{"location":"api/ontix/#autoencodix.ontix.Ontix","title":"<code>Ontix</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>Ontix specific version of the BasePipeline class.</p> <p>Inherits preprocess, fit, predict, evaluate, and visualize methods from BasePipeline.</p> <p>This class extends BasePipeline. See the parent class for a full list of attributes and methods.</p> Additional Attributes <p>_default_config: Is set to OntixConfig here.</p> Source code in <code>src/autoencodix/ontix.py</code> <pre><code>class Ontix(BasePipeline):\n    \"\"\"Ontix specific version of the BasePipeline class.\n\n    Inherits preprocess, fit, predict, evaluate, and visualize methods from BasePipeline.\n\n    This class extends BasePipeline. See the parent class for a full list\n    of attributes and methods.\n\n    Additional Attributes:\n        _default_config: Is set to OntixConfig here.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        ontologies: Union[Tuple, List],  # Addition to Varix, mandotory for Ontix\n        sep: Optional[str] = \"\\t\",  # Addition to Varix, optional to read in ontologies\n        data: Optional[Union[DataPackage, DatasetContainer]] = None,\n        trainer_type: Type[BaseTrainer] = OntixTrainer,\n        dataset_type: Type[BaseDataset] = NumericDataset,\n        model_type: Type[BaseAutoencoder] = OntixArchitecture,\n        loss_type: Type[BaseLoss] = VarixLoss,\n        preprocessor_type: Type[BasePreprocessor] = GeneralPreprocessor,\n        visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n        evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n        result: Optional[Result] = None,\n        datasplitter_type: Type[DataSplitter] = DataSplitter,\n        custom_splits: Optional[Dict[str, np.ndarray]] = None,\n        config: Optional[DefaultConfig] = None,\n    ) -&gt; None:\n        \"\"\"Initialize Ontix pipeline with customizable components.\n\n        Some components are passed as types rather than instances because they require\n        data that is only available after preprocessing.\n\n        See parent class for full list of Arguments.\n\n        Raises:\n            TypeError: if ontologies are not a Tuple or List.\n\n        \"\"\"\n        self._default_config = OntixConfig()\n        if isinstance(ontologies, tuple):\n            self.ontologies = ontologies\n        elif isinstance(ontologies, list):\n            if sep is None:\n                raise ValueError(\n                    \"If ontologies are provided as a list, the seperator 'sep' cannot be None. \"\n                )\n            ontologies_dict_list = [\n                self._read_ont_file(ont_file, sep=sep) for ont_file in ontologies\n            ]\n            self.ontologies = tuple(ontologies_dict_list)\n        else:\n            raise TypeError(\n                f\"Expected ontologies to be of type tuple or list, got {type(ontologies)}.\"\n            )\n\n        super().__init__(\n            data=data,\n            dataset_type=dataset_type,\n            trainer_type=trainer_type,\n            model_type=model_type,\n            loss_type=loss_type,\n            preprocessor_type=preprocessor_type,\n            visualizer=visualizer,\n            evaluator=evaluator,\n            result=result,\n            datasplitter_type=datasplitter_type,\n            config=config,\n            custom_split=custom_splits,\n            ontologies=self.ontologies,\n        )\n        if not isinstance(self.config, OntixConfig):\n            raise TypeError(\n                f\"For Ontix Pipeline, we only allow OntixConfig as type for config, got {type(self.config)}\"\n            )\n\n    def _read_ont_file(self, file_path: str, sep: str = \"\\t\") -&gt; dict:\n        \"\"\"Function to read-in text files of ontologies with format child - separator - parent into an dictionary.\n\n        Args:\n            file_path: Path to file with ontology\n            sep: Separator used in file\n        Returns:\n            ont_dic: Dictionary containing the ontology as described in the text file.\n\n        \"\"\"\n        ont_dic = dict()\n        with open(file_path, \"r\") as ont_file:\n            for line in ont_file:\n                id_parent = line.strip().split(sep)[1]\n                id_child = line.split(sep)[0]\n\n                if id_parent in ont_dic:\n                    ont_dic[id_parent].append(id_child)\n                else:\n                    ont_dic[id_parent] = list()\n                    ont_dic[id_parent].append(id_child)\n\n        return ont_dic\n</code></pre>"},{"location":"api/ontix/#autoencodix.ontix.Ontix.__init__","title":"<code>__init__(ontologies, sep='\\t', data=None, trainer_type=OntixTrainer, dataset_type=NumericDataset, model_type=OntixArchitecture, loss_type=VarixLoss, preprocessor_type=GeneralPreprocessor, visualizer=GeneralVisualizer, evaluator=GeneralEvaluator, result=None, datasplitter_type=DataSplitter, custom_splits=None, config=None)</code>","text":"<p>Initialize Ontix pipeline with customizable components.</p> <p>Some components are passed as types rather than instances because they require data that is only available after preprocessing.</p> <p>See parent class for full list of Arguments.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>if ontologies are not a Tuple or List.</p> Source code in <code>src/autoencodix/ontix.py</code> <pre><code>def __init__(\n    self,\n    ontologies: Union[Tuple, List],  # Addition to Varix, mandotory for Ontix\n    sep: Optional[str] = \"\\t\",  # Addition to Varix, optional to read in ontologies\n    data: Optional[Union[DataPackage, DatasetContainer]] = None,\n    trainer_type: Type[BaseTrainer] = OntixTrainer,\n    dataset_type: Type[BaseDataset] = NumericDataset,\n    model_type: Type[BaseAutoencoder] = OntixArchitecture,\n    loss_type: Type[BaseLoss] = VarixLoss,\n    preprocessor_type: Type[BasePreprocessor] = GeneralPreprocessor,\n    visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n    evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n    result: Optional[Result] = None,\n    datasplitter_type: Type[DataSplitter] = DataSplitter,\n    custom_splits: Optional[Dict[str, np.ndarray]] = None,\n    config: Optional[DefaultConfig] = None,\n) -&gt; None:\n    \"\"\"Initialize Ontix pipeline with customizable components.\n\n    Some components are passed as types rather than instances because they require\n    data that is only available after preprocessing.\n\n    See parent class for full list of Arguments.\n\n    Raises:\n        TypeError: if ontologies are not a Tuple or List.\n\n    \"\"\"\n    self._default_config = OntixConfig()\n    if isinstance(ontologies, tuple):\n        self.ontologies = ontologies\n    elif isinstance(ontologies, list):\n        if sep is None:\n            raise ValueError(\n                \"If ontologies are provided as a list, the seperator 'sep' cannot be None. \"\n            )\n        ontologies_dict_list = [\n            self._read_ont_file(ont_file, sep=sep) for ont_file in ontologies\n        ]\n        self.ontologies = tuple(ontologies_dict_list)\n    else:\n        raise TypeError(\n            f\"Expected ontologies to be of type tuple or list, got {type(ontologies)}.\"\n        )\n\n    super().__init__(\n        data=data,\n        dataset_type=dataset_type,\n        trainer_type=trainer_type,\n        model_type=model_type,\n        loss_type=loss_type,\n        preprocessor_type=preprocessor_type,\n        visualizer=visualizer,\n        evaluator=evaluator,\n        result=result,\n        datasplitter_type=datasplitter_type,\n        config=config,\n        custom_split=custom_splits,\n        ontologies=self.ontologies,\n    )\n    if not isinstance(self.config, OntixConfig):\n        raise TypeError(\n            f\"For Ontix Pipeline, we only allow OntixConfig as type for config, got {type(self.config)}\"\n        )\n</code></pre>"},{"location":"api/stackix/","title":"Stackix Module","text":""},{"location":"api/stackix/#autoencodix.stackix.Stackix","title":"<code>Stackix</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>Stackix pipeline for training multiple VAEs on different modalities and stacking their latent spaces.</p> <p>This pipeline uses: 1. StackixPreprocessor to prepare data for multi-modality training 2. StackixTrainer to train individual VAEs, extract latent spaces, and train the final stacked model</p> <p>Like other pipelines, it follows the standard BasePipeline interface and workflow.</p> Additional Attributes <p>_default_config: Is set to StackixConfig here.</p> Source code in <code>src/autoencodix/stackix.py</code> <pre><code>class Stackix(BasePipeline):\n    \"\"\"Stackix pipeline for training multiple VAEs on different modalities and stacking their latent spaces.\n\n    This pipeline uses:\n    1. StackixPreprocessor to prepare data for multi-modality training\n    2. StackixTrainer to train individual VAEs, extract latent spaces, and train the final stacked model\n\n    Like other pipelines, it follows the standard BasePipeline interface and workflow.\n\n    Additional Attributes:\n        _default_config: Is set to StackixConfig here.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Optional[Union[DataPackage, DatasetContainer]] = None,\n        trainer_type: Type[BaseTrainer] = StackixTrainer,\n        dataset_type: Type[BaseDataset] = StackixDataset,\n        model_type: Type[BaseAutoencoder] = VarixArchitecture,\n        loss_type: Type[BaseLoss] = VarixLoss,\n        preprocessor_type: Type[BasePreprocessor] = StackixPreprocessor,\n        visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n        evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n        result: Optional[Result] = None,\n        datasplitter_type: Type[DataSplitter] = DataSplitter,\n        custom_splits: Optional[Dict[str, np.ndarray]] = None,\n        config: Optional[DefaultConfig] = None,\n        ontologies: Optional[Union[List, Dict]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize the Stackix pipeline.\n\n        See parent class for full list of Args.\n        \"\"\"\n        self._default_config = StackixConfig()\n        super().__init__(\n            data=data,\n            dataset_type=dataset_type\n            or NumericDataset,  # Fallback, but not directly used\n            trainer_type=trainer_type,\n            model_type=model_type,\n            loss_type=loss_type,\n            preprocessor_type=preprocessor_type,\n            visualizer=visualizer,\n            evaluator=evaluator,\n            result=result,\n            datasplitter_type=datasplitter_type,\n            config=config,\n            custom_split=custom_splits,\n            ontologies=ontologies,\n        )\n        if not isinstance(self.config, StackixConfig):\n            raise TypeError(\n                f\"For Stackix Pipeline, we only allow StackixConfig as type for config, got {type(self.config)}\"\n            )\n\n    def _process_latent_results(\n        self, predictor_results: Result, predict_data: DatasetContainer\n    ):\n        \"\"\"Processes the latent spaces from the StackixTrainer prediction results.\n\n        Creates a correctly annotated AnnData object.\n        This method overrides the BasePipeline implementation to specifically handle\n        the aligned latent space from the unpaired/stacked workflow.\n\n\n        Args:\n            predictor_results: Result object after predict step\n            predict_data: not used here, only to keep interface structure\n\n        \"\"\"\n        latent = predictor_results.latentspaces.get(epoch=-1, split=\"test\")\n        sample_ids = predictor_results.sample_ids.get(epoch=-1, split=\"test\")\n        if latent is None:\n            import warnings\n\n            warnings.warn(\n                \"No latent space found in predictor results. Cannot create AnnData object.\"\n            )\n            return\n\n        self.result.adata_latent = ad.AnnData(X=latent)\n        self.result.adata_latent.obs_names = sample_ids\n        self.result.adata_latent.var_names = [\n            f\"Latent_{i}\" for i in range(latent.shape[1])  # ty: ignore\n        ]\n\n        # 4. Update the main result object with the rest of the prediction results.\n        self.result.update(predictor_results)\n\n        print(\"Successfully created annotated latent space object (adata_latent).\")\n</code></pre>"},{"location":"api/stackix/#autoencodix.stackix.Stackix.__init__","title":"<code>__init__(data=None, trainer_type=StackixTrainer, dataset_type=StackixDataset, model_type=VarixArchitecture, loss_type=VarixLoss, preprocessor_type=StackixPreprocessor, visualizer=GeneralVisualizer, evaluator=GeneralEvaluator, result=None, datasplitter_type=DataSplitter, custom_splits=None, config=None, ontologies=None)</code>","text":"<p>Initialize the Stackix pipeline.</p> <p>See parent class for full list of Args.</p> Source code in <code>src/autoencodix/stackix.py</code> <pre><code>def __init__(\n    self,\n    data: Optional[Union[DataPackage, DatasetContainer]] = None,\n    trainer_type: Type[BaseTrainer] = StackixTrainer,\n    dataset_type: Type[BaseDataset] = StackixDataset,\n    model_type: Type[BaseAutoencoder] = VarixArchitecture,\n    loss_type: Type[BaseLoss] = VarixLoss,\n    preprocessor_type: Type[BasePreprocessor] = StackixPreprocessor,\n    visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n    evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n    result: Optional[Result] = None,\n    datasplitter_type: Type[DataSplitter] = DataSplitter,\n    custom_splits: Optional[Dict[str, np.ndarray]] = None,\n    config: Optional[DefaultConfig] = None,\n    ontologies: Optional[Union[List, Dict]] = None,\n) -&gt; None:\n    \"\"\"Initialize the Stackix pipeline.\n\n    See parent class for full list of Args.\n    \"\"\"\n    self._default_config = StackixConfig()\n    super().__init__(\n        data=data,\n        dataset_type=dataset_type\n        or NumericDataset,  # Fallback, but not directly used\n        trainer_type=trainer_type,\n        model_type=model_type,\n        loss_type=loss_type,\n        preprocessor_type=preprocessor_type,\n        visualizer=visualizer,\n        evaluator=evaluator,\n        result=result,\n        datasplitter_type=datasplitter_type,\n        config=config,\n        custom_split=custom_splits,\n        ontologies=ontologies,\n    )\n    if not isinstance(self.config, StackixConfig):\n        raise TypeError(\n            f\"For Stackix Pipeline, we only allow StackixConfig as type for config, got {type(self.config)}\"\n        )\n</code></pre>"},{"location":"api/trainers/","title":"Trainers Module","text":""},{"location":"api/trainers/#autoencodix.trainers.GeneralTrainer","title":"<code>GeneralTrainer</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Handles general training logic for autoencoder models.</p> <p>Attributes:</p> Name Type Description <code>_trainset</code> <p>The dataset used for training.</p> <code>_validset</code> <p>The dataset used for validation, if provided.</p> <code>_result</code> <p>An object to store and manage training results.</p> <code>_config</code> <p>Configuration object containing training hyperparameters and settings.</p> <code>_model_type</code> <p>The autoencoder model class to be trained.</p> <code>_loss_fn</code> <p>Instantiated loss function specific to the model.</p> <code>_trainloader</code> <p>DataLoader for the training dataset.</p> <code>_validloader</code> <p>DataLoader for the validation dataset, if provided.</p> <code>_model</code> <p>The instantiated model architecture.</p> <code>_optimizer</code> <p>The optimizer used for training.</p> <code>_fabric</code> <p>Lightning Fabric wrapper for device and precision management.</p> <code>n_train</code> <p>Number of training samples.</p> <code>n_valid</code> <p>Number of validation samples.</p> <code>n_test</code> <code>Optional[int]</code> <p>Number of test samples (set during prediction).</p> <code>n_features</code> <p>Number of input features.</p> <code>latent_dim</code> <p>Dimensionality of the latent space.</p> <code>device</code> <p>Device on which the model is located.</p> <code>_n_cpus</code> <p>Number of CPU cores available.</p> <code>_reconstruction_buffer</code> <p>Buffer to store reconstructions during training/validation/testing.</p> <code>_latentspace_buffer</code> <p>Buffer to store latent representations during training/validation/testing.</p> <code>_mu_buffer</code> <p>Buffer to store latent means (for VAE) during training/validation</p> <code>_sigma_buffer</code> <p>Buffer to store latent log-variances (for VAE) during training/validation/testing.</p> <code>_sample_ids_buffer</code> <p>Buffer to store sample IDs during training/validation/testing.</p> Source code in <code>src/autoencodix/trainers/_general_trainer.py</code> <pre><code>class GeneralTrainer(BaseTrainer):\n    \"\"\"Handles general training logic for autoencoder models.\n\n    Attributes:\n         _trainset: The dataset used for training.\n         _validset: The dataset used for validation, if provided.\n         _result: An object to store and manage training results.\n         _config: Configuration object containing training hyperparameters and settings.\n         _model_type: The autoencoder model class to be trained.\n         _loss_fn: Instantiated loss function specific to the model.\n         _trainloader: DataLoader for the training dataset.\n         _validloader: DataLoader for the validation dataset, if provided.\n         _model: The instantiated model architecture.\n         _optimizer: The optimizer used for training.\n         _fabric: Lightning Fabric wrapper for device and precision management.\n         n_train: Number of training samples.\n         n_valid: Number of validation samples.\n         n_test: Number of test samples (set during prediction).\n         n_features: Number of input features.\n         latent_dim: Dimensionality of the latent space.\n         device: Device on which the model is located.\n         _n_cpus: Number of CPU cores available.\n         _reconstruction_buffer: Buffer to store reconstructions during training/validation/testing.\n         _latentspace_buffer: Buffer to store latent representations during training/validation/testing.\n         _mu_buffer: Buffer to store latent means (for VAE) during training/validation\n         _sigma_buffer: Buffer to store latent log-variances (for VAE) during training/validation/testing.\n         _sample_ids_buffer: Buffer to store sample IDs during training/validation/testing.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        trainset: Optional[BaseDataset],\n        validset: Optional[BaseDataset],\n        result: Result,\n        config: DefaultConfig,\n        model_type: Type[BaseAutoencoder],\n        loss_type: Type[BaseLoss],\n        ontologies: Optional[Union[Tuple, List]] = None,\n        **kwargs,\n    ):\n        \"\"\"Initializes the GeneralTrainer.\n\n        Args:\n                trainset: The dataset used for training.\n                validset: The dataset used for validation, if provided.\n                result: An object to store and manage training results.\n                config: Configuration object containing training hyperparameters and settings.\n                model_type: The autoencoder model class to be trained.\n                loss_type: The loss function class to be used for training.\n                ontologies: Ontology information, if provided for Ontix\n        \"\"\"\n        self._n_cpus = os.cpu_count()\n        if self._n_cpus is None:\n            self._n_cpus = 0\n\n        super().__init__(\n            trainset, validset, result, config, model_type, loss_type, ontologies\n        )\n\n        self.latent_dim = config.latent_dim\n        if ontologies is not None:\n            if not hasattr(self._model, \"latent_dim\"):\n                raise ValueError(\n                    \"Model must have a 'latent_dim' attribute when ontologies are provided.\"\n                )\n            self.latent_dim = self._model.latent_dim\n\n        # we will set this later, in the predict method\n        self.n_test: Optional[int] = None\n        self.n_train = len(trainset) if trainset else 0\n        self.n_valid = len(validset) if validset else 0\n        self.n_features = trainset.get_input_dim() if trainset else 0\n        self.device = next(self._model.parameters()).device\n        self._init_buffers()\n\n    def _init_buffers(self, input_data: Optional[BaseDataset] = None) -&gt; None:\n        if input_data:\n            self.n_features = input_data.get_input_dim()\n\n        def make_tensor_buffer(size: int, dim: Union[int, Tuple[int, ...]]):\n            if isinstance(dim, int):\n                return torch.zeros((size, dim), device=self.device)\n            else:\n                return torch.zeros((size, *dim), device=self.device)\n\n        def make_numpy_buffer(size: int):\n            return np.empty((size,), dtype=object)\n\n        # Always create sample ID buffers\n        self._sample_ids_buffer = {\n            \"train\": make_numpy_buffer(self.n_train),\n            \"valid\": make_numpy_buffer(self.n_valid) if self.n_valid else None,\n            \"test\": make_numpy_buffer(self.n_test) if self.n_test else None,\n        }\n\n        splits = [\"test\"] if self._config.save_memory else [\"train\", \"valid\", \"test\"]\n\n        def make_split_buffers(dim):\n            return {\n                split: (\n                    make_tensor_buffer(getattr(self, f\"n_{split}\"), dim)\n                    if getattr(self, f\"n_{split}\", 0) and (split in splits)\n                    else None\n                )\n                for split in [\"train\", \"valid\", \"test\"]\n            }\n\n        self._latentspace_buffer = make_split_buffers(self.latent_dim)\n        self._reconstruction_buffer = make_split_buffers(self.n_features)\n        self._mu_buffer = make_split_buffers(self.latent_dim)\n        self._sigma_buffer = make_split_buffers(self.latent_dim)\n\n    def _ontix_hook(self):\n        pass\n\n    def train(self, epochs_overwrite=None) -&gt; Result:\n        \"\"\"Orchestrates training over multiple epochs, including training and validation phases.\n\n        Args:\n            epochs_overwrite: If provided, overrides the number of epochs specified in the config.\n                This is only there so we can use the train method for pretraining.Any\n        Returns:\n            Result object containing training results and dynamics like latent spaces and reconstructions.\n        \"\"\"\n        epochs = self._config.epochs\n        if epochs_overwrite:\n            epochs = epochs_overwrite\n        with self._fabric.autocast():\n            for epoch in range(epochs):\n                self._init_buffers()\n                should_checkpoint: bool = self._should_checkpoint(epoch)\n                self._model.train()\n\n                epoch_loss, epoch_sub_losses = self._train_epoch(\n                    should_checkpoint=should_checkpoint, epoch=epoch\n                )\n                self._log_losses(epoch, \"train\", epoch_loss, epoch_sub_losses)\n\n                if self._validset:\n                    valid_loss, valid_sub_losses = self._validate_epoch(\n                        should_checkpoint=should_checkpoint, epoch=epoch\n                    )\n                    self._log_losses(epoch, \"valid\", valid_loss, valid_sub_losses)\n\n                if should_checkpoint:\n                    self._store_checkpoint(epoch)\n\n        self._result.model = next(self._model.children())\n        return self._result\n\n    def maskix_hook(self, X: torch.Tensor):\n        \"\"\"Only active when override from MaskixTrainer is used\"\"\"\n        return X\n\n    def _train_epoch(\n        self, should_checkpoint: bool, epoch: int\n    ) -&gt; Tuple[float, Dict[str, float]]:\n        \"\"\"Handles loss computation, backwards pass and checkpointing for one epoch.\n\n        Args:\n            should_checkpoint: Whether to checkpoint this epoch.\n            epoch: The current epoch number.\n        Returns:\n            total_loss: The total loss for the epoch.\n            sub_losses: A dictionary of sub-losses accumulated over the epoch.\n        \"\"\"\n        total_loss = 0.0\n        sub_losses: Dict[str, float] = defaultdict(float)\n        current_batch = 0\n        for indices, features, sample_ids in self._trainloader:\n            current_batch += 1\n            self._optimizer.zero_grad()\n            X: torch.Tensor = self.maskix_hook(features)\n            model_outputs = self._model(X)\n            loss, batch_sub_losses = self._loss_fn(\n                model_output=model_outputs,\n                targets=features,\n                corrupted_input=X,  # only relevant for MaskixLoss\n                epoch=epoch,\n                n_samples=len(\n                    self._trainloader.dataset\n                ),  # Pass n_samples for disentangled loss calculations\n            )\n            self._fabric.backward(loss)\n            self._ontix_hook()\n            self._optimizer.step()\n\n            total_loss += loss.item()\n            for k, v in batch_sub_losses.items():\n                if \"_factor\" not in k:  # Skip factor losses\n                    sub_losses[k] += v.item()\n                else:\n                    sub_losses[k] = v.item()\n\n            if should_checkpoint:\n                self._capture_dynamics(model_outputs, \"train\", indices, sample_ids)\n\n        for k, v in sub_losses.items():\n            if \"_factor\" not in k:\n                sub_losses[k] = v / len(\n                    self._trainloader.dataset\n                )  # Average over all samples\n        total_loss = total_loss / len(\n            self._trainloader.dataset\n        )  # Average over all samples\n        return total_loss, sub_losses\n\n    def _validate_epoch(\n        self, should_checkpoint: bool, epoch: int\n    ) -&gt; Tuple[float, Dict[str, float]]:\n        \"\"\"Handles validation for one epoch.\n\n        Args:\n            should_checkpoint: Whether to checkpoint this epoch.\n            epoch: The current epoch number.\n        Returns:\n            total_loss: The total loss for the epoch.\n            sub_losses: A dictionary of sub-losses accumulated over the epoch.\n        \"\"\"\n        total_loss = 0.0\n        sub_losses: Dict[str, float] = defaultdict(float)\n        self._model.eval()\n\n        with torch.no_grad():\n            for indices, features, sample_ids in self._validloader:\n                X: torch.Tensor = self.maskix_hook(features)\n                model_outputs = self._model(X)\n                loss, batch_sub_losses = self._loss_fn(\n                    model_output=model_outputs,\n                    targets=features,\n                    corrupted_input=X,\n                    epoch=epoch,\n                    n_samples=len(\n                        self._validloader.dataset\n                    ),  # Pass n_samples for disentangled loss calculations\n                )\n                total_loss += loss.item()\n                for k, v in batch_sub_losses.items():\n                    if \"_factor\" not in k:  # Skip factor losses\n                        sub_losses[k] += v.item()\n                    else:\n                        sub_losses[k] = v.item()\n                if should_checkpoint:\n                    self._capture_dynamics(model_outputs, \"valid\", indices, sample_ids)\n\n        for k, v in sub_losses.items():\n            if \"_factor\" not in k:\n                sub_losses[k] = v / len(\n                    self._validloader.dataset\n                )  # Average over all samples\n        total_loss = total_loss / len(\n            self._validloader.dataset\n        )  # Average over all samples\n        return total_loss, sub_losses\n\n    def _log_losses(\n        self, epoch: int, split: str, total_loss: float, sub_losses: Dict[str, float]\n    ) -&gt; None:\n        \"\"\"Logs the total and sub-losses for an epoch and stores them in the Result object.\n\n        Args:\n            epoch: The current epoch number.\n            split: The data split (\"train\" or \"valid\").\n            total_loss: The total loss for the epoch.\n            sub_losses: A dictionary of sub-losses for the epoch.\n        \"\"\"\n        # dataset_len = len(\n        #     self._trainloader.dataset if split == \"train\" else self._validloader.dataset\n        # )\n        self._result.losses.add(epoch=epoch, split=split, data=total_loss)\n        self._result.sub_losses.add(\n            epoch=epoch,\n            split=split,\n            data={k: v if \"_factor\" not in k else v for k, v in sub_losses.items()},\n        )\n\n        self._fabric.print(\n            f\"Epoch {epoch + 1} - {split.capitalize()} Loss: {total_loss:.4f}\"\n        )\n        self._fabric.print(\n            f\"Sub-losses: {', '.join([f'{k}: {v:.4f}' for k, v in sub_losses.items()])}\"\n        )\n\n    def _store_checkpoint(self, epoch: int) -&gt; None:\n        \"\"\"Stores model checkpoints and training dynamics to result object.\n\n        Args:\n            epoch: The current epoch number.\n        \"\"\"\n        self._result.model_checkpoints.add(epoch=epoch, data=self._model.state_dict())\n        if self._config.save_memory:\n            return\n        self._dynamics_to_result(epoch, \"train\")\n        if self._validset:\n            self._dynamics_to_result(epoch, \"valid\")\n\n    def _capture_dynamics(\n        self,\n        model_output: Union[ModelOutput, Any],\n        split: str,\n        indices: torch.Tensor,\n        sample_ids: Any,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Writes model dynamics (latent space, reconstructions, etc.) to buffers.\n\n        Args:\n            model_output: The output from the model containing dynamics information.\n            split: The data split (\"train\", \"valid\", or \"test\").\n            indices: The indices of the samples in the current batch.\n            sample_ids: The sample IDs corresponding to the current batch.\n            **kwargs: Additional arguments (not used here).\n\n        \"\"\"\n        indices_np = (\n            indices.cpu().numpy()\n            if isinstance(indices, torch.Tensor)\n            else np.array(indices)\n        )\n\n        self._sample_ids_buffer[split][indices_np] = np.array(sample_ids)\n        if self._config.save_memory and split != \"test\":\n            return\n        indices_np = (\n            indices.cpu().numpy()\n            if isinstance(indices, torch.Tensor)\n            else np.array(indices)\n        )\n\n        self._sample_ids_buffer[split][indices_np] = np.array(sample_ids)\n\n        self._latentspace_buffer[split][indices_np] = model_output.latentspace.detach()\n        self._reconstruction_buffer[split][\n            indices_np\n        ] = model_output.reconstruction.detach()\n\n        if model_output.latent_logvar is not None:\n            self._sigma_buffer[split][indices_np] = model_output.latent_logvar.detach()\n\n        if model_output.latent_mean is not None:\n            self._mu_buffer[split][indices_np] = model_output.latent_mean.detach()\n\n    def _dynamics_to_result(self, epoch: int, split: str) -&gt; None:\n        \"\"\"Transfers buffered dynamics to the Result object.\n\n        Args:\n            epoch: The current epoch number.\n            split: The data split (\"train\", \"valid\", or \"test\").\n        \"\"\"\n\n        def maybe_add(buffer, target):\n            if buffer[split] is not None and buffer[split].sum() != 0:\n                target.add(\n                    epoch=epoch, split=split, data=buffer[split].cpu().detach().numpy()\n                )\n\n        self._result.latentspaces.add(\n            epoch=epoch,\n            split=split,\n            data=self._latentspace_buffer[split].cpu().detach().numpy(),\n        )\n        self._result.reconstructions.add(\n            epoch=epoch,\n            split=split,\n            data=self._reconstruction_buffer[split].cpu().detach().numpy(),\n        )\n        self._result.sample_ids.add(\n            epoch=epoch, split=split, data=self._sample_ids_buffer[split]\n        )\n        maybe_add(self._mu_buffer, self._result.mus)\n        maybe_add(self._sigma_buffer, self._result.sigmas)\n\n    def predict(\n        self, data: BaseDataset, model: Optional[torch.nn.Module] = None, **kwargs\n    ) -&gt; Result:\n        \"\"\"Decided to add predict method to the trainer class.\n\n        This violates SRP, but the trainer class has a lot of attributes and methods\n        that are needed for prediction. So this way we don't need to write so much duplicate code\n\n        Args:\n            data: BaseDataset unseen data to run inference on\n            model: torch.nn.Module model to run inference with\n            **kwargs: Additional arguments (not used here).\n\n        Returns:\n            self._result: Result object containing the inference results\n\n        \"\"\"\n        if model is None:\n            model: torch.nn.Module = self._model\n            import warnings\n\n            warnings.warn(\n                \"No model provided for prediction, using the trained model from the trainer.\"\n            )\n        model.eval()\n        inference_loader = DataLoader(\n            data,\n            batch_size=self._config.batch_size,\n            shuffle=False,\n        )\n        self.n_test = len(data)\n        self._init_buffers(input_data=data)\n        inference_loader = self._fabric.setup_dataloaders(inference_loader)  # type: ignore\n        with self._fabric.autocast(), torch.inference_mode():\n            for idx, data, sample_ids in inference_loader:\n                model_output = model(data)\n                self._capture_dynamics(\n                    model_output=model_output,\n                    split=\"test\",\n                    sample_ids=sample_ids,\n                    indices=idx,\n                )\n        self._dynamics_to_result(epoch=-1, split=\"test\")\n\n        return self._result\n\n    def decode(self, x: torch.Tensor):\n        \"\"\"Decodes the input tensor x using the trained model.\n\n        Args:\n            x: Input tensor to be decoded.\n        Returns:\n            Decoded tensor.\n        \"\"\"\n        with self._fabric.autocast(), torch.no_grad():\n            x = self._fabric.to_device(obj=x)\n            if not isinstance(x, torch.Tensor):\n                raise TypeError(\n                    f\"Expected input to be a torch.Tensor, got {type(x)} instead.\"\n                )\n            return self._model.decode(x=x)\n\n    def get_model(self) -&gt; torch.nn.Module:\n        \"\"\"Getter method for the trained model.\n\n        Returns:\n\n            The trained model as a torch.nn.Module.\n        \"\"\"\n        return self._model\n\n    def purge(self) -&gt; None:\n        \"\"\"Cleans up any resources used during training, such as cached data or large attributes.\"\"\"\n\n        attrs_to_delete = [\n            \"_trainloader\",\n            \"_validloader\",\n            \"_model\",\n            \"_optimizer\",\n            \"_latentspace_buffer\",\n            \"_reconstruction_buffer\",\n            \"_mu_buffer\",\n            \"_sigma_buffer\",\n            \"_sample_ids_buffer\",\n            \"_trainset\",\n            \"_loss_fn\",\n            \"_validset\",\n        ]\n\n        for attr in attrs_to_delete:\n            if hasattr(self, attr):\n                value = getattr(self, attr)\n                # Optional: ensure non-None before deletion\n                if value is not None:\n                    delattr(self, attr)\n\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        gc.collect()\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.GeneralTrainer.__init__","title":"<code>__init__(trainset, validset, result, config, model_type, loss_type, ontologies=None, **kwargs)</code>","text":"<p>Initializes the GeneralTrainer.</p> <p>Parameters:</p> Name Type Description Default <code>trainset</code> <code>Optional[BaseDataset]</code> <p>The dataset used for training.</p> required <code>validset</code> <code>Optional[BaseDataset]</code> <p>The dataset used for validation, if provided.</p> required <code>result</code> <code>Result</code> <p>An object to store and manage training results.</p> required <code>config</code> <code>DefaultConfig</code> <p>Configuration object containing training hyperparameters and settings.</p> required <code>model_type</code> <code>Type[BaseAutoencoder]</code> <p>The autoencoder model class to be trained.</p> required <code>loss_type</code> <code>Type[BaseLoss]</code> <p>The loss function class to be used for training.</p> required <code>ontologies</code> <code>Optional[Union[Tuple, List]]</code> <p>Ontology information, if provided for Ontix</p> <code>None</code> Source code in <code>src/autoencodix/trainers/_general_trainer.py</code> <pre><code>def __init__(\n    self,\n    trainset: Optional[BaseDataset],\n    validset: Optional[BaseDataset],\n    result: Result,\n    config: DefaultConfig,\n    model_type: Type[BaseAutoencoder],\n    loss_type: Type[BaseLoss],\n    ontologies: Optional[Union[Tuple, List]] = None,\n    **kwargs,\n):\n    \"\"\"Initializes the GeneralTrainer.\n\n    Args:\n            trainset: The dataset used for training.\n            validset: The dataset used for validation, if provided.\n            result: An object to store and manage training results.\n            config: Configuration object containing training hyperparameters and settings.\n            model_type: The autoencoder model class to be trained.\n            loss_type: The loss function class to be used for training.\n            ontologies: Ontology information, if provided for Ontix\n    \"\"\"\n    self._n_cpus = os.cpu_count()\n    if self._n_cpus is None:\n        self._n_cpus = 0\n\n    super().__init__(\n        trainset, validset, result, config, model_type, loss_type, ontologies\n    )\n\n    self.latent_dim = config.latent_dim\n    if ontologies is not None:\n        if not hasattr(self._model, \"latent_dim\"):\n            raise ValueError(\n                \"Model must have a 'latent_dim' attribute when ontologies are provided.\"\n            )\n        self.latent_dim = self._model.latent_dim\n\n    # we will set this later, in the predict method\n    self.n_test: Optional[int] = None\n    self.n_train = len(trainset) if trainset else 0\n    self.n_valid = len(validset) if validset else 0\n    self.n_features = trainset.get_input_dim() if trainset else 0\n    self.device = next(self._model.parameters()).device\n    self._init_buffers()\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.GeneralTrainer.decode","title":"<code>decode(x)</code>","text":"<p>Decodes the input tensor x using the trained model.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input tensor to be decoded.</p> required <p>Returns:     Decoded tensor.</p> Source code in <code>src/autoencodix/trainers/_general_trainer.py</code> <pre><code>def decode(self, x: torch.Tensor):\n    \"\"\"Decodes the input tensor x using the trained model.\n\n    Args:\n        x: Input tensor to be decoded.\n    Returns:\n        Decoded tensor.\n    \"\"\"\n    with self._fabric.autocast(), torch.no_grad():\n        x = self._fabric.to_device(obj=x)\n        if not isinstance(x, torch.Tensor):\n            raise TypeError(\n                f\"Expected input to be a torch.Tensor, got {type(x)} instead.\"\n            )\n        return self._model.decode(x=x)\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.GeneralTrainer.get_model","title":"<code>get_model()</code>","text":"<p>Getter method for the trained model.</p> <p>Returns:</p> <pre><code>The trained model as a torch.nn.Module.\n</code></pre> Source code in <code>src/autoencodix/trainers/_general_trainer.py</code> <pre><code>def get_model(self) -&gt; torch.nn.Module:\n    \"\"\"Getter method for the trained model.\n\n    Returns:\n\n        The trained model as a torch.nn.Module.\n    \"\"\"\n    return self._model\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.GeneralTrainer.maskix_hook","title":"<code>maskix_hook(X)</code>","text":"<p>Only active when override from MaskixTrainer is used</p> Source code in <code>src/autoencodix/trainers/_general_trainer.py</code> <pre><code>def maskix_hook(self, X: torch.Tensor):\n    \"\"\"Only active when override from MaskixTrainer is used\"\"\"\n    return X\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.GeneralTrainer.predict","title":"<code>predict(data, model=None, **kwargs)</code>","text":"<p>Decided to add predict method to the trainer class.</p> <p>This violates SRP, but the trainer class has a lot of attributes and methods that are needed for prediction. So this way we don't need to write so much duplicate code</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>BaseDataset</code> <p>BaseDataset unseen data to run inference on</p> required <code>model</code> <code>Optional[Module]</code> <p>torch.nn.Module model to run inference with</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments (not used here).</p> <code>{}</code> <p>Returns:</p> Type Description <code>Result</code> <p>self._result: Result object containing the inference results</p> Source code in <code>src/autoencodix/trainers/_general_trainer.py</code> <pre><code>def predict(\n    self, data: BaseDataset, model: Optional[torch.nn.Module] = None, **kwargs\n) -&gt; Result:\n    \"\"\"Decided to add predict method to the trainer class.\n\n    This violates SRP, but the trainer class has a lot of attributes and methods\n    that are needed for prediction. So this way we don't need to write so much duplicate code\n\n    Args:\n        data: BaseDataset unseen data to run inference on\n        model: torch.nn.Module model to run inference with\n        **kwargs: Additional arguments (not used here).\n\n    Returns:\n        self._result: Result object containing the inference results\n\n    \"\"\"\n    if model is None:\n        model: torch.nn.Module = self._model\n        import warnings\n\n        warnings.warn(\n            \"No model provided for prediction, using the trained model from the trainer.\"\n        )\n    model.eval()\n    inference_loader = DataLoader(\n        data,\n        batch_size=self._config.batch_size,\n        shuffle=False,\n    )\n    self.n_test = len(data)\n    self._init_buffers(input_data=data)\n    inference_loader = self._fabric.setup_dataloaders(inference_loader)  # type: ignore\n    with self._fabric.autocast(), torch.inference_mode():\n        for idx, data, sample_ids in inference_loader:\n            model_output = model(data)\n            self._capture_dynamics(\n                model_output=model_output,\n                split=\"test\",\n                sample_ids=sample_ids,\n                indices=idx,\n            )\n    self._dynamics_to_result(epoch=-1, split=\"test\")\n\n    return self._result\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.GeneralTrainer.purge","title":"<code>purge()</code>","text":"<p>Cleans up any resources used during training, such as cached data or large attributes.</p> Source code in <code>src/autoencodix/trainers/_general_trainer.py</code> <pre><code>def purge(self) -&gt; None:\n    \"\"\"Cleans up any resources used during training, such as cached data or large attributes.\"\"\"\n\n    attrs_to_delete = [\n        \"_trainloader\",\n        \"_validloader\",\n        \"_model\",\n        \"_optimizer\",\n        \"_latentspace_buffer\",\n        \"_reconstruction_buffer\",\n        \"_mu_buffer\",\n        \"_sigma_buffer\",\n        \"_sample_ids_buffer\",\n        \"_trainset\",\n        \"_loss_fn\",\n        \"_validset\",\n    ]\n\n    for attr in attrs_to_delete:\n        if hasattr(self, attr):\n            value = getattr(self, attr)\n            # Optional: ensure non-None before deletion\n            if value is not None:\n                delattr(self, attr)\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    gc.collect()\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.GeneralTrainer.train","title":"<code>train(epochs_overwrite=None)</code>","text":"<p>Orchestrates training over multiple epochs, including training and validation phases.</p> <p>Parameters:</p> Name Type Description Default <code>epochs_overwrite</code> <p>If provided, overrides the number of epochs specified in the config. This is only there so we can use the train method for pretraining.Any</p> <code>None</code> <p>Returns:     Result object containing training results and dynamics like latent spaces and reconstructions.</p> Source code in <code>src/autoencodix/trainers/_general_trainer.py</code> <pre><code>def train(self, epochs_overwrite=None) -&gt; Result:\n    \"\"\"Orchestrates training over multiple epochs, including training and validation phases.\n\n    Args:\n        epochs_overwrite: If provided, overrides the number of epochs specified in the config.\n            This is only there so we can use the train method for pretraining.Any\n    Returns:\n        Result object containing training results and dynamics like latent spaces and reconstructions.\n    \"\"\"\n    epochs = self._config.epochs\n    if epochs_overwrite:\n        epochs = epochs_overwrite\n    with self._fabric.autocast():\n        for epoch in range(epochs):\n            self._init_buffers()\n            should_checkpoint: bool = self._should_checkpoint(epoch)\n            self._model.train()\n\n            epoch_loss, epoch_sub_losses = self._train_epoch(\n                should_checkpoint=should_checkpoint, epoch=epoch\n            )\n            self._log_losses(epoch, \"train\", epoch_loss, epoch_sub_losses)\n\n            if self._validset:\n                valid_loss, valid_sub_losses = self._validate_epoch(\n                    should_checkpoint=should_checkpoint, epoch=epoch\n                )\n                self._log_losses(epoch, \"valid\", valid_loss, valid_sub_losses)\n\n            if should_checkpoint:\n                self._store_checkpoint(epoch)\n\n    self._result.model = next(self._model.children())\n    return self._result\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixOrchestrator","title":"<code>StackixOrchestrator</code>","text":"<p>StackixOrchestrator coordinates the training of multi-modality VAE stacking.</p> <p>This orchestrator manages both parallel and sequential training of modality-specific autoencoders, followed by creating a concatenated latent space for the final stacked model training. It leverages Lightning Fabric's distribution strategies for efficient training.</p> <p>Attributes: _workdir: Directory for saving intermediate models and results modality_models: Dictionary of trained models for each modality modality_results: Dictionary of training results for each modality _modality_latent_dims: Dictionary of latent dimensions for each modality concatenated_latent_spaces: Dictionary of concatenated latent spaces by split _dataset_type: Class to use for creating datasets _fabric: Lightning Fabric instance for distributed operations _trainer_class: Class to use for training (dependency injection) trainset: Training dataset containing multiple modalities validset: Validation dataset containing multiple modalities testset: Test dataset containing multiple modalities loss_type: Type of loss function to use for training model_type: Type of autoencoder model to use for each modality config: Configuration parameters for training and model architecture stacked_model: The final stacked autoencoder model (initialized later) stacked_trainer: Trainer for the stacked model (initialized later) concat_idx: Dictionary tracking the start and end indices of each modality in the concatenated latent space dropped_indices_map: Dictionary tracking dropped sample indices for each modality during alignment reconstruction_shapes: Dictionary storing original shapes of latent spaces for reconstruction common_sample_ids: Common sample IDs across all modalities for alignment</p> Source code in <code>src/autoencodix/trainers/_stackix_orchestrator.py</code> <pre><code>class StackixOrchestrator:\n    \"\"\"StackixOrchestrator coordinates the training of multi-modality VAE stacking.\n\n    This orchestrator manages both parallel and sequential training of modality-specific\n    autoencoders, followed by creating a concatenated latent space for the final\n    stacked model training. It leverages Lightning Fabric's distribution strategies\n    for efficient training.\n\n    Attributes:\n    _workdir: Directory for saving intermediate models and results\n    modality_models: Dictionary of trained models for each modality\n    modality_results: Dictionary of training results for each modality\n    _modality_latent_dims: Dictionary of latent dimensions for each modality\n    concatenated_latent_spaces: Dictionary of concatenated latent spaces by split\n    _dataset_type: Class to use for creating datasets\n    _fabric: Lightning Fabric instance for distributed operations\n    _trainer_class: Class to use for training (dependency injection)\n    trainset: Training dataset containing multiple modalities\n    validset: Validation dataset containing multiple modalities\n    testset: Test dataset containing multiple modalities\n    loss_type: Type of loss function to use for training\n    model_type: Type of autoencoder model to use for each modality\n    config: Configuration parameters for training and model architecture\n    stacked_model: The final stacked autoencoder model (initialized later)\n    stacked_trainer: Trainer for the stacked model (initialized later)\n    concat_idx: Dictionary tracking the start and end indices of each modality in the concatenated latent space\n    dropped_indices_map: Dictionary tracking dropped sample indices for each modality during alignment\n    reconstruction_shapes: Dictionary storing original shapes of latent spaces for reconstruction\n    common_sample_ids: Common sample IDs across all modalities for alignment\n\n    \"\"\"\n\n    def __init__(\n        self,\n        trainset: Optional[MultiModalDataset],\n        validset: Optional[MultiModalDataset],\n        config: DefaultConfig,\n        model_type: Type[BaseAutoencoder],\n        loss_type: Type[BaseLoss],\n        testset: Optional[MultiModalDataset] = None,\n        trainer_type: Type[GeneralTrainer] = GeneralTrainer,\n        dataset_type: Type[BaseDataset] = NumericDataset,\n        workdir: str = \"./stackix_work\",\n    ):\n        \"\"\"\n        Initialize the StackixOrchestrator with datasets and configuration.\n\n        Args:\n            trainset: Training dataset containing multiple modalities\n            validset: Validation dataset containing multiple modalities\n            config: Configuration parameters for training and model architecture\n            model_type: Type of autoencoder model to use for each modality\n            loss_type: Type of loss function to use for training\n            testset: Dataset with test split\n            trainer_type: Type to use for training (default is GeneralTrainer)\n            dataset_type: Type to use for creating datasets (default is NumericDataset)\n            workdir: Directory to save intermediate models and results (default is \"./stackix_work\")\n        \"\"\"\n        self._trainset = trainset\n        self._validset = validset\n        self._testset = testset\n        self._config = config\n        self._model_type = model_type\n        self._loss_type = loss_type\n        self._workdir = workdir\n        self._trainer_class = trainer_type\n        self._dataset_type = dataset_type\n\n        # Initialize fabric for distributed operations\n        strategy = \"auto\"\n        if hasattr(config, \"gpu_strategy\"):\n            strategy = config.gpu_strategy\n\n        self._fabric = lightning_fabric.Fabric(\n            accelerator=(\n                str(config.device) if hasattr(config, \"device\") else None\n            ),  # ty: ignore[invalid-argument-type]\n            devices=config.n_gpus if hasattr(config, \"n_gpus\") else 1,\n            precision=(\n                config.float_precision if hasattr(config, \"float_precision\") else \"32\"\n            ),\n            strategy=strategy,\n        )\n\n        # Initialize storage for models and results\n        self.modality_trainers: Dict[str, BaseAutoencoder] = {}\n        self.modality_results: Dict[str, Result] = {}\n        self._modality_latent_dims: Dict[str, int] = {}\n        self.concatenated_latent_spaces: Dict[str, torch.Tensor] = {}\n\n        # Stacked model and trainer will be created during the process\n        self.stacked_model: Optional[BaseAutoencoder] = None\n        self.stacked_trainer: Optional[GeneralTrainer] = None\n        self.concat_idx: Dict[str, Optional[Tuple[int, int]]] = {}\n        self.dropped_indices_map: Dict[str, np.ndarray] = {}\n        self.reconstruction_shapes: Dict[str, torch.Size] = {}\n        self.common_sample_ids: Optional[pd.Index] = None\n\n        # Create the directory if it doesn't exist\n        os.makedirs(name=self._workdir, exist_ok=True)\n\n    def set_testset(self, testset: MultiModalDataset) -&gt; None:\n        \"\"\"Set the test dataset for the orchestrator.\n\n        Args:\n            testset: Test dataset containing multiple modalities\n        \"\"\"\n        self._testset = testset\n\n    def _train_modality(\n        self,\n        modality: str,\n        modality_dataset: BaseDataset,\n        valid_modality_dataset: Optional[BaseDataset] = None,\n    ) -&gt; None:\n        \"\"\"Trains a single modality and returns the trained model and result.\n\n        This function is designed to be executed within a Lightning Fabric context.\n        It trains an individual modality model and saves the results to disk.\n\n        Args:\n            modality: Modality name/identifier\n            modality_dataset: Training dataset for this modality\n            valid_modality_dataset: Validation dataset for this modality (default is None)\n\n        Returns:\n            Trained model and result object\n        \"\"\"\n        print(f\"Training modality: {modality}\")\n        result = Result()\n        trainer = self._trainer_class(\n            trainset=modality_dataset,\n            validset=valid_modality_dataset,\n            result=result,\n            config=self._config,\n            model_type=self._model_type,\n            loss_type=self._loss_type,\n        )\n\n        result = trainer.train()\n        self.modality_results[modality] = result\n        self.modality_trainers[modality] = trainer\n\n    def _train_distributed(self, keys: List[str]) -&gt; None:\n        \"\"\"Trains modality models in a distributed fashion using Lightning Fabric.\n\n        Uses Lightning Fabric's built-in capabilities for distributing work across devices.\n        Each process trains a subset of modalities, then loads results from other processes.\n\n        Args:\n            keys: List of modality keys to train\n        \"\"\"\n        # For parallel training with multiple devices\n        strategy = (\n            self._config.gpu_strategy\n            if hasattr(self._config, \"gpu_strategy\")\n            else \"auto\"\n        )\n        n_gpus = self._config.n_gpus if hasattr(self._config, \"n_gpus\") else 1\n        device: str = self._config.device\n\n        if strategy == \"auto\" and device in [\"cuda\", \"gpu\"]:\n            strategy = \"ddp\" if n_gpus &gt; 1 else \"dp\"\n\n        # Create a fabric instance with appropriate parallelism strategy\n        fabric = lightning_fabric.Fabric(\n            accelerator=device,\n            devices=min(n_gpus, len(keys)),  # No more devices than modalities\n            precision=(\n                self._config.float_precision\n                if hasattr(self._config, \"float_precision\")\n                else \"32\"\n            ),\n            strategy=strategy,\n        )\n\n        # Launch distributed training\n        fabric.launch()\n\n        # Device ID within the process group\n        local_rank = fabric.local_rank if hasattr(fabric, \"local_rank\") else 0\n        world_size = fabric.world_size if hasattr(fabric, \"world_size\") else 1\n\n        # Distribute modalities across ranks\n        my_modalities = [k for i, k in enumerate(keys) if i % world_size == local_rank]\n\n        # Train assigned modalities\n        for modality in my_modalities:\n            train_dataset = self._trainset.datasets[modality]\n            valid_dataset = (\n                self._validset.datasets.get(modality)\n                if self._validset and hasattr(self._validset, \"datasets\")\n                else None\n            )\n\n            self._train_modality(\n                modality=modality,\n                modality_dataset=train_dataset,\n                valid_modality_dataset=valid_dataset,\n            )\n            self._modality_latent_dims[modality] = train_dataset.get_input_dim()\n\n        # Synchronize across processes to ensure all modalities are trained\n        if world_size &gt; 1:\n            fabric.barrier()\n\n        # Load models and results from other processes\n        for modality in keys:\n            if modality in self.modality_trainers.keys():\n                continue\n            # Load model state dict\n            model_path = os.path.join(self._workdir, f\"{modality}_model.ckpt\")\n\n            # Get input dimension for this modality\n            input_dim = self._trainset.datasets[modality].get_input_dim()\n\n            # Initialize a new model\n            model = self._model_type(config=self._config, input_dim=input_dim)\n            model.load_state_dict(\n                state_dict=torch.load(f=model_path, map_location=\"cpu\")\n            )\n\n            # Load result\n            with open(\n                file=os.path.join(self._workdir, f\"{modality}_result.pkl\"),\n                mode=\"rb\",\n            ) as f:\n                result = pickle.load(file=f)\n\n            self.modality_trainers[modality] = model\n            self.modality_results[modality] = result\n            self._modality_latent_dims[modality] = input_dim\n\n    def _train_sequential(self, keys: List[str]) -&gt; None:\n        \"\"\"Trains modality models sequentially on a single device.\n\n        Used when distributed training is not available or not necessary.\n        Processes each modality one after another.\n\n        Args:\n            keys: List of modality keys to train\n        \"\"\"\n        for modality in keys:\n            print(f\"Training modality: {modality}\")\n            train_dataset = self._trainset.datasets[modality]\n            valid_dataset = (\n                self._validset.datasets.get(modality) if self._validset else None\n            )\n\n            self._train_modality(\n                modality=modality,\n                modality_dataset=train_dataset,\n                valid_modality_dataset=valid_dataset,\n            )\n\n            self._modality_latent_dims[modality] = train_dataset.get_input_dim()\n\n    def _extract_latent_spaces(\n        self, result_dict: Dict[str, Any], split: str = \"train\"\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts, aligns, and concatenates latent spaces, populating instance attributes for later reconstruction.\n\n        Args:\n            result_dict: Dictionary of Result objects from modality training\n            split: Data split to extract latent spaces from (\"train\", \"valid\", \"test\"), default is \"train\"\n        \"\"\"\n        # --- Step 1: Extract Latent Spaces and Sample IDs ---\n        all_latents = {}\n        all_sample_ids = {}\n        for name, result in result_dict.items():\n            try:\n                latent_space = result.latentspaces.get(split=split, epoch=-1)\n                sample_ids = result.sample_ids.get(split=split, epoch=-1)\n\n                if latent_space.shape[0] != len(sample_ids):\n                    raise ValueError(\n                        f\"Mismatch between latent space rows ({latent_space.shape[0]}) and sample IDs ({len(sample_ids)}) for modality '{name}'.\"\n                    )\n\n                all_latents[name] = latent_space\n                all_sample_ids[name] = sample_ids\n                self.reconstruction_shapes[name] = latent_space.shape\n            except Exception as e:\n                raise ValueError(\n                    f\"Failed to extract data for modality {name} on split {split}: {e}\"\n                )\n\n        if not all_latents:\n            raise ValueError(\"No latent spaces were extracted.\")\n\n        # --- Step 2: Find Common Sample IDs ---\n        initial_ids = set(next(iter(all_sample_ids.values())))\n        common_ids_set = initial_ids.intersection(\n            *[set(ids) for ids in all_sample_ids.values()]\n        )\n\n        if not common_ids_set:\n            raise ValueError(\n                \"No common samples found across all modalities for concatenation.\"\n            )\n        # common_ids = [cid for cid in list(common_ids_set) if cid]\n        self.common_sample_ids = pd.Index(sorted(list(common_ids_set)))\n\n        # self.common_sample_ids = pd.Index(sorted(list(common_ids_set)))\n        print(\n            f\"Found {len(self.common_sample_ids)} common samples for the stacked autoencoder.\"\n        )\n\n        # --- Step 3 &amp; 4: Align Latent Spaces and Track Dropped Indices ---\n        aligned_latents_list = []\n        start_idx = 0\n        for name, latent_space in all_latents.items():\n            original_ids = pd.Index(all_sample_ids[name])\n            latent_df = pd.DataFrame(latent_space, index=original_ids)\n\n            aligned_df = latent_df.loc[self.common_sample_ids]\n            aligned_latents_list.append(torch.from_numpy(aligned_df.values))\n\n            end_idx = start_idx + aligned_df.shape[1]\n            self.concat_idx[name] = (start_idx, end_idx)\n            start_idx = end_idx\n\n            dropped_mask = ~original_ids.isin(self.common_sample_ids)\n            self.dropped_indices_map[name] = np.where(dropped_mask)[0]\n\n        # --- Step 5: Concatenate ---\n        stackix_input = torch.cat(aligned_latents_list, dim=1)\n        return stackix_input\n\n    def train_modalities(self) -&gt; Tuple[Dict[str, BaseAutoencoder], Dict[str, Result]]:\n        \"\"\"Trains all modality-specific models.\n\n        This is the first phase of Stackix training where each modality is\n        trained independently before their latent spaces are combined.\n\n        Returns:\n            Dictionary of trained models for each modality and Dictionary of training results\n            for each modality.\n\n        Raises:\n            ValueError: If trainset is not a MultiModalDataset or has no modalities\n        \"\"\"\n        # if not isinstance(self._trainset, MulDataset):\n        #     raise ValueError(\"Trainset must be a MultiModalDataset for Stackix training\")\n\n        keys = self._trainset.datasets.keys()\n        if not keys:\n            raise ValueError(\"No modalities found in trainset\")\n\n        n_modalities = len(keys)\n\n        # Determine if we should use distributed training\n        n_gpus = self._config.n_gpus if hasattr(self._config, \"n_gpus\") else 0\n        use_distributed = bool(n_gpus and n_gpus &gt; 1 and n_modalities &gt; 1)\n\n        if use_distributed:\n            self._train_distributed(keys=keys)\n        else:\n            self._train_sequential(keys=keys)\n\n        return self.modality_trainers, self.modality_results\n\n    def prepare_latent_datasets(self, split: str) -&gt; NumericDataset:\n        \"\"\"Prepares datasets with concatenated latent spaces for stacked model training.\n\n        This is the second phase of Stackix training where latent spaces from\n        all modalities are extracted and concatenated.\n\n        Returns:\n            Training and validation datasets with concatenated latent spaces\n\n        Raises:\n            ValueError: If no modality models have been trained or no latent spaces could be extracted\n        \"\"\"\n        if not self.modality_trainers:\n            raise ValueError(\n                \"No modality models have been trained. Call train_modalities() first.\"\n            )\n\n        # Extract and concatenate latent spaces\n        if split == \"test\":\n            if self._testset is None:\n                raise ValueError(\n                    \"No test dataset available. Please provide a test dataset for prediction.\"\n                )\n            latent = self._extract_latent_spaces(\n                result_dict=self.predict_modalities(data=self._testset), split=split\n            )\n            ds = NumericDataset(\n                data=latent,\n                config=self._config,\n                sample_ids=self.common_sample_ids,\n                feature_ids=[\n                    f\"{k}_latent_{i}\"\n                    for k, (start, end) in self.concat_idx.items()\n                    for i in range(start, end)\n                ],\n            )\n            return ds\n\n        latent = self._extract_latent_spaces(\n            result_dict=self.modality_results, split=split\n        )\n        # Create datasets for the concatenated latent spaces\n        feature_ids = [\n            f\"{k}_latent_{i}\"\n            for k, (start, end) in self.concat_idx.items()\n            for i in range(start, end)\n        ]\n        ds = NumericDataset(\n            data=latent,\n            config=self._config,\n            sample_ids=self.common_sample_ids,\n            feature_ids=feature_ids,\n        )\n\n        return ds\n\n    def predict_modalities(self, data: MultiModalDataset) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Predicts using the trained models for each modality.\n\n        Args:\n            data: Input data for prediction, uses test data if not provided\n\n        Returns:\n            Dictionary of reconstructed tensors by modality\n\n        Raises:\n            ValueError: If model has not been trained yet or no data is available\n        \"\"\"\n        if not self.modality_trainers:\n            raise ValueError(\n                \"No modality models have been trained. Call train_modalities() first.\"\n            )\n\n        predictions = {}\n        for modality, trainer in self.modality_trainers.items():\n            predictions[modality] = trainer.predict(\n                data=data.datasets[modality],\n                model=trainer._model,\n            )\n        return predictions\n\n    def reconstruct_from_stack(\n        self, reconstructed_stack: torch.Tensor\n    ) -&gt; Dict[str, torch.Tensor]:\n        \"\"\"Reconstructs the full data for each modality from the stacked latent reconstruction.\n\n        Args:\n            reconstructed_stack: Tensor with the reconstructed concatenated latent space\n        Returns:\n            Dictionary of reconstructed tensors by modality\n        \"\"\"\n        modality_reconstructions: Dict[str, torch.Tensor] = {}\n\n        for trainer in self.modality_trainers.values():\n            trainer._model.eval()\n\n        for name, (start_idx, end_idx) in self.concat_idx.items():\n            # =================================================================\n            # STEP 1: DE-CONCATENATE THE ALIGNED PART\n            # This is the small, dense reconstruction matching the common samples.\n            # =================================================================\n            aligned_latent_recon = reconstructed_stack[:, start_idx:end_idx]\n\n            # =================================================================\n            # STEP 2: RE-ASSEMBLE THE FULL-SIZE LATENT SPACE\n            # This is the logic from the old `reconstruct_full_latent` function,\n            # now integrated directly here.\n            # =================================================================\n            original_shape = self.reconstruction_shapes[name]\n            dropped_indices = self.dropped_indices_map[name]\n            n_original_samples = original_shape[0]\n\n            # Determine the original integer positions of the samples that were KEPT.\n            kept_indices = np.delete(np.arange(n_original_samples), dropped_indices)\n\n            # Create a full-size placeholder tensor matching the original latent space.\n            # We use zeros, as they are a neutral input for most decoders.\n            full_latent_recon = torch.zeros(\n                size=original_shape,\n                dtype=aligned_latent_recon.dtype,\n                device=self._fabric.device,\n            )\n\n            # Place the aligned reconstruction data into the correct rows of the full tensor.\n            full_latent_recon[kept_indices, :] = self._fabric.to_device(\n                aligned_latent_recon\n            )\n\n            # =================================================================\n            # STEP 3: DECODE THE FULL-SIZE LATENT TENSOR\n            # The decoder now receives a tensor with the correct number of samples,\n            # including the rows of zeros for the dropped samples.\n            # =================================================================\n            with torch.no_grad():\n                model = self.modality_trainers[name].get_model()\n                model = self._fabric.to_device(model)\n\n                final_recon = model.decode(full_latent_recon)\n                modality_reconstructions[name] = final_recon.cpu()\n\n        return modality_reconstructions\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixOrchestrator.__init__","title":"<code>__init__(trainset, validset, config, model_type, loss_type, testset=None, trainer_type=GeneralTrainer, dataset_type=NumericDataset, workdir='./stackix_work')</code>","text":"<p>Initialize the StackixOrchestrator with datasets and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>trainset</code> <code>Optional[MultiModalDataset]</code> <p>Training dataset containing multiple modalities</p> required <code>validset</code> <code>Optional[MultiModalDataset]</code> <p>Validation dataset containing multiple modalities</p> required <code>config</code> <code>DefaultConfig</code> <p>Configuration parameters for training and model architecture</p> required <code>model_type</code> <code>Type[BaseAutoencoder]</code> <p>Type of autoencoder model to use for each modality</p> required <code>loss_type</code> <code>Type[BaseLoss]</code> <p>Type of loss function to use for training</p> required <code>testset</code> <code>Optional[MultiModalDataset]</code> <p>Dataset with test split</p> <code>None</code> <code>trainer_type</code> <code>Type[GeneralTrainer]</code> <p>Type to use for training (default is GeneralTrainer)</p> <code>GeneralTrainer</code> <code>dataset_type</code> <code>Type[BaseDataset]</code> <p>Type to use for creating datasets (default is NumericDataset)</p> <code>NumericDataset</code> <code>workdir</code> <code>str</code> <p>Directory to save intermediate models and results (default is \"./stackix_work\")</p> <code>'./stackix_work'</code> Source code in <code>src/autoencodix/trainers/_stackix_orchestrator.py</code> <pre><code>def __init__(\n    self,\n    trainset: Optional[MultiModalDataset],\n    validset: Optional[MultiModalDataset],\n    config: DefaultConfig,\n    model_type: Type[BaseAutoencoder],\n    loss_type: Type[BaseLoss],\n    testset: Optional[MultiModalDataset] = None,\n    trainer_type: Type[GeneralTrainer] = GeneralTrainer,\n    dataset_type: Type[BaseDataset] = NumericDataset,\n    workdir: str = \"./stackix_work\",\n):\n    \"\"\"\n    Initialize the StackixOrchestrator with datasets and configuration.\n\n    Args:\n        trainset: Training dataset containing multiple modalities\n        validset: Validation dataset containing multiple modalities\n        config: Configuration parameters for training and model architecture\n        model_type: Type of autoencoder model to use for each modality\n        loss_type: Type of loss function to use for training\n        testset: Dataset with test split\n        trainer_type: Type to use for training (default is GeneralTrainer)\n        dataset_type: Type to use for creating datasets (default is NumericDataset)\n        workdir: Directory to save intermediate models and results (default is \"./stackix_work\")\n    \"\"\"\n    self._trainset = trainset\n    self._validset = validset\n    self._testset = testset\n    self._config = config\n    self._model_type = model_type\n    self._loss_type = loss_type\n    self._workdir = workdir\n    self._trainer_class = trainer_type\n    self._dataset_type = dataset_type\n\n    # Initialize fabric for distributed operations\n    strategy = \"auto\"\n    if hasattr(config, \"gpu_strategy\"):\n        strategy = config.gpu_strategy\n\n    self._fabric = lightning_fabric.Fabric(\n        accelerator=(\n            str(config.device) if hasattr(config, \"device\") else None\n        ),  # ty: ignore[invalid-argument-type]\n        devices=config.n_gpus if hasattr(config, \"n_gpus\") else 1,\n        precision=(\n            config.float_precision if hasattr(config, \"float_precision\") else \"32\"\n        ),\n        strategy=strategy,\n    )\n\n    # Initialize storage for models and results\n    self.modality_trainers: Dict[str, BaseAutoencoder] = {}\n    self.modality_results: Dict[str, Result] = {}\n    self._modality_latent_dims: Dict[str, int] = {}\n    self.concatenated_latent_spaces: Dict[str, torch.Tensor] = {}\n\n    # Stacked model and trainer will be created during the process\n    self.stacked_model: Optional[BaseAutoencoder] = None\n    self.stacked_trainer: Optional[GeneralTrainer] = None\n    self.concat_idx: Dict[str, Optional[Tuple[int, int]]] = {}\n    self.dropped_indices_map: Dict[str, np.ndarray] = {}\n    self.reconstruction_shapes: Dict[str, torch.Size] = {}\n    self.common_sample_ids: Optional[pd.Index] = None\n\n    # Create the directory if it doesn't exist\n    os.makedirs(name=self._workdir, exist_ok=True)\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixOrchestrator.predict_modalities","title":"<code>predict_modalities(data)</code>","text":"<p>Predicts using the trained models for each modality.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>MultiModalDataset</code> <p>Input data for prediction, uses test data if not provided</p> required <p>Returns:</p> Type Description <code>Dict[str, Tensor]</code> <p>Dictionary of reconstructed tensors by modality</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model has not been trained yet or no data is available</p> Source code in <code>src/autoencodix/trainers/_stackix_orchestrator.py</code> <pre><code>def predict_modalities(self, data: MultiModalDataset) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Predicts using the trained models for each modality.\n\n    Args:\n        data: Input data for prediction, uses test data if not provided\n\n    Returns:\n        Dictionary of reconstructed tensors by modality\n\n    Raises:\n        ValueError: If model has not been trained yet or no data is available\n    \"\"\"\n    if not self.modality_trainers:\n        raise ValueError(\n            \"No modality models have been trained. Call train_modalities() first.\"\n        )\n\n    predictions = {}\n    for modality, trainer in self.modality_trainers.items():\n        predictions[modality] = trainer.predict(\n            data=data.datasets[modality],\n            model=trainer._model,\n        )\n    return predictions\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixOrchestrator.prepare_latent_datasets","title":"<code>prepare_latent_datasets(split)</code>","text":"<p>Prepares datasets with concatenated latent spaces for stacked model training.</p> <p>This is the second phase of Stackix training where latent spaces from all modalities are extracted and concatenated.</p> <p>Returns:</p> Type Description <code>NumericDataset</code> <p>Training and validation datasets with concatenated latent spaces</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no modality models have been trained or no latent spaces could be extracted</p> Source code in <code>src/autoencodix/trainers/_stackix_orchestrator.py</code> <pre><code>def prepare_latent_datasets(self, split: str) -&gt; NumericDataset:\n    \"\"\"Prepares datasets with concatenated latent spaces for stacked model training.\n\n    This is the second phase of Stackix training where latent spaces from\n    all modalities are extracted and concatenated.\n\n    Returns:\n        Training and validation datasets with concatenated latent spaces\n\n    Raises:\n        ValueError: If no modality models have been trained or no latent spaces could be extracted\n    \"\"\"\n    if not self.modality_trainers:\n        raise ValueError(\n            \"No modality models have been trained. Call train_modalities() first.\"\n        )\n\n    # Extract and concatenate latent spaces\n    if split == \"test\":\n        if self._testset is None:\n            raise ValueError(\n                \"No test dataset available. Please provide a test dataset for prediction.\"\n            )\n        latent = self._extract_latent_spaces(\n            result_dict=self.predict_modalities(data=self._testset), split=split\n        )\n        ds = NumericDataset(\n            data=latent,\n            config=self._config,\n            sample_ids=self.common_sample_ids,\n            feature_ids=[\n                f\"{k}_latent_{i}\"\n                for k, (start, end) in self.concat_idx.items()\n                for i in range(start, end)\n            ],\n        )\n        return ds\n\n    latent = self._extract_latent_spaces(\n        result_dict=self.modality_results, split=split\n    )\n    # Create datasets for the concatenated latent spaces\n    feature_ids = [\n        f\"{k}_latent_{i}\"\n        for k, (start, end) in self.concat_idx.items()\n        for i in range(start, end)\n    ]\n    ds = NumericDataset(\n        data=latent,\n        config=self._config,\n        sample_ids=self.common_sample_ids,\n        feature_ids=feature_ids,\n    )\n\n    return ds\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixOrchestrator.reconstruct_from_stack","title":"<code>reconstruct_from_stack(reconstructed_stack)</code>","text":"<p>Reconstructs the full data for each modality from the stacked latent reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>reconstructed_stack</code> <code>Tensor</code> <p>Tensor with the reconstructed concatenated latent space</p> required <p>Returns:     Dictionary of reconstructed tensors by modality</p> Source code in <code>src/autoencodix/trainers/_stackix_orchestrator.py</code> <pre><code>def reconstruct_from_stack(\n    self, reconstructed_stack: torch.Tensor\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Reconstructs the full data for each modality from the stacked latent reconstruction.\n\n    Args:\n        reconstructed_stack: Tensor with the reconstructed concatenated latent space\n    Returns:\n        Dictionary of reconstructed tensors by modality\n    \"\"\"\n    modality_reconstructions: Dict[str, torch.Tensor] = {}\n\n    for trainer in self.modality_trainers.values():\n        trainer._model.eval()\n\n    for name, (start_idx, end_idx) in self.concat_idx.items():\n        # =================================================================\n        # STEP 1: DE-CONCATENATE THE ALIGNED PART\n        # This is the small, dense reconstruction matching the common samples.\n        # =================================================================\n        aligned_latent_recon = reconstructed_stack[:, start_idx:end_idx]\n\n        # =================================================================\n        # STEP 2: RE-ASSEMBLE THE FULL-SIZE LATENT SPACE\n        # This is the logic from the old `reconstruct_full_latent` function,\n        # now integrated directly here.\n        # =================================================================\n        original_shape = self.reconstruction_shapes[name]\n        dropped_indices = self.dropped_indices_map[name]\n        n_original_samples = original_shape[0]\n\n        # Determine the original integer positions of the samples that were KEPT.\n        kept_indices = np.delete(np.arange(n_original_samples), dropped_indices)\n\n        # Create a full-size placeholder tensor matching the original latent space.\n        # We use zeros, as they are a neutral input for most decoders.\n        full_latent_recon = torch.zeros(\n            size=original_shape,\n            dtype=aligned_latent_recon.dtype,\n            device=self._fabric.device,\n        )\n\n        # Place the aligned reconstruction data into the correct rows of the full tensor.\n        full_latent_recon[kept_indices, :] = self._fabric.to_device(\n            aligned_latent_recon\n        )\n\n        # =================================================================\n        # STEP 3: DECODE THE FULL-SIZE LATENT TENSOR\n        # The decoder now receives a tensor with the correct number of samples,\n        # including the rows of zeros for the dropped samples.\n        # =================================================================\n        with torch.no_grad():\n            model = self.modality_trainers[name].get_model()\n            model = self._fabric.to_device(model)\n\n            final_recon = model.decode(full_latent_recon)\n            modality_reconstructions[name] = final_recon.cpu()\n\n    return modality_reconstructions\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixOrchestrator.set_testset","title":"<code>set_testset(testset)</code>","text":"<p>Set the test dataset for the orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>testset</code> <code>MultiModalDataset</code> <p>Test dataset containing multiple modalities</p> required Source code in <code>src/autoencodix/trainers/_stackix_orchestrator.py</code> <pre><code>def set_testset(self, testset: MultiModalDataset) -&gt; None:\n    \"\"\"Set the test dataset for the orchestrator.\n\n    Args:\n        testset: Test dataset containing multiple modalities\n    \"\"\"\n    self._testset = testset\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixOrchestrator.train_modalities","title":"<code>train_modalities()</code>","text":"<p>Trains all modality-specific models.</p> <p>This is the first phase of Stackix training where each modality is trained independently before their latent spaces are combined.</p> <p>Returns:</p> Type Description <code>Dict[str, BaseAutoencoder]</code> <p>Dictionary of trained models for each modality and Dictionary of training results</p> <code>Dict[str, Result]</code> <p>for each modality.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If trainset is not a MultiModalDataset or has no modalities</p> Source code in <code>src/autoencodix/trainers/_stackix_orchestrator.py</code> <pre><code>def train_modalities(self) -&gt; Tuple[Dict[str, BaseAutoencoder], Dict[str, Result]]:\n    \"\"\"Trains all modality-specific models.\n\n    This is the first phase of Stackix training where each modality is\n    trained independently before their latent spaces are combined.\n\n    Returns:\n        Dictionary of trained models for each modality and Dictionary of training results\n        for each modality.\n\n    Raises:\n        ValueError: If trainset is not a MultiModalDataset or has no modalities\n    \"\"\"\n    # if not isinstance(self._trainset, MulDataset):\n    #     raise ValueError(\"Trainset must be a MultiModalDataset for Stackix training\")\n\n    keys = self._trainset.datasets.keys()\n    if not keys:\n        raise ValueError(\"No modalities found in trainset\")\n\n    n_modalities = len(keys)\n\n    # Determine if we should use distributed training\n    n_gpus = self._config.n_gpus if hasattr(self._config, \"n_gpus\") else 0\n    use_distributed = bool(n_gpus and n_gpus &gt; 1 and n_modalities &gt; 1)\n\n    if use_distributed:\n        self._train_distributed(keys=keys)\n    else:\n        self._train_sequential(keys=keys)\n\n    return self.modality_trainers, self.modality_results\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixTrainer","title":"<code>StackixTrainer</code>","text":"<p>               Bases: <code>GeneralTrainer</code></p> <p>StackixTrainer is a wrapper for StackixOrchestrator that conforms to the BaseTrainer interface.</p> <p>This trainer maintains compatibility with the BasePipeline interface while leveraging the more modular and well-designed StackixOrchestrator classes for the actual implementation.</p> <p>Attributes: _workdir: Directory for saving intermediate models and results _result: Result object to store training outcomes _config: Configuration parameters for training and model architecture _model_type: Type of autoencoder model to use for each modality _loss_type: Type of loss function to use for training _trainset: Training dataset containing multiple modalities _validset: Validation dataset containing multiple modalities _orchestrator_type: Type to use for orchestrating modality training (default is StackixOrchestrator) _trainer_type: Type to use for training each modality model (default is GeneralTrainer) _modality_trainers: Dictionary of trained models for each modality _modality_results: Dictionary of training results for each modality _trainer: Trainer for the stacked model _fabric: Lightning Fabric wrapper for device and precision management _train_latent_ds: Training dataset with concatenated latent spaces _valid_latent_ds: Validation dataset with concatenated latent spaces concat_idx: Indices used for concatenating latent spaces _model: The instantiated stacked model architecture _optimizer: The optimizer used for training _orchestrator: The orchestrator that manages modality model training and latent space preparation _workdir: Directory for saving intermediate models and results</p> Source code in <code>src/autoencodix/trainers/_stackix_trainer.py</code> <pre><code>class StackixTrainer(GeneralTrainer):\n    \"\"\"StackixTrainer is a wrapper for StackixOrchestrator that conforms to the BaseTrainer interface.\n\n    This trainer maintains compatibility with the BasePipeline interface while\n    leveraging the more modular and well-designed StackixOrchestrator\n    classes for the actual implementation.\n\n    Attributes:\n    _workdir: Directory for saving intermediate models and results\n    _result: Result object to store training outcomes\n    _config: Configuration parameters for training and model architecture\n    _model_type: Type of autoencoder model to use for each modality\n    _loss_type: Type of loss function to use for training\n    _trainset: Training dataset containing multiple modalities\n    _validset: Validation dataset containing multiple modalities\n    _orchestrator_type: Type to use for orchestrating modality training (default is StackixOrchestrator)\n    _trainer_type: Type to use for training each modality model (default is GeneralTrainer)\n    _modality_trainers: Dictionary of trained models for each modality\n    _modality_results: Dictionary of training results for each modality\n    _trainer: Trainer for the stacked model\n    _fabric: Lightning Fabric wrapper for device and precision management\n    _train_latent_ds: Training dataset with concatenated latent spaces\n    _valid_latent_ds: Validation dataset with concatenated latent spaces\n    concat_idx: Indices used for concatenating latent spaces\n    _model: The instantiated stacked model architecture\n    _optimizer: The optimizer used for training\n    _orchestrator: The orchestrator that manages modality model training and latent space preparation\n    _workdir: Directory for saving intermediate models and results\n    \"\"\"\n\n    def __init__(\n        self,\n        trainset: Optional[StackixDataset],\n        validset: Optional[StackixDataset],\n        result: Result,\n        config: DefaultConfig,\n        model_type: Type[BaseAutoencoder],\n        loss_type: Type[BaseLoss],\n        orchestrator_type: Type[StackixOrchestrator] = StackixOrchestrator,\n        trainer_type: Type[BaseTrainer] = GeneralTrainer,\n        workdir: str = \"./stackix_work\",\n        ontologies: Optional[Tuple] = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize the StackixTrainer with datasets and configuration.\n\n        Args:\n            trainset: Training dataset containing multiple modalities\n            validset: Validation dataset containing multiple modalities\n            result: Result object to store training outcomes\n            config: Configuration parameters for training and model architecture\n            model_type: Type of autoencoder model to use for each modality\n            loss_type: Type of loss function to use for training\n            orchestrator_type: Type to use for orchestrating modality training (default is StackixOrchestrator)\n            trainer_type: Type to use for training each modality model (default is GeneralTrainer)\n            workdir: Directory to save intermediate models and results (default is \"./stackix_work\")\n            onotologies: Ontology information, if provided for Ontix compatibility\n        \"\"\"\n        self._workdir = workdir\n        self._result = result\n        self._config = config\n        self._model_type = model_type\n        self._loss_type = loss_type\n        self._trainset = trainset\n        self._validset = validset\n        self._orchestrator_type = orchestrator_type\n        self._trainer_type = trainer_type\n\n        self._orchestrator = self._orchestrator_type(\n            trainset=trainset,\n            validset=validset,\n            config=config,\n            model_type=model_type,\n            loss_type=loss_type,\n            workdir=workdir,\n        )\n        self._modality_trainers: Optional[Dict[str, BaseAutoencoder]] = None\n        self._modality_results: Optional[Dict[str, Result]] = None\n\n        self._fabric = Fabric(\n            accelerator=self._config.device,\n            devices=self._config.n_gpus,\n            precision=self._config.float_precision,\n            strategy=self._config.gpu_strategy,\n        )\n\n        super().__init__(\n            trainset=trainset,\n            validset=validset,\n            result=result,\n            config=config,\n            model_type=model_type,\n            loss_type=loss_type,\n        )\n\n    def get_model(self) -&gt; torch.nn.Module:\n        \"\"\"Getter for the the trained model.\n\n        Returns:\n            The trained model\n        \"\"\"\n        return self._model\n\n    def train(self) -&gt; Result:\n        \"\"\"Train the stacked model on the concatenated latent space.\n\n        Uses the standard BaseTrainer training process but with the stacked model.\n\n        Returns:\n            Training results including losses, latent spaces, and other metrics\n        \"\"\"\n        print(\"Training each modality model...\")\n        self._train_modalities()\n        print(\"finished training each modality model\")\n        self._trainer = self._trainer_type(\n            trainset=self._train_latent_ds,\n            validset=self._valid_latent_ds,\n            result=self._result,\n            config=self._config,\n            model_type=self._model_type,\n            loss_type=self._loss_type,\n        )\n\n        self._model = self._trainer._model\n        self._result = self._trainer.train()\n\n        return self._result\n\n    def _train_modalities(self) -&gt; None:\n        \"\"\"Trains a Autoencoder for each modality in the dataset.\n        This method orchestrates the training of individual modality models\n\n        This method orchestrates the complete training process:\n        1. Train individual modality models\n        2. Extract and concatenate latent spaces\n        3. Populates the self._modality_models and self._modality_results attributes\n\n        \"\"\"\n        # Step 1: Train individual modality models\n        self._modality_trainers, self._modality_results = (\n            self._orchestrator.train_modalities()\n        )\n\n        self._result.sub_results = self._modality_results\n        # Step 2: Prepare concatenated latent space datasets\n        self._train_latent_ds = self._orchestrator.prepare_latent_datasets(\n            split=\"train\"\n        )\n        self._valid_latent_ds = self._orchestrator.prepare_latent_datasets(\n            split=\"valid\"\n        )\n        self.concat_idx = self._orchestrator.concat_idx\n\n    def _reconstruct(self, split: str) -&gt; None:\n        \"\"\"Orchestrates the reconstruction by delegating the task to the StackixOrchestrator.\n\n        Args:\n            split: The data split to reconstruct ('train', 'valid', 'test').\n        \"\"\"\n        stacked_recon = self._result.reconstructions.get(epoch=-1, split=split)\n        if stacked_recon is None:\n            print(f\"Warning: No reconstruction found for split '{split}'. Skipping.\")\n            return\n\n        # The orchestrator handles the de-concatenation, re-assembly, and decoding.\n        modality_reconstructions = self._orchestrator.reconstruct_from_stack(\n            reconstructed_stack=torch.from_numpy(stacked_recon).to(self._fabric.device)\n        )\n\n        # The result is the final, full-sized data reconstructions.\n        self._result.sub_reconstructions = modality_reconstructions\n\n    def predict(\n        self, data: BaseDataset, model: Optional[torch.nn.Module] = None, **kwargs\n    ) -&gt; Result:\n        \"\"\"Make predictions on the given dataset.\n\n        Args:\n            data: The dataset to make predictions on.\n            model: The model to use for predictions. If None, uses the trained model.\n            **kwargs: Additional keyword arguments.\n        Returns:\n            Result: The prediction results including reconstructions and latent spaces.\n        \"\"\"\n        self.n_test = len(data) if data is not None else 0\n        self._orchestrator.set_testset(testset=data)\n        test_ds = self._orchestrator.prepare_latent_datasets(split=\"test\")\n        self.testset = test_ds\n        print(test_ds)\n        pred_result = super().predict(data=test_ds, model=model)\n        self._result.update(other=pred_result)\n        self._reconstruct(split=\"test\")\n        return self._result\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixTrainer.__init__","title":"<code>__init__(trainset, validset, result, config, model_type, loss_type, orchestrator_type=StackixOrchestrator, trainer_type=GeneralTrainer, workdir='./stackix_work', ontologies=None, **kwargs)</code>","text":"<p>Initialize the StackixTrainer with datasets and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>trainset</code> <code>Optional[StackixDataset]</code> <p>Training dataset containing multiple modalities</p> required <code>validset</code> <code>Optional[StackixDataset]</code> <p>Validation dataset containing multiple modalities</p> required <code>result</code> <code>Result</code> <p>Result object to store training outcomes</p> required <code>config</code> <code>DefaultConfig</code> <p>Configuration parameters for training and model architecture</p> required <code>model_type</code> <code>Type[BaseAutoencoder]</code> <p>Type of autoencoder model to use for each modality</p> required <code>loss_type</code> <code>Type[BaseLoss]</code> <p>Type of loss function to use for training</p> required <code>orchestrator_type</code> <code>Type[StackixOrchestrator]</code> <p>Type to use for orchestrating modality training (default is StackixOrchestrator)</p> <code>StackixOrchestrator</code> <code>trainer_type</code> <code>Type[BaseTrainer]</code> <p>Type to use for training each modality model (default is GeneralTrainer)</p> <code>GeneralTrainer</code> <code>workdir</code> <code>str</code> <p>Directory to save intermediate models and results (default is \"./stackix_work\")</p> <code>'./stackix_work'</code> <code>onotologies</code> <p>Ontology information, if provided for Ontix compatibility</p> required Source code in <code>src/autoencodix/trainers/_stackix_trainer.py</code> <pre><code>def __init__(\n    self,\n    trainset: Optional[StackixDataset],\n    validset: Optional[StackixDataset],\n    result: Result,\n    config: DefaultConfig,\n    model_type: Type[BaseAutoencoder],\n    loss_type: Type[BaseLoss],\n    orchestrator_type: Type[StackixOrchestrator] = StackixOrchestrator,\n    trainer_type: Type[BaseTrainer] = GeneralTrainer,\n    workdir: str = \"./stackix_work\",\n    ontologies: Optional[Tuple] = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the StackixTrainer with datasets and configuration.\n\n    Args:\n        trainset: Training dataset containing multiple modalities\n        validset: Validation dataset containing multiple modalities\n        result: Result object to store training outcomes\n        config: Configuration parameters for training and model architecture\n        model_type: Type of autoencoder model to use for each modality\n        loss_type: Type of loss function to use for training\n        orchestrator_type: Type to use for orchestrating modality training (default is StackixOrchestrator)\n        trainer_type: Type to use for training each modality model (default is GeneralTrainer)\n        workdir: Directory to save intermediate models and results (default is \"./stackix_work\")\n        onotologies: Ontology information, if provided for Ontix compatibility\n    \"\"\"\n    self._workdir = workdir\n    self._result = result\n    self._config = config\n    self._model_type = model_type\n    self._loss_type = loss_type\n    self._trainset = trainset\n    self._validset = validset\n    self._orchestrator_type = orchestrator_type\n    self._trainer_type = trainer_type\n\n    self._orchestrator = self._orchestrator_type(\n        trainset=trainset,\n        validset=validset,\n        config=config,\n        model_type=model_type,\n        loss_type=loss_type,\n        workdir=workdir,\n    )\n    self._modality_trainers: Optional[Dict[str, BaseAutoencoder]] = None\n    self._modality_results: Optional[Dict[str, Result]] = None\n\n    self._fabric = Fabric(\n        accelerator=self._config.device,\n        devices=self._config.n_gpus,\n        precision=self._config.float_precision,\n        strategy=self._config.gpu_strategy,\n    )\n\n    super().__init__(\n        trainset=trainset,\n        validset=validset,\n        result=result,\n        config=config,\n        model_type=model_type,\n        loss_type=loss_type,\n    )\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixTrainer.get_model","title":"<code>get_model()</code>","text":"<p>Getter for the the trained model.</p> <p>Returns:</p> Type Description <code>Module</code> <p>The trained model</p> Source code in <code>src/autoencodix/trainers/_stackix_trainer.py</code> <pre><code>def get_model(self) -&gt; torch.nn.Module:\n    \"\"\"Getter for the the trained model.\n\n    Returns:\n        The trained model\n    \"\"\"\n    return self._model\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixTrainer.predict","title":"<code>predict(data, model=None, **kwargs)</code>","text":"<p>Make predictions on the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>BaseDataset</code> <p>The dataset to make predictions on.</p> required <code>model</code> <code>Optional[Module]</code> <p>The model to use for predictions. If None, uses the trained model.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:     Result: The prediction results including reconstructions and latent spaces.</p> Source code in <code>src/autoencodix/trainers/_stackix_trainer.py</code> <pre><code>def predict(\n    self, data: BaseDataset, model: Optional[torch.nn.Module] = None, **kwargs\n) -&gt; Result:\n    \"\"\"Make predictions on the given dataset.\n\n    Args:\n        data: The dataset to make predictions on.\n        model: The model to use for predictions. If None, uses the trained model.\n        **kwargs: Additional keyword arguments.\n    Returns:\n        Result: The prediction results including reconstructions and latent spaces.\n    \"\"\"\n    self.n_test = len(data) if data is not None else 0\n    self._orchestrator.set_testset(testset=data)\n    test_ds = self._orchestrator.prepare_latent_datasets(split=\"test\")\n    self.testset = test_ds\n    print(test_ds)\n    pred_result = super().predict(data=test_ds, model=model)\n    self._result.update(other=pred_result)\n    self._reconstruct(split=\"test\")\n    return self._result\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.StackixTrainer.train","title":"<code>train()</code>","text":"<p>Train the stacked model on the concatenated latent space.</p> <p>Uses the standard BaseTrainer training process but with the stacked model.</p> <p>Returns:</p> Type Description <code>Result</code> <p>Training results including losses, latent spaces, and other metrics</p> Source code in <code>src/autoencodix/trainers/_stackix_trainer.py</code> <pre><code>def train(self) -&gt; Result:\n    \"\"\"Train the stacked model on the concatenated latent space.\n\n    Uses the standard BaseTrainer training process but with the stacked model.\n\n    Returns:\n        Training results including losses, latent spaces, and other metrics\n    \"\"\"\n    print(\"Training each modality model...\")\n    self._train_modalities()\n    print(\"finished training each modality model\")\n    self._trainer = self._trainer_type(\n        trainset=self._train_latent_ds,\n        validset=self._valid_latent_ds,\n        result=self._result,\n        config=self._config,\n        model_type=self._model_type,\n        loss_type=self._loss_type,\n    )\n\n    self._model = self._trainer._model\n    self._result = self._trainer.train()\n\n    return self._result\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.XModalTrainer","title":"<code>XModalTrainer</code>","text":"<p>               Bases: <code>BaseTrainer</code></p> <p>Trainer for cross-modal autoencoders, implements multimodal training with adversarial component.</p> <p>Attributes:</p> Name Type Description <code>_trainset</code> <p>The dataset used for training, must be a MultiModalDataset.</p> <code>_n_train_modalities</code> <p>Number of modalities in the training dataset.</p> <code>model_map</code> <p>Mapping from DataSetTypes to specific autoencoder architectures.</p> <code>model_trainer_map</code> <p>Mapping from autoencoder architectures to their corresponding trainer classes.</p> <code>_n_cpus</code> <p>Number of CPU cores available for data loading.</p> <code>latent_dim</code> <p>Dimensionality of the shared latent space.</p> <code>n_test</code> <code>Optional[int]</code> <p>Number of samples in the test set, if provided.</p> <code>n_train</code> <p>Number of samples in the training set.</p> <code>n_valid</code> <p>Number of samples in the validation set, if provided.</p> <code>n_features</code> <p>Total number of features across all modalities.</p> <code>_cur_epoch</code> <code>int</code> <p>Current epoch number during training.</p> <code>_is_checkpoint_epoch</code> <code>Optional[bool]</code> <p>Flag indicating if the current epoch is a checkpoint epoch.</p> <code>_sub_loss_type</code> <p>Loss function type for individual modality autoencoders.</p> <code>sub_loss</code> <p>Instantiated loss function for individual modality autoencoders.</p> <code>_clf_epoch_loss</code> <code>float</code> <p>Cumulative classifier loss for the current epoch.</p> <code>_epoch_loss</code> <code>float</code> <p>Cumulative total loss for the current epoch.</p> <code>_epoch_loss_valid</code> <code>float</code> <p>Cumulative total validation loss for the current epoch.</p> <code>_modality_dynamics</code> <code>float</code> <p>Dictionary holding model, optimizer, and training state for each modality.</p> <code>_latent_clf</code> <code>float</code> <p>Classifier model for adversarial training on latent spaces.</p> <code>_clf_optim</code> <code>float</code> <p>Optimizer for the classifier model.</p> <code>_clf_loss_fn</code> <code>float</code> <p>Loss function for the classifier.</p> <code>_trainloader</code> <code>float</code> <p>DataLoader for the training dataset.</p> <code>_validloader</code> <code>float</code> <p>DataLoader for the validation dataset, if provided.</p> <code>_model</code> <code>float</code> <p>The instantiated stacked model architecture.</p> <code>_validset</code> <p>The dataset used for validation, if provided, must be a MultiModalDataset.</p> <code>_fabric</code> <p>Lightning Fabric wrapper for device and precision management.</p> Source code in <code>src/autoencodix/trainers/_xmodal_trainer.py</code> <pre><code>class XModalTrainer(BaseTrainer):\n    \"\"\"Trainer for cross-modal autoencoders, implements multimodal training with adversarial component.\n\n    Attributes:\n        _trainset: The dataset used for training, must be a MultiModalDataset.\n        _n_train_modalities: Number of modalities in the training dataset.\n        model_map: Mapping from DataSetTypes to specific autoencoder architectures.\n        model_trainer_map: Mapping from autoencoder architectures to their corresponding trainer classes.\n        _n_cpus: Number of CPU cores available for data loading.\n        latent_dim: Dimensionality of the shared latent space.\n        n_test: Number of samples in the test set, if provided.\n        n_train: Number of samples in the training set.\n        n_valid: Number of samples in the validation set, if provided.\n        n_features: Total number of features across all modalities.\n        _cur_epoch: Current epoch number during training.\n        _is_checkpoint_epoch: Flag indicating if the current epoch is a checkpoint epoch.\n        _sub_loss_type: Loss function type for individual modality autoencoders.\n        sub_loss: Instantiated loss function for individual modality autoencoders.\n        _clf_epoch_loss: Cumulative classifier loss for the current epoch.\n        _epoch_loss: Cumulative total loss for the current epoch.\n        _epoch_loss_valid: Cumulative total validation loss for the current epoch.\n        _modality_dynamics: Dictionary holding model, optimizer, and training state for each modality.\n        _latent_clf: Classifier model for adversarial training on latent spaces.\n        _clf_optim: Optimizer for the classifier model.\n        _clf_loss_fn: Loss function for the classifier.\n        _trainloader: DataLoader for the training dataset.\n        _validloader: DataLoader for the validation dataset, if provided.\n        _model: The instantiated stacked model architecture.\n        _validset: The dataset used for validation, if provided, must be a MultiModalDataset.\n        _fabric: Lightning Fabric wrapper for device and precision management.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        trainset: MultiModalDataset,\n        validset: MultiModalDataset,\n        result: Result,\n        config: DefaultConfig,\n        model_type: Type[BaseAutoencoder],\n        loss_type: Type[BaseLoss],\n        sub_loss_type: Type[BaseLoss] = VarixLoss,\n        model_map: Dict[DataSetTypes, Type[BaseAutoencoder]] = {\n            DataSetTypes.NUM: VarixArchitecture,\n            DataSetTypes.IMG: ImageVAEArchitecture,\n        },\n        ontologies: Optional[Union[Tuple, List]] = None,\n        **kwargs,\n    ):\n        \"\"\"Initializes the XModalTrainer with datasets and configuration.\n\n        Args:\n            trainset: Training dataset containing multiple modalities\n            validset: Validation dataset containing multiple modalities\n            result: Result object to store training outcomes\n            config: Configuration parameters for training and model architecture\n            model_type: Type of autoencoder model to use for each modality (not used directly)\n            loss_type: Type of loss function to use for training the stacked model\n            sub_loss_type: Type of loss function to use for individual modality autoencoders\n            model_map: Mapping from DataSetTypes to specific autoencoder architectures\n            ontologies: Ontology information, for compatibility with Ontix\n        \"\"\"\n        self._trainset = trainset\n        self._n_train_modalities = self._trainset.n_modalities  # type: ignore\n        self.model_map = model_map\n        self.model_trainer_map = model_trainer_map\n        self._n_cpus = os.cpu_count()\n        if self._n_cpus is None:\n            self._n_cpus = 0\n\n        super().__init__(\n            trainset, validset, result, config, model_type, loss_type, ontologies\n        )\n        # init attributes ------------------------------------------------\n        self.latent_dim = config.latent_dim\n        self.n_test: Optional[int] = None\n        self.n_train = len(trainset.data) if trainset else 0\n        self.n_valid = len(validset.data) if validset else 0\n        self.n_features = trainset.get_input_dim() if trainset else 0\n        self._cur_epoch: int = 0\n        self._is_checkpoint_epoch: Optional[bool] = None\n        self._sub_loss_type = sub_loss_type\n        self.sub_loss = sub_loss_type(config=self._config)\n        self._clf_epoch_loss: float = 0.0\n        self._epoch_loss: float = 0.0\n        self._epoch_loss_valid: float = 0.0\n        self._all_dynamics: Dict[str, Dict[int, Dict]] = {\n            \"test\": defaultdict(dict),\n            \"train\": defaultdict(dict),\n            \"valid\": defaultdict(dict),\n        }\n\n    def _init_model_architecture(self, ontologies: Optional[Tuple] = None):\n        \"\"\"Override parent's model init - we don't use a single model, needs to be there because is abstract in parent.\"\"\"\n        pass\n\n    def _setup_fabric(self, old_model=None):\n        \"\"\"Sets up the models, optimizers, and data loaders with Lightning Fabric.\"\"\"\n\n        self._fabric.launch()\n        self._init_adversarial_training()\n        self._init_modality_training()\n        self._latent_clf, self._clf_optim = self._fabric.setup(\n            self._latent_clf, self._clf_optim\n        )\n\n        self._trainloader = self._fabric.setup_dataloaders(self._trainloader)\n        if self._validloader is not None:\n            self._validloader = self._fabric.setup_dataloaders(self._validloader)\n\n        for dynamics in self._modality_dynamics.values():\n            dynamics[\"model\"], dynamics[\"optim\"] = self._fabric.setup(\n                dynamics[\"model\"], dynamics[\"optim\"]\n            )\n\n    # def _init_loaders(self):\n    #     \"\"\"Initializes DataLoaders for training and validation datasets.\"\"\"\n    #     trainsampler = CoverageEnsuringSampler(\n    #         datasets=self._trainset, batch_size=self._config.batch_size\n    #     )\n    #     validsampler = CoverageEnsuringSampler(\n    #         datasets=self._validset, batch_size=self._config.batch_size\n    #     )\n    #     collate_fn = create_multimodal_collate_fn(datasets=self._trainset)\n    #     valid_collate_fn = create_multimodal_collate_fn(\n    #         datasets=self._validset\n    #     )\n    #     # drop_last handled in custom sampler\n    #     self._trainloader = DataLoader(\n    #         self._trainset,\n    #         batch_sampler=trainsampler,\n    #         collate_fn=collate_fn,\n    #     )\n    #     self._validloader = DataLoader(\n    #         self._validset,\n    #         batch_sampler=validsampler,\n    #         collate_fn=valid_collate_fn,\n    #     )\n\n    def _init_loaders(self):\n        \"\"\"Initializes DataLoaders with smart sampler selection based on pairing.\"\"\"\n\n        def _build_loader(dataset: MultiModalDataset, is_train: bool):\n            batch_size = self._config.batch_size\n            collate_fn = create_multimodal_collate_fn(multimodal_dataset=dataset)\n\n            if dataset.is_fully_paired:\n                return DataLoader(\n                    dataset,\n                    batch_size=batch_size,\n                    shuffle=is_train,\n                    drop_last=is_train,\n                    collate_fn=collate_fn,\n                )\n            else:\n                print(\n                    f\"Dataset has UNPAIRED samples \u2192 using CoverageEnsuringSampler \"\n                    f\"({len(dataset.paired_sample_ids)} paired + {len(dataset.unpaired_sample_ids)} unpaired)\"\n                )\n                sampler = CoverageEnsuringSampler(\n                    multimodal_dataset=dataset,\n                    batch_size=batch_size,\n                )\n                return DataLoader(\n                    dataset,\n                    batch_sampler=sampler,  # note: batch_sampler, not sampler\n                    collate_fn=collate_fn,\n                )\n\n        # Build train and validation loaders\n        self._trainloader = _build_loader(self._trainset, is_train=True)\n        self._validloader = _build_loader(self._validset, is_train=False)\n\n        # Optional: expose for logging\n        self._train_is_fully_paired = len(self._trainset.unpaired_sample_ids) == 0\n\n    def _init_modality_training(self):\n        \"\"\"Initializes models, optimizers, and training state for each modality.\"\"\"\n        self._modality_dynamics = {\n            mod_name: None for mod_name in self._trainset.datasets.keys()\n        }\n\n        self._result.sub_results = {\n            mod_name: None for mod_name in self._trainset.datasets.keys()\n        }\n\n        data_info = self._config.data_config.data_info\n        for mod_name, ds in self._trainset.datasets.items():\n            simple_name = mod_name.split(\".\")[1]\n            local_epochs = data_info[simple_name].pretrain_epochs\n\n            pretrain_epochs = (\n                local_epochs\n                if local_epochs is not None\n                else self._config.pretrain_epochs\n            )\n\n            model_type = self.model_map.get(ds.mytype)\n            if model_type is None:\n                raise ValueError(\n                    f\"No Mapping exists for {ds.mytype}, you passed this mapping: {self.model_map}\"\n                )\n            model = model_type(config=self._config, input_dim=ds.get_input_dim())\n            optimizer = torch.optim.AdamW(\n                params=model.parameters(),\n                lr=self._config.learning_rate,\n                weight_decay=self._config.weight_decay,\n            )\n            self._modality_dynamics[mod_name] = {\n                \"model\": model,\n                \"optim\": optimizer,\n                \"mp\": [],\n                \"losses\": [],\n                \"config_name\": simple_name,\n                \"pretrain_epochs\": pretrain_epochs,\n                \"mytype\": ds.mytype,\n                \"pretrain_result\": None,\n            }\n\n    def _init_adversarial_training(self):\n        \"\"\"Initializes the classifier and its optimizer for adversarial training.\"\"\"\n        self._latent_clf = Classifier(\n            input_dim=self._config.latent_dim, n_modalities=self._n_train_modalities\n        )\n        self._clf_optim = torch.optim.AdamW(\n            params=self._latent_clf.parameters(),\n            lr=self._config.learning_rate,\n            weight_decay=self._config.weight_decay,\n        )\n        self._clf_loss_fn = torch.nn.CrossEntropyLoss(\n            reduction=self._config.loss_reduction\n        )\n\n    def _modalities_forward(self, batch: Dict[str, Dict[str, Any]]):\n        \"\"\"Performs forward pass for each modality in the batch and computes losses.\"\"\"\n        for k, v in batch.items():\n            model = self._modality_dynamics[k][\"model\"]\n            data = v[\"data\"]\n\n            mp = model(data)\n\n            loss, loss_stats = self.sub_loss(\n                model_output=mp, targets=data, epoch=self._cur_epoch\n            )\n\n            self._modality_dynamics[k][\"loss_stats\"] = loss_stats\n            self._modality_dynamics[k][\"loss\"] = loss\n            self._modality_dynamics[k][\"mp\"] = mp\n\n    def _prep_adver_training(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Prepares concatenated latent spaces and corresponding labels for adversarial training.\n\n        Returns:\n            A tuple containing:\n            - latents: Concatenated latent space tensor of shape (total_samples, latent_dim)\n            - labels: Tensor of modality labels of shape (total_samples,)\n        \"\"\"\n        all_latents: List[torch.Tensor] = []\n        all_labels: List[torch.Tensor] = []\n        mod2idx = {\n            mod_name: idx for idx, mod_name in enumerate(self._modality_dynamics.keys())\n        }\n\n        for mod_name, helper in self._modality_dynamics.items():\n            output: ModelOutput = helper[\"mp\"]\n            latents = output.latentspace\n            label_id = mod2idx[mod_name]\n            all_latents.append(latents)\n            all_labels.append(\n                torch.full(\n                    (latents.size(0),),\n                    fill_value=label_id,\n                    dtype=torch.long,\n                    device=self._fabric.device,\n                )\n            )\n\n        return torch.cat(all_latents, dim=0), torch.cat(all_labels, dim=0)\n\n    def _train_clf(self, latents: torch.Tensor, labels: torch.Tensor) -&gt; None:\n        \"\"\"Performs a single optimization step for the classifier.\n        Args:\n            latents: Concatenated latent space tensor of shape (total_samples, latent_dim)\n            labels: Tensor of modality labels of shape (total_samples,)\n\n        \"\"\"\n        self._clf_optim.zero_grad()\n        # We must detach the latents from the computation graph. This serves two purposes:\n        # 1. It isolates the classifier, ensuring that gradients only update its weights.\n        # 2. It prevents a \"two backward passes\" error by leaving the graph to the\n        #    autoencoders untouched, ready for use in the next stage.\n        detached_latents = latents.detach()\n        clf_scores = self._latent_clf(detached_latents)\n        clf_loss = self._clf_loss_fn(clf_scores, labels)\n        self._fabric.backward(clf_loss)\n        self._clf_optim.step()\n        self._clf_epoch_loss += clf_loss.item()\n\n    def _validate_one_epoch(self) -&gt; Tuple[List[Dict], Dict[str, float], int]:\n        \"\"\"Runs a single, combined validation pass for the entire model, including dedicated metrics for the classifier.\n\n        Returns:\n            A tuple containing:\n            - epoch_dynamics: List of dictionaries capturing dynamics for each batch\n            - sub_losses: Dictionary of accumulated sub-losses for the epoch\n            - n_samples_total: Total number of samples processed in validation\n        \"\"\"\n        for dynamics in self._modality_dynamics.values():\n            dynamics[\"model\"].eval()\n        self._latent_clf.eval()\n\n        self._epoch_loss_valid = 0.0\n        total_clf_loss = 0.0\n        n_samples_total: int = 0\n        epoch_dynamics: List[Dict] = []\n        sub_losses: Dict[str, float] = defaultdict(float)\n\n        with torch.no_grad():\n            for batch in self._validloader:\n                with self._fabric.autocast():\n                    self._modalities_forward(batch=batch)\n                    latents, labels = self._prep_adver_training()\n                    n_samples_total += len(labels)\n                    clf_scores = self._latent_clf(latents)\n\n                    batch_loss, loss_dict = self._loss_fn(\n                        batch=batch,\n                        modality_dynamics=self._modality_dynamics,\n                        clf_scores=clf_scores,\n                        labels=labels,\n                        clf_loss_fn=self._clf_loss_fn,\n                        is_training=False,\n                    )\n                    self._epoch_loss_valid += batch_loss.item()\n                    for k, v in loss_dict.items():\n                        value = v.item() if hasattr(v, \"item\") else v\n                        if \"_factor\" not in k:\n                            sub_losses[k] += value\n                        else:\n                            sub_losses[k] = value\n\n                    clf_loss = self._clf_loss_fn(clf_scores, labels)\n                    total_clf_loss += clf_loss.item()\n                    if self._is_checkpoint_epoch:\n                        batch_capture = self._capture_dynamics(batch)\n                        epoch_dynamics.append(batch_capture)\n\n        sub_losses[\"clf_loss\"] = total_clf_loss\n        n_samples_total /= (\n            self._n_train_modalities\n        )  # n_modalities because each sample is counted once per modality and n_modalites is the same for train and valid\n        for k, v in sub_losses.items():\n            if \"_factor\" not in k:\n                sub_losses[k] = v / n_samples_total  # Average over all samples\n        self._epoch_loss_valid = (\n            self._epoch_loss_valid / n_samples_total\n        )  # Average over all samples\n        return epoch_dynamics, sub_losses, len(self._validset)\n\n    def _train_one_epoch(self) -&gt; Tuple[List[Dict], Dict[str, float], int]:\n        \"\"\"Runs the training loop with corrected adversarial logic.\n        Returns:\n            A tuple containing:\n            - epoch_dynamics: List of dictionaries capturing dynamics for each batch\n            - sub_losses: Dictionary of accumulated sub-losses for the epoch\n            - n_samples_total: Total number of samples processed in training\n\n        \"\"\"\n        for dynamics in self._modality_dynamics.values():\n            dynamics[\"model\"].train()\n        self._latent_clf.train()\n\n        self._clf_epoch_loss = 0\n        self._epoch_loss = 0\n        epoch_dynamics: List[Dict] = []\n        sub_losses: Dict[str, float] = defaultdict(float)\n        n_samples_total: int = (\n            0  # because of unpaired training we need to sum the samples instead of using len(dataset)\n        )\n\n        for batch in self._trainloader:\n            with self._fabric.autocast():\n                # --- Stage 1: forward for each data modality ---\n                self._modalities_forward(batch=batch)\n\n                # --- Stage 2: Train the Classifier ---\n                latents, labels = self._prep_adver_training()\n                n_samples_total += latents.size(0)\n                self._train_clf(latents=latents, labels=labels)\n\n                # --- Stage 3: Train the Autoencoders ---\n                for _, dynamics in self._modality_dynamics.items():\n                    dynamics[\"optim\"].zero_grad()\n\n                # We re-calculate scores here and cannot reuse the `clf_scores` from Stage 2.\n                # The previous scores were from detached latents and would block the gradients\n                # needed to train the autoencoders adversarially.\n                clf_scores_for_adv = self._latent_clf(latents)\n\n                batch_loss, loss_dict = self._loss_fn(\n                    batch=batch,\n                    modality_dynamics=self._modality_dynamics,\n                    clf_scores=clf_scores_for_adv,\n                    labels=labels,\n                    clf_loss_fn=self._clf_loss_fn,\n                    is_training=True,\n                )\n            self._fabric.backward(batch_loss)\n            for _, dynamics in self._modality_dynamics.items():\n                dynamics[\"optim\"].step()\n\n            # --- Logging and Capturing ---\n            self._epoch_loss += batch_loss.item()\n            for k, v in loss_dict.items():\n                value_to_add = v.item() if hasattr(v, \"item\") else v\n                if \"_factor\" not in k:\n                    sub_losses[k] += value_to_add\n                else:\n                    sub_losses[k] = value_to_add\n\n            if self._is_checkpoint_epoch:\n                batch_capture = self._capture_dynamics(batch)\n                epoch_dynamics.append(batch_capture)\n        n_samples_total /= self._n_train_modalities\n        sub_losses[\"clf_loss\"] = self._clf_epoch_loss\n        for k, v in sub_losses.items():\n            if \"_factor\" not in k:\n                sub_losses[k] = v / n_samples_total  # Average over all samples\n        self._epoch_loss = (\n            self._epoch_loss / n_samples_total\n        )  # Average over all samples\n\n        return epoch_dynamics, sub_losses, n_samples_total\n\n    def _pretraining(self):\n        \"\"\"Pretrain each modality's model if needed.\"\"\"\n        for mod_name, dynamic in self._modality_dynamics.items():\n            mytype = dynamic.get(\"mytype\")\n            pretrain_epochs = dynamic.get(\"pretrain_epochs\")\n            print(f\"Check if we need to pretrain: {mod_name}\")\n            print(f\"pretrain epochs : {pretrain_epochs}\")\n            if not pretrain_epochs:\n                print(f\"No pretraining for {mod_name}\")\n                continue\n            model_type = self.model_map.get(mytype)\n            pretrainer_type = self.model_trainer_map.get(model_type)\n            print(f\"Starting Pretraining for: {mod_name} with {pretrainer_type}\")\n            trainset = self._trainset.datasets.get(mod_name)\n            validset = self._validset.datasets.get(mod_name)\n            pretrainer = pretrainer_type(\n                trainset=trainset,\n                validset=validset,\n                result=Result(),\n                config=self._config,\n                model_type=model_type,\n                loss_type=self._sub_loss_type,\n                ontologies=self.ontologies,\n            )\n            pretrain_result = pretrainer.train(epochs_overwrite=pretrain_epochs)\n            self._result.sub_results[f\"pretrain.{mod_name}\"] = pretrain_result\n            self._modality_dynamics[mod_name][\"pretrain_result\"] = pretrain_result\n            self._modality_dynamics[mod_name][\"model\"] = pretrain_result.model\n\n    def train(self):\n        \"\"\"Orchestrates the full training process for the cross-modal autoencoder.\"\"\"\n        self._pretraining()\n        for epoch in range(self._config.epochs):\n            self._cur_epoch = epoch\n            self._is_checkpoint_epoch = self._should_checkpoint(epoch=epoch)\n            self._fabric.print(f\"--- Epoch {epoch + 1}/{self._config.epochs} ---\")\n            train_epoch_dynamics, train_sub_losses, n_samples_train = (\n                self._train_one_epoch()\n            )\n\n            self._log_losses(\n                split=\"train\",\n                total_loss=self._epoch_loss,\n                sub_losses=train_sub_losses,\n                n_samples=n_samples_train,\n            )\n\n            if self._validset:\n                valid_epoch_dynamics, valid_sub_losses, n_samples_valid = (\n                    self._validate_one_epoch()\n                )\n                self._log_losses(\n                    split=\"valid\",\n                    total_loss=self._epoch_loss_valid,\n                    sub_losses=valid_sub_losses,\n                    n_samples=n_samples_valid,\n                )\n            if self._is_checkpoint_epoch:\n                self._fabric.print(f\"Storing checkpoint for epoch {epoch}...\")\n                self._store_checkpoint(\n                    split=\"train\", epoch_dynamics=train_epoch_dynamics\n                )\n                if self._validset:\n                    self._store_checkpoint(\n                        split=\"valid\",\n                        epoch_dynamics=valid_epoch_dynamics,\n                    )\n        # save final model\n        self._store_checkpoint(split=\"train\", epoch_dynamics=train_epoch_dynamics)\n        self._store_final_models()\n        return self._result\n\n    def decode(self, x: torch.Tensor):\n        \"\"\"Decodes input latent representations\n        Args:\n            x: Latent representations to decode, shape (n_samples, latent_dim)\n        \"\"\"\n        raise NotImplementedError(\"General decode step is not implemented for XModalix\")\n\n    def predict(\n        self,\n        data: BaseDataset,\n        model: Optional[Dict[str, torch.nn.Module]] = None,\n        **kwargs,\n    ) -&gt; Result:\n        \"\"\"Performs cross-modal prediction from a specified 'from' modality to a 'to' modality.\n\n        The direction is determined by the 'translate_direction' attribute in the config.\n        Results are stored in the Result object under split='test' and epoch=-1.\n\n        Args:\n            data: A MultiModalDataset containing the input data for the 'from' modality.\n\n        Returns:\n            The Result object populated with prediction results.\n        \"\"\"\n        split: str = kwargs.pop(\"split\", \"test\")\n        if split not in [\"train\", \"valid\", \"test\"]:\n            raise ValueError(\n                f\"split must be one of 'train', 'valid', or 'test', got {split}\"\n            )\n        if not isinstance(data, MultiModalDataset):\n            raise TypeError(\n                f\"type of data has to be MultiModalDataset, got: {type(data)}\"\n            )\n        if model is not None and not hasattr(self, \"_modality_dynamics\"):\n            self._modality_dynamics = {mod_name: {} for mod_name in model.keys()}\n\n            for mod_name, mod_model in model.items():\n                mod_model.eval()\n                if not isinstance(mod_model, _FabricModule):\n                    mod_model = self._fabric.setup_module(mod_model)\n                mod_model.to(self._fabric.device)\n                self._modality_dynamics[mod_name][\"model\"] = mod_model\n            # self._setup_fabric(old_model=model)\n\n        from_key = kwargs.pop(\"from_key\", None)\n        to_key = kwargs.pop(\"to_key\", None)\n        predict_keys = find_translation_keys(\n            config=self._config,\n            trained_modalities=list(self._modality_dynamics.keys()),\n            from_key=from_key,\n            to_key=to_key,\n        )\n        from_key, to_key = predict_keys[\"from\"], predict_keys[\"to\"]\n        from_modality = self._modality_dynamics[from_key]\n        to_modality = self._modality_dynamics[to_key]\n        from_model = from_modality[\"model\"]\n        to_model = to_modality[\"model\"]\n        from_model.eval(), to_model.eval()\n        # drop_last handled in custom sampler\n        inference_loader = DataLoader(\n            data,\n            batch_sampler=CoverageEnsuringSampler(\n                multimodal_dataset=data, batch_size=self._config.batch_size\n            ),\n            collate_fn=create_multimodal_collate_fn(multimodal_dataset=data),\n        )\n        inference_loader = self._fabric.setup_dataloaders(inference_loader)  # type: ignore\n        epoch_dynamics: List[Dict] = []\n        with (\n            self._fabric.autocast(),\n            torch.inference_mode(),\n        ):\n            for batch in inference_loader:\n                # needed for visualize later\n                self._get_vis_dynamics(batch=batch)\n                from_z = self._modality_dynamics[from_key][\"mp\"].latentspace\n                translated = to_model.decode(x=from_z)\n\n                # for reference\n                to_z = self._modality_dynamics[to_key][\"mp\"].latentspace\n                to_to_reference = to_model.decode(x=to_z)\n\n                batch_capture = self._capture_dynamics(batch)\n                translation_key = \"translation\"\n\n                reference_key = f\"reference_{to_key}_to_{to_key}\"\n                batch_capture[\"reconstructions\"][\n                    translation_key\n                ] = translated.cpu().numpy()\n\n                batch_capture[\"reconstructions\"][\n                    reference_key\n                ] = to_to_reference.cpu().numpy()\n\n                if \"sample_ids\" in batch[from_key]:\n                    batch_capture[\"sample_ids\"][translation_key] = np.array(\n                        batch[from_key][\"sample_ids\"]\n                    )\n                if \"sample_ids\" in batch[to_key]:\n                    batch_capture[\"sample_ids\"][reference_key] = np.array(\n                        batch[to_key][\"sample_ids\"]\n                    )\n\n                epoch_dynamics.append(batch_capture)\n\n        self._dynamics_to_result(split=split, epoch_dynamics=epoch_dynamics)\n\n        print(\"Prediction complete.\")\n        return self._result\n\n    def _get_vis_dynamics(self, batch: Dict[str, Dict[str, Any]]):\n        \"\"\"Runs a forward pass for each modality in the batch and stores the model outputs.\n\n        This is used to capture dynamics for visualization or analysis not for actual translation as specified in predict.\n        Args:\n            batch: A dictionary where keys are modality names and values are dictionaries containing 'data' tensors.\n        \"\"\"\n        for mod_name, mod_data in batch.items():\n            model = self._modality_dynamics[mod_name][\"model\"]\n            # Store model output (containing latents, recons, etc.)\n            self._modality_dynamics[mod_name][\"mp\"] = model(mod_data[\"data\"])\n\n    def _capture_dynamics(\n        self,\n        batch_data: Union[Dict[str, Dict[str, Any]], Any],\n    ) -&gt; Union[Dict[str, Dict[str, np.ndarray]], Any]:\n        \"\"\"Captures and returns the dynamics (latents, reconstructions, etc.) for each modality in the batch.\n\n        Args:\n            batch_data: A dictionary where keys are modality names and values are dictionaries containing 'data' tensors\n                         and optionally 'sample_ids'.\n        Returns:\n            A dictionary with keys 'latentspaces', 'reconstructions', 'mus', 'sigmas', and 'sample_ids',\n            each mapping to another dictionary where keys are modality names and values are numpy arrays.\n        \"\"\"\n        captured_data: Dict[str, Dict[str, Any]] = {\n            \"latentspaces\": {},\n            \"reconstructions\": {},\n            \"mus\": {},\n            \"sigmas\": {},\n            \"sample_ids\": {},\n        }\n        for mod_name, dynamics in self._modality_dynamics.items():\n            sample_ids = batch_data[mod_name].get(\"sample_ids\")\n            # get index of\n            if sample_ids is not None:\n                captured_data[\"sample_ids\"][mod_name] = np.array(sample_ids)\n\n            model_output = dynamics[\"mp\"]\n            captured_data[\"latentspaces\"][mod_name] = (\n                model_output.latentspace.detach().cpu().numpy()\n            )\n            captured_data[\"reconstructions\"][mod_name] = (\n                model_output.reconstruction.detach().cpu().numpy()\n            )\n            if model_output.latent_mean is not None:\n                captured_data[\"mus\"][mod_name] = (\n                    model_output.latent_mean.detach().cpu().numpy()\n                )\n            if model_output.latent_logvar is not None:\n                captured_data[\"sigmas\"][mod_name] = (\n                    model_output.latent_logvar.detach().cpu().numpy()\n                )\n\n        return captured_data\n\n    def _dynamics_to_result(self, split: str, epoch_dynamics: List[Dict]) -&gt; None:\n        \"\"\"Aggregates and stores epoch dynamics into the Result object.\n\n        Due to our multi modal Traning with unpaired data, we can see sample_ids more than once per epoch.\n        For more consistent downstream analysis, we only keep the first occurence of this sample in the epoch\n        and report the dynamics for this sample\n\n        Args:\n            split: The data split name (e.g., 'train', 'valid', 'test').\n            epoch_dynamics: List of dictionaries capturing dynamics for each batch in the epoch.\n\n        \"\"\"\n        final_data: Dict[str, Any] = defaultdict(lambda: defaultdict(list))\n        for batch_data in epoch_dynamics:\n            for dynamic_type, mod_data in batch_data.items():\n                for mod_name, data in mod_data.items():\n                    final_data[dynamic_type][mod_name].append(data)\n\n        sample_ids: Optional[Dict[str, np.ndarray]] = final_data.get(\"sample_ids\")\n        if sample_ids is None:\n            raise ValueError(\"No Sample Ids in TrainingDynamics\")\n        concat_ids: Dict[str, np.ndarray] = {\n            k: np.concatenate(v) for k, v in sample_ids.items()\n        }\n        unique_idx: Dict[str, np.ndarray] = {\n            k: np.unique(v, return_index=True)[1] for k, v in concat_ids.items()\n        }\n\n        deduplicated_data: Dict[str, Dict[str, np.ndarray]] = defaultdict(dict)\n        # all_dynamics_inner: Dict[str, Any] = {}\n        for dynamic_type, dynamic_data in final_data.items():\n            concat_dynamic: Dict[str, np.ndarray] = {\n                k: np.concatenate(v) for k, v in dynamic_data.items()\n            }\n            # all_dynamics_inner[dynamic_type] = concat_dynamic\n            deduplicated_helper: Dict[str, np.ndarray] = {\n                k: v[unique_idx[k]] for k, v in concat_dynamic.items()\n            }\n            deduplicated_data[dynamic_type] = deduplicated_helper\n            # Store the deduplicated data\n        # self._all_dynamics[split][self._cur_epoch] = all_dynamics_inner\n        self._result.latentspaces.add(\n            epoch=self._cur_epoch,\n            split=split,\n            data=deduplicated_data.get(\"latentspaces\", {}),\n        )\n\n        self._result.reconstructions.add(\n            epoch=self._cur_epoch,\n            split=split,\n            data=deduplicated_data.get(\"reconstructions\", {}),\n        )\n\n        if deduplicated_data.get(\"sample_ids\"):\n            self._result.sample_ids.add(\n                epoch=self._cur_epoch,\n                split=split,\n                data=deduplicated_data[\"sample_ids\"],\n            )\n\n        if deduplicated_data.get(\"mus\"):\n            self._result.mus.add(\n                epoch=self._cur_epoch,\n                split=split,\n                data=deduplicated_data[\"mus\"],\n            )\n\n        if deduplicated_data.get(\"sigmas\"):\n            self._result.sigmas.add(\n                epoch=self._cur_epoch,\n                split=split,\n                data=deduplicated_data[\"sigmas\"],\n            )\n\n    def _log_losses(\n        self,\n        split: str,\n        total_loss: float,\n        sub_losses: Dict[str, float],\n        n_samples: int,\n    ) -&gt; None:\n        \"\"\"Logs and stores average losses for the epoch.\n        Args:\n            split: The data split name (e.g., 'train', 'valid').\n            total_loss: The cumulative total loss for the epoch.\n            sub_losses: Dictionary of accumulated sub-losses for the epoch.\n            n_samples: Total number of samples processed in the epoch.\n\n        \"\"\"\n\n        n_samples = max(n_samples, 1)\n        print(f\"split: {split}, n_samples: {n_samples}\")\n        # avg_total_loss = total_loss / n_samples ## Now already normalized\n        self._result.losses.add(epoch=self._cur_epoch, split=split, data=total_loss)\n\n        # avg_sub_losses = {\n        #     k: v / n_samples if \"_factor\" not in k else v for k, v in sub_losses.items()\n        # }\n        self._result.sub_losses.add(\n            epoch=self._cur_epoch,\n            split=split,\n            data=sub_losses,\n        )\n        self._fabric.print(\n            f\"Epoch {self._cur_epoch + 1}/{self._config.epochs} - {split.capitalize()} Loss: {total_loss:.4f}\"\n        )\n        # Detailed sub-loss logging in one line\n        sub_loss_str = \", \".join(\n            [f\"{k}: {v:.4f}\" for k, v in sub_losses.items() if \"_factor\" not in k]\n        )\n        self._fabric.print(f\"Sub-losses - {sub_loss_str}\")\n\n    def _store_checkpoint(self, split: str, epoch_dynamics: List[Dict]) -&gt; None:\n        \"\"\"Stores model checkpoints and epoch dynamics into the Result object.\n\n        Args:\n            split: The data split name (e.g., 'train', 'valid').\n            epoch_dynamics: List of dictionaries capturing dynamics for each batch in the epoch.\n\n        \"\"\"\n\n        state_to_save = {\n            mod_name: dynamics[\"model\"].state_dict()\n            for mod_name, dynamics in self._modality_dynamics.items()\n        }\n        state_to_save[\"latent_clf\"] = self._latent_clf.state_dict()\n        self._result.model_checkpoints.add(epoch=self._cur_epoch, data=state_to_save)\n        self._dynamics_to_result(split=split, epoch_dynamics=epoch_dynamics)\n\n    def _store_final_models(self) -&gt; None:\n        \"\"\"Stores the final trained models into the Result object.\"\"\"\n        final_models = {\n            mod_name: dynamics[\"model\"]\n            for mod_name, dynamics in self._modality_dynamics.items()\n        }\n        self._result.model = final_models\n\n    def purge(self) -&gt; None:\n        \"\"\"\n        Cleans up all instantiated resources used during training, including\n        all modality-specific models/optimizers and the adversarial classifier.\n        \"\"\"\n\n        if hasattr(self, \"_modality_dynamics\"):\n            for mod_name, dynamics in self._modality_dynamics.items():\n                if \"model\" in dynamics and dynamics[\"model\"] is not None:\n                    # Remove the model and its optimizer from the dynamics dict\n                    del dynamics[\"model\"]\n                if \"optim\" in dynamics and dynamics[\"optim\"] is not None:\n                    del dynamics[\"optim\"]\n            # Delete the container itself after cleaning contents\n            del self._modality_dynamics\n\n        if hasattr(self, \"_latent_clf\"):\n            del self._latent_clf\n        if hasattr(self, \"_clf_optim\"):\n            del self._clf_optim\n\n        if hasattr(self, \"_trainloader\"):\n            del self._trainloader\n        if hasattr(self, \"_validloader\") and self._validloader is not None:\n            del self._validloader\n        if hasattr(self, \"_trainset\"):\n            del self._trainset\n        if hasattr(self, \"_validset\"):\n            del self._validset\n        if hasattr(self, \"_loss_fn\"):\n            del self._loss_fn\n        if hasattr(self, \"sub_loss\"):  # Directly accessible sub_loss instance\n            del self.sub_loss\n        if hasattr(self, \"_clf_loss_fn\"):\n            del self._clf_loss_fn\n\n        if hasattr(self, \"_all_dynamics\"):\n            del self._all_dynamics\n\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n        gc.collect()\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.XModalTrainer.__init__","title":"<code>__init__(trainset, validset, result, config, model_type, loss_type, sub_loss_type=VarixLoss, model_map={DataSetTypes.NUM: VarixArchitecture, DataSetTypes.IMG: ImageVAEArchitecture}, ontologies=None, **kwargs)</code>","text":"<p>Initializes the XModalTrainer with datasets and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>trainset</code> <code>MultiModalDataset</code> <p>Training dataset containing multiple modalities</p> required <code>validset</code> <code>MultiModalDataset</code> <p>Validation dataset containing multiple modalities</p> required <code>result</code> <code>Result</code> <p>Result object to store training outcomes</p> required <code>config</code> <code>DefaultConfig</code> <p>Configuration parameters for training and model architecture</p> required <code>model_type</code> <code>Type[BaseAutoencoder]</code> <p>Type of autoencoder model to use for each modality (not used directly)</p> required <code>loss_type</code> <code>Type[BaseLoss]</code> <p>Type of loss function to use for training the stacked model</p> required <code>sub_loss_type</code> <code>Type[BaseLoss]</code> <p>Type of loss function to use for individual modality autoencoders</p> <code>VarixLoss</code> <code>model_map</code> <code>Dict[DataSetTypes, Type[BaseAutoencoder]]</code> <p>Mapping from DataSetTypes to specific autoencoder architectures</p> <code>{NUM: VarixArchitecture, IMG: ImageVAEArchitecture}</code> <code>ontologies</code> <code>Optional[Union[Tuple, List]]</code> <p>Ontology information, for compatibility with Ontix</p> <code>None</code> Source code in <code>src/autoencodix/trainers/_xmodal_trainer.py</code> <pre><code>def __init__(\n    self,\n    trainset: MultiModalDataset,\n    validset: MultiModalDataset,\n    result: Result,\n    config: DefaultConfig,\n    model_type: Type[BaseAutoencoder],\n    loss_type: Type[BaseLoss],\n    sub_loss_type: Type[BaseLoss] = VarixLoss,\n    model_map: Dict[DataSetTypes, Type[BaseAutoencoder]] = {\n        DataSetTypes.NUM: VarixArchitecture,\n        DataSetTypes.IMG: ImageVAEArchitecture,\n    },\n    ontologies: Optional[Union[Tuple, List]] = None,\n    **kwargs,\n):\n    \"\"\"Initializes the XModalTrainer with datasets and configuration.\n\n    Args:\n        trainset: Training dataset containing multiple modalities\n        validset: Validation dataset containing multiple modalities\n        result: Result object to store training outcomes\n        config: Configuration parameters for training and model architecture\n        model_type: Type of autoencoder model to use for each modality (not used directly)\n        loss_type: Type of loss function to use for training the stacked model\n        sub_loss_type: Type of loss function to use for individual modality autoencoders\n        model_map: Mapping from DataSetTypes to specific autoencoder architectures\n        ontologies: Ontology information, for compatibility with Ontix\n    \"\"\"\n    self._trainset = trainset\n    self._n_train_modalities = self._trainset.n_modalities  # type: ignore\n    self.model_map = model_map\n    self.model_trainer_map = model_trainer_map\n    self._n_cpus = os.cpu_count()\n    if self._n_cpus is None:\n        self._n_cpus = 0\n\n    super().__init__(\n        trainset, validset, result, config, model_type, loss_type, ontologies\n    )\n    # init attributes ------------------------------------------------\n    self.latent_dim = config.latent_dim\n    self.n_test: Optional[int] = None\n    self.n_train = len(trainset.data) if trainset else 0\n    self.n_valid = len(validset.data) if validset else 0\n    self.n_features = trainset.get_input_dim() if trainset else 0\n    self._cur_epoch: int = 0\n    self._is_checkpoint_epoch: Optional[bool] = None\n    self._sub_loss_type = sub_loss_type\n    self.sub_loss = sub_loss_type(config=self._config)\n    self._clf_epoch_loss: float = 0.0\n    self._epoch_loss: float = 0.0\n    self._epoch_loss_valid: float = 0.0\n    self._all_dynamics: Dict[str, Dict[int, Dict]] = {\n        \"test\": defaultdict(dict),\n        \"train\": defaultdict(dict),\n        \"valid\": defaultdict(dict),\n    }\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.XModalTrainer.decode","title":"<code>decode(x)</code>","text":"<p>Decodes input latent representations Args:     x: Latent representations to decode, shape (n_samples, latent_dim)</p> Source code in <code>src/autoencodix/trainers/_xmodal_trainer.py</code> <pre><code>def decode(self, x: torch.Tensor):\n    \"\"\"Decodes input latent representations\n    Args:\n        x: Latent representations to decode, shape (n_samples, latent_dim)\n    \"\"\"\n    raise NotImplementedError(\"General decode step is not implemented for XModalix\")\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.XModalTrainer.predict","title":"<code>predict(data, model=None, **kwargs)</code>","text":"<p>Performs cross-modal prediction from a specified 'from' modality to a 'to' modality.</p> <p>The direction is determined by the 'translate_direction' attribute in the config. Results are stored in the Result object under split='test' and epoch=-1.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>BaseDataset</code> <p>A MultiModalDataset containing the input data for the 'from' modality.</p> required <p>Returns:</p> Type Description <code>Result</code> <p>The Result object populated with prediction results.</p> Source code in <code>src/autoencodix/trainers/_xmodal_trainer.py</code> <pre><code>def predict(\n    self,\n    data: BaseDataset,\n    model: Optional[Dict[str, torch.nn.Module]] = None,\n    **kwargs,\n) -&gt; Result:\n    \"\"\"Performs cross-modal prediction from a specified 'from' modality to a 'to' modality.\n\n    The direction is determined by the 'translate_direction' attribute in the config.\n    Results are stored in the Result object under split='test' and epoch=-1.\n\n    Args:\n        data: A MultiModalDataset containing the input data for the 'from' modality.\n\n    Returns:\n        The Result object populated with prediction results.\n    \"\"\"\n    split: str = kwargs.pop(\"split\", \"test\")\n    if split not in [\"train\", \"valid\", \"test\"]:\n        raise ValueError(\n            f\"split must be one of 'train', 'valid', or 'test', got {split}\"\n        )\n    if not isinstance(data, MultiModalDataset):\n        raise TypeError(\n            f\"type of data has to be MultiModalDataset, got: {type(data)}\"\n        )\n    if model is not None and not hasattr(self, \"_modality_dynamics\"):\n        self._modality_dynamics = {mod_name: {} for mod_name in model.keys()}\n\n        for mod_name, mod_model in model.items():\n            mod_model.eval()\n            if not isinstance(mod_model, _FabricModule):\n                mod_model = self._fabric.setup_module(mod_model)\n            mod_model.to(self._fabric.device)\n            self._modality_dynamics[mod_name][\"model\"] = mod_model\n        # self._setup_fabric(old_model=model)\n\n    from_key = kwargs.pop(\"from_key\", None)\n    to_key = kwargs.pop(\"to_key\", None)\n    predict_keys = find_translation_keys(\n        config=self._config,\n        trained_modalities=list(self._modality_dynamics.keys()),\n        from_key=from_key,\n        to_key=to_key,\n    )\n    from_key, to_key = predict_keys[\"from\"], predict_keys[\"to\"]\n    from_modality = self._modality_dynamics[from_key]\n    to_modality = self._modality_dynamics[to_key]\n    from_model = from_modality[\"model\"]\n    to_model = to_modality[\"model\"]\n    from_model.eval(), to_model.eval()\n    # drop_last handled in custom sampler\n    inference_loader = DataLoader(\n        data,\n        batch_sampler=CoverageEnsuringSampler(\n            multimodal_dataset=data, batch_size=self._config.batch_size\n        ),\n        collate_fn=create_multimodal_collate_fn(multimodal_dataset=data),\n    )\n    inference_loader = self._fabric.setup_dataloaders(inference_loader)  # type: ignore\n    epoch_dynamics: List[Dict] = []\n    with (\n        self._fabric.autocast(),\n        torch.inference_mode(),\n    ):\n        for batch in inference_loader:\n            # needed for visualize later\n            self._get_vis_dynamics(batch=batch)\n            from_z = self._modality_dynamics[from_key][\"mp\"].latentspace\n            translated = to_model.decode(x=from_z)\n\n            # for reference\n            to_z = self._modality_dynamics[to_key][\"mp\"].latentspace\n            to_to_reference = to_model.decode(x=to_z)\n\n            batch_capture = self._capture_dynamics(batch)\n            translation_key = \"translation\"\n\n            reference_key = f\"reference_{to_key}_to_{to_key}\"\n            batch_capture[\"reconstructions\"][\n                translation_key\n            ] = translated.cpu().numpy()\n\n            batch_capture[\"reconstructions\"][\n                reference_key\n            ] = to_to_reference.cpu().numpy()\n\n            if \"sample_ids\" in batch[from_key]:\n                batch_capture[\"sample_ids\"][translation_key] = np.array(\n                    batch[from_key][\"sample_ids\"]\n                )\n            if \"sample_ids\" in batch[to_key]:\n                batch_capture[\"sample_ids\"][reference_key] = np.array(\n                    batch[to_key][\"sample_ids\"]\n                )\n\n            epoch_dynamics.append(batch_capture)\n\n    self._dynamics_to_result(split=split, epoch_dynamics=epoch_dynamics)\n\n    print(\"Prediction complete.\")\n    return self._result\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.XModalTrainer.purge","title":"<code>purge()</code>","text":"<p>Cleans up all instantiated resources used during training, including all modality-specific models/optimizers and the adversarial classifier.</p> Source code in <code>src/autoencodix/trainers/_xmodal_trainer.py</code> <pre><code>def purge(self) -&gt; None:\n    \"\"\"\n    Cleans up all instantiated resources used during training, including\n    all modality-specific models/optimizers and the adversarial classifier.\n    \"\"\"\n\n    if hasattr(self, \"_modality_dynamics\"):\n        for mod_name, dynamics in self._modality_dynamics.items():\n            if \"model\" in dynamics and dynamics[\"model\"] is not None:\n                # Remove the model and its optimizer from the dynamics dict\n                del dynamics[\"model\"]\n            if \"optim\" in dynamics and dynamics[\"optim\"] is not None:\n                del dynamics[\"optim\"]\n        # Delete the container itself after cleaning contents\n        del self._modality_dynamics\n\n    if hasattr(self, \"_latent_clf\"):\n        del self._latent_clf\n    if hasattr(self, \"_clf_optim\"):\n        del self._clf_optim\n\n    if hasattr(self, \"_trainloader\"):\n        del self._trainloader\n    if hasattr(self, \"_validloader\") and self._validloader is not None:\n        del self._validloader\n    if hasattr(self, \"_trainset\"):\n        del self._trainset\n    if hasattr(self, \"_validset\"):\n        del self._validset\n    if hasattr(self, \"_loss_fn\"):\n        del self._loss_fn\n    if hasattr(self, \"sub_loss\"):  # Directly accessible sub_loss instance\n        del self.sub_loss\n    if hasattr(self, \"_clf_loss_fn\"):\n        del self._clf_loss_fn\n\n    if hasattr(self, \"_all_dynamics\"):\n        del self._all_dynamics\n\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n\n    gc.collect()\n</code></pre>"},{"location":"api/trainers/#autoencodix.trainers.XModalTrainer.train","title":"<code>train()</code>","text":"<p>Orchestrates the full training process for the cross-modal autoencoder.</p> Source code in <code>src/autoencodix/trainers/_xmodal_trainer.py</code> <pre><code>def train(self):\n    \"\"\"Orchestrates the full training process for the cross-modal autoencoder.\"\"\"\n    self._pretraining()\n    for epoch in range(self._config.epochs):\n        self._cur_epoch = epoch\n        self._is_checkpoint_epoch = self._should_checkpoint(epoch=epoch)\n        self._fabric.print(f\"--- Epoch {epoch + 1}/{self._config.epochs} ---\")\n        train_epoch_dynamics, train_sub_losses, n_samples_train = (\n            self._train_one_epoch()\n        )\n\n        self._log_losses(\n            split=\"train\",\n            total_loss=self._epoch_loss,\n            sub_losses=train_sub_losses,\n            n_samples=n_samples_train,\n        )\n\n        if self._validset:\n            valid_epoch_dynamics, valid_sub_losses, n_samples_valid = (\n                self._validate_one_epoch()\n            )\n            self._log_losses(\n                split=\"valid\",\n                total_loss=self._epoch_loss_valid,\n                sub_losses=valid_sub_losses,\n                n_samples=n_samples_valid,\n            )\n        if self._is_checkpoint_epoch:\n            self._fabric.print(f\"Storing checkpoint for epoch {epoch}...\")\n            self._store_checkpoint(\n                split=\"train\", epoch_dynamics=train_epoch_dynamics\n            )\n            if self._validset:\n                self._store_checkpoint(\n                    split=\"valid\",\n                    epoch_dynamics=valid_epoch_dynamics,\n                )\n    # save final model\n    self._store_checkpoint(split=\"train\", epoch_dynamics=train_epoch_dynamics)\n    self._store_final_models()\n    return self._result\n</code></pre>"},{"location":"api/utils/","title":"Utils Module","text":""},{"location":"api/utils/#autoencodix.utils.AnnDataConverter","title":"<code>AnnDataConverter</code>","text":"<p>Utility class for converting datasets into AnnData or multimodal AnnData dictionaries.</p> Source code in <code>src/autoencodix/utils/adata_converter.py</code> <pre><code>class AnnDataConverter:\n    \"\"\"Utility class for converting datasets into AnnData or multimodal AnnData dictionaries.\"\"\"\n\n    @staticmethod\n    def _numeric_ds_to_adata(ds: NumericDataset) -&gt; Dict[str, ad.AnnData]:\n        \"\"\"Convert a NumericDataset to an AnnData object.\n\n        Args:\n            ds: The numeric dataset to convert.\n\n        Returns:\n            An AnnData object containing the dataset's data, features, and metadata.\n        \"\"\"\n        if not isinstance(ds.metadata, pd.DataFrame):\n            raise ValueError(\n                f\"metadata needs to be pd.DataFrame, got {type(ds.metadata)}\"\n            )\n        metadata = ds.metadata.copy()\n        metadata.index = metadata.index.astype(str)\n\n        var = pd.DataFrame(index=pd.Index(ds.feature_ids, dtype=str))\n        return {\n            \"global\": ad.AnnData(\n                X=ds.data.detach().cpu().numpy(),\n                var=var,\n                obs=metadata,\n            )\n        }\n\n    @staticmethod\n    def _parse_multimodal(mds: MultiModalDataset) -&gt; Dict[str, ad.AnnData]:\n        \"\"\"Convert a MultiModalDataset into a dictionary of AnnData objects.\n\n        Args:\n            mds: The multimodal dataset to convert.\n\n        Returns:\n            A dictionary mapping modality names to AnnData objects.\n\n        Raises:\n            NotImplementedError: If any modality is not a NumericDataset.\n        \"\"\"\n        result_dict: Dict[str, ad.AnnData] = {}\n        for mod_name, dataset in mds.datasets.items():\n            if not isinstance(dataset, NumericDataset):\n                raise NotImplementedError(\n                    f\"Feature Importance is only implemented for NumericDataset, got type: {type(dataset)}\"\n                )\n            result_dict[mod_name] = AnnDataConverter._numeric_ds_to_adata(dataset)  # type: ignore\n        return result_dict\n\n    @staticmethod\n    def dataset_to_adata(\n        datasetcontainer: DatasetContainer,\n        split: Literal[\"train\", \"valid\", \"test\"] = \"train\",\n    ) -&gt; Optional[Dict[str, ad.AnnData]]:\n        \"\"\"Convert a DatasetContainer split to an AnnData or multimodal AnnData dictionary.\n\n        Args:\n            datasetcontainer: Container holding train/valid/test datasets.\n            split: The dataset split to convert. Defaults to \"train\".\n\n        Returns:\n            A single AnnData object (for NumericDataset) or a dictionary of AnnData objects (for MultiModalDataset).\n\n        Raises:\n            ValueError: If the specified split does not exist in the DatasetContainer.\n            NotImplementedError: If the dataset type is not supported.\n        \"\"\"\n        if not hasattr(datasetcontainer, split):\n            raise ValueError(\n                f\"Split: {split} not present in DatasetContainer: {datasetcontainer}\"\n            )\n\n        ds = datasetcontainer[split]\n\n        if isinstance(ds, MultiModalDataset):\n            return AnnDataConverter._parse_multimodal(ds)\n        elif isinstance(ds, NumericDataset):\n            return AnnDataConverter._numeric_ds_to_adata(ds)\n        elif ds is None:\n            import warnings\n\n            warnings.warn(f\"No dataset found for split: {split}, returning None\")\n            return None\n\n        else:\n            raise NotImplementedError(\n                f\"Conversion not implemented for type: {type(ds)}\"\n            )\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.AnnDataConverter.dataset_to_adata","title":"<code>dataset_to_adata(datasetcontainer, split='train')</code>  <code>staticmethod</code>","text":"<p>Convert a DatasetContainer split to an AnnData or multimodal AnnData dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>datasetcontainer</code> <code>DatasetContainer</code> <p>Container holding train/valid/test datasets.</p> required <code>split</code> <code>Literal['train', 'valid', 'test']</code> <p>The dataset split to convert. Defaults to \"train\".</p> <code>'train'</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, AnnData]]</code> <p>A single AnnData object (for NumericDataset) or a dictionary of AnnData objects (for MultiModalDataset).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified split does not exist in the DatasetContainer.</p> <code>NotImplementedError</code> <p>If the dataset type is not supported.</p> Source code in <code>src/autoencodix/utils/adata_converter.py</code> <pre><code>@staticmethod\ndef dataset_to_adata(\n    datasetcontainer: DatasetContainer,\n    split: Literal[\"train\", \"valid\", \"test\"] = \"train\",\n) -&gt; Optional[Dict[str, ad.AnnData]]:\n    \"\"\"Convert a DatasetContainer split to an AnnData or multimodal AnnData dictionary.\n\n    Args:\n        datasetcontainer: Container holding train/valid/test datasets.\n        split: The dataset split to convert. Defaults to \"train\".\n\n    Returns:\n        A single AnnData object (for NumericDataset) or a dictionary of AnnData objects (for MultiModalDataset).\n\n    Raises:\n        ValueError: If the specified split does not exist in the DatasetContainer.\n        NotImplementedError: If the dataset type is not supported.\n    \"\"\"\n    if not hasattr(datasetcontainer, split):\n        raise ValueError(\n            f\"Split: {split} not present in DatasetContainer: {datasetcontainer}\"\n        )\n\n    ds = datasetcontainer[split]\n\n    if isinstance(ds, MultiModalDataset):\n        return AnnDataConverter._parse_multimodal(ds)\n    elif isinstance(ds, NumericDataset):\n        return AnnDataConverter._numeric_ds_to_adata(ds)\n    elif ds is None:\n        import warnings\n\n        warnings.warn(f\"No dataset found for split: {split}, returning None\")\n        return None\n\n    else:\n        raise NotImplementedError(\n            f\"Conversion not implemented for type: {type(ds)}\"\n        )\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.BulkDataReader","title":"<code>BulkDataReader</code>","text":"<p>Reads bulk data from files based on configuration.</p> <p>Supports both paired and unpaired data reading strategies.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration object</p> Source code in <code>src/autoencodix/utils/_bulkreader.py</code> <pre><code>class BulkDataReader:\n    \"\"\"Reads bulk data from files based on configuration.\n\n    Supports both paired and unpaired data reading strategies.\n\n    Attributes:\n        config: Configuration object\n    \"\"\"\n\n    def __init__(self, config: DefaultConfig):\n        \"\"\"Initialize the BulkDataReader with a configuration.\n\n        Args:\n            config: Configuration object containing data paths and specifications.\n        \"\"\"\n        self.config = config\n\n    def read_data(self) -&gt; Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n        \"\"\"Read all data according to the configuration.\n\n        Returns:\n            A tuple containing (bulk_dataframes, annotation_dataframes)\n        \"\"\"\n        if self.config.requires_paired or self.config.requires_paired is None:\n            return self.read_paired_data()\n        else:\n            return self.read_unpaired_data()\n\n    def read_paired_data(\n        self,\n    ) -&gt; Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n        \"\"\"Reads numeric paired data\n\n        Returns:\n            Tuple containing two Dicts:\n                 1. with name of the data as key and pandas DataFrame as value\n                 2. with  str 'paired' as key and a common annotaion/metadata as DataFrame\n        \"\"\"\n        common_samples: Optional[Set[str]] = None\n        bulk_dfs: Dict[str, pd.DataFrame] = {}\n        annotation_df = pd.DataFrame()\n        has_annotation = False\n\n        # First pass: read all data files and track common samples\n        for key, info in self.config.data_config.data_info.items():\n            if info.data_type == \"IMG\":\n                continue\n\n            file_path = os.path.join(info.file_path)\n            df = self._read_tabular_data(file_path, info.sep or \"\\t\")\n\n            if df is None:\n                continue\n\n            if info.data_type == \"NUMERIC\" and not info.is_single_cell:\n                current_samples = set(df.index)\n                if common_samples is None:\n                    common_samples = current_samples\n                else:\n                    common_samples &amp;= current_samples\n\n                bulk_dfs[key] = df\n\n            elif info.data_type == \"ANNOTATION\":\n                has_annotation = True\n                annotation_df = df\n\n        # Second pass: filter to common samples\n        if common_samples:\n            common_samples_list = list(common_samples)\n\n            # Reindex bulk dataframes to common samples\n            for key in bulk_dfs:\n                bulk_dfs[key] = bulk_dfs[key].reindex(common_samples_list)\n\n            # Handle annotation dataframe\n            if has_annotation:\n                annotation = annotation_df.reindex(common_samples_list)\n            else:\n                # Create empty annotation with common sample indices\n                annotation_df = pd.DataFrame(index=common_samples_list)\n                annotation = annotation_df\n        else:\n            print(\"Warning: No common samples found across datasets\")\n            annotation = annotation_df\n\n        return bulk_dfs, {\"paired\": annotation}\n\n    def read_unpaired_data(\n        self,\n    ) -&gt; Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n        \"\"\"Read data without enforcing sample alignment across modalities.\n\n        Returns:\n            A tuple containing (bulk_dataframes, annotation_dataframes)\n        \"\"\"\n        bulk_dfs: Dict[str, pd.DataFrame] = {}\n        annotations: Dict[str, pd.DataFrame] = {}\n\n        for key, info in self.config.data_config.data_info.items():\n            if info.data_type == \"IMG\" or info.is_single_cell:\n                continue  # Skip image and single-cell data\n\n            # Read main data file\n            file_path = os.path.join(info.file_path)\n            df = self._read_tabular_data(file_path=file_path, sep=info.sep)\n\n            if df is None:\n                continue\n\n            if info.data_type == \"NUMERIC\":\n                bulk_dfs[key] = df\n\n                if hasattr(info, \"extra_anno_file\") and info.extra_anno_file:\n                    extra_anno_file = os.path.join(info.extra_anno_file)\n                    extra_anno_df = self._read_tabular_data(\n                        file_path=extra_anno_file, sep=info.sep\n                    )\n                    if extra_anno_df is not None:\n                        annotations[key] = extra_anno_df\n\n            elif info.data_type == \"ANNOTATION\":\n                annotations[key] = df\n\n        bulk_dfs, annotations = self._validate_and_filter_unpaired(\n            bulk_dfs, annotations\n        )\n\n        return bulk_dfs, annotations\n\n    def _validate_and_filter_unpaired(\n        self,\n        bulk_dfs: Dict[str, pd.DataFrame],\n        annotations: Dict[str, pd.DataFrame],\n    ) -&gt; Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n        \"\"\"Validates that all samples in bulk data have a corresponding annotation.\n\n        If a single global annotation file is provided, it creates a perfectly\n        matched annotation dataframe for each bulk dataframe.\n\n        Warns and drops samples that do not have a corresponding annotation.\n\n        Args:\n            bulk_dfs: Dictionary of bulk data modalities and their dataframes.\n            annotations: Dictionary of annotation dataframes, possibly one global one.\n\n        Returns:\n            A tuple of two dictionaries:\n            1. The filtered bulk dataframes.\n            2. The new, synchronized annotation dataframes, with keys matching the bulk dataframes.\n        \"\"\"\n        if not annotations:\n            warnings.warn(\n                \"No annotation files were provided. Cannot validate sample annotations.\"\n            )\n            return bulk_dfs, {}\n\n        # If annotations have keys that match bulk_dfs, we assume they are already paired.\n        # This logic focuses on the case where one annotation file is meant for all bulk files.\n        # A simple heuristic: if there is one annotation file and its key is not in bulk_dfs.\n        annotation_keys = set(annotations.keys())\n        bulk_keys = set(bulk_dfs.keys())\n\n        # Check for the global annotation case\n        if len(annotation_keys) == 1 and not annotation_keys.intersection(bulk_keys):\n            global_annotation_key = list(annotation_keys)[0]\n            global_annotation_df = annotations[global_annotation_key]\n\n            filtered_bulk_dfs = {}\n            synchronized_annotations = {}\n\n            for key, data_df in bulk_dfs.items():\n                data_samples = data_df.index\n                annotation_samples = global_annotation_df.index\n\n                # Find the intersection of valid sample IDs\n                valid_ids = data_samples.intersection(annotation_samples)\n\n                # Check for and warn about dropped samples\n                if len(valid_ids) &lt; len(data_samples):\n                    missing_ids = sorted(list(set(data_samples) - set(valid_ids)))\n                    warnings.warn(\n                        f\"For data modality '{key}', {len(missing_ids)} sample(s) \"\n                        f\"were found without a corresponding annotation and will be dropped: {missing_ids}\"\n                    )\n\n                # Filter both the data and the annotation to the valid IDs\n                filtered_bulk_dfs[key] = data_df.loc[valid_ids]\n                synchronized_annotations[key] = global_annotation_df.loc[valid_ids]\n\n            return filtered_bulk_dfs, synchronized_annotations\n        else:\n            # Handle the case where annotations are already meant to be paired by key\n            # (Or a more complex case we are not handling yet)\n            warnings.warn(\n                \"Proceeding without global annotation synchronization. Assuming annotations are pre-aligned by key.\"\n            )\n            return bulk_dfs, annotations\n\n    def _read_tabular_data(\n        self, file_path: str, sep: Union[str, None] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Read tabular data from a file with error handling.\n\n        Args:\n        file_path: Path to the data file.\n        sep: Separator character for CSV/TSV files.\n\n        Returns:\n            The loaded DataFrame.\n        \"\"\"\n        try:\n            if file_path.endswith(\".parquet\"):\n                print(f\"reading parquet: {file_path}\")\n                return pd.read_parquet(file_path)\n            elif file_path.endswith((\".csv\", \".txt\", \".tsv\")):\n                return pd.read_csv(file_path, sep=sep, index_col=0)\n            else:\n                raise ValueError(\n                    f\"Unsupported file type for {file_path}. Supported formats: .parquet, .csv, .txt, .tsv\"\n                )\n        except Exception as e:\n            raise e\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.BulkDataReader.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize the BulkDataReader with a configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DefaultConfig</code> <p>Configuration object containing data paths and specifications.</p> required Source code in <code>src/autoencodix/utils/_bulkreader.py</code> <pre><code>def __init__(self, config: DefaultConfig):\n    \"\"\"Initialize the BulkDataReader with a configuration.\n\n    Args:\n        config: Configuration object containing data paths and specifications.\n    \"\"\"\n    self.config = config\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.BulkDataReader.read_data","title":"<code>read_data()</code>","text":"<p>Read all data according to the configuration.</p> <p>Returns:</p> Type Description <code>Tuple[Dict[str, DataFrame], Dict[str, DataFrame]]</code> <p>A tuple containing (bulk_dataframes, annotation_dataframes)</p> Source code in <code>src/autoencodix/utils/_bulkreader.py</code> <pre><code>def read_data(self) -&gt; Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n    \"\"\"Read all data according to the configuration.\n\n    Returns:\n        A tuple containing (bulk_dataframes, annotation_dataframes)\n    \"\"\"\n    if self.config.requires_paired or self.config.requires_paired is None:\n        return self.read_paired_data()\n    else:\n        return self.read_unpaired_data()\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.BulkDataReader.read_paired_data","title":"<code>read_paired_data()</code>","text":"<p>Reads numeric paired data</p> <p>Returns:</p> Type Description <code>Tuple[Dict[str, DataFrame], Dict[str, DataFrame]]</code> <p>Tuple containing two Dicts:  1. with name of the data as key and pandas DataFrame as value  2. with  str 'paired' as key and a common annotaion/metadata as DataFrame</p> Source code in <code>src/autoencodix/utils/_bulkreader.py</code> <pre><code>def read_paired_data(\n    self,\n) -&gt; Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n    \"\"\"Reads numeric paired data\n\n    Returns:\n        Tuple containing two Dicts:\n             1. with name of the data as key and pandas DataFrame as value\n             2. with  str 'paired' as key and a common annotaion/metadata as DataFrame\n    \"\"\"\n    common_samples: Optional[Set[str]] = None\n    bulk_dfs: Dict[str, pd.DataFrame] = {}\n    annotation_df = pd.DataFrame()\n    has_annotation = False\n\n    # First pass: read all data files and track common samples\n    for key, info in self.config.data_config.data_info.items():\n        if info.data_type == \"IMG\":\n            continue\n\n        file_path = os.path.join(info.file_path)\n        df = self._read_tabular_data(file_path, info.sep or \"\\t\")\n\n        if df is None:\n            continue\n\n        if info.data_type == \"NUMERIC\" and not info.is_single_cell:\n            current_samples = set(df.index)\n            if common_samples is None:\n                common_samples = current_samples\n            else:\n                common_samples &amp;= current_samples\n\n            bulk_dfs[key] = df\n\n        elif info.data_type == \"ANNOTATION\":\n            has_annotation = True\n            annotation_df = df\n\n    # Second pass: filter to common samples\n    if common_samples:\n        common_samples_list = list(common_samples)\n\n        # Reindex bulk dataframes to common samples\n        for key in bulk_dfs:\n            bulk_dfs[key] = bulk_dfs[key].reindex(common_samples_list)\n\n        # Handle annotation dataframe\n        if has_annotation:\n            annotation = annotation_df.reindex(common_samples_list)\n        else:\n            # Create empty annotation with common sample indices\n            annotation_df = pd.DataFrame(index=common_samples_list)\n            annotation = annotation_df\n    else:\n        print(\"Warning: No common samples found across datasets\")\n        annotation = annotation_df\n\n    return bulk_dfs, {\"paired\": annotation}\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.BulkDataReader.read_unpaired_data","title":"<code>read_unpaired_data()</code>","text":"<p>Read data without enforcing sample alignment across modalities.</p> <p>Returns:</p> Type Description <code>Tuple[Dict[str, DataFrame], Dict[str, DataFrame]]</code> <p>A tuple containing (bulk_dataframes, annotation_dataframes)</p> Source code in <code>src/autoencodix/utils/_bulkreader.py</code> <pre><code>def read_unpaired_data(\n    self,\n) -&gt; Tuple[Dict[str, pd.DataFrame], Dict[str, pd.DataFrame]]:\n    \"\"\"Read data without enforcing sample alignment across modalities.\n\n    Returns:\n        A tuple containing (bulk_dataframes, annotation_dataframes)\n    \"\"\"\n    bulk_dfs: Dict[str, pd.DataFrame] = {}\n    annotations: Dict[str, pd.DataFrame] = {}\n\n    for key, info in self.config.data_config.data_info.items():\n        if info.data_type == \"IMG\" or info.is_single_cell:\n            continue  # Skip image and single-cell data\n\n        # Read main data file\n        file_path = os.path.join(info.file_path)\n        df = self._read_tabular_data(file_path=file_path, sep=info.sep)\n\n        if df is None:\n            continue\n\n        if info.data_type == \"NUMERIC\":\n            bulk_dfs[key] = df\n\n            if hasattr(info, \"extra_anno_file\") and info.extra_anno_file:\n                extra_anno_file = os.path.join(info.extra_anno_file)\n                extra_anno_df = self._read_tabular_data(\n                    file_path=extra_anno_file, sep=info.sep\n                )\n                if extra_anno_df is not None:\n                    annotations[key] = extra_anno_df\n\n        elif info.data_type == \"ANNOTATION\":\n            annotations[key] = df\n\n    bulk_dfs, annotations = self._validate_and_filter_unpaired(\n        bulk_dfs, annotations\n    )\n\n    return bulk_dfs, annotations\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.ImageDataReader","title":"<code>ImageDataReader</code>","text":"<p>Reads and processes image data.</p> <p>Reads all images from the specified directory, processes them, and returns a list of ImgData objects.</p> Source code in <code>src/autoencodix/utils/_imgreader.py</code> <pre><code>class ImageDataReader:\n    \"\"\"Reads and processes image data.\n\n    Reads all images from the specified directory, processes them,\n    and returns a list of ImgData objects.\n    \"\"\"\n\n    def __init__(self, config: DefaultConfig):\n        self.config = config\n\n    def validate_image_path(self, image_path: Union[str, Path]) -&gt; bool:\n        \"\"\"Checks if file extension is allowed:\n\n        Allowed are (independent of capitalization):\n            - jpg\n            - jpeg\n            - png\n            - tif\n            - tiff\n\n        Args:\n            image_path: path or str of image to read\n        \"\"\"\n        path = Path(image_path) if isinstance(image_path, str) else image_path\n        return (\n            path.exists()\n            and path.is_file()\n            and path.suffix.lower() in {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n        )\n\n    def parse_image_to_tensor(\n        self,\n        image_path: Union[str, Path],\n        to_h: Optional[int] = None,\n        to_w: Optional[int] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Reads an image from the given path, optionally resizes it, and converts it to a tensor.\n\n        Args:\n            image_path: The path to the image file.\n            to_h: The desired height of the output tensor, by default None.\n            to_w: The desired width of the output tensor, by default None.\n\n        Returns:\n            The processed image as a tensor.\n\n        Raises:\n            FileNotFoundError: If the image path is invalid or the image cannot be read.\n            ImageProcessingError: If the image format is unsupported or an unexpected error occurs during processing.\n        \"\"\"\n\n        if not self.validate_image_path(image_path):\n            raise FileNotFoundError(f\"Invalid image path: {image_path}\")\n        image_path = Path(image_path)\n        SUPPORTED_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n        if image_path.suffix.lower() not in SUPPORTED_EXTENSIONS:\n            raise ImageProcessingError(\n                f\"Unsupported image format: {image_path.suffix}. \"\n                f\"Supported formats are: {', '.join(SUPPORTED_EXTENSIONS)}\"\n            )\n        try:\n            if image_path.suffix.lower() in {\".tif\", \".tiff\"}:\n                image = cv2.imread(str(image_path), cv2.IMREAD_UNCHANGED)\n            else:\n                image = cv2.imread(str(image_path))\n\n            if image is None:\n                raise FileNotFoundError(f\"Failed to read image: {image_path}\")\n\n            (h, w, _) = image.shape[:3]\n            if to_h is None:\n                to_h = h\n            if to_w is None:\n                to_w = w\n\n            if not (2 &lt;= len(image.shape) &lt;= 3):\n                raise ImageProcessingError(\n                    f\"Image has unsupported shape: {image.shape}. \"\n                    \"Supported shapes are 2D and 3D.\"\n                )\n\n            image = cv2.resize(image, (to_w, to_h), interpolation=cv2.INTER_AREA)\n\n            if len(image.shape) == 3:\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n            if len(image.shape) == 2:\n                image = np.expand_dims(image, axis=2)\n\n            image = image.transpose(2, 0, 1)\n            return image\n\n        except Exception as e:\n            raise e\n\n    def read_all_images_from_dir(\n        self,\n        img_dir: str,\n        to_h: Optional[int],\n        to_w: Optional[int],\n        annotation_df: pd.DataFrame,\n        is_paired: Union[bool, None] = None,\n    ) -&gt; List[ImgData]:\n        \"\"\"Reads all images from a specified directory, processes them, returns list of ImgData objects.\n\n        Args:\n            img_dir: The directory containing the images.\n            to_h: The desired height of the output tensors.\n            to_w: The desired width of the output tensors.\n            annotation_df: DataFrame containing image annotations.\n            is_paired: Whether the images are paired with annotations.\n\n        Returns:\n            List of processed image data objects.\n\n        Raises:\n            ValueError: If the annotation DataFrame is missing required columns.\n        \"\"\"\n        if self.config.img_path_col not in annotation_df.columns:\n            raise ValueError(\n                f\" The defined column for image paths: {self.config.img_path_col} column is missing in the annotation_df\\\n                             you can define this in the config via the param `img_path_col`\"\n            )\n\n        SUPPORTED_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n        paths = [\n            os.path.join(img_dir, f)\n            for f in os.listdir(img_dir)\n            if Path(f).suffix.lower() in SUPPORTED_EXTENSIONS\n        ]\n        if is_paired or is_paired is None:\n            paths = [\n                p\n                for p in paths\n                if os.path.basename(p)\n                in annotation_df[self.config.img_path_col].tolist()\n            ]\n        imgs = []\n        for p in paths:\n            img = self.parse_image_to_tensor(image_path=p, to_h=to_h, to_w=to_w)\n            img_path = os.path.basename(p)\n            subset: Union[pd.Series, pd.DataFrame] = annotation_df[\n                annotation_df[self.config.img_path_col] == img_path\n            ]\n            if not subset.empty:\n                imgs.append(\n                    ImgData(\n                        img=img,\n                        sample_id=str(subset.index[0]),\n                        annotation=subset,\n                    )\n                )\n        return imgs\n\n    def read_annotation_file(self, data_info: DataInfo) -&gt; pd.DataFrame:\n        \"\"\"Reads annotation file and returns DataFrame with file contents\n        Args:\n            data_info: specific part of the Configuration object for input data\n        Returns:\n            DataFrame with annotation data.\n\n        \"\"\"\n        anno_file = (\n            os.path.join(data_info.file_path)\n            if data_info.extra_anno_file is None\n            else os.path.join(data_info.extra_anno_file)\n        )\n        sep = data_info.sep\n        if anno_file.endswith(\".parquet\"):\n            annotation = pd.read_parquet(anno_file)\n        elif anno_file.endswith((\".csv\", \".txt\", \".tsv\")):\n            annotation = pd.read_csv(anno_file, sep=sep, index_col=0, engine=\"python\")\n        else:\n            raise ValueError(f\"Unsupported file type for: {anno_file}\")\n        return annotation\n\n    def read_data(\n        self, config: DefaultConfig\n    ) -&gt; Tuple[Dict[str, List[ImgData]], Dict[str, pd.DataFrame]]:\n        \"\"\"Read image data from the specified directory based on configuration.\n\n        Args:\n            config: The configuration object containing the data configuration.\n\n        Returns:\n            A Tuple of Dicts:\n            1. Dict with type of image data as key and actual List of ImgData as value.\n            2. Dict with type of image data as key and DataFrame of annotation data as value.\n\n        Raises:\n            Exception: If no image data is found in the configuration or other validation errors occur.\n        \"\"\"\n        # Find all image data sources in config\n        image_sources = {\n            k: v\n            for k, v in config.data_config.data_info.items()\n            if v.data_type == \"IMG\"\n        }\n\n        if not image_sources:\n            raise ValueError(\"No image data found in the configuration.\")\n\n        result = {}\n        annotation = {}\n        for key, img_info in image_sources.items():\n            try:\n                result[key], annotation[key] = self._read_data(config, img_info)\n                print(f\"Successfully loaded {len(result[key])} images for {key}\")\n            except Exception as e:\n                print(f\"Error loading images for {key}: {str(e)}\")\n                # Decide whether to raise or continue based on your requirements\n\n        return result, annotation\n\n    def _read_data(\n        self, config: DefaultConfig, img_info: DataInfo\n    ) -&gt; Tuple[List[ImgData], pd.DataFrame]:\n        \"\"\"Read data for a specific image source.\n\n        Args:\n            config: The configuration object containing the data configuration.\n            img_info: The specific image info configuration.\n\n        Returns:\n            A Tuple of Dicts:\n            1. Dict with type of image data as key and actual List of ImgData as value.\n            2. Dict with type of image data as key and DataFrame of annotation data as value.\n\n        \"\"\"\n        img_dir = img_info.file_path\n        img_size_finder: ImageSizeFinder = ImageSizeFinder(config)\n        to_h, to_w = img_size_finder.get_nearest_quadratic_image_size()\n\n        if img_info.extra_anno_file is not None:\n            # Use image-specific annotation file if provided\n            annotation = self.read_annotation_file(img_info)\n        else:\n            # Otherwise use the global annotation file\n            try:\n                anno_info = next(\n                    f\n                    for f in config.data_config.data_info.values()\n                    if f.data_type == \"ANNOTATION\"\n                )\n                annotation = self.read_annotation_file(anno_info)\n            except StopIteration:\n                raise ValueError(\"No annotation data found in the configuration.\")\n\n        images = self.read_all_images_from_dir(\n            img_dir=img_dir,\n            to_h=to_h,\n            to_w=to_w,\n            annotation_df=annotation,\n            is_paired=config.requires_paired,\n        )\n        annotations: pd.DataFrame = pd.concat([img.annotation for img in images])\n\n        return images, annotations\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.ImageDataReader.parse_image_to_tensor","title":"<code>parse_image_to_tensor(image_path, to_h=None, to_w=None)</code>","text":"<p>Reads an image from the given path, optionally resizes it, and converts it to a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Union[str, Path]</code> <p>The path to the image file.</p> required <code>to_h</code> <code>Optional[int]</code> <p>The desired height of the output tensor, by default None.</p> <code>None</code> <code>to_w</code> <code>Optional[int]</code> <p>The desired width of the output tensor, by default None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The processed image as a tensor.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the image path is invalid or the image cannot be read.</p> <code>ImageProcessingError</code> <p>If the image format is unsupported or an unexpected error occurs during processing.</p> Source code in <code>src/autoencodix/utils/_imgreader.py</code> <pre><code>def parse_image_to_tensor(\n    self,\n    image_path: Union[str, Path],\n    to_h: Optional[int] = None,\n    to_w: Optional[int] = None,\n) -&gt; np.ndarray:\n    \"\"\"Reads an image from the given path, optionally resizes it, and converts it to a tensor.\n\n    Args:\n        image_path: The path to the image file.\n        to_h: The desired height of the output tensor, by default None.\n        to_w: The desired width of the output tensor, by default None.\n\n    Returns:\n        The processed image as a tensor.\n\n    Raises:\n        FileNotFoundError: If the image path is invalid or the image cannot be read.\n        ImageProcessingError: If the image format is unsupported or an unexpected error occurs during processing.\n    \"\"\"\n\n    if not self.validate_image_path(image_path):\n        raise FileNotFoundError(f\"Invalid image path: {image_path}\")\n    image_path = Path(image_path)\n    SUPPORTED_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n    if image_path.suffix.lower() not in SUPPORTED_EXTENSIONS:\n        raise ImageProcessingError(\n            f\"Unsupported image format: {image_path.suffix}. \"\n            f\"Supported formats are: {', '.join(SUPPORTED_EXTENSIONS)}\"\n        )\n    try:\n        if image_path.suffix.lower() in {\".tif\", \".tiff\"}:\n            image = cv2.imread(str(image_path), cv2.IMREAD_UNCHANGED)\n        else:\n            image = cv2.imread(str(image_path))\n\n        if image is None:\n            raise FileNotFoundError(f\"Failed to read image: {image_path}\")\n\n        (h, w, _) = image.shape[:3]\n        if to_h is None:\n            to_h = h\n        if to_w is None:\n            to_w = w\n\n        if not (2 &lt;= len(image.shape) &lt;= 3):\n            raise ImageProcessingError(\n                f\"Image has unsupported shape: {image.shape}. \"\n                \"Supported shapes are 2D and 3D.\"\n            )\n\n        image = cv2.resize(image, (to_w, to_h), interpolation=cv2.INTER_AREA)\n\n        if len(image.shape) == 3:\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        if len(image.shape) == 2:\n            image = np.expand_dims(image, axis=2)\n\n        image = image.transpose(2, 0, 1)\n        return image\n\n    except Exception as e:\n        raise e\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.ImageDataReader.read_all_images_from_dir","title":"<code>read_all_images_from_dir(img_dir, to_h, to_w, annotation_df, is_paired=None)</code>","text":"<p>Reads all images from a specified directory, processes them, returns list of ImgData objects.</p> <p>Parameters:</p> Name Type Description Default <code>img_dir</code> <code>str</code> <p>The directory containing the images.</p> required <code>to_h</code> <code>Optional[int]</code> <p>The desired height of the output tensors.</p> required <code>to_w</code> <code>Optional[int]</code> <p>The desired width of the output tensors.</p> required <code>annotation_df</code> <code>DataFrame</code> <p>DataFrame containing image annotations.</p> required <code>is_paired</code> <code>Union[bool, None]</code> <p>Whether the images are paired with annotations.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ImgData]</code> <p>List of processed image data objects.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the annotation DataFrame is missing required columns.</p> Source code in <code>src/autoencodix/utils/_imgreader.py</code> <pre><code>def read_all_images_from_dir(\n    self,\n    img_dir: str,\n    to_h: Optional[int],\n    to_w: Optional[int],\n    annotation_df: pd.DataFrame,\n    is_paired: Union[bool, None] = None,\n) -&gt; List[ImgData]:\n    \"\"\"Reads all images from a specified directory, processes them, returns list of ImgData objects.\n\n    Args:\n        img_dir: The directory containing the images.\n        to_h: The desired height of the output tensors.\n        to_w: The desired width of the output tensors.\n        annotation_df: DataFrame containing image annotations.\n        is_paired: Whether the images are paired with annotations.\n\n    Returns:\n        List of processed image data objects.\n\n    Raises:\n        ValueError: If the annotation DataFrame is missing required columns.\n    \"\"\"\n    if self.config.img_path_col not in annotation_df.columns:\n        raise ValueError(\n            f\" The defined column for image paths: {self.config.img_path_col} column is missing in the annotation_df\\\n                         you can define this in the config via the param `img_path_col`\"\n        )\n\n    SUPPORTED_EXTENSIONS = {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n    paths = [\n        os.path.join(img_dir, f)\n        for f in os.listdir(img_dir)\n        if Path(f).suffix.lower() in SUPPORTED_EXTENSIONS\n    ]\n    if is_paired or is_paired is None:\n        paths = [\n            p\n            for p in paths\n            if os.path.basename(p)\n            in annotation_df[self.config.img_path_col].tolist()\n        ]\n    imgs = []\n    for p in paths:\n        img = self.parse_image_to_tensor(image_path=p, to_h=to_h, to_w=to_w)\n        img_path = os.path.basename(p)\n        subset: Union[pd.Series, pd.DataFrame] = annotation_df[\n            annotation_df[self.config.img_path_col] == img_path\n        ]\n        if not subset.empty:\n            imgs.append(\n                ImgData(\n                    img=img,\n                    sample_id=str(subset.index[0]),\n                    annotation=subset,\n                )\n            )\n    return imgs\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.ImageDataReader.read_annotation_file","title":"<code>read_annotation_file(data_info)</code>","text":"<p>Reads annotation file and returns DataFrame with file contents Args:     data_info: specific part of the Configuration object for input data Returns:     DataFrame with annotation data.</p> Source code in <code>src/autoencodix/utils/_imgreader.py</code> <pre><code>def read_annotation_file(self, data_info: DataInfo) -&gt; pd.DataFrame:\n    \"\"\"Reads annotation file and returns DataFrame with file contents\n    Args:\n        data_info: specific part of the Configuration object for input data\n    Returns:\n        DataFrame with annotation data.\n\n    \"\"\"\n    anno_file = (\n        os.path.join(data_info.file_path)\n        if data_info.extra_anno_file is None\n        else os.path.join(data_info.extra_anno_file)\n    )\n    sep = data_info.sep\n    if anno_file.endswith(\".parquet\"):\n        annotation = pd.read_parquet(anno_file)\n    elif anno_file.endswith((\".csv\", \".txt\", \".tsv\")):\n        annotation = pd.read_csv(anno_file, sep=sep, index_col=0, engine=\"python\")\n    else:\n        raise ValueError(f\"Unsupported file type for: {anno_file}\")\n    return annotation\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.ImageDataReader.read_data","title":"<code>read_data(config)</code>","text":"<p>Read image data from the specified directory based on configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>DefaultConfig</code> <p>The configuration object containing the data configuration.</p> required <p>Returns:</p> Type Description <code>Dict[str, List[ImgData]]</code> <p>A Tuple of Dicts:</p> <code>Dict[str, DataFrame]</code> <ol> <li>Dict with type of image data as key and actual List of ImgData as value.</li> </ol> <code>Tuple[Dict[str, List[ImgData]], Dict[str, DataFrame]]</code> <ol> <li>Dict with type of image data as key and DataFrame of annotation data as value.</li> </ol> <p>Raises:</p> Type Description <code>Exception</code> <p>If no image data is found in the configuration or other validation errors occur.</p> Source code in <code>src/autoencodix/utils/_imgreader.py</code> <pre><code>def read_data(\n    self, config: DefaultConfig\n) -&gt; Tuple[Dict[str, List[ImgData]], Dict[str, pd.DataFrame]]:\n    \"\"\"Read image data from the specified directory based on configuration.\n\n    Args:\n        config: The configuration object containing the data configuration.\n\n    Returns:\n        A Tuple of Dicts:\n        1. Dict with type of image data as key and actual List of ImgData as value.\n        2. Dict with type of image data as key and DataFrame of annotation data as value.\n\n    Raises:\n        Exception: If no image data is found in the configuration or other validation errors occur.\n    \"\"\"\n    # Find all image data sources in config\n    image_sources = {\n        k: v\n        for k, v in config.data_config.data_info.items()\n        if v.data_type == \"IMG\"\n    }\n\n    if not image_sources:\n        raise ValueError(\"No image data found in the configuration.\")\n\n    result = {}\n    annotation = {}\n    for key, img_info in image_sources.items():\n        try:\n            result[key], annotation[key] = self._read_data(config, img_info)\n            print(f\"Successfully loaded {len(result[key])} images for {key}\")\n        except Exception as e:\n            print(f\"Error loading images for {key}: {str(e)}\")\n            # Decide whether to raise or continue based on your requirements\n\n    return result, annotation\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.ImageDataReader.validate_image_path","title":"<code>validate_image_path(image_path)</code>","text":"<p>Checks if file extension is allowed:</p> <p>Allowed are (independent of capitalization):     - jpg     - jpeg     - png     - tif     - tiff</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Union[str, Path]</code> <p>path or str of image to read</p> required Source code in <code>src/autoencodix/utils/_imgreader.py</code> <pre><code>def validate_image_path(self, image_path: Union[str, Path]) -&gt; bool:\n    \"\"\"Checks if file extension is allowed:\n\n    Allowed are (independent of capitalization):\n        - jpg\n        - jpeg\n        - png\n        - tif\n        - tiff\n\n    Args:\n        image_path: path or str of image to read\n    \"\"\"\n    path = Path(image_path) if isinstance(image_path, str) else image_path\n    return (\n        path.exists()\n        and path.is_file()\n        and path.suffix.lower() in {\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"}\n    )\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.ModelOutput","title":"<code>ModelOutput</code>  <code>dataclass</code>","text":"<p>A structured output dataclass for autoencoder models.</p> <p>This class is used to encapsulate the outputs of autoencoder models in a consistent format, allowing for flexibility in the type of outputs returned by different architectures.</p> <p>Attributes:</p> Name Type Description <code>reconstruction</code> <code>Tensor</code> <p>The reconstructed input data.</p> <code>latent_mean</code> <code>Optional[Tensor]</code> <p>The mean of the latent space distribution, applicable for models like VAEs.</p> <code>latent_logvar</code> <code>Optional[Tensor]</code> <p>The log variance of the latent space distribution, applicable for models like VAEs.</p> <code>additional_info</code> <code>Optional[dict]</code> <p>A dictionary to store any additional information or intermediate outputs.</p> Source code in <code>src/autoencodix/utils/_model_output.py</code> <pre><code>@dataclass\nclass ModelOutput:\n    \"\"\"A structured output dataclass for autoencoder models.\n\n    This class is used to encapsulate the outputs of autoencoder models in a\n    consistent format, allowing for flexibility in the type of outputs returned\n    by different architectures.\n\n    Attributes:\n        reconstruction: The reconstructed input data.\n        latent_mean: The mean of the latent space distribution, applicable for models like VAEs.\n        latent_logvar: The log variance of the latent space distribution, applicable for models like VAEs.\n        additional_info:  A dictionary to store any additional information or intermediate outputs.\n    \"\"\"\n\n    reconstruction: torch.Tensor\n    latentspace: torch.Tensor\n    latent_mean: Optional[torch.Tensor] = None\n    latent_logvar: Optional[torch.Tensor] = None\n    additional_info: Optional[dict] = None\n\n    def __iter__(self):\n        yield self\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.Result","title":"<code>Result</code>  <code>dataclass</code>","text":"<p>A dataclass to store results from the pipeline with predefined keys.</p> <p>Attributes:</p> Name Type Description <code>latentspaces</code> <code>TrainingDynamics</code> <p>TrainingDynamics object storing latent space representations for 'train', 'valid', and 'test' splits.</p> <code>sample_ids</code> <code>TrainingDynamics</code> <p>TrainingDynamics object storing sample identifiers for 'train', 'valid', and 'test' splits.</p> <code>reconstructions</code> <code>TrainingDynamics</code> <p>TrainingDynamics object storing reconstructed outputs for 'train', 'valid', and 'test' splits.</p> <code>mus</code> <code>TrainingDynamics</code> <p>TrainingDynamics object storing mean values of latent distributions for 'train', 'valid', and 'test' splits.</p> <code>sigmas</code> <code>TrainingDynamics</code> <p>TrainingDynamics object storing standard deviations of latent distributions for 'train', 'valid', and 'test' splits.</p> <code>losses</code> <code>TrainingDynamics</code> <p>TrainingDynamics object storing the total loss for different epochs and splits ('train', 'valid', 'test').</p> <code>sub_losses</code> <code>LossRegistry</code> <p>LossRegistry object (extendable) for all sublosses.</p> <code>preprocessed_data</code> <code>Tensor</code> <p>torch.Tensor containing data after preprocessing.</p> <code>model</code> <code>Union[Dict[str, Module], Module]</code> <p>final trained torch.nn.Module model.</p> <code>model_checkpoints</code> <code>TrainingDynamics</code> <p>TrainingDynamics object storing model state at each checkpoint.</p> <code>datasets</code> <code>Optional[DatasetContainer]</code> <p>Optional[DatasetContainer] containing train, valid, and test datasets.</p> <code>new_datasets</code> <code>Optional[DatasetContainer]</code> <p>Optional[DatasetContainer] containing new train, valid, and test datasets.</p> <code>adata_latent</code> <code>Optional[AnnData]</code> <p>Optional[AnnData] containing latent representations as AnnData.</p> <code>final_reconstruction</code> <code>Optional[Union[DataPackage, MuData]]</code> <p>Optional[Union[DataPackage, MuData]] containing final reconstruction results.</p> <code>sub_results</code> <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]] containing sub-results for multi-task or multi-modal models.</p> <code>sub_reconstructions</code> <code>Optional[Dict[str, Any]]</code> <p>Optional[Dict[str, Any]] containing sub-reconstructions for multi-task or multi-modal models.</p> <code>embedding_evaluation</code> <code>DataFrame</code> <p>pd.DataFrame containing embedding evaluation results.</p> Source code in <code>src/autoencodix/utils/_result.py</code> <pre><code>@dataclass\nclass Result:\n    \"\"\"A dataclass to store results from the pipeline with predefined keys.\n\n    Attributes:\n        latentspaces: TrainingDynamics object storing latent space representations for 'train', 'valid', and 'test' splits.\n        sample_ids: TrainingDynamics object storing sample identifiers for 'train', 'valid', and 'test' splits.\n        reconstructions: TrainingDynamics object storing reconstructed outputs for 'train', 'valid', and 'test' splits.\n        mus: TrainingDynamics object storing mean values of latent distributions for 'train', 'valid', and 'test' splits.\n        sigmas: TrainingDynamics object storing standard deviations of latent distributions for 'train', 'valid', and 'test' splits.\n        losses: TrainingDynamics object storing the total loss for different epochs and splits ('train', 'valid', 'test').\n        sub_losses: LossRegistry object (extendable) for all sublosses.\n        preprocessed_data: torch.Tensor containing data after preprocessing.\n        model: final trained torch.nn.Module model.\n        model_checkpoints: TrainingDynamics object storing model state at each checkpoint.\n        datasets: Optional[DatasetContainer] containing train, valid, and test datasets.\n        new_datasets: Optional[DatasetContainer] containing new train, valid, and test datasets.\n        adata_latent: Optional[AnnData] containing latent representations as AnnData.\n        final_reconstruction: Optional[Union[DataPackage, MuData]] containing final reconstruction results.\n        sub_results: Optional[Dict[str, Any]] containing sub-results for multi-task or multi-modal models.\n        sub_reconstructions: Optional[Dict[str, Any]] containing sub-reconstructions for multi-task or multi-modal models.\n        embedding_evaluation: pd.DataFrame containing embedding evaluation results.\n    \"\"\"\n\n    latentspaces: TrainingDynamics = field(default_factory=TrainingDynamics)\n    sample_ids: TrainingDynamics = field(default_factory=TrainingDynamics)\n    reconstructions: TrainingDynamics = field(default_factory=TrainingDynamics)\n    mus: TrainingDynamics = field(default_factory=TrainingDynamics)\n    sigmas: TrainingDynamics = field(default_factory=TrainingDynamics)\n    losses: TrainingDynamics = field(default_factory=TrainingDynamics)\n    sub_losses: LossRegistry = field(default_factory=LossRegistry)\n    preprocessed_data: torch.Tensor = field(default_factory=torch.Tensor)\n    model: Union[Dict[str, torch.nn.Module], torch.nn.Module] = field(\n        default_factory=torch.nn.Module\n    )\n    model_checkpoints: TrainingDynamics = field(default_factory=TrainingDynamics)\n\n    datasets: Optional[DatasetContainer] = field(\n        default_factory=lambda: DatasetContainer(train=None, valid=None, test=None)\n    )\n    new_datasets: Optional[DatasetContainer] = field(\n        default_factory=lambda: DatasetContainer(train=None, valid=None, test=None)\n    )\n\n    adata_latent: Optional[AnnData] = field(default_factory=AnnData)\n    final_reconstruction: Optional[\n        Union[DataPackage, MuData]  # ty: ignore[invalid-type-form]\n    ] = field(default=None)\n    sub_results: Optional[Dict[str, Any]] = field(default=None)\n    sub_reconstructions: Optional[Dict[str, Any]] = field(default=None)\n\n    # Embedding evaluation results\n    embedding_evaluation: pd.DataFrame = field(default_factory=pd.DataFrame)\n\n    # plots: Dict[str, Any] = field(\n    #     default_factory=nested_dict\n    # )  ## Nested dictionary of plots as figure handles\n\n    def __getitem__(self, key: str) -&gt; Any:\n        \"\"\"Retrieve the value associated with a specific key.\n\n        Args:\n            key: The name of the attribute to retrieve.\n        Returns:\n            The value of the specified attribute.\n        Raises:\n            KeyError - If the key is not a valid attribute of the Results class.\n\n        \"\"\"\n        if not hasattr(self, key):\n            raise KeyError(\n                f\"Invalid key: '{key}'. Allowed keys are: {', '.join(self.__annotations__.keys())}\"\n            )\n        return getattr(self, key)\n\n    def __setitem__(self, key: str, value: Any) -&gt; None:\n        \"\"\"Assign a value to a specific attribute.\n\n        Args:\n            key: The name of the attribute to set.\n            value: The value to assign to the attribute.\n        Raises:\n            KeyError: If the key is not a valid attribute of the Results class.\n\n        \"\"\"\n        if not hasattr(self, key):\n            raise KeyError(\n                f\"Invalid key: '{key}'. Allowed keys are: {', '.join(self.__annotations__.keys())}\"\n            )\n        setattr(self, key, value)\n\n    def _is_empty_value(self, value: Any) -&gt; bool:\n        \"\"\"\n        Helper method to check if an attribute of the Result object is empty.\n\n        Parameters:\n            value (Any): The value to check\n        Returns:\n            bool: True if the value is empty, False otherwise\n\n        \"\"\"\n\n        if isinstance(value, TrainingDynamics):\n            return len(value._data) == 0\n        elif isinstance(value, torch.Tensor):\n            return value.numel() == 0\n        elif isinstance(value, torch.nn.Module):\n            return sum(p.numel() for p in value.parameters()) == 0\n        elif isinstance(value, DatasetContainer):\n            return all(v is None for v in [value.train, value.valid, value.test])\n        elif isinstance(value, LossRegistry):\n            # single Nones are handled in update method (skipped)\n            return all(v is None for _, v in value.losses())\n\n        return False\n\n    def update(self, other: \"Result\") -&gt; None:\n        \"\"\"Update the current Result object with values from another Result object.\n\n        For TrainingDynamics, merges the data across epochs and splits and overwrites if already exists.\n        For all other attributes, replaces the current value with the other value.\n\n        Args:\n            other: The Result object to update from.\n        Raises:\n            TypeError: If the input object is not a Result instance\n\n        \"\"\"\n        if not isinstance(other, Result):\n            raise TypeError(f\"Expected Result object, got {type(other)}\")\n\n        for field_name in self.__annotations__.keys():\n            current_value = getattr(self, field_name)\n            other_value = getattr(other, field_name)\n            if self._is_empty_value(other_value):\n                continue\n\n            # Handle TrainingDynamics - merge data\n            if isinstance(current_value, TrainingDynamics):\n                current_value = self._update_traindynamics(current_value, other_value)\n            # For all other types - replace with other value\n            if isinstance(current_value, LossRegistry):\n                for key, value in other_value.losses():\n                    if value is None:\n                        continue\n                    if not isinstance(value, TrainingDynamics):\n                        raise ValueError(\n                            f\"Expected TrainingDynamics object, got {type(value)}\"\n                        )\n                    updated_dynamic = self._update_traindynamics(\n                        current_value=current_value.get(key=key), other_value=value\n                    )\n                    current_value.set(key=key, value=updated_dynamic)\n            else:\n                setattr(self, field_name, other_value)\n\n    def _update_traindynamics(\n        self, current_value: TrainingDynamics, other_value: TrainingDynamics\n    ) -&gt; TrainingDynamics:\n        \"\"\"Update TrainingDynamics object with values from another TrainingDynamics object.\n\n        Args:\n        current_value: The current TrainingDynamics object to update.\n        other_value: The TrainingDynamics object to update from.\n\n        Returns:\n            Updated TrainingDynamics object.\n\n        Examples:\n            &gt;&gt;&gt; current = TrainingDynamics()\n            &gt;&gt;&gt; current._data = {1: {\"train\": np.array([1, 2, 3])},\n            ...                   2: None}\n\n            &gt;&gt;&gt; other = TrainingDynamics()\n            &gt;&gt;&gt; other._data = {1: {\"train\": np.array([4, 5, 6])},\n            ...                 2: {\"train\": np.array([7, 8, 9])}}\n            &gt;&gt;&gt; # after update\n            &gt;&gt;&gt; print(current._data)\n            {1: {\"train\": np.array([4, 5, 6])}, # updated\n            2: {\"train\": np.array([7, 8, 9])}} # kept, because other was None\n\n        \"\"\"\n\n        if current_value is None:\n            return other_value\n        if current_value._data is None:\n            return other_value\n\n        for epoch, split_data in other_value._data.items():\n            if split_data is None:\n                continue\n            if len(split_data) == 0:\n                continue\n\n            # If current epoch is None, it should be updated\n            if epoch in current_value._data and current_value._data[epoch] is None:\n                current_value._data[epoch] = {}\n                for split, data in split_data.items():\n                    if data is None:\n                        continue\n                    current_value.add(epoch=epoch, data=data, split=split)\n                continue\n\n            if epoch not in current_value._data:\n                for split, data in split_data.items():\n                    if data is None:\n                        continue\n                    current_value.add(epoch=epoch, data=data, split=split)\n                continue\n            # case when current epoch exists, then update all but None values\n            for split, value in split_data.items():\n                if value is not None:\n                    current_value.add(epoch=epoch, data=value, split=split)\n\n        # Ensure ordering\n        current_value._data = dict(sorted(current_value._data.items()))\n\n        return current_value\n\n    def __str__(self) -&gt; str:\n        \"\"\"Provide a readable string representation of the Result object's public attributes.\n\n        Returns:\n            Formatted string showing all public attributes and their values\n        \"\"\"\n        output = [\"Result Object Public Attributes:\", \"-\" * 30]\n\n        for name in self.__annotations__.keys():\n            if name.startswith(\"__\"):\n                continue\n\n            value = getattr(self, name)\n            if isinstance(value, TrainingDynamics):\n                output.append(f\"{name}: TrainingDynamics object\")\n            elif isinstance(value, torch.nn.Module):\n                output.append(f\"{name}: {value.__class__.__name__}\")\n            elif isinstance(value, dict):\n                output.append(f\"{name}: Dict with {len(value)} items\")\n            elif isinstance(value, torch.Tensor):\n                output.append(f\"{name}: Tensor of shape {tuple(value.shape)}\")\n            else:\n                output.append(f\"{name}: {value}\")\n\n        return \"\\n\".join(output)\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Return the same representation as __str__ for consistency.\"\"\"\n        return self.__str__()\n\n    def get_latent_df(\n        self, epoch: int, split: str, modality: Optional[str] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Return latent representations as a DataFrame.\n\n        Retrieves latent vectors and their corresponding sample IDs for a given\n        epoch and data split. If a specific modality is provided, the results\n        are restricted to that modality. Column names are inferred from model\n        ontologies if available; otherwise, generic latent dimension labels are\n        used.\n\n        Args:\n            epoch: The epoch number to retrieve latents from.\n            split: The dataset split to query (e.g., \"train\", \"valid\", \"test\").\n            modality: Optional modality name to filter the latents and sample IDs.\n\n        Returns:\n            A DataFrame where rows correspond to samples, columns represent latent\n            dimensions, and the index contains sample IDs.\n        \"\"\"\n        try:\n            latents = self.latentspaces.get(epoch=epoch, split=split)\n            ids = self.sample_ids.get(epoch=epoch, split=split)\n            if modality is not None:  # for x-modalix and other multi-modal models\n                latents = latents[modality]\n                ids = ids[modality]\n            if hasattr(self.model, \"ontologies\") and self.model.ontologies is not None:\n                cols = list(self.model.ontologies[0].keys())\n            else:\n                cols = [\"LatDim_\" + str(i) for i in range(latents.shape[1])]\n            return pd.DataFrame(latents, index=ids, columns=cols)\n        except Exception as e:\n            import warnings\n\n            warnings.warn(\n                f\"We could not create visualizations for the loss plots.\\n\"\n                f\"This usually happens if you try to visualize after saving and loading \"\n                f\"the pipeline object with `save_all=False`. This memory-efficient saving mode \"\n                f\"does not retain past training loss data.\\n\\n\"\n                f\"Original error message: {e}\"\n            )\n\n            return pd.DataFrame()\n\n    def get_reconstructions_df(\n        self, epoch: int, split: str, modality: Optional[str] = None\n    ) -&gt; pd.DataFrame:\n        \"\"\"Return reconstructions as a DataFrame.\n\n        Retrieves reconstructed features and their corresponding sample IDs for a\n        given epoch and data split. If a specific modality is provided, the results\n        are restricted to that modality. Column names are based on the dataset's\n        feature identifiers.\n\n        Args:\n            epoch: The epoch number to retrieve reconstructions from.\n            split: The dataset split to query (e.g., \"train\", \"valid\", \"test\").\n            modality: Optional modality name to filter the reconstructions and\n                sample IDs.\n\n        Returns:\n            A DataFrame where rows correspond to samples, columns represent\n            reconstructed features, and the index contains sample IDs.\n        \"\"\"\n        reconstructions = self.reconstructions.get(epoch=epoch, split=split)\n        ids = self.sample_ids.get(epoch=epoch, split=split)\n        if modality is not None:\n            reconstructions = reconstructions[modality]\n            ids = ids[modality]\n\n        cols = self.datasets.train.feature_ids\n        return pd.DataFrame(reconstructions, index=ids, columns=cols)\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.Result.__getitem__","title":"<code>__getitem__(key)</code>","text":"<p>Retrieve the value associated with a specific key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The name of the attribute to retrieve.</p> required <p>Returns:     The value of the specified attribute. Raises:     KeyError - If the key is not a valid attribute of the Results class.</p> Source code in <code>src/autoencodix/utils/_result.py</code> <pre><code>def __getitem__(self, key: str) -&gt; Any:\n    \"\"\"Retrieve the value associated with a specific key.\n\n    Args:\n        key: The name of the attribute to retrieve.\n    Returns:\n        The value of the specified attribute.\n    Raises:\n        KeyError - If the key is not a valid attribute of the Results class.\n\n    \"\"\"\n    if not hasattr(self, key):\n        raise KeyError(\n            f\"Invalid key: '{key}'. Allowed keys are: {', '.join(self.__annotations__.keys())}\"\n        )\n    return getattr(self, key)\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.Result.__repr__","title":"<code>__repr__()</code>","text":"<p>Return the same representation as str for consistency.</p> Source code in <code>src/autoencodix/utils/_result.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Return the same representation as __str__ for consistency.\"\"\"\n    return self.__str__()\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.Result.__setitem__","title":"<code>__setitem__(key, value)</code>","text":"<p>Assign a value to a specific attribute.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>The name of the attribute to set.</p> required <code>value</code> <code>Any</code> <p>The value to assign to the attribute.</p> required <p>Raises:     KeyError: If the key is not a valid attribute of the Results class.</p> Source code in <code>src/autoencodix/utils/_result.py</code> <pre><code>def __setitem__(self, key: str, value: Any) -&gt; None:\n    \"\"\"Assign a value to a specific attribute.\n\n    Args:\n        key: The name of the attribute to set.\n        value: The value to assign to the attribute.\n    Raises:\n        KeyError: If the key is not a valid attribute of the Results class.\n\n    \"\"\"\n    if not hasattr(self, key):\n        raise KeyError(\n            f\"Invalid key: '{key}'. Allowed keys are: {', '.join(self.__annotations__.keys())}\"\n        )\n    setattr(self, key, value)\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.Result.__str__","title":"<code>__str__()</code>","text":"<p>Provide a readable string representation of the Result object's public attributes.</p> <p>Returns:</p> Type Description <code>str</code> <p>Formatted string showing all public attributes and their values</p> Source code in <code>src/autoencodix/utils/_result.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Provide a readable string representation of the Result object's public attributes.\n\n    Returns:\n        Formatted string showing all public attributes and their values\n    \"\"\"\n    output = [\"Result Object Public Attributes:\", \"-\" * 30]\n\n    for name in self.__annotations__.keys():\n        if name.startswith(\"__\"):\n            continue\n\n        value = getattr(self, name)\n        if isinstance(value, TrainingDynamics):\n            output.append(f\"{name}: TrainingDynamics object\")\n        elif isinstance(value, torch.nn.Module):\n            output.append(f\"{name}: {value.__class__.__name__}\")\n        elif isinstance(value, dict):\n            output.append(f\"{name}: Dict with {len(value)} items\")\n        elif isinstance(value, torch.Tensor):\n            output.append(f\"{name}: Tensor of shape {tuple(value.shape)}\")\n        else:\n            output.append(f\"{name}: {value}\")\n\n    return \"\\n\".join(output)\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.Result.get_latent_df","title":"<code>get_latent_df(epoch, split, modality=None)</code>","text":"<p>Return latent representations as a DataFrame.</p> <p>Retrieves latent vectors and their corresponding sample IDs for a given epoch and data split. If a specific modality is provided, the results are restricted to that modality. Column names are inferred from model ontologies if available; otherwise, generic latent dimension labels are used.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The epoch number to retrieve latents from.</p> required <code>split</code> <code>str</code> <p>The dataset split to query (e.g., \"train\", \"valid\", \"test\").</p> required <code>modality</code> <code>Optional[str]</code> <p>Optional modality name to filter the latents and sample IDs.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where rows correspond to samples, columns represent latent</p> <code>DataFrame</code> <p>dimensions, and the index contains sample IDs.</p> Source code in <code>src/autoencodix/utils/_result.py</code> <pre><code>def get_latent_df(\n    self, epoch: int, split: str, modality: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Return latent representations as a DataFrame.\n\n    Retrieves latent vectors and their corresponding sample IDs for a given\n    epoch and data split. If a specific modality is provided, the results\n    are restricted to that modality. Column names are inferred from model\n    ontologies if available; otherwise, generic latent dimension labels are\n    used.\n\n    Args:\n        epoch: The epoch number to retrieve latents from.\n        split: The dataset split to query (e.g., \"train\", \"valid\", \"test\").\n        modality: Optional modality name to filter the latents and sample IDs.\n\n    Returns:\n        A DataFrame where rows correspond to samples, columns represent latent\n        dimensions, and the index contains sample IDs.\n    \"\"\"\n    try:\n        latents = self.latentspaces.get(epoch=epoch, split=split)\n        ids = self.sample_ids.get(epoch=epoch, split=split)\n        if modality is not None:  # for x-modalix and other multi-modal models\n            latents = latents[modality]\n            ids = ids[modality]\n        if hasattr(self.model, \"ontologies\") and self.model.ontologies is not None:\n            cols = list(self.model.ontologies[0].keys())\n        else:\n            cols = [\"LatDim_\" + str(i) for i in range(latents.shape[1])]\n        return pd.DataFrame(latents, index=ids, columns=cols)\n    except Exception as e:\n        import warnings\n\n        warnings.warn(\n            f\"We could not create visualizations for the loss plots.\\n\"\n            f\"This usually happens if you try to visualize after saving and loading \"\n            f\"the pipeline object with `save_all=False`. This memory-efficient saving mode \"\n            f\"does not retain past training loss data.\\n\\n\"\n            f\"Original error message: {e}\"\n        )\n\n        return pd.DataFrame()\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.Result.get_reconstructions_df","title":"<code>get_reconstructions_df(epoch, split, modality=None)</code>","text":"<p>Return reconstructions as a DataFrame.</p> <p>Retrieves reconstructed features and their corresponding sample IDs for a given epoch and data split. If a specific modality is provided, the results are restricted to that modality. Column names are based on the dataset's feature identifiers.</p> <p>Parameters:</p> Name Type Description Default <code>epoch</code> <code>int</code> <p>The epoch number to retrieve reconstructions from.</p> required <code>split</code> <code>str</code> <p>The dataset split to query (e.g., \"train\", \"valid\", \"test\").</p> required <code>modality</code> <code>Optional[str]</code> <p>Optional modality name to filter the reconstructions and sample IDs.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A DataFrame where rows correspond to samples, columns represent</p> <code>DataFrame</code> <p>reconstructed features, and the index contains sample IDs.</p> Source code in <code>src/autoencodix/utils/_result.py</code> <pre><code>def get_reconstructions_df(\n    self, epoch: int, split: str, modality: Optional[str] = None\n) -&gt; pd.DataFrame:\n    \"\"\"Return reconstructions as a DataFrame.\n\n    Retrieves reconstructed features and their corresponding sample IDs for a\n    given epoch and data split. If a specific modality is provided, the results\n    are restricted to that modality. Column names are based on the dataset's\n    feature identifiers.\n\n    Args:\n        epoch: The epoch number to retrieve reconstructions from.\n        split: The dataset split to query (e.g., \"train\", \"valid\", \"test\").\n        modality: Optional modality name to filter the reconstructions and\n            sample IDs.\n\n    Returns:\n        A DataFrame where rows correspond to samples, columns represent\n        reconstructed features, and the index contains sample IDs.\n    \"\"\"\n    reconstructions = self.reconstructions.get(epoch=epoch, split=split)\n    ids = self.sample_ids.get(epoch=epoch, split=split)\n    if modality is not None:\n        reconstructions = reconstructions[modality]\n        ids = ids[modality]\n\n    cols = self.datasets.train.feature_ids\n    return pd.DataFrame(reconstructions, index=ids, columns=cols)\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.Result.update","title":"<code>update(other)</code>","text":"<p>Update the current Result object with values from another Result object.</p> <p>For TrainingDynamics, merges the data across epochs and splits and overwrites if already exists. For all other attributes, replaces the current value with the other value.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Result</code> <p>The Result object to update from.</p> required <p>Raises:     TypeError: If the input object is not a Result instance</p> Source code in <code>src/autoencodix/utils/_result.py</code> <pre><code>def update(self, other: \"Result\") -&gt; None:\n    \"\"\"Update the current Result object with values from another Result object.\n\n    For TrainingDynamics, merges the data across epochs and splits and overwrites if already exists.\n    For all other attributes, replaces the current value with the other value.\n\n    Args:\n        other: The Result object to update from.\n    Raises:\n        TypeError: If the input object is not a Result instance\n\n    \"\"\"\n    if not isinstance(other, Result):\n        raise TypeError(f\"Expected Result object, got {type(other)}\")\n\n    for field_name in self.__annotations__.keys():\n        current_value = getattr(self, field_name)\n        other_value = getattr(other, field_name)\n        if self._is_empty_value(other_value):\n            continue\n\n        # Handle TrainingDynamics - merge data\n        if isinstance(current_value, TrainingDynamics):\n            current_value = self._update_traindynamics(current_value, other_value)\n        # For all other types - replace with other value\n        if isinstance(current_value, LossRegistry):\n            for key, value in other_value.losses():\n                if value is None:\n                    continue\n                if not isinstance(value, TrainingDynamics):\n                    raise ValueError(\n                        f\"Expected TrainingDynamics object, got {type(value)}\"\n                    )\n                updated_dynamic = self._update_traindynamics(\n                    current_value=current_value.get(key=key), other_value=value\n                )\n                current_value.set(key=key, value=updated_dynamic)\n        else:\n            setattr(self, field_name, other_value)\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.SingleCellDataReader","title":"<code>SingleCellDataReader</code>","text":"<p>Reader for multi-modal single-cell data.</p> Source code in <code>src/autoencodix/utils/_screader.py</code> <pre><code>class SingleCellDataReader:\n    \"\"\"Reader for multi-modal single-cell data.\"\"\"\n\n    @staticmethod\n    def read_data(\n        config: DefaultConfig,\n    ) -&gt; Dict[str, MuData]:  # ty: ignore[invalid-type-form]\n        \"\"\"Read multiple single-cell modalities into MuData object(s).\n\n        Args:\n        config: Configuration object containing data paths and parameters.\n\n        Returns:\n            For non-paired translation: Dict of Dicts with {'multi_sc': DataDict} as outer dict and with modalty keys and mudata obj as inner dict.\n            For paired translation and non translation cases: dict with \"multi_sc\" as key and mudata as value\n        \"\"\"\n        modalities: Dict[str, AnnData] = {}\n\n        for mod_key, mod_info in config.data_config.data_info.items():\n            if not mod_info.is_single_cell:\n                continue\n            adata = sc.read_h5ad(mod_info.file_path)\n            modalities[mod_key] = adata\n\n        # if config.requires_paired:\n        #     mdata = md.MuData(modalities)\n        #     common_cells = list(\n        #         set.intersection(\n        #             *(set(adata.obs_names) for adata in modalities.values())\n        #         )\n        #     )\n        #     print(f\"Number of common cells: {len(common_cells)}\")\n        #     mdata = mdata[common_cells]\n        #     return {\"multi_sc\": mdata}\n\n        if config.requires_paired:\n            common_cells_set = set.intersection(\n                *(set(adata.obs_names) for adata in modalities.values())\n            )\n            common_cells_sorted = sorted(list(common_cells_set))\n\n            # Subset EACH modality individually with the sorted common cells\n            # This ensures each modality is aligned to the same order\n            aligned_modalities = {}\n            for mod_key, adata in modalities.items():\n                aligned_modalities[mod_key] = adata[common_cells_sorted].copy()\n            mdata = md.MuData(aligned_modalities)\n\n            print(f\"Number of common cells: {len(common_cells_sorted)}\")\n\n            # Clean obs_names: remove modality prefixes\n            cleaned_names = [\n                name.split(\":\")[-1] if \":\" in name else name\n                for name in mdata.obs.columns\n            ]\n            mdata.obs.columns = cleaned_names\n\n            # Remove duplicate columns from obs\n            mdata.obs = mdata.obs.loc[:, ~mdata.obs.columns.duplicated(keep=\"first\")]\n\n            return {\"multi_sc\": mdata}\n        return {\"multi_sc\": modalities}\n</code></pre>"},{"location":"api/utils/#autoencodix.utils.SingleCellDataReader.read_data","title":"<code>read_data(config)</code>  <code>staticmethod</code>","text":"<p>Read multiple single-cell modalities into MuData object(s).</p> <p>Args: config: Configuration object containing data paths and parameters.</p> <p>Returns:</p> Type Description <code>Dict[str, MuData]</code> <p>For non-paired translation: Dict of Dicts with {'multi_sc': DataDict} as outer dict and with modalty keys and mudata obj as inner dict.</p> <code>Dict[str, MuData]</code> <p>For paired translation and non translation cases: dict with \"multi_sc\" as key and mudata as value</p> Source code in <code>src/autoencodix/utils/_screader.py</code> <pre><code>@staticmethod\ndef read_data(\n    config: DefaultConfig,\n) -&gt; Dict[str, MuData]:  # ty: ignore[invalid-type-form]\n    \"\"\"Read multiple single-cell modalities into MuData object(s).\n\n    Args:\n    config: Configuration object containing data paths and parameters.\n\n    Returns:\n        For non-paired translation: Dict of Dicts with {'multi_sc': DataDict} as outer dict and with modalty keys and mudata obj as inner dict.\n        For paired translation and non translation cases: dict with \"multi_sc\" as key and mudata as value\n    \"\"\"\n    modalities: Dict[str, AnnData] = {}\n\n    for mod_key, mod_info in config.data_config.data_info.items():\n        if not mod_info.is_single_cell:\n            continue\n        adata = sc.read_h5ad(mod_info.file_path)\n        modalities[mod_key] = adata\n\n    # if config.requires_paired:\n    #     mdata = md.MuData(modalities)\n    #     common_cells = list(\n    #         set.intersection(\n    #             *(set(adata.obs_names) for adata in modalities.values())\n    #         )\n    #     )\n    #     print(f\"Number of common cells: {len(common_cells)}\")\n    #     mdata = mdata[common_cells]\n    #     return {\"multi_sc\": mdata}\n\n    if config.requires_paired:\n        common_cells_set = set.intersection(\n            *(set(adata.obs_names) for adata in modalities.values())\n        )\n        common_cells_sorted = sorted(list(common_cells_set))\n\n        # Subset EACH modality individually with the sorted common cells\n        # This ensures each modality is aligned to the same order\n        aligned_modalities = {}\n        for mod_key, adata in modalities.items():\n            aligned_modalities[mod_key] = adata[common_cells_sorted].copy()\n        mdata = md.MuData(aligned_modalities)\n\n        print(f\"Number of common cells: {len(common_cells_sorted)}\")\n\n        # Clean obs_names: remove modality prefixes\n        cleaned_names = [\n            name.split(\":\")[-1] if \":\" in name else name\n            for name in mdata.obs.columns\n        ]\n        mdata.obs.columns = cleaned_names\n\n        # Remove duplicate columns from obs\n        mdata.obs = mdata.obs.loc[:, ~mdata.obs.columns.duplicated(keep=\"first\")]\n\n        return {\"multi_sc\": mdata}\n    return {\"multi_sc\": modalities}\n</code></pre>"},{"location":"api/vanillix/","title":"Vanillix Module","text":""},{"location":"api/vanillix/#autoencodix.vanillix.Vanillix","title":"<code>Vanillix</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>Vanillix specific version of the BasePipeline class.</p> <p>Inherits preprocess, fit, predict, evaluate, and visualize methods from BasePipeline. This class extends BasePipeline. See the parent class for a full list of attributes and methods.</p> Additional Attributes <p>_default_config: Is set to VanillixConfig here.</p> Source code in <code>src/autoencodix/vanillix.py</code> <pre><code>class Vanillix(BasePipeline):\n    \"\"\"Vanillix specific version of the BasePipeline class.\n\n    Inherits preprocess, fit, predict, evaluate, and visualize methods from BasePipeline.\n    This class extends BasePipeline. See the parent class for a full list\n    of attributes and methods.\n\n    Additional Attributes:\n        _default_config: Is set to VanillixConfig here.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Optional[Union[DataPackage, DatasetContainer]] = None,\n        trainer_type: Type[BaseTrainer] = GeneralTrainer,\n        dataset_type: Type[BaseDataset] = NumericDataset,\n        model_type: Type[BaseAutoencoder] = VanillixArchitecture,\n        loss_type: Type[BaseLoss] = VanillixLoss,\n        preprocessor_type: Type[BasePreprocessor] = GeneralPreprocessor,\n        visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n        evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n        result: Optional[Result] = None,\n        datasplitter_type: Type[DataSplitter] = DataSplitter,\n        custom_splits: Optional[Dict[str, np.ndarray]] = None,\n        config: Optional[DefaultConfig] = None,\n        ontologies: Optional[Union[List, Dict]] = None,\n    ) -&gt; None:\n        \"\"\"Initialize Vanillix pipeline with customizable components.\n\n        Some components are passed as types rather than instances because they require\n        data that is only available after preprocessing.\n\n        See implementation of parent class for list of full Args.\n        \"\"\"\n        self._default_config = VanillixConfig()\n        super().__init__(\n            data=data,\n            dataset_type=dataset_type,\n            trainer_type=trainer_type,\n            model_type=model_type,\n            loss_type=loss_type,\n            preprocessor_type=preprocessor_type,\n            visualizer=visualizer,\n            evaluator=evaluator,\n            result=result,\n            datasplitter_type=datasplitter_type,\n            config=config,\n            custom_split=custom_splits,\n            ontologies=ontologies,\n        )\n\n    def sample_latent_space(\n        self,\n        n_samples: int,\n        split: str = \"test\",\n        epoch: int = -1,\n    ) -&gt; torch.Tensor:\n        \"\"\"Samples latent space points from the empirical latent distribution.\n\n        This method draws new latent points by fitting a diagonal Gaussian\n        distribution to the latent codes of the specified split and epoch, and\n        sampling from it. This enables approximate generative sampling for\n        autoencoders that do not model uncertainty explicitly.\n\n        Args:\n            n_samples: The number of latent points to sample. Must be a positive\n                integer.\n            split: The split to sample from (train, valid, test), default is test.\n            epoch: The epoch to sample from, default is the last epoch (-1).\n\n        Returns:\n            z: torch.Tensor - The sampled latent space points.\n\n        Raises:\n            ValueError: If the model has not been trained, latent codes have not\n                been computed, or n_samples is not a positive integer.\n            TypeError: If the stored latent codes are not numpy arrays.\n        \"\"\"\n\n        if not hasattr(self, \"_trainer\") or self._trainer is None:\n            raise ValueError(\"Model is not trained yet. Please train the model first.\")\n        if self.result.latentspaces is None:\n            raise ValueError(\"Model has no stored latent codes for sampling.\")\n        if not isinstance(n_samples, int) or n_samples &lt;= 0:\n            raise ValueError(\"n_samples must be a positive integer.\")\n\n        Z = self.result.latentspaces.get(split=split, epoch=epoch)\n\n        if not isinstance(Z, np.ndarray):\n            raise TypeError(\n                f\"Expected latent codes to be of type numpy.ndarray, got {type(Z)}.\"\n            )\n\n        Z_t = torch.from_numpy(Z).to(\n            device=self._trainer._model.device,\n            dtype=self._trainer._model.dtype,\n        )\n\n        with torch.no_grad():\n            # Fit empirical diagonal Gaussian\n            global_mu = Z_t.mean(dim=0)\n            global_std = Z_t.std(dim=0)\n\n            eps = torch.randn(\n                n_samples,\n                Z_t.shape[1],\n                device=Z_t.device,\n                dtype=Z_t.dtype,\n            )\n\n            z = global_mu + eps * global_std\n            return z\n</code></pre>"},{"location":"api/vanillix/#autoencodix.vanillix.Vanillix.__init__","title":"<code>__init__(data=None, trainer_type=GeneralTrainer, dataset_type=NumericDataset, model_type=VanillixArchitecture, loss_type=VanillixLoss, preprocessor_type=GeneralPreprocessor, visualizer=GeneralVisualizer, evaluator=GeneralEvaluator, result=None, datasplitter_type=DataSplitter, custom_splits=None, config=None, ontologies=None)</code>","text":"<p>Initialize Vanillix pipeline with customizable components.</p> <p>Some components are passed as types rather than instances because they require data that is only available after preprocessing.</p> <p>See implementation of parent class for list of full Args.</p> Source code in <code>src/autoencodix/vanillix.py</code> <pre><code>def __init__(\n    self,\n    data: Optional[Union[DataPackage, DatasetContainer]] = None,\n    trainer_type: Type[BaseTrainer] = GeneralTrainer,\n    dataset_type: Type[BaseDataset] = NumericDataset,\n    model_type: Type[BaseAutoencoder] = VanillixArchitecture,\n    loss_type: Type[BaseLoss] = VanillixLoss,\n    preprocessor_type: Type[BasePreprocessor] = GeneralPreprocessor,\n    visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n    evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n    result: Optional[Result] = None,\n    datasplitter_type: Type[DataSplitter] = DataSplitter,\n    custom_splits: Optional[Dict[str, np.ndarray]] = None,\n    config: Optional[DefaultConfig] = None,\n    ontologies: Optional[Union[List, Dict]] = None,\n) -&gt; None:\n    \"\"\"Initialize Vanillix pipeline with customizable components.\n\n    Some components are passed as types rather than instances because they require\n    data that is only available after preprocessing.\n\n    See implementation of parent class for list of full Args.\n    \"\"\"\n    self._default_config = VanillixConfig()\n    super().__init__(\n        data=data,\n        dataset_type=dataset_type,\n        trainer_type=trainer_type,\n        model_type=model_type,\n        loss_type=loss_type,\n        preprocessor_type=preprocessor_type,\n        visualizer=visualizer,\n        evaluator=evaluator,\n        result=result,\n        datasplitter_type=datasplitter_type,\n        config=config,\n        custom_split=custom_splits,\n        ontologies=ontologies,\n    )\n</code></pre>"},{"location":"api/vanillix/#autoencodix.vanillix.Vanillix.sample_latent_space","title":"<code>sample_latent_space(n_samples, split='test', epoch=-1)</code>","text":"<p>Samples latent space points from the empirical latent distribution.</p> <p>This method draws new latent points by fitting a diagonal Gaussian distribution to the latent codes of the specified split and epoch, and sampling from it. This enables approximate generative sampling for autoencoders that do not model uncertainty explicitly.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>The number of latent points to sample. Must be a positive integer.</p> required <code>split</code> <code>str</code> <p>The split to sample from (train, valid, test), default is test.</p> <code>'test'</code> <code>epoch</code> <code>int</code> <p>The epoch to sample from, default is the last epoch (-1).</p> <code>-1</code> <p>Returns:</p> Name Type Description <code>z</code> <code>Tensor</code> <p>torch.Tensor - The sampled latent space points.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model has not been trained, latent codes have not been computed, or n_samples is not a positive integer.</p> <code>TypeError</code> <p>If the stored latent codes are not numpy arrays.</p> Source code in <code>src/autoencodix/vanillix.py</code> <pre><code>def sample_latent_space(\n    self,\n    n_samples: int,\n    split: str = \"test\",\n    epoch: int = -1,\n) -&gt; torch.Tensor:\n    \"\"\"Samples latent space points from the empirical latent distribution.\n\n    This method draws new latent points by fitting a diagonal Gaussian\n    distribution to the latent codes of the specified split and epoch, and\n    sampling from it. This enables approximate generative sampling for\n    autoencoders that do not model uncertainty explicitly.\n\n    Args:\n        n_samples: The number of latent points to sample. Must be a positive\n            integer.\n        split: The split to sample from (train, valid, test), default is test.\n        epoch: The epoch to sample from, default is the last epoch (-1).\n\n    Returns:\n        z: torch.Tensor - The sampled latent space points.\n\n    Raises:\n        ValueError: If the model has not been trained, latent codes have not\n            been computed, or n_samples is not a positive integer.\n        TypeError: If the stored latent codes are not numpy arrays.\n    \"\"\"\n\n    if not hasattr(self, \"_trainer\") or self._trainer is None:\n        raise ValueError(\"Model is not trained yet. Please train the model first.\")\n    if self.result.latentspaces is None:\n        raise ValueError(\"Model has no stored latent codes for sampling.\")\n    if not isinstance(n_samples, int) or n_samples &lt;= 0:\n        raise ValueError(\"n_samples must be a positive integer.\")\n\n    Z = self.result.latentspaces.get(split=split, epoch=epoch)\n\n    if not isinstance(Z, np.ndarray):\n        raise TypeError(\n            f\"Expected latent codes to be of type numpy.ndarray, got {type(Z)}.\"\n        )\n\n    Z_t = torch.from_numpy(Z).to(\n        device=self._trainer._model.device,\n        dtype=self._trainer._model.dtype,\n    )\n\n    with torch.no_grad():\n        # Fit empirical diagonal Gaussian\n        global_mu = Z_t.mean(dim=0)\n        global_std = Z_t.std(dim=0)\n\n        eps = torch.randn(\n            n_samples,\n            Z_t.shape[1],\n            device=Z_t.device,\n            dtype=Z_t.dtype,\n        )\n\n        z = global_mu + eps * global_std\n        return z\n</code></pre>"},{"location":"api/varix/","title":"Varix Module","text":""},{"location":"api/varix/#autoencodix.varix.Varix","title":"<code>Varix</code>","text":"<p>               Bases: <code>BasePipeline</code></p> <p>Varix specific version of the BasePipeline class.</p> <p>Inherits preprocess, fit, predict, evaluate, and visualize methods from BasePipeline. This class extends BasePipeline. See the parent class for a full list of attributes and methods.</p> Additional Attributes <p>_default_config: Is set to VarixConfig here.</p> Source code in <code>src/autoencodix/varix.py</code> <pre><code>class Varix(BasePipeline):\n    \"\"\"Varix specific version of the BasePipeline class.\n\n    Inherits preprocess, fit, predict, evaluate, and visualize methods from BasePipeline.\n    This class extends BasePipeline. See the parent class for a full list\n    of attributes and methods.\n\n    Additional Attributes:\n        _default_config: Is set to VarixConfig here.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Optional[Union[DataPackage, DatasetContainer]] = None,\n        trainer_type: Type[BaseTrainer] = GeneralTrainer,\n        dataset_type: Type[BaseDataset] = NumericDataset,\n        model_type: Type[BaseAutoencoder] = VarixArchitecture,\n        loss_type: Type[BaseLoss] = VarixLoss,\n        preprocessor_type: Type[BasePreprocessor] = GeneralPreprocessor,\n        visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n        evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n        result: Optional[Result] = None,\n        datasplitter_type: Type[DataSplitter] = DataSplitter,\n        custom_splits: Optional[Dict[str, np.ndarray]] = None,\n        ontologies: Optional[Union[List, Dict]] = None,\n        config: Optional[DefaultConfig] = None,\n    ) -&gt; None:\n        \"\"\"Initialize Varix pipeline with customizable components.\n\n        Some components are passed as types rather than instances because they require\n        data that is only available after preprocessing.\n\n        See parent class for full list of Args.\n\n        \"\"\"\n        self._default_config = VarixConfig()\n        super().__init__(\n            data=data,\n            dataset_type=dataset_type,\n            trainer_type=trainer_type,\n            model_type=model_type,\n            loss_type=loss_type,\n            preprocessor_type=preprocessor_type,\n            visualizer=visualizer,\n            evaluator=evaluator,\n            result=result,\n            datasplitter_type=datasplitter_type,\n            config=config,\n            custom_split=custom_splits,\n            ontologies=ontologies,\n        )\n</code></pre>"},{"location":"api/varix/#autoencodix.varix.Varix.__init__","title":"<code>__init__(data=None, trainer_type=GeneralTrainer, dataset_type=NumericDataset, model_type=VarixArchitecture, loss_type=VarixLoss, preprocessor_type=GeneralPreprocessor, visualizer=GeneralVisualizer, evaluator=GeneralEvaluator, result=None, datasplitter_type=DataSplitter, custom_splits=None, ontologies=None, config=None)</code>","text":"<p>Initialize Varix pipeline with customizable components.</p> <p>Some components are passed as types rather than instances because they require data that is only available after preprocessing.</p> <p>See parent class for full list of Args.</p> Source code in <code>src/autoencodix/varix.py</code> <pre><code>def __init__(\n    self,\n    data: Optional[Union[DataPackage, DatasetContainer]] = None,\n    trainer_type: Type[BaseTrainer] = GeneralTrainer,\n    dataset_type: Type[BaseDataset] = NumericDataset,\n    model_type: Type[BaseAutoencoder] = VarixArchitecture,\n    loss_type: Type[BaseLoss] = VarixLoss,\n    preprocessor_type: Type[BasePreprocessor] = GeneralPreprocessor,\n    visualizer: Type[BaseVisualizer] = GeneralVisualizer,\n    evaluator: Optional[Type[BaseEvaluator]] = GeneralEvaluator,\n    result: Optional[Result] = None,\n    datasplitter_type: Type[DataSplitter] = DataSplitter,\n    custom_splits: Optional[Dict[str, np.ndarray]] = None,\n    ontologies: Optional[Union[List, Dict]] = None,\n    config: Optional[DefaultConfig] = None,\n) -&gt; None:\n    \"\"\"Initialize Varix pipeline with customizable components.\n\n    Some components are passed as types rather than instances because they require\n    data that is only available after preprocessing.\n\n    See parent class for full list of Args.\n\n    \"\"\"\n    self._default_config = VarixConfig()\n    super().__init__(\n        data=data,\n        dataset_type=dataset_type,\n        trainer_type=trainer_type,\n        model_type=model_type,\n        loss_type=loss_type,\n        preprocessor_type=preprocessor_type,\n        visualizer=visualizer,\n        evaluator=evaluator,\n        result=result,\n        datasplitter_type=datasplitter_type,\n        config=config,\n        custom_split=custom_splits,\n        ontologies=ontologies,\n    )\n</code></pre>"},{"location":"api/visualize/","title":"Visualize Module","text":""},{"location":"api/visualize/#autoencodix.visualize.GeneralVisualizer","title":"<code>GeneralVisualizer</code>","text":"<p>               Bases: <code>BaseVisualizer</code></p> Source code in <code>src/autoencodix/visualize/_general_visualizer.py</code> <pre><code>class GeneralVisualizer(BaseVisualizer):\n    plots: Dict[str, Any] = field(\n        default_factory=nested_dict\n    )  ## Nested dictionary of plots as figure handles\n\n    def __init__(self):\n        self.plots = nested_dict()\n\n    def __setitem__(self, key, elem):\n        self.plots[key] = elem\n\n    def visualize(self, result: Result, config: DefaultConfig) -&gt; Result:\n        ## Make Model Weights plot\n        if result.model.input_dim &lt;= 3000:\n            self.plots[\"ModelWeights\"] = self._plot_model_weights(model=result.model)\n        else:\n            warnings.warn(\n                f\"Model weights plot is skipped since input dimension {result.model.input_dim} is larger than 3000 and heatmap would be too large.\"\n            )\n\n        ## Make long format of losses\n        try:\n            loss_df_melt = self._make_loss_format(result=result, config=config)\n\n            ## Make plot loss absolute\n            self.plots[\"loss_absolute\"] = self._make_loss_plot(\n                df_plot=loss_df_melt, plot_type=\"absolute\"\n            )\n            ## Make plot loss relative\n            self.plots[\"loss_relative\"] = self._make_loss_plot(\n                df_plot=loss_df_melt, plot_type=\"relative\"\n            )\n        except Exception as e:\n            warnings.warn(\n                f\"We could not create visualizations for the loss plots.\\n\"\n                f\"This usually happens if you try to visualize after saving and loading \"\n                f\"the pipeline object with `save_all=False`. This memory-efficient saving mode \"\n                f\"does not retain past training loss data.\\n\\n\"\n                f\"Original error message: {e}\"\n            )\n\n        return result\n\n    ## Plotting methods ##\n    @no_type_check\n    def show_latent_space(\n        self,\n        result: Result,\n        plot_type: Literal[\n            \"2D-scatter\", \"Ridgeline\", \"Coverage-Correlation\"\n        ] = \"2D-scatter\",\n        labels: Optional[Union[list, pd.Series, None]] = None,\n        param: Optional[Union[list, str]] = None,\n        epoch: Optional[Union[int, None]] = None,\n        split: str = \"all\",\n        n_downsample: Optional[int] = 10000,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Visualizes the latent space of the given result using different types of plots.\n\n        Args:\n            result: The result object containing latent spaces and losses.\n            plot_type: The type of plot to generate. Options are \"2D-scatter\", \"Ridgeline\", and \"Coverage-Correlation\". Default is \"2D-scatter\".\n            labels: List of labels for the data points in the latent space. Default is None.\n            param: List of parameters provided and stored as metadata. Strings must match column names. If not a list, string \"all\" is expected for convenient way to make plots for all parameters available. Default is None where no colored labels are plotted.\n            epoch: The epoch number to visualize. If None, the last epoch is inferred from the losses. Default is None.\n            split: The data split to visualize. Options are \"train\", \"valid\", \"test\", and \"all\". Default is \"all\".\n            n_downsample: If provided, downsample the data to this number of samples for faster visualization. Default is 10000. Set to None to disable downsampling.\n            **kwargs: additional arguments.\n\n        \"\"\"\n        plt.ioff()\n        if plot_type == \"Coverage-Correlation\":\n            if \"Coverage-Correlation\" in self.plots:\n                fig = self.plots[\"Coverage-Correlation\"]\n                show_figure(fig)\n                plt.show()\n            else:\n                results = []\n                for epoch in range(\n                    result.model.config.checkpoint_interval,\n                    result.model.config.epochs + 1,\n                    result.model.config.checkpoint_interval,\n                ):\n                    for split in [\"train\", \"valid\"]:\n                        latent_df = result.get_latent_df(epoch=epoch - 1, split=split)\n                        tc = self._total_correlation(latent_df)\n                        cov = self._coverage_calc(latent_df)\n                        results.append(\n                            {\n                                \"epoch\": epoch,\n                                \"split\": split,\n                                \"total_correlation\": tc,\n                                \"coverage\": cov,\n                            }\n                        )\n\n                df_metrics = pd.DataFrame(results)\n\n                fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n                # Total Correlation plot\n                _ = sns.lineplot(\n                    data=df_metrics,\n                    x=\"epoch\",\n                    y=\"total_correlation\",\n                    hue=\"split\",\n                    ax=axes[0],\n                )\n                axes[0].set_title(\"Total Correlation\")\n                axes[0].set_xlabel(\"Epoch\")\n                axes[0].set_ylabel(\"Total Correlation\")\n\n                # Coverage plot\n                _ = sns.lineplot(\n                    data=df_metrics, x=\"epoch\", y=\"coverage\", hue=\"split\", ax=axes[1]\n                )\n                axes[1].set_title(\"Coverage\")\n                axes[1].set_xlabel(\"Epoch\")\n                axes[1].set_ylabel(\"Coverage\")\n\n                plt.tight_layout()\n                self.plots[\"Coverage-Correlation\"] = fig\n                show_figure(fig)\n                plt.show()\n\n        else:\n            # Set Defaults\n            if epoch is None:\n                epoch = result.model.config.epochs - 1\n\n            # ## Getting clin_data\n            clin_data = self._collect_all_metadata(result=result)\n            # if hasattr(result.datasets.train, \"metadata\"):\n            #     # Check if metadata is a dictionary and contains 'paired'\n            #     if isinstance(result.datasets.train.metadata, dict):\n            #         if \"paired\" in result.datasets.train.metadata:\n            #             clin_data = result.datasets.train.metadata[\"paired\"]\n            #             if hasattr(result.datasets, \"test\"):\n            #                 clin_data = pd.concat(\n            #                     [\n            #                         clin_data,\n            #                         result.datasets.test.metadata[  # ty: ignore\n            #                             \"paired\"\n            #                         ],  # ty: ignore\n            #                     ],  # ty: ignore\n            #                     axis=0,\n            #                 )\n            #             if hasattr(result.datasets, \"valid\"):\n            #                 clin_data = pd.concat(\n            #                     [\n            #                         clin_data,\n            #                         result.datasets.valid.metadata[  # ty: ignore\n            #                             \"paired\"\n            #                         ],  # ty: ignore\n            #                     ],  # ty: ignore\n            #                     axis=0,\n            #                 )\n            #         else:\n            #             # Iterate over all splits and keys, concatenate if DataFrame\n            #             clin_data = pd.DataFrame()\n            #             for split_name in [\"train\", \"test\", \"valid\"]:\n            #                 split_temp = getattr(result.datasets, split_name, None)\n            #                 if split_temp is not None and hasattr(\n            #                     split_temp, \"metadata\"\n            #                 ):\n            #                     for key in split_temp.metadata.keys():\n            #                         if isinstance(\n            #                             split_temp.metadata[key], pd.DataFrame\n            #                         ):\n            #                             clin_data = pd.concat(\n            #                                 [\n            #                                     clin_data,\n            #                                     split_temp.metadata[key],\n            #                                 ],\n            #                                 axis=0,\n            #                             )\n            #             # remove duplicate rows\n            #             clin_data = clin_data[~clin_data.index.duplicated(keep=\"first\")]\n            #             # if clin_data.empty:\n            #             #     # Raise error no annotation given\n            #             #     raise ValueError(\n            #             #         \"Please provide paired annotation data with key 'paired' in metadata dictionary.\"\n            #             #     )\n            #     elif isinstance(result.datasets.train.metadata, pd.DataFrame):\n            #         clin_data = result.datasets.train.metadata\n            #         if hasattr(result.datasets, \"test\"):\n            #             clin_data = pd.concat(\n            #                 [clin_data, result.datasets.test.metadata],  # ty: ignore\n            #                 axis=0,\n            #             )\n            #         if hasattr(result.datasets, \"valid\"):\n            #             clin_data = pd.concat(\n            #                 [clin_data, result.datasets.valid.metadata],  # ty: ignore\n            #                 axis=0,\n            #             )\n            #     else:\n            #         # Raise error no annotation given\n            #         raise ValueError(\n            #             \"Metadata is not a dictionary or DataFrame. Please provide a valid annotation data type.\"\n            #         )\n            # else:\n            #     # Iterate over all splits and keys, concatenate if DataFrame\n            #     clin_data = pd.DataFrame()\n            #     for split_name in [\"train\", \"test\", \"valid\"]:\n            #         split_temp = getattr(result.datasets, split_name, None)\n            #         if split_temp is not None:\n            #             for key in split_temp.datasets.keys():\n            #                 if isinstance(\n            #                     split_temp.datasets[key].metadata, pd.DataFrame\n            #                 ):\n            #                     clin_data = pd.concat(\n            #                         [\n            #                             clin_data,\n            #                             split_temp.datasets[key].metadata,\n            #                         ],\n            #                         axis=0,\n            #                     )\n            #     if len(clin_data) == 0: ## New predict case\n            #         for split_name in [\"train\", \"test\", \"valid\"]:\n            #             split_temp = getattr(result.new_datasets, split_name, None)\n            #             if split_temp is not None:\n            #                 if len(split_temp.datasets.keys()) &gt; 0:\n            #                     for key in split_temp.datasets.keys():\n            #                         if isinstance(\n            #                             split_temp.datasets[key].metadata, pd.DataFrame\n            #                         ):\n            #                             clin_data = pd.concat(\n            #                                 [\n            #                                     clin_data,\n            #                                     split_temp.datasets[key].metadata,\n            #                                 ],\n            #                                 axis=0,\n            #                             )\n            #                 else:\n            #                     if isinstance(\n            #                         split_temp.metadata, pd.DataFrame\n            #                     ):\n            #                         clin_data = pd.concat(\n            #                             [\n            #                                 clin_data,\n            #                                 split_temp.metadata,\n            #                             ],\n            #                             axis=0,\n            #                         )\n            #     # remove duplicate rows\n            #     clin_data = clin_data[~clin_data.index.duplicated(keep=\"first\")]\n\n            # # Raise error no annotation given\n            # raise ValueError(\n            #     \"No annotation data found. Please provide a valid annotation data type.\"\n            # )\n\n            if split == \"all\":\n                df_latent = pd.concat(\n                    [\n                        result.get_latent_df(epoch=epoch, split=\"train\"),\n                        result.get_latent_df(epoch=epoch, split=\"valid\"),\n                        result.get_latent_df(epoch=-1, split=\"test\"),\n                    ]\n                )\n            else:\n                if split == \"test\":\n                    df_latent = result.get_latent_df(epoch=-1, split=split)\n                else:\n                    df_latent = result.get_latent_df(epoch=epoch, split=split)\n\n            ## Label options\n            if labels is None and param is None:\n                labels = [\"all\"] * df_latent.shape[0]\n\n            if labels is None and isinstance(param, str):\n                if param == \"all\":\n                    param = list(clin_data.columns)\n                else:\n                    raise ValueError(\n                        \"Please provide parameter to plot as a list not as string. If you want to plot all parameters, set param to 'all' and labels to None.\"\n                    )\n\n            if labels is not None and param is not None:\n                raise ValueError(\n                    \"Please provide either labels or param, not both. If you want to plot all parameters, set param to 'all' and labels to None.\"\n                )\n\n            if labels is not None and param is None:\n                if isinstance(labels, pd.Series):\n                    param = [labels.name]\n                    # Order by index of df_latent first, fill missing with \"unknown\"\n                    labels = labels.reindex(\n                        df_latent.index, fill_value=\"unknown\"\n                    ).tolist()\n                else:\n                    param = [\"user_label\"]  # Default label if none provided\n            if not isinstance(param, list):\n                raise TypeError(\"Param needs to be converted to a list\")\n            for p in param:\n                if p in clin_data.columns:\n                    labels = clin_data.loc[df_latent.index, p].tolist()  # ty: ignore\n\n                if n_downsample is not None:\n                    if df_latent.shape[0] &gt; n_downsample:\n                        sample_idx = np.random.choice(\n                            df_latent.shape[0], n_downsample, replace=False\n                        )\n                        df_latent = df_latent.iloc[sample_idx]\n                        if labels is not None:\n                            labels = [labels[i] for i in sample_idx]\n\n                if plot_type == \"2D-scatter\":\n                    ## Make 2D Embedding with UMAP\n                    if df_latent.shape[1] &gt; 2:\n                        reducer = UMAP(n_components=2)\n                        embedding = pd.DataFrame(reducer.fit_transform(df_latent))\n                    else:\n                        embedding = df_latent\n\n                    self.plots[\"2D-scatter\"][epoch][split][p] = self._plot_2D(\n                        embedding=embedding,\n                        labels=labels,\n                        param=p,\n                        layer=f\"2D latent space (epoch {epoch+1})\",  # we start counting epochs at 0, so add 1 for display\n                        figsize=(12, 8),\n                        center=True,\n                    )\n\n                    fig = self.plots[\"2D-scatter\"][epoch][split][p]\n                    show_figure(fig)\n                    plt.show()\n\n                if plot_type == \"Ridgeline\":\n                    ## Make ridgeline plot\n\n                    self.plots[\"Ridgeline\"][epoch][split][p] = self._plot_latent_ridge(\n                        lat_space=df_latent, labels=labels, param=p\n                    )\n\n                    fig = self.plots[\"Ridgeline\"][epoch][split][p].figure\n                    show_figure(fig)\n                    plt.show()\n\n                if plot_type == \"Clustermap\":\n                    ## Make clustermap plot\n\n                    self.plots[\"Clustermap\"][epoch][split][p] = (\n                        self._plot_latent_clustermap(\n                            lat_space=df_latent, labels=labels, param=p\n                        )\n                    )\n\n                    fig = self.plots[\"Clustermap\"][epoch][split][p]\n                    show_figure(fig)\n                    plt.show()\n\n    def show_weights(self) -&gt; None:\n        \"\"\"Display the model weights plot if it exists in the plots dictionary.\"\"\"\n\n        if \"ModelWeights\" not in self.plots.keys():\n            print(\"Model weights not found in the plots dictionary\")\n            print(\"You need to run visualize() method first\")\n        else:\n            fig = self.plots[\"ModelWeights\"]\n            show_figure(fig)\n            plt.show()\n\n    ### Moved to Base\n    # def show_evaluation(\n    #     self,\n    #     param: str,\n    #     metric: str,\n    #     ml_alg: Optional[str] = None,\n    # ) -&gt; None:\n\n    ### Utilities ###\n    @staticmethod\n    def _plot_2D(\n        embedding: pd.DataFrame,\n        labels: list,\n        param: Optional[Union[str, None]] = None,\n        layer: str = \"latent space\",\n        figsize: tuple = (24, 15),\n        center: bool = True,\n        plot_numeric: bool = False,\n        xlim: Optional[Union[tuple, None]] = None,\n        ylim: Optional[Union[tuple, None]] = None,\n        scale: Optional[Union[str, None]] = None,\n        no_leg: bool = False,\n    ) -&gt; matplotlib.figure.Figure:\n        \"\"\"Plots a 2D scatter plot of the given embedding with labels.\n\n        Args:\n            embedding: DataFrame containing the 2D embedding coordinates.\n            labels: List of labels corresponding to each point in the embedding.\n            param: Title for the legend. Defaults to None.\n            layer: Title for the plot. Defaults to \"latent space\".\n            figsize: Size of the figure. Defaults to (24, 15).\n            center: If True, centers the plot based on label means. Defaults to True.\n            plot_numeric: If True, treats labels as numeric. Defaults to False.\n            xlim: Limits for the x-axis. Defaults to None.\n            ylim: Limits for the y-axis. Defaults to None.\n            scale:: Scale for the axes (e.g., 'log'). Defaults to None.\n            no_leg: If True, no legend is displayed. Defaults to False.\n\n        Returns:\n            The resulting matplotlib figure.\n        \"\"\"\n\n        numeric = False\n        if not isinstance(labels[0], str):\n            if len(np.unique(labels)) &gt; 3:\n                if not plot_numeric:\n                    print(\n                        \"The provided label column is numeric and converted to categories.\"\n                    )\n                    labels = [\n                        float(\"nan\") if not isinstance(x, float) else x for x in labels\n                    ]\n                    labels = (\n                        pd.qcut(\n                            x=pd.Series(labels),\n                            q=4,\n                            labels=[\"1stQ\", \"2ndQ\", \"3rdQ\", \"4thQ\"],\n                        )\n                        .astype(str)\n                        .to_list()\n                    )\n                else:\n                    center = False  ## Disable centering for numeric params\n                    numeric = True\n            else:\n                labels = [str(x) for x in labels]\n\n        fig, ax1 = plt.subplots(figsize=figsize)\n\n        # check if label or embedding is longerm and duplicate the shorter one\n        if len(labels) &lt; embedding.shape[0]:\n            print(\n                \"Given labels do not have the same length as given sample size. Labels will be duplicated.\"\n            )\n            labels = [\n                label\n                for label in labels\n                for _ in range(embedding.shape[0] // len(labels))\n            ]\n        elif len(labels) &gt; embedding.shape[0]:\n            labels = list(set(labels))\n\n        if numeric:\n            ax2 = sns.scatterplot(\n                x=embedding.iloc[:, 0],\n                y=embedding.iloc[:, 1],\n                hue=labels,\n                palette=\"bwr\",\n                s=40,\n                alpha=0.5,\n                ec=\"black\",\n            )\n        else:\n            if len(np.unique(labels)) &gt; 8:\n                cat_pal = sns.color_palette(\"tab20\", n_colors=len(np.unique(labels)))\n            else:\n                cat_pal = sns.color_palette(\"tab10\", n_colors=len(np.unique(labels)))\n            ax2 = sns.scatterplot(\n                x=embedding.iloc[:, 0],\n                y=embedding.iloc[:, 1],\n                hue=labels,\n                hue_order=np.unique(labels),\n                palette=cat_pal,\n                s=40,\n                alpha=0.5,\n                ec=\"black\",\n            )\n        if center:\n            means = embedding.groupby(by=labels).mean()\n\n            ax2 = sns.scatterplot(\n                x=means.iloc[:, 0],\n                y=means.iloc[:, 1],\n                hue=np.unique(labels),\n                hue_order=np.unique(labels),\n                palette=cat_pal,\n                s=200,\n                ec=\"black\",\n                alpha=0.9,\n                marker=\"*\",\n                legend=False,\n                ax=ax2,\n            )\n\n        if xlim is not None:\n            ax2.set_xlim(xlim[0], xlim[1])\n\n        if ylim is not None:\n            ax2.set_ylim(ylim[0], ylim[1])\n\n        if scale is not None:\n            plt.yscale(scale)\n            plt.xscale(scale)\n        ax2.set_xlabel(\"Dim 1\")\n        ax2.set_ylabel(\"Dim 2\")\n        legend_cols = 1\n        if len(np.unique(labels)) &gt; 10:\n            legend_cols = 2\n\n        if no_leg:\n            plt.legend([], [], frameon=False)\n        else:\n            sns.move_legend(\n                ax2,\n                \"upper left\",\n                bbox_to_anchor=(1, 1),\n                ncol=legend_cols,\n                title=param,\n                frameon=False,\n            )\n\n        # Add title to the plot\n        ax2.set_title(layer)\n\n        plt.close()\n        return fig\n\n    @staticmethod\n    def _plot_latent_clustermap(\n        lat_space: pd.DataFrame,\n        labels: Optional[Union[list, pd.Series, None]] = None,\n        param: Optional[Union[str, None]] = None,\n    ) -&gt; matplotlib.figure.Figure:\n        \"\"\"Creates a clustermap of the latent space dimension where each row shows the intensity of a latent dimension and columns are clustered.\n\n        Args:\n            lat_space: DataFrame containing the latent space intensities for samples (rows) and latent dimensions (columns)\n            labels: List of labels for each sample. If None, all samples are considered as one group.\n            param: Clinical parameter to create groupings and coloring of ridges. Must be a column name (str) of clin_data\n        Returns:\n            fig: Figure object containing the clustermap\n        \"\"\"\n        lat_space[param] = labels\n\n        cluster_figure = sns.clustermap(\n            lat_space.groupby(param).mean(),\n            col_cluster=False,\n            row_cluster=True,\n            figsize=(1 * lat_space.shape[1], 4 + 0.5 * len(set(labels))),\n            dendrogram_ratio=0.1,\n            cmap=\"icefire\",\n            cbar_kws={\"orientation\": \"horizontal\"},\n            cbar_pos=(0.2, 0.95, 0.3, 0.02),\n        ).fig\n\n        plt.close()\n        lat_space.drop(columns=[param], inplace=True)\n        return cluster_figure\n\n    @staticmethod\n    def _plot_latent_ridge(\n        lat_space: pd.DataFrame,\n        labels: Optional[Union[list, pd.Series, None]] = None,\n        param: Optional[Union[str, None]] = None,\n    ) -&gt; sns.FacetGrid:\n        \"\"\"Creates a ridge line plot of latent space dimension where each row shows the density of a latent dimension and groups (ridges).\n\n        Args:\n            lat_space: DataFrame containing the latent space intensities for samples (rows) and latent dimensions (columns)\n            labels: List of labels for each sample. If None, all samples are considered as one group.\n            param: Clinical parameter to create groupings and coloring of ridges. Must be a column name (str) of clin_data\n        Returns:\n            g: FacetGrid object containing the ridge line plot\n        \"\"\"\n        sns.set_theme(\n            style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)}\n        )  ## Necessary to enforce overplotting\n\n        df = pd.melt(lat_space, var_name=\"latent dim\", value_name=\"latent intensity\")\n        df[\"sample\"] = len(lat_space.columns) * list(lat_space.index)\n\n        if labels is None:\n            param = \"all\"\n            labels = [\"all\"] * len(df)\n\n        # print(labels[0])\n        if not isinstance(labels[0], str):\n            if len(np.unique(labels)) &gt; 3:\n                # Change all non-float labels to NaN\n                labels = [x if isinstance(x, float) else float(\"nan\") for x in labels]\n                labels = list(\n                    pd.qcut(\n                        x=pd.Series(labels),\n                        q=4,\n                        labels=[\"1stQ\", \"2ndQ\", \"3rdQ\", \"4thQ\"],\n                    ).astype(str)\n                )\n            else:\n                labels = [str(x) for x in labels]\n\n        df[param] = len(lat_space.columns) * labels  # type: ignore\n\n        exclude_missing_info = (df[param] == \"unknown\") | (df[param] == \"nan\")\n\n        xmin = (\n            df.loc[~exclude_missing_info, [\"latent intensity\", \"latent dim\", param]]\n            .groupby([param, \"latent dim\"], observed=False)\n            .quantile(0.05)\n            .min()\n        )\n        xmax = (\n            df.loc[~exclude_missing_info, [\"latent intensity\", \"latent dim\", param]]\n            .groupby([param, \"latent dim\"], observed=False)\n            .quantile(0.9)\n            .max()\n        )\n\n        # if len(np.unique(df[param])) &gt; 8:\n        #     cat_pal = sns.husl_palette(len(np.unique(df[param])))\n        # else:\n        #     cat_pal = sns.color_palette(n_colors=len(np.unique(df[param])))\n\n        if len(np.unique(labels)) &gt; 8:\n            cat_pal = sns.color_palette(\"tab20\", n_colors=len(labels))\n        else:\n            cat_pal = sns.color_palette(\"tab10\", n_colors=len(labels))\n\n        g = sns.FacetGrid(\n            df[~exclude_missing_info],\n            row=\"latent dim\",\n            hue=param,\n            aspect=12,\n            height=0.8,\n            xlim=(xmin.iloc[0], xmax.iloc[0]),\n            palette=cat_pal,\n        )\n\n        g.map_dataframe(\n            sns.kdeplot,\n            \"latent intensity\",\n            bw_adjust=0.5,\n            clip_on=True,\n            fill=True,\n            alpha=0.5,\n            warn_singular=False,\n            ec=\"k\",\n            lw=1,\n        )\n\n        def label(data, color, label, text=\"latent dim\"):\n            ax = plt.gca()\n            label_text = data[text].unique()[0]\n            ax.text(\n                0.0,\n                0.2,\n                label_text,\n                fontweight=\"bold\",\n                ha=\"right\",\n                va=\"center\",\n                transform=ax.transAxes,\n            )\n\n        g.map_dataframe(label, text=\"latent dim\")\n\n        g.set(xlim=(xmin.iloc[0], xmax.iloc[0]))\n        # Set the subplots to overlap\n        g.figure.subplots_adjust(hspace=-0.5)\n\n        # Remove axes details that don't play well with overlap\n        g.set_titles(\"\")\n        g.set(yticks=[], ylabel=\"\")\n        g.despine(bottom=True, left=True)\n\n        g.add_legend()\n\n        plt.close()\n        return g\n\n    def _plot_evaluation(\n        self,\n        result: Result,\n    ) -&gt; dict:\n        \"\"\"Plots the evaluation results from the Result object.\n\n        Args:\n            result: The Result object containing evaluation data.\n\n        Returns:\n            The generated dictionary containing the evaluation plots.\n        \"\"\"\n        ## Plot all results\n\n        ml_plots = dict()\n        plt.ioff()\n        if not hasattr(result.embedding_evaluation, \"CLINIC_PARAM\"):\n            warnings.warn(\n                \"We could not create visualizations for the evaluation plots.\\n\"\n                \"This usually happens if you try to visualize after saving and loading \"\n                \"the pipeline object with `save_all=False`. This memory-efficient saving mode \"\n                \"Set save_all=True to avoid this, also this might be fixed soon.\"\n            )\n            return {}\n\n        for c in pd.unique(result.embedding_evaluation.CLINIC_PARAM):\n            ml_plots[c] = dict()\n            for m in pd.unique(  # ty: ignore\n                result.embedding_evaluation.loc[\n                    result.embedding_evaluation.CLINIC_PARAM == c, \"metric\"\n                ]\n            ):\n                ml_plots[c][m] = dict()\n                for alg in pd.unique(  # ty: ignore\n                    result.embedding_evaluation.loc[\n                        (result.embedding_evaluation.CLINIC_PARAM == c)\n                        &amp; (result.embedding_evaluation.metric == m),\n                        \"ML_ALG\",\n                    ]\n                ):\n                    data = result.embedding_evaluation[\n                        (result.embedding_evaluation.metric == m)\n                        &amp; (result.embedding_evaluation.CLINIC_PARAM == c)\n                        &amp; (result.embedding_evaluation.ML_ALG == alg)\n                    ]\n\n                    # Check for missing values\n                    if data[\"value\"].isnull().any():\n                        warnings.warn(\n                            f\"Missing values found in evaluation data for parameter '{c}', metric '{m}', and algorithm '{alg}'. These will be ignored in the plot.\"\n                        )\n                        data = data.dropna()\n\n                    sns_plot = sns.catplot(\n                        data=data,\n                        x=\"score_split\",\n                        y=\"value\",\n                        col=\"ML_TASK\",\n                        hue=\"score_split\",\n                        kind=\"bar\",\n                    )\n\n                    min_y = data.value.min()\n                    if min_y &gt; 0:\n                        min_y = 0\n\n                    ml_plots[c][m][alg] = sns_plot.set(ylim=(min_y, None))\n\n        self.plots[\"ML_Evaluation\"] = ml_plots\n\n        return ml_plots\n\n    @staticmethod\n    def _total_correlation(latent_space: pd.DataFrame) -&gt; float:\n        \"\"\"Function to compute the total correlation as described here (Equation2): https://doi.org/10.3390/e21100921\n\n        Args:\n            latent_space: latent space with dimension sample vs. latent dimensions\n        Returns:\n            tc: total correlation across latent dimensions\n        \"\"\"\n        lat_cov = np.cov(latent_space.T)\n        tc = 0.5 * (np.sum(np.log(np.diag(lat_cov))) - np.linalg.slogdet(lat_cov)[1])\n        return tc\n\n    @staticmethod\n    def _coverage_calc(latent_space: pd.DataFrame) -&gt; float:\n        \"\"\"Function to compute the coverage as described here (Equation3): https://doi.org/10.3390/e21100921\n\n        Args:\n            latent_space: latent space with dimension sample vs. latent dimensions\n        Returns:\n            cov: coverage across latent dimensions\n        \"\"\"\n        bins_per_dim = int(\n            np.power(len(latent_space.index), 1 / len(latent_space.columns))\n        )\n        if bins_per_dim &lt; 2:\n            warnings.warn(\n                \"Coverage calculation fails since combination of sample size and latent dimension results in less than 2 bins.\"\n            )\n            cov = np.nan\n        else:\n            latent_bins = latent_space.apply(lambda x: pd.cut(x, bins=bins_per_dim))\n            latent_bins = pd.Series(zip(*[latent_bins[col] for col in latent_bins]))\n            cov = len(latent_bins.unique()) / np.power(\n                bins_per_dim, len(latent_space.columns)\n            )\n\n        return cov\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.GeneralVisualizer.show_latent_space","title":"<code>show_latent_space(result, plot_type='2D-scatter', labels=None, param=None, epoch=None, split='all', n_downsample=10000, **kwargs)</code>","text":"<p>Visualizes the latent space of the given result using different types of plots.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Result</code> <p>The result object containing latent spaces and losses.</p> required <code>plot_type</code> <code>Literal['2D-scatter', 'Ridgeline', 'Coverage-Correlation']</code> <p>The type of plot to generate. Options are \"2D-scatter\", \"Ridgeline\", and \"Coverage-Correlation\". Default is \"2D-scatter\".</p> <code>'2D-scatter'</code> <code>labels</code> <code>Optional[Union[list, Series, None]]</code> <p>List of labels for the data points in the latent space. Default is None.</p> <code>None</code> <code>param</code> <code>Optional[Union[list, str]]</code> <p>List of parameters provided and stored as metadata. Strings must match column names. If not a list, string \"all\" is expected for convenient way to make plots for all parameters available. Default is None where no colored labels are plotted.</p> <code>None</code> <code>epoch</code> <code>Optional[Union[int, None]]</code> <p>The epoch number to visualize. If None, the last epoch is inferred from the losses. Default is None.</p> <code>None</code> <code>split</code> <code>str</code> <p>The data split to visualize. Options are \"train\", \"valid\", \"test\", and \"all\". Default is \"all\".</p> <code>'all'</code> <code>n_downsample</code> <code>Optional[int]</code> <p>If provided, downsample the data to this number of samples for faster visualization. Default is 10000. Set to None to disable downsampling.</p> <code>10000</code> <code>**kwargs</code> <p>additional arguments.</p> <code>{}</code> Source code in <code>src/autoencodix/visualize/_general_visualizer.py</code> <pre><code>@no_type_check\ndef show_latent_space(\n    self,\n    result: Result,\n    plot_type: Literal[\n        \"2D-scatter\", \"Ridgeline\", \"Coverage-Correlation\"\n    ] = \"2D-scatter\",\n    labels: Optional[Union[list, pd.Series, None]] = None,\n    param: Optional[Union[list, str]] = None,\n    epoch: Optional[Union[int, None]] = None,\n    split: str = \"all\",\n    n_downsample: Optional[int] = 10000,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Visualizes the latent space of the given result using different types of plots.\n\n    Args:\n        result: The result object containing latent spaces and losses.\n        plot_type: The type of plot to generate. Options are \"2D-scatter\", \"Ridgeline\", and \"Coverage-Correlation\". Default is \"2D-scatter\".\n        labels: List of labels for the data points in the latent space. Default is None.\n        param: List of parameters provided and stored as metadata. Strings must match column names. If not a list, string \"all\" is expected for convenient way to make plots for all parameters available. Default is None where no colored labels are plotted.\n        epoch: The epoch number to visualize. If None, the last epoch is inferred from the losses. Default is None.\n        split: The data split to visualize. Options are \"train\", \"valid\", \"test\", and \"all\". Default is \"all\".\n        n_downsample: If provided, downsample the data to this number of samples for faster visualization. Default is 10000. Set to None to disable downsampling.\n        **kwargs: additional arguments.\n\n    \"\"\"\n    plt.ioff()\n    if plot_type == \"Coverage-Correlation\":\n        if \"Coverage-Correlation\" in self.plots:\n            fig = self.plots[\"Coverage-Correlation\"]\n            show_figure(fig)\n            plt.show()\n        else:\n            results = []\n            for epoch in range(\n                result.model.config.checkpoint_interval,\n                result.model.config.epochs + 1,\n                result.model.config.checkpoint_interval,\n            ):\n                for split in [\"train\", \"valid\"]:\n                    latent_df = result.get_latent_df(epoch=epoch - 1, split=split)\n                    tc = self._total_correlation(latent_df)\n                    cov = self._coverage_calc(latent_df)\n                    results.append(\n                        {\n                            \"epoch\": epoch,\n                            \"split\": split,\n                            \"total_correlation\": tc,\n                            \"coverage\": cov,\n                        }\n                    )\n\n            df_metrics = pd.DataFrame(results)\n\n            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n            # Total Correlation plot\n            _ = sns.lineplot(\n                data=df_metrics,\n                x=\"epoch\",\n                y=\"total_correlation\",\n                hue=\"split\",\n                ax=axes[0],\n            )\n            axes[0].set_title(\"Total Correlation\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"Total Correlation\")\n\n            # Coverage plot\n            _ = sns.lineplot(\n                data=df_metrics, x=\"epoch\", y=\"coverage\", hue=\"split\", ax=axes[1]\n            )\n            axes[1].set_title(\"Coverage\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"Coverage\")\n\n            plt.tight_layout()\n            self.plots[\"Coverage-Correlation\"] = fig\n            show_figure(fig)\n            plt.show()\n\n    else:\n        # Set Defaults\n        if epoch is None:\n            epoch = result.model.config.epochs - 1\n\n        # ## Getting clin_data\n        clin_data = self._collect_all_metadata(result=result)\n        # if hasattr(result.datasets.train, \"metadata\"):\n        #     # Check if metadata is a dictionary and contains 'paired'\n        #     if isinstance(result.datasets.train.metadata, dict):\n        #         if \"paired\" in result.datasets.train.metadata:\n        #             clin_data = result.datasets.train.metadata[\"paired\"]\n        #             if hasattr(result.datasets, \"test\"):\n        #                 clin_data = pd.concat(\n        #                     [\n        #                         clin_data,\n        #                         result.datasets.test.metadata[  # ty: ignore\n        #                             \"paired\"\n        #                         ],  # ty: ignore\n        #                     ],  # ty: ignore\n        #                     axis=0,\n        #                 )\n        #             if hasattr(result.datasets, \"valid\"):\n        #                 clin_data = pd.concat(\n        #                     [\n        #                         clin_data,\n        #                         result.datasets.valid.metadata[  # ty: ignore\n        #                             \"paired\"\n        #                         ],  # ty: ignore\n        #                     ],  # ty: ignore\n        #                     axis=0,\n        #                 )\n        #         else:\n        #             # Iterate over all splits and keys, concatenate if DataFrame\n        #             clin_data = pd.DataFrame()\n        #             for split_name in [\"train\", \"test\", \"valid\"]:\n        #                 split_temp = getattr(result.datasets, split_name, None)\n        #                 if split_temp is not None and hasattr(\n        #                     split_temp, \"metadata\"\n        #                 ):\n        #                     for key in split_temp.metadata.keys():\n        #                         if isinstance(\n        #                             split_temp.metadata[key], pd.DataFrame\n        #                         ):\n        #                             clin_data = pd.concat(\n        #                                 [\n        #                                     clin_data,\n        #                                     split_temp.metadata[key],\n        #                                 ],\n        #                                 axis=0,\n        #                             )\n        #             # remove duplicate rows\n        #             clin_data = clin_data[~clin_data.index.duplicated(keep=\"first\")]\n        #             # if clin_data.empty:\n        #             #     # Raise error no annotation given\n        #             #     raise ValueError(\n        #             #         \"Please provide paired annotation data with key 'paired' in metadata dictionary.\"\n        #             #     )\n        #     elif isinstance(result.datasets.train.metadata, pd.DataFrame):\n        #         clin_data = result.datasets.train.metadata\n        #         if hasattr(result.datasets, \"test\"):\n        #             clin_data = pd.concat(\n        #                 [clin_data, result.datasets.test.metadata],  # ty: ignore\n        #                 axis=0,\n        #             )\n        #         if hasattr(result.datasets, \"valid\"):\n        #             clin_data = pd.concat(\n        #                 [clin_data, result.datasets.valid.metadata],  # ty: ignore\n        #                 axis=0,\n        #             )\n        #     else:\n        #         # Raise error no annotation given\n        #         raise ValueError(\n        #             \"Metadata is not a dictionary or DataFrame. Please provide a valid annotation data type.\"\n        #         )\n        # else:\n        #     # Iterate over all splits and keys, concatenate if DataFrame\n        #     clin_data = pd.DataFrame()\n        #     for split_name in [\"train\", \"test\", \"valid\"]:\n        #         split_temp = getattr(result.datasets, split_name, None)\n        #         if split_temp is not None:\n        #             for key in split_temp.datasets.keys():\n        #                 if isinstance(\n        #                     split_temp.datasets[key].metadata, pd.DataFrame\n        #                 ):\n        #                     clin_data = pd.concat(\n        #                         [\n        #                             clin_data,\n        #                             split_temp.datasets[key].metadata,\n        #                         ],\n        #                         axis=0,\n        #                     )\n        #     if len(clin_data) == 0: ## New predict case\n        #         for split_name in [\"train\", \"test\", \"valid\"]:\n        #             split_temp = getattr(result.new_datasets, split_name, None)\n        #             if split_temp is not None:\n        #                 if len(split_temp.datasets.keys()) &gt; 0:\n        #                     for key in split_temp.datasets.keys():\n        #                         if isinstance(\n        #                             split_temp.datasets[key].metadata, pd.DataFrame\n        #                         ):\n        #                             clin_data = pd.concat(\n        #                                 [\n        #                                     clin_data,\n        #                                     split_temp.datasets[key].metadata,\n        #                                 ],\n        #                                 axis=0,\n        #                             )\n        #                 else:\n        #                     if isinstance(\n        #                         split_temp.metadata, pd.DataFrame\n        #                     ):\n        #                         clin_data = pd.concat(\n        #                             [\n        #                                 clin_data,\n        #                                 split_temp.metadata,\n        #                             ],\n        #                             axis=0,\n        #                         )\n        #     # remove duplicate rows\n        #     clin_data = clin_data[~clin_data.index.duplicated(keep=\"first\")]\n\n        # # Raise error no annotation given\n        # raise ValueError(\n        #     \"No annotation data found. Please provide a valid annotation data type.\"\n        # )\n\n        if split == \"all\":\n            df_latent = pd.concat(\n                [\n                    result.get_latent_df(epoch=epoch, split=\"train\"),\n                    result.get_latent_df(epoch=epoch, split=\"valid\"),\n                    result.get_latent_df(epoch=-1, split=\"test\"),\n                ]\n            )\n        else:\n            if split == \"test\":\n                df_latent = result.get_latent_df(epoch=-1, split=split)\n            else:\n                df_latent = result.get_latent_df(epoch=epoch, split=split)\n\n        ## Label options\n        if labels is None and param is None:\n            labels = [\"all\"] * df_latent.shape[0]\n\n        if labels is None and isinstance(param, str):\n            if param == \"all\":\n                param = list(clin_data.columns)\n            else:\n                raise ValueError(\n                    \"Please provide parameter to plot as a list not as string. If you want to plot all parameters, set param to 'all' and labels to None.\"\n                )\n\n        if labels is not None and param is not None:\n            raise ValueError(\n                \"Please provide either labels or param, not both. If you want to plot all parameters, set param to 'all' and labels to None.\"\n            )\n\n        if labels is not None and param is None:\n            if isinstance(labels, pd.Series):\n                param = [labels.name]\n                # Order by index of df_latent first, fill missing with \"unknown\"\n                labels = labels.reindex(\n                    df_latent.index, fill_value=\"unknown\"\n                ).tolist()\n            else:\n                param = [\"user_label\"]  # Default label if none provided\n        if not isinstance(param, list):\n            raise TypeError(\"Param needs to be converted to a list\")\n        for p in param:\n            if p in clin_data.columns:\n                labels = clin_data.loc[df_latent.index, p].tolist()  # ty: ignore\n\n            if n_downsample is not None:\n                if df_latent.shape[0] &gt; n_downsample:\n                    sample_idx = np.random.choice(\n                        df_latent.shape[0], n_downsample, replace=False\n                    )\n                    df_latent = df_latent.iloc[sample_idx]\n                    if labels is not None:\n                        labels = [labels[i] for i in sample_idx]\n\n            if plot_type == \"2D-scatter\":\n                ## Make 2D Embedding with UMAP\n                if df_latent.shape[1] &gt; 2:\n                    reducer = UMAP(n_components=2)\n                    embedding = pd.DataFrame(reducer.fit_transform(df_latent))\n                else:\n                    embedding = df_latent\n\n                self.plots[\"2D-scatter\"][epoch][split][p] = self._plot_2D(\n                    embedding=embedding,\n                    labels=labels,\n                    param=p,\n                    layer=f\"2D latent space (epoch {epoch+1})\",  # we start counting epochs at 0, so add 1 for display\n                    figsize=(12, 8),\n                    center=True,\n                )\n\n                fig = self.plots[\"2D-scatter\"][epoch][split][p]\n                show_figure(fig)\n                plt.show()\n\n            if plot_type == \"Ridgeline\":\n                ## Make ridgeline plot\n\n                self.plots[\"Ridgeline\"][epoch][split][p] = self._plot_latent_ridge(\n                    lat_space=df_latent, labels=labels, param=p\n                )\n\n                fig = self.plots[\"Ridgeline\"][epoch][split][p].figure\n                show_figure(fig)\n                plt.show()\n\n            if plot_type == \"Clustermap\":\n                ## Make clustermap plot\n\n                self.plots[\"Clustermap\"][epoch][split][p] = (\n                    self._plot_latent_clustermap(\n                        lat_space=df_latent, labels=labels, param=p\n                    )\n                )\n\n                fig = self.plots[\"Clustermap\"][epoch][split][p]\n                show_figure(fig)\n                plt.show()\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.GeneralVisualizer.show_weights","title":"<code>show_weights()</code>","text":"<p>Display the model weights plot if it exists in the plots dictionary.</p> Source code in <code>src/autoencodix/visualize/_general_visualizer.py</code> <pre><code>def show_weights(self) -&gt; None:\n    \"\"\"Display the model weights plot if it exists in the plots dictionary.\"\"\"\n\n    if \"ModelWeights\" not in self.plots.keys():\n        print(\"Model weights not found in the plots dictionary\")\n        print(\"You need to run visualize() method first\")\n    else:\n        fig = self.plots[\"ModelWeights\"]\n        show_figure(fig)\n        plt.show()\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer","title":"<code>Visualizer</code>","text":"<p>               Bases: <code>BaseVisualizer</code></p> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>class Visualizer(BaseVisualizer):\n    plots: Dict[str, Any] = field(\n        default_factory=nested_dict\n    )  ## Nested dictionary of plots as figure handles\n\n    def __init__(self):\n        self.plots = nested_dict()\n\n    def __setitem__(self, key, elem):\n        self.plots[key] = elem\n\n    def visualize(self, result: Result, config: DefaultConfig) -&gt; Result:\n        ## Make Model Weights plot\n        self.plots[\"ModelWeights\"] = self.plot_model_weights(model=result.model)\n\n        ## Make long format of losses\n        loss_df_melt = self.make_loss_format(result=result, config=config)\n\n        ## Make plot loss absolute\n        self.plots[\"loss_absolute\"] = self.make_loss_plot(\n            df_plot=loss_df_melt, plot_type=\"absolute\"\n        )\n        ## Make plot loss relative\n        self.plots[\"loss_relative\"] = self.make_loss_plot(\n            df_plot=loss_df_melt, plot_type=\"relative\"\n        )\n\n        return result\n\n    ## Plotting methods ##\n\n    def save_plots(\n        self, path: str, which: Union[str, list] = \"all\", format: str = \"png\"\n    ) -&gt; None:\n        \"\"\"Save specified plots to the given path in the specified format.\n\n        Args:\n            path: The directory path where the plots will be saved.\n            which: A list of plot names to save or a string specifying which plots to save.\n                                If 'all', all plots in the plots dictionary will be saved.\n                                If a single plot name is provided as a string, only that plot will be saved.\n            format: The file format in which to save the plots (e.g., 'png', 'jpg').\n\n        Raises:\n            ValueError: If the 'which' parameter is not a list or a string.\n        \"\"\"\n        if not isinstance(which, list):\n            ## Case when which is a string\n            if which == \"all\":\n                ## Case when all plots are to be saved\n                if len(self.plots) == 0:\n                    print(\"No plots found in the plots dictionary\")\n                    print(\"You need to run  visualize() method first\")\n                else:\n                    for item in nested_to_tuple(self.plots):\n                        fig = item[-1]  ## Figure is in last element of the tuple\n                        filename = \"_\".join(str(x) for x in item[0:-1])\n                        fullpath = os.path.join(path, filename)\n                        fig.savefig(f\"{fullpath}.{format}\")\n            else:\n                ## Case when a single plot is provided as string\n                if which not in self.plots.keys():\n                    print(f\"Plot {which} not found in the plots dictionary\")\n                    print(f\"All available plots are: {list(self.plots.keys())}\")\n                else:\n                    for item in nested_to_tuple(\n                        self.plots[which]\n                    ):  # Plot all epochs and splits of type which\n                        fig = item[-1]  ## Figure is in last element of the tuple\n                        filename = (\n                            which  # ty: ignore\n                            + \"_\"\n                            + \"_\".join(str(x) for x in item[0:-1])\n                        )\n                        fullpath = os.path.join(path, filename)\n                        fig.savefig(f\"{fullpath}.{format}\")\n        else:\n            ## Case when which is a list of plot specified as strings\n            for key in which:\n                if key not in self.plots.keys():\n                    print(f\"Plot {key} not found in the plots dictionary\")\n                    print(f\"All available plots are: {list(self.plots.keys())}\")\n                    continue\n                else:\n                    for item in nested_to_tuple(\n                        self.plots[key]\n                    ):  # Plot all epochs and splits of type key\n                        fig = item[-1]  ## Figure is in last element of the tuple\n                        filename = key + \"_\" + \"_\".join(str(x) for x in item[0:-1])\n                        fullpath = os.path.join(path, filename)\n                        fig.savefig(f\"{fullpath}.{format}\")\n\n    def show_loss(\n        self, plot_type: Literal[\"absolute\", \"relative\"] = \"absolute\"\n    ) -&gt; None:\n        \"\"\"Display the loss plot.\n\n        Args:\n            plot_type: The type of loss plot to display. Defaults to \"absolute\".\n        \"\"\"\n        if plot_type == \"absolute\":\n            if \"loss_absolute\" not in self.plots.keys():\n                print(\"Absolute loss plot not found in the plots dictionary\")\n                print(\"You need to run visualize() method first\")\n            else:\n                fig = self.plots[\"loss_absolute\"]\n                show_figure(fig)\n                plt.show()\n        if plot_type == \"relative\":\n            if \"loss_relative\" not in self.plots.keys():\n                print(\"Relative loss plot not found in the plots dictionary\")\n                print(\"You need to run visualize() method first\")\n            else:\n                fig = self.plots[\"loss_relative\"]\n                show_figure(fig)\n                plt.show()\n\n        if plot_type not in [\"absolute\", \"relative\"]:\n            print(\n                \"Type of loss plot not recognized. Please use 'absolute' or 'relative'\"\n            )\n\n    @no_type_check\n    def show_latent_space(\n        self,\n        result: Result,\n        plot_type: str = \"2D-scatter\",\n        labels: Optional[Union[list, pd.Series, None]] = None,\n        param: Optional[Union[list, str]] = None,\n        epoch: Optional[Union[int, None]] = None,\n        split: str = \"all\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Visualizes the latent space of the given result using different types of plots.\n\n        Args:\n            result: The result object containing latent spaces and losses.\n            plot_type The type of plot to generate. Options are \"2D-scatter\", \"Ridgeline\", and \"Coverage-Correlation\". Default is \"2D-scatter\".\n            labels: List of labels for the data points in the latent space. Default is None.\n            param : List of parameters provided and stored as metadata. Strings must match column names. If not a list, string \"all\" is expected for convenient way to make plots for all parameters available. Default is None where no colored labels are plotted.\n            epoch: The epoch number to visualize. If None, the last epoch is inferred from the losses. Default is None.\n            split: The data split to visualize. Options are \"train\", \"valid\", \"test\", and \"all\". Default is \"all\".\n\n        \"\"\"\n        plt.ioff()\n        if plot_type == \"Coverage-Correlation\":\n            if \"Coverage-Correlation\" in self.plots:\n                fig = self.plots[\"Coverage-Correlation\"]\n                show_figure(fig)\n                plt.show()\n            else:\n                results = []\n                for epoch in range(\n                    result.model.config.checkpoint_interval,\n                    result.model.config.epochs + 1,\n                    result.model.config.checkpoint_interval,\n                ):\n                    for split in [\"train\", \"valid\"]:\n                        latent_df = result.get_latent_df(epoch=epoch - 1, split=split)\n                        tc = self._total_correlation(latent_df)\n                        cov = self._coverage_calc(latent_df)\n                        results.append(\n                            {\n                                \"epoch\": epoch,\n                                \"split\": split,\n                                \"total_correlation\": tc,\n                                \"coverage\": cov,\n                            }\n                        )\n\n                df_metrics = pd.DataFrame(results)\n\n                fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n                # Total Correlation plot\n                _ = sns.lineplot(\n                    data=df_metrics,\n                    x=\"epoch\",\n                    y=\"total_correlation\",\n                    hue=\"split\",\n                    ax=axes[0],\n                )\n                axes[0].set_title(\"Total Correlation\")\n                axes[0].set_xlabel(\"Epoch\")\n                axes[0].set_ylabel(\"Total Correlation\")\n\n                # Coverage plot\n                _ = sns.lineplot(\n                    data=df_metrics, x=\"epoch\", y=\"coverage\", hue=\"split\", ax=axes[1]\n                )\n                axes[1].set_title(\"Coverage\")\n                axes[1].set_xlabel(\"Epoch\")\n                axes[1].set_ylabel(\"Coverage\")\n\n                plt.tight_layout()\n                self.plots[\"Coverage-Correlation\"] = fig\n                show_figure(fig)\n                plt.show()\n\n        else:\n            # Set Defaults\n            if epoch is None:\n                epoch = result.model.config.epochs - 1\n\n            ## Getting clin_data\n            if not hasattr(result.datasets, \"train\"):\n                raise ValueError(\"no train split in datasets\")\n\n            if not hasattr(result.datasets, \"valid\"):\n                raise ValueError(\"no valid split in datasets\")\n            if result.datasets.train is None:\n                raise ValueError(\"train is None\")\n            if result.datasets.valid is None:\n                raise ValueError(\"train is None\")\n            if result.datasets.test is None:\n                raise ValueError(\"train is None\")\n\n            if not hasattr(result.datasets.train, \"metadata\"):\n                raise ValueError(\"train dataset has no metadata\")\n            if not hasattr(result.datasets.valid, \"metadata\"):\n                raise ValueError(\"valid dataset has no metadata\")\n\n            # Check if metadata is a dictionary and contains 'paired'\n            if isinstance(result.datasets.train.metadata, dict):\n                if \"paired\" in result.datasets.train.metadata:\n                    clin_data = result.datasets.train.metadata[\"paired\"]\n                    if hasattr(result.datasets, \"test\"):\n                        clin_data = pd.concat(\n                            [clin_data, result.datasets.test.metadata[\"paired\"]],\n                            axis=0,\n                        )\n                    if hasattr(result.datasets, \"valid\"):\n                        clin_data = pd.concat(\n                            [clin_data, result.datasets.valid.metadata[\"paired\"]],\n                            axis=0,\n                        )\n                    else:\n                        # Raise error no annotation given\n                        raise ValueError(\n                            \"Please provide paired annotation data with key 'paired' in metadata dictionary.\"\n                        )\n                elif isinstance(result.datasets.train.metadata, pd.DataFrame):\n                    clin_data = result.datasets.train.metadata\n                    if hasattr(result.datasets, \"test\"):\n                        clin_data = pd.concat(\n                            [clin_data, result.datasets.test.metadata],\n                            axis=0,\n                        )\n                    if hasattr(result.datasets, \"valid\"):\n                        clin_data = pd.concat(\n                            [clin_data, result.datasets.valid.metadata],\n                            axis=0,\n                        )\n                else:\n                    # Raise error no annotation given\n                    raise ValueError(\n                        \"Metadata is not a dictionary or DataFrame. Please provide a valid annotation data type.\"\n                    )\n            else:\n                # Raise error no annotation given\n                raise ValueError(\n                    \"No annotation data found. Please provide a valid annotation data type.\"\n                )\n\n            if split == \"all\":\n                df_latent = pd.concat(\n                    [\n                        result.get_latent_df(epoch=epoch, split=\"train\"),\n                        result.get_latent_df(epoch=epoch, split=\"valid\"),\n                        result.get_latent_df(epoch=-1, split=\"test\"),\n                    ]\n                )\n            else:\n                if split == \"test\":\n                    df_latent = result.get_latent_df(epoch=-1, split=split)\n                else:\n                    df_latent = result.get_latent_df(epoch=epoch, split=split)\n\n            if labels is None and param is None:\n                labels = [\"all\"] * df_latent.shape[0]\n\n            if labels is None and isinstance(param, str):\n                if param == \"all\":\n                    param = list(clin_data.columns)\n                else:\n                    raise ValueError(\n                        \"Please provide parameter to plot as a list not as string. If you want to plot all parameters, set param to 'all' and labels to None.\"\n                    )\n\n            if labels is not None and param is not None:\n                raise ValueError(\n                    \"Please provide either labels or param, not both. If you want to plot all parameters, set param to 'all' and labels to None.\"\n                )\n\n            if labels is not None and param is None:\n                if isinstance(labels, pd.Series):\n                    param = [labels.name]\n                    # Order by index of df_latent first, fill missing with \"unknown\"\n                    labels = labels.reindex(\n                        df_latent.index, fill_value=\"unknown\"\n                    ).tolist()\n                else:\n                    param = [\"user_label\"]  # Default label if none provided\n\n            for p in param:\n                if p in clin_data.columns:\n                    labels = clin_data.loc[df_latent.index, p].tolist()\n\n                if plot_type == \"2D-scatter\":\n                    ## Make 2D Embedding with UMAP\n                    if df_latent.shape[1] &gt; 2:\n                        reducer = UMAP(n_components=2)\n                        embedding = pd.DataFrame(reducer.fit_transform(df_latent))\n                    else:\n                        embedding = df_latent\n\n                    self.plots[\"2D-scatter\"][epoch][split][p] = self.plot_2D(\n                        embedding=embedding,\n                        labels=labels,\n                        param=p,\n                        layer=f\"2D latent space (epoch {epoch + 1})\",  # we start counting epochs at 0, so add 1 for display\n                        figsize=(12, 8),\n                        center=True,\n                    )\n\n                    fig = self.plots[\"2D-scatter\"][epoch][split][p]\n                    show_figure(fig)\n                    plt.show()\n\n                if plot_type == \"Ridgeline\":\n                    ## Make ridgeline plot\n\n                    self.plots[\"Ridgeline\"][epoch][split][p] = self.plot_latent_ridge(\n                        lat_space=df_latent, labels=labels, param=p\n                    )\n\n                    fig = self.plots[\"Ridgeline\"][epoch][split][p].figure\n                    show_figure(fig)\n                    plt.show()\n\n    def show_weights(self) -&gt; None:\n        \"\"\"Display the model weights plot if it exists in the plots dictionary.\"\"\"\n\n        if \"ModelWeights\" not in self.plots.keys():\n            print(\"Model weights not found in the plots dictionary\")\n            print(\"You need to run visualize() method first\")\n        else:\n            fig = self.plots[\"ModelWeights\"]\n            show_figure(fig)\n            plt.show()\n\n    # def plot_model_weights(model: torch.nn.Module) -&gt; matplotlib.figure.Figure:\n    #     \"\"\"\n    #     Visualization of model weights in encoder and decoder layers as heatmap for each layer as subplot.\n    #     ARGS:\n    #         model (torch.nn.Module): PyTorch model instance.\n    #         filepath (str): Path specifying save name and location.\n    #     RETURNS:\n    #         fig (matplotlib.figure): Figure handle (of last plot)\n    #     \"\"\"\n    #     all_weights = []\n    #     names = []\n    #     if hasattr(model, \"ontologies\"):\n    #         if model.ontologies is not None:\n    #             # If model is Ontix\n    #             # Get node names from ontologies\n    #             node_names = list()\n    #             for ontology in model.ontologies:\n    #                 node_names.append(ontology.keys())\n\n    #             node_names.append(model.feature_order)  # Add feature order as last layer\n\n    #     for name, param in model.named_parameters():\n    #         if \"weight\" in name and len(param.shape) == 2:\n    #             if \"var\" not in name:  ## For VAE plot only mu weights\n    #                 all_weights.append(param.detach().cpu().numpy())\n    #                 names.append(name[:-7])\n\n    #     layers = int(len(all_weights) / 2)\n    #     fig, axes = plt.subplots(2, layers, sharex=False, figsize=(20, 10))\n\n    #     for layer in range(layers):\n    #         ## Encoder Layer\n    #         if layers &gt; 1:\n    #             sns.heatmap(\n    #                 all_weights[layer],\n    #                 cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n    #                 ax=axes[0, layer],\n    #             ).set(title=names[layer])\n    #             ## Decoder Layer\n    #             sns.heatmap(\n    #                 all_weights[layers + layer],\n    #                 cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n    #                 ax=axes[1, layer],\n    #             ).set(title=names[layers + layer])\n    #             axes[1, layer].set_xlabel(\"In Node\", size=12)\n    #             if model.ontologies is not None:\n    #                 axes[1, layer].set_xticks(\n    #                     ticks=range(len(node_names[layer])),\n    #                     labels=node_names[layer],\n    #                     rotation=90,\n    #                     fontsize=8,\n    #                 )\n    #                 axes[1, layer].set_yticks(\n    #                     ticks=range(len(node_names[layer + 1])),\n    #                     labels=node_names[layer + 1],\n    #                     rotation=0,\n    #                     fontsize=8,\n    #                 )\n    #         else:\n    #             sns.heatmap(\n    #                 all_weights[layer],\n    #                 cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n    #                 ax=axes[layer],\n    #             ).set(title=names[layer])\n    #             ## Decoder Layer\n    #             sns.heatmap(\n    #                 all_weights[layer + 2],\n    #                 cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n    #                 ax=axes[layer + 1],\n    #             ).set(title=names[layer + 2])\n    #             axes[1].set_xlabel(\"In Node\", size=12)\n\n    #     if layers &gt; 1:\n    #         axes[1, 0].set_ylabel(\"Out Node\", size=12)\n    #         axes[0, 0].set_ylabel(\"Out Node\", size=12)\n    #     else:\n    #         axes[1].set_ylabel(\"Out Node\", size=12)\n    #         axes[0].set_ylabel(\"Out Node\", size=12)\n\n    #     ## Add title\n    #     fig.suptitle(\"Model Weights\", size=20)\n    #     plt.close()\n    #     return fig\n\n    ## NEW VERSION\n    # @staticmethod\n    # def plot_model_weights(model: torch.nn.Module) -&gt; matplotlib.figure.Figure:\n    #     \"\"\"\n    #     Visualization of model weights in encoder and decoder layers as heatmap for each layer as subplot.\n    #     ARGS:\n    #         model (torch.nn.Module): PyTorch model instance.\n    #         filepath (str): Path specifying save name and location.\n    #     RETURNS:\n    #         fig (matplotlib.figure): Figure handle (of last plot)\n    #     \"\"\"\n    #     all_weights = []\n    #     names = []\n    #     if hasattr(model, \"ontologies\"):\n    #         if model.ontologies is not None:\n    #             # If model is Ontix\n    #             # Get node names from ontologies\n    #             node_names = list()\n    #             for ontology in model.ontologies:\n    #                 node_names.append(ontology.keys())\n\n    #             node_names.append(model.feature_order)  # Add feature order as last layer\n\n    #     for name, param in model.named_parameters():\n    #         if \"weight\" in name and len(param.shape) == 2:\n    #             if \"var\" not in name:  ## For VAE plot only mu weights\n    #                 all_weights.append(param.detach().cpu().numpy())\n    #                 names.append(name[:-7])\n\n    #     layers = int(len(all_weights) / 2)\n    #     fig, axes = plt.subplots(2, layers, sharex=False, figsize=(20, 10))\n\n    #     for layer in range(layers):\n    #         ## Encoder Layer\n    #         if layers &gt; 1:\n    #             sns.heatmap(\n    #                 all_weights[layer],\n    #                 cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n    #                 ax=axes[0, layer],\n    #             ).set(title=names[layer])\n    #             ## Decoder Layer\n    #             sns.heatmap(\n    #                 all_weights[layers + layer],\n    #                 cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n    #                 ax=axes[1, layer],\n    #             ).set(title=names[layers + layer])\n    #             axes[1, layer].set_xlabel(\"In Node\", size=12)\n    #             if model.ontologies is not None:\n    #                 axes[1, layer].set_xticks(\n    #                     ticks=range(len(node_names[layer])),\n    #                     labels=node_names[layer],\n    #                     rotation=90,\n    #                     fontsize=8,\n    #                 )\n    #                 axes[1, layer].set_yticks(\n    #                     ticks=range(len(node_names[layer + 1])),\n    #                     labels=node_names[layer + 1],\n    #                     rotation=0,\n    #                     fontsize=8,\n    #                 )\n    #         else:\n    #             sns.heatmap(\n    #                 all_weights[layer],\n    #                 cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n    #                 ax=axes[layer],\n    #             ).set(title=names[layer])\n    #             ## Decoder Layer\n    #             sns.heatmap(\n    #                 all_weights[layer + 2],\n    #                 cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n    #                 ax=axes[layer + 1],\n    #             ).set(title=names[layer + 2])\n    #             axes[1].set_xlabel(\"In Node\", size=12)\n\n    #     if layers &gt; 1:\n    #         axes[1, 0].set_ylabel(\"Out Node\", size=12)\n    #         axes[0, 0].set_ylabel(\"Out Node\", size=12)\n    #     else:\n    #         axes[1].set_ylabel(\"Out Node\", size=12)\n    #         axes[0].set_ylabel(\"Out Node\", size=12)\n\n    #     ## Add title\n    #     fig.suptitle(\"Model Weights\", size=20)\n    #     plt.close()\n    #     return fig\n\n    ## NEW VERSION\n    def plot_model_weights(model: torch.nn.Module) -&gt; matplotlib.figure.Figure:\n        \"\"\"Visualization of model weights in encoder and decoder layers as heatmap for each layer as subplot.\n\n        Handles non-symmetrical autoencoder architectures.\n        Plots _mu layer for encoder as well.\n        Uses node_names for decoder layers if model has ontologies.\n\n        Args:\n            model: PyTorch model instance.\n        Returns:\n            fig: Figure handle (of last plot)\n        \"\"\"\n        all_weights = []\n        names = []\n        node_names = []\n        if hasattr(model, \"ontologies\"):\n            if model.ontologies is not None:\n                node_names = []\n                for ontology in model.ontologies:\n                    node_names.append(list(ontology.keys()))\n                node_names.append(model.feature_order)\n\n        # Collect encoder and decoder weights separately\n        encoder_weights = []\n        encoder_names = []\n        decoder_weights = []\n        decoder_names = []\n        for name, param in model.named_parameters():\n            # print(name)\n            if \"weight\" in name and len(param.shape) == 2:\n                if \"encoder\" in name and \"var\" not in name and \"_mu\" not in name:\n                    encoder_weights.append(param.detach().cpu().numpy())\n                    encoder_names.append(name[:-7])\n                elif \"_mu\" in name:\n                    encoder_weights.append(param.detach().cpu().numpy())\n                    encoder_names.append(name[:-7])\n                elif \"decoder\" in name and \"var\" not in name:\n                    decoder_weights.append(param.detach().cpu().numpy())\n                    decoder_names.append(name[:-7])\n                elif (\n                    \"encoder\" not in name\n                    and \"decoder\" not in name\n                    and \"var\" not in name\n                ):\n                    # fallback for models without explicit encoder/decoder in name\n                    all_weights.append(param.detach().cpu().numpy())\n                    names.append(name[:-7])\n\n        if encoder_weights or decoder_weights:\n            n_enc = len(encoder_weights)\n            n_dec = len(decoder_weights)\n            n_cols = max(n_enc, n_dec)\n            fig, axes = plt.subplots(2, n_cols, sharex=False, figsize=(15 * n_cols, 15))\n            if n_cols == 1:\n                axes = axes.reshape(2, 1)\n            # Plot encoder weights\n            for i in range(n_enc):\n                ax = axes[0, i]\n                sns.heatmap(\n                    encoder_weights[i],\n                    cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                    center=0,\n                    ax=ax,\n                ).set(title=encoder_names[i])\n                ax.set_ylabel(\"Out Node\", size=12)\n            # Hide unused encoder subplots\n            for i in range(n_enc, n_cols):\n                axes[0, i].axis(\"off\")\n            # Plot decoder weights\n            for i in range(n_dec):\n                ax = axes[1, i]\n                heatmap_kwargs = {}\n\n                sns.heatmap(\n                    decoder_weights[i],\n                    cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                    center=0,\n                    ax=ax,\n                    **heatmap_kwargs,\n                ).set(title=decoder_names[i])\n                if model.ontologies is not None:\n                    axes[1, i].set_xticks(\n                        ticks=range(len(node_names[i])),\n                        labels=node_names[i],\n                        rotation=90,\n                        fontsize=8,\n                    )\n                    axes[1, i].set_yticks(\n                        ticks=range(len(node_names[i + 1])),\n                        labels=node_names[i + 1],\n                        rotation=0,\n                        fontsize=8,\n                    )\n                ax.set_xlabel(\"In Node\", size=12)\n                ax.set_ylabel(\"Out Node\", size=12)\n            # Hide unused decoder subplots\n            for i in range(n_dec, n_cols):\n                axes[1, i].axis(\"off\")\n        else:\n            # fallback: plot all weights in order, split in half for encoder/decoder\n            n_layers = len(all_weights) // 2\n            fig, axes = plt.subplots(\n                2, n_layers, sharex=False, figsize=(5 * n_layers, 10)\n            )\n            for layer in range(n_layers):\n                sns.heatmap(\n                    all_weights[layer],\n                    cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                    center=0,\n                    ax=axes[0, layer],\n                ).set(title=names[layer])\n                sns.heatmap(\n                    all_weights[n_layers + layer],\n                    cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                    center=0,\n                    ax=axes[1, layer],\n                ).set(title=names[n_layers + layer])\n                axes[1, layer].set_xlabel(\"In Node\", size=12)\n                axes[0, layer].set_ylabel(\"Out Node\", size=12)\n                axes[1, layer].set_ylabel(\"Out Node\", size=12)\n\n        fig.suptitle(\"Model Weights\", size=20)\n        plt.close()\n        return fig\n\n    @staticmethod\n    def plot_2D(\n        embedding: pd.DataFrame,\n        labels: list,\n        param: Optional[Union[str, None]] = None,\n        layer: str = \"latent space\",\n        figsize: tuple = (24, 15),\n        center: bool = True,\n        plot_numeric: bool = False,\n        xlim: Optional[Union[tuple, None]] = None,\n        ylim: Optional[Union[tuple, None]] = None,\n        scale: Optional[Union[str, None]] = None,\n        no_leg: bool = False,\n    ) -&gt; matplotlib.figure.Figure:\n        \"\"\"Plots a 2D scatter plot of the given embedding with labels.\n\n        Args:\n            embedding: DataFrame containing the 2D embedding coordinates.\n            labels: List of labels corresponding to each point in the embedding.\n            param: Title for the legend. Defaults to None.\n            layer: Title for the plot. Defaults to \"latent space\".\n            figsize: Size of the figure. Defaults to (24, 15).\n            center: If True, centers the plot based on label means. Defaults to True.\n            plot_numeric Defaults to False.\n            xlim: Defaults to None.\n            ylim: Defaults to None.\n            scale: Defaults to None.\n            no_leg: Defaults to False.\n\n        Returns:\n            The resulting matplotlib figure.\n        \"\"\"\n\n        numeric = False\n        if not isinstance(labels[0], str):\n            if len(np.unique(labels)) &gt; 3:\n                if not plot_numeric:\n                    print(\n                        \"The provided label column is numeric and converted to categories.\"\n                    )\n                    # Change non-float labels to NaN\n                    labels = [\n                        x if isinstance(x, float) else float(\"nan\") for x in labels\n                    ]\n                    labels = (\n                        pd.qcut(\n                            x=pd.Series(labels),\n                            q=4,\n                            labels=[\"1stQ\", \"2ndQ\", \"3rdQ\", \"4thQ\"],\n                        )\n                        .astype(str)\n                        .to_list()\n                    )\n                else:\n                    center = False  ## Disable centering for numeric params\n                    numeric = True\n            else:\n                labels = [str(x) for x in labels]\n\n        fig, ax1 = plt.subplots(figsize=figsize)\n\n        # check if label or embedding is longerm and duplicate the shorter one\n        if len(labels) &lt; embedding.shape[0]:\n            print(\n                \"Given labels do not have the same length as given sample size. Labels will be duplicated.\"\n            )\n            labels = [\n                label\n                for label in labels\n                for _ in range(embedding.shape[0] // len(labels))\n            ]\n        elif len(labels) &gt; embedding.shape[0]:\n            labels = list(set(labels))\n\n        if numeric:\n            ax2 = sns.scatterplot(\n                x=embedding.iloc[:, 0],\n                y=embedding.iloc[:, 1],\n                hue=labels,\n                palette=\"bwr\",\n                s=40,\n                alpha=0.5,\n                ec=\"black\",\n            )\n        else:\n            ax2 = sns.scatterplot(\n                x=embedding.iloc[:, 0],\n                y=embedding.iloc[:, 1],\n                hue=labels,\n                hue_order=np.unique(labels),\n                s=40,\n                alpha=0.5,\n                ec=\"black\",\n            )\n        if center:\n            means = embedding.groupby(by=labels).mean()\n\n            ax2 = sns.scatterplot(\n                x=means.iloc[:, 0],\n                y=means.iloc[:, 1],\n                hue=np.unique(labels),\n                hue_order=np.unique(labels),\n                s=200,\n                ec=\"black\",\n                alpha=0.9,\n                marker=\"*\",\n                legend=False,\n                ax=ax2,\n            )\n\n        if xlim is not None:\n            ax2.set_xlim(xlim[0], xlim[1])\n\n        if ylim is not None:\n            ax2.set_ylim(ylim[0], ylim[1])\n\n        if scale is not None:\n            plt.yscale(scale)\n            plt.xscale(scale)\n        ax2.set_xlabel(\"Dim 1\")\n        ax2.set_ylabel(\"Dim 2\")\n        legend_cols = 1\n        if len(np.unique(labels)) &gt; 10:\n            legend_cols = 2\n\n        if no_leg:\n            plt.legend([], [], frameon=False)\n        else:\n            sns.move_legend(\n                ax2,\n                \"upper left\",\n                bbox_to_anchor=(1, 1),\n                ncol=legend_cols,\n                title=param,\n                frameon=False,\n            )\n\n        # Add title to the plot\n        ax2.set_title(layer)\n\n        plt.close()\n        return fig\n\n    @staticmethod\n    def plot_latent_ridge(\n        lat_space: pd.DataFrame,\n        labels: Optional[Union[list, pd.Series, None]] = None,\n        param: Optional[Union[str, None]] = None,\n    ) -&gt; sns.FacetGrid:\n        \"\"\"Creates a ridge line plot of latent space dimension where each row shows the density of a latent dimension and groups (ridges).\n        Args:\n            lat_space: If None, all samples are considered as one group.\n            param: Must be a column name (str) of clin_data\n        Returns:\n            g: FacetGrid object containing the ridge line plot\n        \"\"\"\n        sns.set_theme(\n            style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)}\n        )  ## Necessary to enforce overplotting\n\n        df = pd.melt(lat_space, var_name=\"latent dim\", value_name=\"latent intensity\")\n        df[\"sample\"] = len(lat_space.columns) * list(lat_space.index)\n\n        if labels is None:\n            param = \"all\"\n            labels = [\"all\"] * len(df)\n\n        # print(labels[0])\n        if not isinstance(labels[0], str):\n            if len(np.unique(labels)) &gt; 3:\n                # Change non-float labels to NaN\n                labels = [x if isinstance(x, float) else float(\"nan\") for x in labels]\n                labels = pd.qcut(\n                    x=pd.Series(labels),\n                    q=4,\n                    labels=[\"1stQ\", \"2ndQ\", \"3rdQ\", \"4thQ\"],\n                ).astype(str)\n            else:\n                labels = [str(x) for x in labels]\n\n        df[param] = len(lat_space.columns) * labels  # type: ignore\n\n        exclude_missing_info = (df[param] == \"unknown\") | (df[param] == \"nan\")\n\n        xmin = (\n            df.loc[~exclude_missing_info, [\"latent intensity\", \"latent dim\", param]]\n            .groupby([param, \"latent dim\"], observed=False)\n            .quantile(0.05)\n            .min()\n        )\n        xmax = (\n            df.loc[~exclude_missing_info, [\"latent intensity\", \"latent dim\", param]]\n            .groupby([param, \"latent dim\"], observed=False)\n            .quantile(0.9)\n            .max()\n        )\n\n        if len(np.unique(df[param])) &gt; 8:\n            cat_pal = sns.husl_palette(len(np.unique(df[param])))\n        else:\n            cat_pal = sns.color_palette(n_colors=len(np.unique(df[param])))\n\n        g = sns.FacetGrid(\n            df[~exclude_missing_info],\n            row=\"latent dim\",\n            hue=param,\n            aspect=12,\n            height=0.8,\n            xlim=(xmin.iloc[0], xmax.iloc[0]),\n            palette=cat_pal,\n        )\n\n        g.map_dataframe(\n            sns.kdeplot,\n            \"latent intensity\",\n            bw_adjust=0.5,\n            clip_on=True,\n            fill=True,\n            alpha=0.5,\n            warn_singular=False,\n            ec=\"k\",\n            lw=1,\n        )\n\n        def label(data, color, label, text=\"latent dim\"):\n            ax = plt.gca()\n            label_text = data[text].unique()[0]\n            ax.text(\n                0.0,\n                0.2,\n                label_text,\n                fontweight=\"bold\",\n                ha=\"right\",\n                va=\"center\",\n                transform=ax.transAxes,\n            )\n\n        g.map_dataframe(label, text=\"latent dim\")\n\n        g.set(xlim=(xmin.iloc[0], xmax.iloc[0]))\n        # Set the subplots to overlap\n        g.figure.subplots_adjust(hspace=-0.5)\n\n        # Remove axes details that don't play well with overlap\n        g.set_titles(\"\")\n        g.set(yticks=[], ylabel=\"\")\n        g.despine(bottom=True, left=True)\n\n        g.add_legend()\n\n        plt.close()\n        return g\n\n    @staticmethod\n    def make_loss_plot(\n        df_plot: pd.DataFrame, plot_type: str\n    ) -&gt; matplotlib.figure.Figure:\n        \"\"\"Generates a plot for visualizing loss values from a DataFrame.\n\n        Args:\n            df_plot: DataFrame containing the loss values to be plotted. It should have the columns:\n                - \"Loss Term\": The type of loss term (e.g., \"total_loss\", \"reconstruction_loss\").\n                - \"Epoch\": The epoch number.\n                - \"Loss Value\": The value of the loss.\n                - \"Split\": The data split (e.g., \"train\", \"validation\").\n\n            plot_type: The type of plot to generate. It can be either \"absolute\" or \"relative\".\n                - \"absolute\": Generates a line plot for each unique loss term.\n                - \"relative\": Generates a density plot for each data split, excluding the \"total_loss\" term.\n\n        Returns:\n            The generated matplotlib figure containing the loss plots.\n        \"\"\"\n        fig_width_abs = 5 * len(df_plot[\"Loss Term\"].unique())\n        fig_width_rel = 5 * len(df_plot[\"Split\"].unique())\n        if plot_type == \"absolute\":\n            fig, axes = plt.subplots(\n                1,\n                len(df_plot[\"Loss Term\"].unique()),\n                figsize=(fig_width_abs, 5),\n                sharey=False,\n            )\n            ax = 0\n            for term in df_plot[\"Loss Term\"].unique():\n                axes[ax] = sns.lineplot(\n                    data=df_plot[(df_plot[\"Loss Term\"] == term)],\n                    x=\"Epoch\",\n                    y=\"Loss Value\",\n                    hue=\"Split\",\n                    ax=axes[ax],\n                ).set_title(term)\n                ax += 1\n\n            plt.close()\n\n        if plot_type == \"relative\":\n            # Check if loss values are positive\n            if (df_plot[\"Loss Value\"] &lt; 0).any():\n                # Warning\n                warnings.warn(\n                    \"Loss values contain negative values. Check your loss function if correct. Loss will be clipped to zero for plotting.\"\n                )\n                df_plot[\"Loss Value\"] = df_plot[\"Loss Value\"].clip(lower=0)\n\n            # Exclude loss terms where all Loss Value are zero or NaN over all epochs\n            valid_terms = [\n                term\n                for term in df_plot[\"Loss Term\"].unique()\n                if (\n                    (df_plot[df_plot[\"Loss Term\"] == term][\"Loss Value\"].notna().any())\n                    and (df_plot[df_plot[\"Loss Term\"] == term][\"Loss Value\"] != 0).any()\n                )\n            ]\n            exclude = (\n                (df_plot[\"Loss Term\"] != \"total_loss\")\n                &amp; ~(df_plot[\"Loss Term\"].str.contains(\"_factor\"))\n                &amp; (df_plot[\"Loss Term\"].isin(valid_terms))\n            )\n\n            fig, axes = plt.subplots(1, 2, figsize=(fig_width_rel, 5), sharey=True)\n\n            ax = 0\n\n            for split in df_plot[\"Split\"].unique():\n                axes[ax] = sns.kdeplot(\n                    data=df_plot[exclude &amp; (df_plot[\"Split\"] == split)],\n                    x=\"Epoch\",\n                    hue=\"Loss Term\",\n                    multiple=\"fill\",\n                    weights=\"Loss Value\",\n                    clip=[0, df_plot[\"Epoch\"].max()],\n                    ax=axes[ax],\n                ).set_title(split)\n                ax += 1\n\n            plt.close()\n\n        return fig\n\n    @staticmethod\n    def make_loss_format(result: Result, config: DefaultConfig) -&gt; pd.DataFrame:\n        loss_df_melt = pd.DataFrame()\n        for term in result.sub_losses.keys():\n            # Get the loss values and ensure it's a dictionary\n            loss_values = result.sub_losses.get(key=term).get()\n\n            # Add explicit type checking/conversion\n            if not isinstance(loss_values, dict):\n                # If it's not a dict, try to convert it or handle appropriately\n                if hasattr(loss_values, \"to_dict\"):\n                    loss_values = loss_values.to_dict()  # type: ignore\n                else:\n                    # For non-convertible types, you might need a custom solution\n                    # For numpy arrays, you could do something like:\n                    if hasattr(loss_values, \"shape\"):\n                        # For numpy arrays, create a dict with indices as keys\n                        loss_values = {i: val for i, val in enumerate(loss_values)}\n\n            # Now create the DataFrame\n            loss_df = pd.DataFrame.from_dict(loss_values, orient=\"index\")  # type: ignore\n\n            # Rest of your code remains the same\n            if term == \"var_loss\":\n                loss_df = loss_df * config.beta\n            loss_df[\"Epoch\"] = loss_df.index + 1\n            loss_df[\"Loss Term\"] = term\n\n            loss_df_melt = pd.concat(\n                [\n                    loss_df_melt,\n                    loss_df.melt(\n                        id_vars=[\"Epoch\", \"Loss Term\"],\n                        var_name=\"Split\",\n                        value_name=\"Loss Value\",\n                    ),\n                ],\n                axis=0,\n            ).reset_index(drop=True)\n\n        # Similar handling for the total losses\n        loss_values = result.losses.get()\n        if not isinstance(loss_values, dict):\n            if hasattr(loss_values, \"to_dict\"):\n                loss_values = loss_values.to_dict()  # ty: ignore\n            else:\n                if hasattr(loss_values, \"shape\"):\n                    loss_values = {i: val for i, val in enumerate(loss_values)}\n\n        loss_df = pd.DataFrame.from_dict(loss_values, orient=\"index\")  # type: ignore\n        loss_df[\"Epoch\"] = loss_df.index + 1\n        loss_df[\"Loss Term\"] = \"total_loss\"\n\n        loss_df_melt = pd.concat(\n            [\n                loss_df_melt,\n                loss_df.melt(\n                    id_vars=[\"Epoch\", \"Loss Term\"],\n                    var_name=\"Split\",\n                    value_name=\"Loss Value\",\n                ),\n            ],\n            axis=0,\n        ).reset_index(drop=True)\n\n        loss_df_melt[\"Loss Value\"] = loss_df_melt[\"Loss Value\"].astype(float)\n        return loss_df_melt\n\n    @no_type_check\n    def plot_evaluation(\n        self,\n        result: Result,\n    ) -&gt; dict:\n        \"\"\"Plots the evaluation results from the Result object.\n\n        Args:\n            result: The Result object containing evaluation data.\n\n        Returns:\n            The generated dictionary containing the evaluation plots.\n        \"\"\"\n        ## Plot all results\n\n        ml_plots = dict()\n        plt.ioff()\n\n        for c in pd.unique(result.embedding_evaluation.CLINIC_PARAM):\n            ml_plots[c] = dict()\n            for m in pd.unique(\n                result.embedding_evaluation.loc[\n                    result.embedding_evaluation.CLINIC_PARAM == c, \"metric\"\n                ]\n            ):\n                ml_plots[c][m] = dict()\n                for alg in pd.unique(\n                    result.embedding_evaluation.loc[\n                        (result.embedding_evaluation.CLINIC_PARAM == c)\n                        &amp; (result.embedding_evaluation.metric == m),\n                        \"ML_ALG\",\n                    ]\n                ):\n                    data = result.embedding_evaluation[\n                        (result.embedding_evaluation.metric == m)\n                        &amp; (result.embedding_evaluation.CLINIC_PARAM == c)\n                        &amp; (result.embedding_evaluation.ML_ALG == alg)\n                    ]\n\n                    sns_plot = sns.catplot(\n                        data=data,\n                        x=\"score_split\",\n                        y=\"value\",\n                        col=\"ML_TASK\",\n                        hue=\"score_split\",\n                        kind=\"bar\",\n                    )\n\n                    min_y = data.value.min()\n                    if min_y &gt; 0:\n                        min_y = 0\n\n                    ml_plots[c][m][alg] = sns_plot.set(ylim=(min_y, None))\n\n        self.plots[\"ML_Evaluation\"] = ml_plots\n\n        return ml_plots\n\n    def show_evaluation(\n        self,\n        param: str,\n        metric: str,\n        ml_alg: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Displays the evaluation plot for a specific clinical parameter, metric, and optionally ML algorithm.\n\n        Args:\n            param: The clinical parameter to visualize.\n            metric: The metric to visualize.\n            ml_alg: If None, plots all available algorithms.\n        \"\"\"\n        plt.ioff()\n        if \"ML_Evaluation\" not in self.plots.keys():\n            print(\"ML Evaluation plots not found in the plots dictionary\")\n            print(\"You need to run evaluate() method first\")\n            return None\n        if param not in self.plots[\"ML_Evaluation\"].keys():\n            print(f\"Parameter {param} not found in the ML Evaluation plots\")\n            print(f\"Available parameters: {list(self.plots['ML_Evaluation'].keys())}\")\n            return None\n        if metric not in self.plots[\"ML_Evaluation\"][param].keys():\n            print(f\"Metric {metric} not found in the ML Evaluation plots for {param}\")\n            print(\n                f\"Available metrics: {list(self.plots['ML_Evaluation'][param].keys())}\"\n            )\n            return None\n\n        algs = list(self.plots[\"ML_Evaluation\"][param][metric].keys())\n        if ml_alg is not None:\n            if ml_alg not in algs:\n                print(f\"ML algorithm {ml_alg} not found for {param} and {metric}\")\n                print(f\"Available ML algorithms: {algs}\")\n                return None\n            fig = self.plots[\"ML_Evaluation\"][param][metric][ml_alg].figure\n            show_figure(fig)\n            plt.show()\n        else:\n            for alg in algs:\n                print(f\"Showing plot for ML algorithm: {alg}\")\n                fig = self.plots[\"ML_Evaluation\"][param][metric][alg].figure\n                show_figure(fig)\n                plt.show()\n\n    @staticmethod\n    def _total_correlation(latent_space: pd.DataFrame) -&gt; float:\n        \"\"\"Function to compute the total correlation as described here (Equation2): https://doi.org/10.3390/e21100921\n\n        Args:\n            latent_space - (pd.DataFrame): latent space with dimension sample vs. latent dimensions\n        Returns:\n            tc - (float): total correlation across latent dimensions\n        \"\"\"\n        lat_cov = np.cov(latent_space.T)\n        tc = 0.5 * (np.sum(np.log(np.diag(lat_cov))) - np.linalg.slogdet(lat_cov)[1])\n        return tc\n\n    @staticmethod\n    def _coverage_calc(latent_space: pd.DataFrame) -&gt; float:\n        \"\"\"Function to compute the coverage as described here (Equation3): https://doi.org/10.3390/e21100921\n\n        Args:\n            latent_space: latent dimensions\n        Returns:\n            cov: coverage across latent dimensions\n        \"\"\"\n        bins_per_dim = int(\n            np.power(len(latent_space.index), 1 / len(latent_space.columns))\n        )\n        if bins_per_dim &lt; 2:\n            warnings.warn(\n                \"Coverage calculation fails since combination of sample size and latent dimension results in less than 2 bins.\"\n            )\n            cov = np.nan\n        else:\n            latent_bins = latent_space.apply(lambda x: pd.cut(x, bins=bins_per_dim))\n            latent_bins = pd.Series(zip(*[latent_bins[col] for col in latent_bins]))\n            cov = len(latent_bins.unique()) / np.power(\n                bins_per_dim, len(latent_space.columns)\n            )\n\n        return cov\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.make_loss_plot","title":"<code>make_loss_plot(df_plot, plot_type)</code>  <code>staticmethod</code>","text":"<p>Generates a plot for visualizing loss values from a DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df_plot</code> <code>DataFrame</code> <p>DataFrame containing the loss values to be plotted. It should have the columns: - \"Loss Term\": The type of loss term (e.g., \"total_loss\", \"reconstruction_loss\"). - \"Epoch\": The epoch number. - \"Loss Value\": The value of the loss. - \"Split\": The data split (e.g., \"train\", \"validation\").</p> required <code>plot_type</code> <code>str</code> <p>The type of plot to generate. It can be either \"absolute\" or \"relative\". - \"absolute\": Generates a line plot for each unique loss term. - \"relative\": Generates a density plot for each data split, excluding the \"total_loss\" term.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>The generated matplotlib figure containing the loss plots.</p> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>@staticmethod\ndef make_loss_plot(\n    df_plot: pd.DataFrame, plot_type: str\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Generates a plot for visualizing loss values from a DataFrame.\n\n    Args:\n        df_plot: DataFrame containing the loss values to be plotted. It should have the columns:\n            - \"Loss Term\": The type of loss term (e.g., \"total_loss\", \"reconstruction_loss\").\n            - \"Epoch\": The epoch number.\n            - \"Loss Value\": The value of the loss.\n            - \"Split\": The data split (e.g., \"train\", \"validation\").\n\n        plot_type: The type of plot to generate. It can be either \"absolute\" or \"relative\".\n            - \"absolute\": Generates a line plot for each unique loss term.\n            - \"relative\": Generates a density plot for each data split, excluding the \"total_loss\" term.\n\n    Returns:\n        The generated matplotlib figure containing the loss plots.\n    \"\"\"\n    fig_width_abs = 5 * len(df_plot[\"Loss Term\"].unique())\n    fig_width_rel = 5 * len(df_plot[\"Split\"].unique())\n    if plot_type == \"absolute\":\n        fig, axes = plt.subplots(\n            1,\n            len(df_plot[\"Loss Term\"].unique()),\n            figsize=(fig_width_abs, 5),\n            sharey=False,\n        )\n        ax = 0\n        for term in df_plot[\"Loss Term\"].unique():\n            axes[ax] = sns.lineplot(\n                data=df_plot[(df_plot[\"Loss Term\"] == term)],\n                x=\"Epoch\",\n                y=\"Loss Value\",\n                hue=\"Split\",\n                ax=axes[ax],\n            ).set_title(term)\n            ax += 1\n\n        plt.close()\n\n    if plot_type == \"relative\":\n        # Check if loss values are positive\n        if (df_plot[\"Loss Value\"] &lt; 0).any():\n            # Warning\n            warnings.warn(\n                \"Loss values contain negative values. Check your loss function if correct. Loss will be clipped to zero for plotting.\"\n            )\n            df_plot[\"Loss Value\"] = df_plot[\"Loss Value\"].clip(lower=0)\n\n        # Exclude loss terms where all Loss Value are zero or NaN over all epochs\n        valid_terms = [\n            term\n            for term in df_plot[\"Loss Term\"].unique()\n            if (\n                (df_plot[df_plot[\"Loss Term\"] == term][\"Loss Value\"].notna().any())\n                and (df_plot[df_plot[\"Loss Term\"] == term][\"Loss Value\"] != 0).any()\n            )\n        ]\n        exclude = (\n            (df_plot[\"Loss Term\"] != \"total_loss\")\n            &amp; ~(df_plot[\"Loss Term\"].str.contains(\"_factor\"))\n            &amp; (df_plot[\"Loss Term\"].isin(valid_terms))\n        )\n\n        fig, axes = plt.subplots(1, 2, figsize=(fig_width_rel, 5), sharey=True)\n\n        ax = 0\n\n        for split in df_plot[\"Split\"].unique():\n            axes[ax] = sns.kdeplot(\n                data=df_plot[exclude &amp; (df_plot[\"Split\"] == split)],\n                x=\"Epoch\",\n                hue=\"Loss Term\",\n                multiple=\"fill\",\n                weights=\"Loss Value\",\n                clip=[0, df_plot[\"Epoch\"].max()],\n                ax=axes[ax],\n            ).set_title(split)\n            ax += 1\n\n        plt.close()\n\n    return fig\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.plot_2D","title":"<code>plot_2D(embedding, labels, param=None, layer='latent space', figsize=(24, 15), center=True, plot_numeric=False, xlim=None, ylim=None, scale=None, no_leg=False)</code>  <code>staticmethod</code>","text":"<p>Plots a 2D scatter plot of the given embedding with labels.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>DataFrame</code> <p>DataFrame containing the 2D embedding coordinates.</p> required <code>labels</code> <code>list</code> <p>List of labels corresponding to each point in the embedding.</p> required <code>param</code> <code>Optional[Union[str, None]]</code> <p>Title for the legend. Defaults to None.</p> <code>None</code> <code>layer</code> <code>str</code> <p>Title for the plot. Defaults to \"latent space\".</p> <code>'latent space'</code> <code>figsize</code> <code>tuple</code> <p>Size of the figure. Defaults to (24, 15).</p> <code>(24, 15)</code> <code>center</code> <code>bool</code> <p>If True, centers the plot based on label means. Defaults to True.</p> <code>True</code> <code>xlim</code> <code>Optional[Union[tuple, None]]</code> <p>Defaults to None.</p> <code>None</code> <code>ylim</code> <code>Optional[Union[tuple, None]]</code> <p>Defaults to None.</p> <code>None</code> <code>scale</code> <code>Optional[Union[str, None]]</code> <p>Defaults to None.</p> <code>None</code> <code>no_leg</code> <code>bool</code> <p>Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Figure</code> <p>The resulting matplotlib figure.</p> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>@staticmethod\ndef plot_2D(\n    embedding: pd.DataFrame,\n    labels: list,\n    param: Optional[Union[str, None]] = None,\n    layer: str = \"latent space\",\n    figsize: tuple = (24, 15),\n    center: bool = True,\n    plot_numeric: bool = False,\n    xlim: Optional[Union[tuple, None]] = None,\n    ylim: Optional[Union[tuple, None]] = None,\n    scale: Optional[Union[str, None]] = None,\n    no_leg: bool = False,\n) -&gt; matplotlib.figure.Figure:\n    \"\"\"Plots a 2D scatter plot of the given embedding with labels.\n\n    Args:\n        embedding: DataFrame containing the 2D embedding coordinates.\n        labels: List of labels corresponding to each point in the embedding.\n        param: Title for the legend. Defaults to None.\n        layer: Title for the plot. Defaults to \"latent space\".\n        figsize: Size of the figure. Defaults to (24, 15).\n        center: If True, centers the plot based on label means. Defaults to True.\n        plot_numeric Defaults to False.\n        xlim: Defaults to None.\n        ylim: Defaults to None.\n        scale: Defaults to None.\n        no_leg: Defaults to False.\n\n    Returns:\n        The resulting matplotlib figure.\n    \"\"\"\n\n    numeric = False\n    if not isinstance(labels[0], str):\n        if len(np.unique(labels)) &gt; 3:\n            if not plot_numeric:\n                print(\n                    \"The provided label column is numeric and converted to categories.\"\n                )\n                # Change non-float labels to NaN\n                labels = [\n                    x if isinstance(x, float) else float(\"nan\") for x in labels\n                ]\n                labels = (\n                    pd.qcut(\n                        x=pd.Series(labels),\n                        q=4,\n                        labels=[\"1stQ\", \"2ndQ\", \"3rdQ\", \"4thQ\"],\n                    )\n                    .astype(str)\n                    .to_list()\n                )\n            else:\n                center = False  ## Disable centering for numeric params\n                numeric = True\n        else:\n            labels = [str(x) for x in labels]\n\n    fig, ax1 = plt.subplots(figsize=figsize)\n\n    # check if label or embedding is longerm and duplicate the shorter one\n    if len(labels) &lt; embedding.shape[0]:\n        print(\n            \"Given labels do not have the same length as given sample size. Labels will be duplicated.\"\n        )\n        labels = [\n            label\n            for label in labels\n            for _ in range(embedding.shape[0] // len(labels))\n        ]\n    elif len(labels) &gt; embedding.shape[0]:\n        labels = list(set(labels))\n\n    if numeric:\n        ax2 = sns.scatterplot(\n            x=embedding.iloc[:, 0],\n            y=embedding.iloc[:, 1],\n            hue=labels,\n            palette=\"bwr\",\n            s=40,\n            alpha=0.5,\n            ec=\"black\",\n        )\n    else:\n        ax2 = sns.scatterplot(\n            x=embedding.iloc[:, 0],\n            y=embedding.iloc[:, 1],\n            hue=labels,\n            hue_order=np.unique(labels),\n            s=40,\n            alpha=0.5,\n            ec=\"black\",\n        )\n    if center:\n        means = embedding.groupby(by=labels).mean()\n\n        ax2 = sns.scatterplot(\n            x=means.iloc[:, 0],\n            y=means.iloc[:, 1],\n            hue=np.unique(labels),\n            hue_order=np.unique(labels),\n            s=200,\n            ec=\"black\",\n            alpha=0.9,\n            marker=\"*\",\n            legend=False,\n            ax=ax2,\n        )\n\n    if xlim is not None:\n        ax2.set_xlim(xlim[0], xlim[1])\n\n    if ylim is not None:\n        ax2.set_ylim(ylim[0], ylim[1])\n\n    if scale is not None:\n        plt.yscale(scale)\n        plt.xscale(scale)\n    ax2.set_xlabel(\"Dim 1\")\n    ax2.set_ylabel(\"Dim 2\")\n    legend_cols = 1\n    if len(np.unique(labels)) &gt; 10:\n        legend_cols = 2\n\n    if no_leg:\n        plt.legend([], [], frameon=False)\n    else:\n        sns.move_legend(\n            ax2,\n            \"upper left\",\n            bbox_to_anchor=(1, 1),\n            ncol=legend_cols,\n            title=param,\n            frameon=False,\n        )\n\n    # Add title to the plot\n    ax2.set_title(layer)\n\n    plt.close()\n    return fig\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.plot_evaluation","title":"<code>plot_evaluation(result)</code>","text":"<p>Plots the evaluation results from the Result object.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Result</code> <p>The Result object containing evaluation data.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The generated dictionary containing the evaluation plots.</p> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>@no_type_check\ndef plot_evaluation(\n    self,\n    result: Result,\n) -&gt; dict:\n    \"\"\"Plots the evaluation results from the Result object.\n\n    Args:\n        result: The Result object containing evaluation data.\n\n    Returns:\n        The generated dictionary containing the evaluation plots.\n    \"\"\"\n    ## Plot all results\n\n    ml_plots = dict()\n    plt.ioff()\n\n    for c in pd.unique(result.embedding_evaluation.CLINIC_PARAM):\n        ml_plots[c] = dict()\n        for m in pd.unique(\n            result.embedding_evaluation.loc[\n                result.embedding_evaluation.CLINIC_PARAM == c, \"metric\"\n            ]\n        ):\n            ml_plots[c][m] = dict()\n            for alg in pd.unique(\n                result.embedding_evaluation.loc[\n                    (result.embedding_evaluation.CLINIC_PARAM == c)\n                    &amp; (result.embedding_evaluation.metric == m),\n                    \"ML_ALG\",\n                ]\n            ):\n                data = result.embedding_evaluation[\n                    (result.embedding_evaluation.metric == m)\n                    &amp; (result.embedding_evaluation.CLINIC_PARAM == c)\n                    &amp; (result.embedding_evaluation.ML_ALG == alg)\n                ]\n\n                sns_plot = sns.catplot(\n                    data=data,\n                    x=\"score_split\",\n                    y=\"value\",\n                    col=\"ML_TASK\",\n                    hue=\"score_split\",\n                    kind=\"bar\",\n                )\n\n                min_y = data.value.min()\n                if min_y &gt; 0:\n                    min_y = 0\n\n                ml_plots[c][m][alg] = sns_plot.set(ylim=(min_y, None))\n\n    self.plots[\"ML_Evaluation\"] = ml_plots\n\n    return ml_plots\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.plot_latent_ridge","title":"<code>plot_latent_ridge(lat_space, labels=None, param=None)</code>  <code>staticmethod</code>","text":"<p>Creates a ridge line plot of latent space dimension where each row shows the density of a latent dimension and groups (ridges). Args:     lat_space: If None, all samples are considered as one group.     param: Must be a column name (str) of clin_data Returns:     g: FacetGrid object containing the ridge line plot</p> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>@staticmethod\ndef plot_latent_ridge(\n    lat_space: pd.DataFrame,\n    labels: Optional[Union[list, pd.Series, None]] = None,\n    param: Optional[Union[str, None]] = None,\n) -&gt; sns.FacetGrid:\n    \"\"\"Creates a ridge line plot of latent space dimension where each row shows the density of a latent dimension and groups (ridges).\n    Args:\n        lat_space: If None, all samples are considered as one group.\n        param: Must be a column name (str) of clin_data\n    Returns:\n        g: FacetGrid object containing the ridge line plot\n    \"\"\"\n    sns.set_theme(\n        style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)}\n    )  ## Necessary to enforce overplotting\n\n    df = pd.melt(lat_space, var_name=\"latent dim\", value_name=\"latent intensity\")\n    df[\"sample\"] = len(lat_space.columns) * list(lat_space.index)\n\n    if labels is None:\n        param = \"all\"\n        labels = [\"all\"] * len(df)\n\n    # print(labels[0])\n    if not isinstance(labels[0], str):\n        if len(np.unique(labels)) &gt; 3:\n            # Change non-float labels to NaN\n            labels = [x if isinstance(x, float) else float(\"nan\") for x in labels]\n            labels = pd.qcut(\n                x=pd.Series(labels),\n                q=4,\n                labels=[\"1stQ\", \"2ndQ\", \"3rdQ\", \"4thQ\"],\n            ).astype(str)\n        else:\n            labels = [str(x) for x in labels]\n\n    df[param] = len(lat_space.columns) * labels  # type: ignore\n\n    exclude_missing_info = (df[param] == \"unknown\") | (df[param] == \"nan\")\n\n    xmin = (\n        df.loc[~exclude_missing_info, [\"latent intensity\", \"latent dim\", param]]\n        .groupby([param, \"latent dim\"], observed=False)\n        .quantile(0.05)\n        .min()\n    )\n    xmax = (\n        df.loc[~exclude_missing_info, [\"latent intensity\", \"latent dim\", param]]\n        .groupby([param, \"latent dim\"], observed=False)\n        .quantile(0.9)\n        .max()\n    )\n\n    if len(np.unique(df[param])) &gt; 8:\n        cat_pal = sns.husl_palette(len(np.unique(df[param])))\n    else:\n        cat_pal = sns.color_palette(n_colors=len(np.unique(df[param])))\n\n    g = sns.FacetGrid(\n        df[~exclude_missing_info],\n        row=\"latent dim\",\n        hue=param,\n        aspect=12,\n        height=0.8,\n        xlim=(xmin.iloc[0], xmax.iloc[0]),\n        palette=cat_pal,\n    )\n\n    g.map_dataframe(\n        sns.kdeplot,\n        \"latent intensity\",\n        bw_adjust=0.5,\n        clip_on=True,\n        fill=True,\n        alpha=0.5,\n        warn_singular=False,\n        ec=\"k\",\n        lw=1,\n    )\n\n    def label(data, color, label, text=\"latent dim\"):\n        ax = plt.gca()\n        label_text = data[text].unique()[0]\n        ax.text(\n            0.0,\n            0.2,\n            label_text,\n            fontweight=\"bold\",\n            ha=\"right\",\n            va=\"center\",\n            transform=ax.transAxes,\n        )\n\n    g.map_dataframe(label, text=\"latent dim\")\n\n    g.set(xlim=(xmin.iloc[0], xmax.iloc[0]))\n    # Set the subplots to overlap\n    g.figure.subplots_adjust(hspace=-0.5)\n\n    # Remove axes details that don't play well with overlap\n    g.set_titles(\"\")\n    g.set(yticks=[], ylabel=\"\")\n    g.despine(bottom=True, left=True)\n\n    g.add_legend()\n\n    plt.close()\n    return g\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.plot_model_weights","title":"<code>plot_model_weights(model)</code>","text":"<p>Visualization of model weights in encoder and decoder layers as heatmap for each layer as subplot.</p> <p>Handles non-symmetrical autoencoder architectures. Plots _mu layer for encoder as well. Uses node_names for decoder layers if model has ontologies.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Module</code> <p>PyTorch model instance.</p> required <p>Returns:     fig: Figure handle (of last plot)</p> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>def plot_model_weights(model: torch.nn.Module) -&gt; matplotlib.figure.Figure:\n    \"\"\"Visualization of model weights in encoder and decoder layers as heatmap for each layer as subplot.\n\n    Handles non-symmetrical autoencoder architectures.\n    Plots _mu layer for encoder as well.\n    Uses node_names for decoder layers if model has ontologies.\n\n    Args:\n        model: PyTorch model instance.\n    Returns:\n        fig: Figure handle (of last plot)\n    \"\"\"\n    all_weights = []\n    names = []\n    node_names = []\n    if hasattr(model, \"ontologies\"):\n        if model.ontologies is not None:\n            node_names = []\n            for ontology in model.ontologies:\n                node_names.append(list(ontology.keys()))\n            node_names.append(model.feature_order)\n\n    # Collect encoder and decoder weights separately\n    encoder_weights = []\n    encoder_names = []\n    decoder_weights = []\n    decoder_names = []\n    for name, param in model.named_parameters():\n        # print(name)\n        if \"weight\" in name and len(param.shape) == 2:\n            if \"encoder\" in name and \"var\" not in name and \"_mu\" not in name:\n                encoder_weights.append(param.detach().cpu().numpy())\n                encoder_names.append(name[:-7])\n            elif \"_mu\" in name:\n                encoder_weights.append(param.detach().cpu().numpy())\n                encoder_names.append(name[:-7])\n            elif \"decoder\" in name and \"var\" not in name:\n                decoder_weights.append(param.detach().cpu().numpy())\n                decoder_names.append(name[:-7])\n            elif (\n                \"encoder\" not in name\n                and \"decoder\" not in name\n                and \"var\" not in name\n            ):\n                # fallback for models without explicit encoder/decoder in name\n                all_weights.append(param.detach().cpu().numpy())\n                names.append(name[:-7])\n\n    if encoder_weights or decoder_weights:\n        n_enc = len(encoder_weights)\n        n_dec = len(decoder_weights)\n        n_cols = max(n_enc, n_dec)\n        fig, axes = plt.subplots(2, n_cols, sharex=False, figsize=(15 * n_cols, 15))\n        if n_cols == 1:\n            axes = axes.reshape(2, 1)\n        # Plot encoder weights\n        for i in range(n_enc):\n            ax = axes[0, i]\n            sns.heatmap(\n                encoder_weights[i],\n                cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                center=0,\n                ax=ax,\n            ).set(title=encoder_names[i])\n            ax.set_ylabel(\"Out Node\", size=12)\n        # Hide unused encoder subplots\n        for i in range(n_enc, n_cols):\n            axes[0, i].axis(\"off\")\n        # Plot decoder weights\n        for i in range(n_dec):\n            ax = axes[1, i]\n            heatmap_kwargs = {}\n\n            sns.heatmap(\n                decoder_weights[i],\n                cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                center=0,\n                ax=ax,\n                **heatmap_kwargs,\n            ).set(title=decoder_names[i])\n            if model.ontologies is not None:\n                axes[1, i].set_xticks(\n                    ticks=range(len(node_names[i])),\n                    labels=node_names[i],\n                    rotation=90,\n                    fontsize=8,\n                )\n                axes[1, i].set_yticks(\n                    ticks=range(len(node_names[i + 1])),\n                    labels=node_names[i + 1],\n                    rotation=0,\n                    fontsize=8,\n                )\n            ax.set_xlabel(\"In Node\", size=12)\n            ax.set_ylabel(\"Out Node\", size=12)\n        # Hide unused decoder subplots\n        for i in range(n_dec, n_cols):\n            axes[1, i].axis(\"off\")\n    else:\n        # fallback: plot all weights in order, split in half for encoder/decoder\n        n_layers = len(all_weights) // 2\n        fig, axes = plt.subplots(\n            2, n_layers, sharex=False, figsize=(5 * n_layers, 10)\n        )\n        for layer in range(n_layers):\n            sns.heatmap(\n                all_weights[layer],\n                cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                center=0,\n                ax=axes[0, layer],\n            ).set(title=names[layer])\n            sns.heatmap(\n                all_weights[n_layers + layer],\n                cmap=sns.color_palette(\"Spectral\", as_cmap=True),\n                center=0,\n                ax=axes[1, layer],\n            ).set(title=names[n_layers + layer])\n            axes[1, layer].set_xlabel(\"In Node\", size=12)\n            axes[0, layer].set_ylabel(\"Out Node\", size=12)\n            axes[1, layer].set_ylabel(\"Out Node\", size=12)\n\n    fig.suptitle(\"Model Weights\", size=20)\n    plt.close()\n    return fig\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.save_plots","title":"<code>save_plots(path, which='all', format='png')</code>","text":"<p>Save specified plots to the given path in the specified format.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The directory path where the plots will be saved.</p> required <code>which</code> <code>Union[str, list]</code> <p>A list of plot names to save or a string specifying which plots to save.                 If 'all', all plots in the plots dictionary will be saved.                 If a single plot name is provided as a string, only that plot will be saved.</p> <code>'all'</code> <code>format</code> <code>str</code> <p>The file format in which to save the plots (e.g., 'png', 'jpg').</p> <code>'png'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'which' parameter is not a list or a string.</p> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>def save_plots(\n    self, path: str, which: Union[str, list] = \"all\", format: str = \"png\"\n) -&gt; None:\n    \"\"\"Save specified plots to the given path in the specified format.\n\n    Args:\n        path: The directory path where the plots will be saved.\n        which: A list of plot names to save or a string specifying which plots to save.\n                            If 'all', all plots in the plots dictionary will be saved.\n                            If a single plot name is provided as a string, only that plot will be saved.\n        format: The file format in which to save the plots (e.g., 'png', 'jpg').\n\n    Raises:\n        ValueError: If the 'which' parameter is not a list or a string.\n    \"\"\"\n    if not isinstance(which, list):\n        ## Case when which is a string\n        if which == \"all\":\n            ## Case when all plots are to be saved\n            if len(self.plots) == 0:\n                print(\"No plots found in the plots dictionary\")\n                print(\"You need to run  visualize() method first\")\n            else:\n                for item in nested_to_tuple(self.plots):\n                    fig = item[-1]  ## Figure is in last element of the tuple\n                    filename = \"_\".join(str(x) for x in item[0:-1])\n                    fullpath = os.path.join(path, filename)\n                    fig.savefig(f\"{fullpath}.{format}\")\n        else:\n            ## Case when a single plot is provided as string\n            if which not in self.plots.keys():\n                print(f\"Plot {which} not found in the plots dictionary\")\n                print(f\"All available plots are: {list(self.plots.keys())}\")\n            else:\n                for item in nested_to_tuple(\n                    self.plots[which]\n                ):  # Plot all epochs and splits of type which\n                    fig = item[-1]  ## Figure is in last element of the tuple\n                    filename = (\n                        which  # ty: ignore\n                        + \"_\"\n                        + \"_\".join(str(x) for x in item[0:-1])\n                    )\n                    fullpath = os.path.join(path, filename)\n                    fig.savefig(f\"{fullpath}.{format}\")\n    else:\n        ## Case when which is a list of plot specified as strings\n        for key in which:\n            if key not in self.plots.keys():\n                print(f\"Plot {key} not found in the plots dictionary\")\n                print(f\"All available plots are: {list(self.plots.keys())}\")\n                continue\n            else:\n                for item in nested_to_tuple(\n                    self.plots[key]\n                ):  # Plot all epochs and splits of type key\n                    fig = item[-1]  ## Figure is in last element of the tuple\n                    filename = key + \"_\" + \"_\".join(str(x) for x in item[0:-1])\n                    fullpath = os.path.join(path, filename)\n                    fig.savefig(f\"{fullpath}.{format}\")\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.show_evaluation","title":"<code>show_evaluation(param, metric, ml_alg=None)</code>","text":"<p>Displays the evaluation plot for a specific clinical parameter, metric, and optionally ML algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>param</code> <code>str</code> <p>The clinical parameter to visualize.</p> required <code>metric</code> <code>str</code> <p>The metric to visualize.</p> required <code>ml_alg</code> <code>Optional[str]</code> <p>If None, plots all available algorithms.</p> <code>None</code> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>def show_evaluation(\n    self,\n    param: str,\n    metric: str,\n    ml_alg: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Displays the evaluation plot for a specific clinical parameter, metric, and optionally ML algorithm.\n\n    Args:\n        param: The clinical parameter to visualize.\n        metric: The metric to visualize.\n        ml_alg: If None, plots all available algorithms.\n    \"\"\"\n    plt.ioff()\n    if \"ML_Evaluation\" not in self.plots.keys():\n        print(\"ML Evaluation plots not found in the plots dictionary\")\n        print(\"You need to run evaluate() method first\")\n        return None\n    if param not in self.plots[\"ML_Evaluation\"].keys():\n        print(f\"Parameter {param} not found in the ML Evaluation plots\")\n        print(f\"Available parameters: {list(self.plots['ML_Evaluation'].keys())}\")\n        return None\n    if metric not in self.plots[\"ML_Evaluation\"][param].keys():\n        print(f\"Metric {metric} not found in the ML Evaluation plots for {param}\")\n        print(\n            f\"Available metrics: {list(self.plots['ML_Evaluation'][param].keys())}\"\n        )\n        return None\n\n    algs = list(self.plots[\"ML_Evaluation\"][param][metric].keys())\n    if ml_alg is not None:\n        if ml_alg not in algs:\n            print(f\"ML algorithm {ml_alg} not found for {param} and {metric}\")\n            print(f\"Available ML algorithms: {algs}\")\n            return None\n        fig = self.plots[\"ML_Evaluation\"][param][metric][ml_alg].figure\n        show_figure(fig)\n        plt.show()\n    else:\n        for alg in algs:\n            print(f\"Showing plot for ML algorithm: {alg}\")\n            fig = self.plots[\"ML_Evaluation\"][param][metric][alg].figure\n            show_figure(fig)\n            plt.show()\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.show_latent_space","title":"<code>show_latent_space(result, plot_type='2D-scatter', labels=None, param=None, epoch=None, split='all', **kwargs)</code>","text":"<p>Visualizes the latent space of the given result using different types of plots.</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>Result</code> <p>The result object containing latent spaces and losses.</p> required <code>labels</code> <code>Optional[Union[list, Series, None]]</code> <p>List of labels for the data points in the latent space. Default is None.</p> <code>None</code> <code>param </code> <p>List of parameters provided and stored as metadata. Strings must match column names. If not a list, string \"all\" is expected for convenient way to make plots for all parameters available. Default is None where no colored labels are plotted.</p> required <code>epoch</code> <code>Optional[Union[int, None]]</code> <p>The epoch number to visualize. If None, the last epoch is inferred from the losses. Default is None.</p> <code>None</code> <code>split</code> <code>str</code> <p>The data split to visualize. Options are \"train\", \"valid\", \"test\", and \"all\". Default is \"all\".</p> <code>'all'</code> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>@no_type_check\ndef show_latent_space(\n    self,\n    result: Result,\n    plot_type: str = \"2D-scatter\",\n    labels: Optional[Union[list, pd.Series, None]] = None,\n    param: Optional[Union[list, str]] = None,\n    epoch: Optional[Union[int, None]] = None,\n    split: str = \"all\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Visualizes the latent space of the given result using different types of plots.\n\n    Args:\n        result: The result object containing latent spaces and losses.\n        plot_type The type of plot to generate. Options are \"2D-scatter\", \"Ridgeline\", and \"Coverage-Correlation\". Default is \"2D-scatter\".\n        labels: List of labels for the data points in the latent space. Default is None.\n        param : List of parameters provided and stored as metadata. Strings must match column names. If not a list, string \"all\" is expected for convenient way to make plots for all parameters available. Default is None where no colored labels are plotted.\n        epoch: The epoch number to visualize. If None, the last epoch is inferred from the losses. Default is None.\n        split: The data split to visualize. Options are \"train\", \"valid\", \"test\", and \"all\". Default is \"all\".\n\n    \"\"\"\n    plt.ioff()\n    if plot_type == \"Coverage-Correlation\":\n        if \"Coverage-Correlation\" in self.plots:\n            fig = self.plots[\"Coverage-Correlation\"]\n            show_figure(fig)\n            plt.show()\n        else:\n            results = []\n            for epoch in range(\n                result.model.config.checkpoint_interval,\n                result.model.config.epochs + 1,\n                result.model.config.checkpoint_interval,\n            ):\n                for split in [\"train\", \"valid\"]:\n                    latent_df = result.get_latent_df(epoch=epoch - 1, split=split)\n                    tc = self._total_correlation(latent_df)\n                    cov = self._coverage_calc(latent_df)\n                    results.append(\n                        {\n                            \"epoch\": epoch,\n                            \"split\": split,\n                            \"total_correlation\": tc,\n                            \"coverage\": cov,\n                        }\n                    )\n\n            df_metrics = pd.DataFrame(results)\n\n            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n            # Total Correlation plot\n            _ = sns.lineplot(\n                data=df_metrics,\n                x=\"epoch\",\n                y=\"total_correlation\",\n                hue=\"split\",\n                ax=axes[0],\n            )\n            axes[0].set_title(\"Total Correlation\")\n            axes[0].set_xlabel(\"Epoch\")\n            axes[0].set_ylabel(\"Total Correlation\")\n\n            # Coverage plot\n            _ = sns.lineplot(\n                data=df_metrics, x=\"epoch\", y=\"coverage\", hue=\"split\", ax=axes[1]\n            )\n            axes[1].set_title(\"Coverage\")\n            axes[1].set_xlabel(\"Epoch\")\n            axes[1].set_ylabel(\"Coverage\")\n\n            plt.tight_layout()\n            self.plots[\"Coverage-Correlation\"] = fig\n            show_figure(fig)\n            plt.show()\n\n    else:\n        # Set Defaults\n        if epoch is None:\n            epoch = result.model.config.epochs - 1\n\n        ## Getting clin_data\n        if not hasattr(result.datasets, \"train\"):\n            raise ValueError(\"no train split in datasets\")\n\n        if not hasattr(result.datasets, \"valid\"):\n            raise ValueError(\"no valid split in datasets\")\n        if result.datasets.train is None:\n            raise ValueError(\"train is None\")\n        if result.datasets.valid is None:\n            raise ValueError(\"train is None\")\n        if result.datasets.test is None:\n            raise ValueError(\"train is None\")\n\n        if not hasattr(result.datasets.train, \"metadata\"):\n            raise ValueError(\"train dataset has no metadata\")\n        if not hasattr(result.datasets.valid, \"metadata\"):\n            raise ValueError(\"valid dataset has no metadata\")\n\n        # Check if metadata is a dictionary and contains 'paired'\n        if isinstance(result.datasets.train.metadata, dict):\n            if \"paired\" in result.datasets.train.metadata:\n                clin_data = result.datasets.train.metadata[\"paired\"]\n                if hasattr(result.datasets, \"test\"):\n                    clin_data = pd.concat(\n                        [clin_data, result.datasets.test.metadata[\"paired\"]],\n                        axis=0,\n                    )\n                if hasattr(result.datasets, \"valid\"):\n                    clin_data = pd.concat(\n                        [clin_data, result.datasets.valid.metadata[\"paired\"]],\n                        axis=0,\n                    )\n                else:\n                    # Raise error no annotation given\n                    raise ValueError(\n                        \"Please provide paired annotation data with key 'paired' in metadata dictionary.\"\n                    )\n            elif isinstance(result.datasets.train.metadata, pd.DataFrame):\n                clin_data = result.datasets.train.metadata\n                if hasattr(result.datasets, \"test\"):\n                    clin_data = pd.concat(\n                        [clin_data, result.datasets.test.metadata],\n                        axis=0,\n                    )\n                if hasattr(result.datasets, \"valid\"):\n                    clin_data = pd.concat(\n                        [clin_data, result.datasets.valid.metadata],\n                        axis=0,\n                    )\n            else:\n                # Raise error no annotation given\n                raise ValueError(\n                    \"Metadata is not a dictionary or DataFrame. Please provide a valid annotation data type.\"\n                )\n        else:\n            # Raise error no annotation given\n            raise ValueError(\n                \"No annotation data found. Please provide a valid annotation data type.\"\n            )\n\n        if split == \"all\":\n            df_latent = pd.concat(\n                [\n                    result.get_latent_df(epoch=epoch, split=\"train\"),\n                    result.get_latent_df(epoch=epoch, split=\"valid\"),\n                    result.get_latent_df(epoch=-1, split=\"test\"),\n                ]\n            )\n        else:\n            if split == \"test\":\n                df_latent = result.get_latent_df(epoch=-1, split=split)\n            else:\n                df_latent = result.get_latent_df(epoch=epoch, split=split)\n\n        if labels is None and param is None:\n            labels = [\"all\"] * df_latent.shape[0]\n\n        if labels is None and isinstance(param, str):\n            if param == \"all\":\n                param = list(clin_data.columns)\n            else:\n                raise ValueError(\n                    \"Please provide parameter to plot as a list not as string. If you want to plot all parameters, set param to 'all' and labels to None.\"\n                )\n\n        if labels is not None and param is not None:\n            raise ValueError(\n                \"Please provide either labels or param, not both. If you want to plot all parameters, set param to 'all' and labels to None.\"\n            )\n\n        if labels is not None and param is None:\n            if isinstance(labels, pd.Series):\n                param = [labels.name]\n                # Order by index of df_latent first, fill missing with \"unknown\"\n                labels = labels.reindex(\n                    df_latent.index, fill_value=\"unknown\"\n                ).tolist()\n            else:\n                param = [\"user_label\"]  # Default label if none provided\n\n        for p in param:\n            if p in clin_data.columns:\n                labels = clin_data.loc[df_latent.index, p].tolist()\n\n            if plot_type == \"2D-scatter\":\n                ## Make 2D Embedding with UMAP\n                if df_latent.shape[1] &gt; 2:\n                    reducer = UMAP(n_components=2)\n                    embedding = pd.DataFrame(reducer.fit_transform(df_latent))\n                else:\n                    embedding = df_latent\n\n                self.plots[\"2D-scatter\"][epoch][split][p] = self.plot_2D(\n                    embedding=embedding,\n                    labels=labels,\n                    param=p,\n                    layer=f\"2D latent space (epoch {epoch + 1})\",  # we start counting epochs at 0, so add 1 for display\n                    figsize=(12, 8),\n                    center=True,\n                )\n\n                fig = self.plots[\"2D-scatter\"][epoch][split][p]\n                show_figure(fig)\n                plt.show()\n\n            if plot_type == \"Ridgeline\":\n                ## Make ridgeline plot\n\n                self.plots[\"Ridgeline\"][epoch][split][p] = self.plot_latent_ridge(\n                    lat_space=df_latent, labels=labels, param=p\n                )\n\n                fig = self.plots[\"Ridgeline\"][epoch][split][p].figure\n                show_figure(fig)\n                plt.show()\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.show_loss","title":"<code>show_loss(plot_type='absolute')</code>","text":"<p>Display the loss plot.</p> <p>Parameters:</p> Name Type Description Default <code>plot_type</code> <code>Literal['absolute', 'relative']</code> <p>The type of loss plot to display. Defaults to \"absolute\".</p> <code>'absolute'</code> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>def show_loss(\n    self, plot_type: Literal[\"absolute\", \"relative\"] = \"absolute\"\n) -&gt; None:\n    \"\"\"Display the loss plot.\n\n    Args:\n        plot_type: The type of loss plot to display. Defaults to \"absolute\".\n    \"\"\"\n    if plot_type == \"absolute\":\n        if \"loss_absolute\" not in self.plots.keys():\n            print(\"Absolute loss plot not found in the plots dictionary\")\n            print(\"You need to run visualize() method first\")\n        else:\n            fig = self.plots[\"loss_absolute\"]\n            show_figure(fig)\n            plt.show()\n    if plot_type == \"relative\":\n        if \"loss_relative\" not in self.plots.keys():\n            print(\"Relative loss plot not found in the plots dictionary\")\n            print(\"You need to run visualize() method first\")\n        else:\n            fig = self.plots[\"loss_relative\"]\n            show_figure(fig)\n            plt.show()\n\n    if plot_type not in [\"absolute\", \"relative\"]:\n        print(\n            \"Type of loss plot not recognized. Please use 'absolute' or 'relative'\"\n        )\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.Visualizer.show_weights","title":"<code>show_weights()</code>","text":"<p>Display the model weights plot if it exists in the plots dictionary.</p> Source code in <code>src/autoencodix/visualize/visualize.py</code> <pre><code>def show_weights(self) -&gt; None:\n    \"\"\"Display the model weights plot if it exists in the plots dictionary.\"\"\"\n\n    if \"ModelWeights\" not in self.plots.keys():\n        print(\"Model weights not found in the plots dictionary\")\n        print(\"You need to run visualize() method first\")\n    else:\n        fig = self.plots[\"ModelWeights\"]\n        show_figure(fig)\n        plt.show()\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.XModalVisualizer","title":"<code>XModalVisualizer</code>","text":"<p>               Bases: <code>BaseVisualizer</code></p> Source code in <code>src/autoencodix/visualize/_xmodal_visualizer.py</code> <pre><code>class XModalVisualizer(BaseVisualizer):\n    plots: Dict[str, Any] = field(\n        default_factory=nested_dict\n    )  ## Nested dictionary of plots as figure handles\n\n    def __init__(self):\n        self.plots = nested_dict()\n\n    def __setitem__(self, key, elem):\n        self.plots[key] = elem\n\n    def visualize(self, result: Result, config: DefaultConfig) -&gt; Result:\n        ## Make Model Weights plot\n        ## TODO needs to be adjusted for X-Modalix ##\n        ## Plot Model weights for each sub-VAE ##\n        # self.plots[\"ModelWeights\"] = self._plot_model_weights(model=result.model)\n\n        ## Make long format of losses\n        loss_df_melt = self._make_loss_format(result=result, config=config)\n\n        ## X-Modalix specific ##\n        # Filter loss terms which are specific for each modality VAE\n        # Plot only combined loss terms as in old autoencodix framework\n        if not hasattr(result.datasets, \"train\"):\n            raise ValueError(\"result.datasets has no attribute train\")\n        if result.datasets.train is None:\n            raise ValueError(\"Train attribute of datasets is None\")\n        loss_df_melt = loss_df_melt[\n            ~loss_df_melt[\"Loss Term\"].str.startswith(\n                tuple(result.datasets.train.datasets.keys())\n            )\n        ]\n        if not result.losses._data:\n            import warnings\n\n            warnings.warn(\n                \"No loss data: This usually happens if you try to visualize after saving and loading the pipeline object with `save_all=False`. This memory-efficient saving mode does not retain past training loss data.\"\n            )\n            return result\n        ## Make plot loss absolute\n        self.plots[\"loss_absolute\"] = self._make_loss_plot(\n            df_plot=loss_df_melt, plot_type=\"absolute\"\n        )\n        ## Make plot loss relative\n        self.plots[\"loss_relative\"] = self._make_loss_plot(\n            df_plot=loss_df_melt, plot_type=\"relative\"\n        )\n\n        return result\n\n    def show_latent_space(\n        self,\n        result: Result,\n        plot_type: str = \"2D-scatter\",\n        labels: Optional[Union[list, pd.Series, None]] = None,\n        param: Optional[Union[list, str]] = None,\n        epoch: Optional[Union[int, None]] = None,\n        split: str = \"all\",\n    ) -&gt; None:\n        plt.ioff()\n        if plot_type == \"Coverage-Correlation\":\n            print(\"TODO: Implement Coverage-Correlation plot for X-Modalix\")\n            # if \"Coverage-Correlation\" in self.plots:\n            #     fig = self.plots[\"Coverage-Correlation\"]\n            #     show_figure(fig)\n            #     plt.show()\n            # else:\n            #     results = []\n            #     for epoch in range(result.model.config.checkpoint_interval, result.model.config.epochs + 1, result.model.config.checkpoint_interval):\n            #         for split in [\"train\", \"valid\"]:\n            #             latent_df = result.get_latent_df(epoch=epoch-1, split=split)\n            #             tc = self._total_correlation(latent_df)\n            #             cov = self._coverage_calc(latent_df)\n            #             results.append({\"epoch\": epoch, \"split\": split, \"total_correlation\": tc, \"coverage\": cov})\n\n            #     df_metrics = pd.DataFrame(results)\n\n            #     fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n            #     # Total Correlation plot\n            #     ax1 = sns.lineplot(data=df_metrics, x=\"epoch\", y=\"total_correlation\", hue=\"split\", ax=axes[0])\n            #     axes[0].set_title(\"Total Correlation\")\n            #     axes[0].set_xlabel(\"Epoch\")\n            #     axes[0].set_ylabel(\"Total Correlation\")\n\n            #     # Coverage plot\n            #     ax2 = sns.lineplot(data=df_metrics, x=\"epoch\", y=\"coverage\", hue=\"split\", ax=axes[1])\n            #     axes[1].set_title(\"Coverage\")\n            #     axes[1].set_xlabel(\"Epoch\")\n            #     axes[1].set_ylabel(\"Coverage\")\n\n            #     plt.tight_layout()\n            #     self.plots[\"Coverage-Correlation\"] = fig\n            #     show_figure(fig)\n            #     plt.show()\n        else:\n            # Set Defaults\n            if epoch is None:\n                epoch = -1\n\n            ## Collect all metadata and latent spaces from datasets\n            clin_data = []\n            latent_data = []\n\n            if split == \"all\":\n                split_list = [\"train\", \"test\", \"valid\"]\n            else:\n                split_list = [split]\n            for s in split_list:\n                split_ds = getattr(result.datasets, s, None)\n                if split_ds is not None:\n                    for key, ds in split_ds.datasets.items():\n                        if s == \"test\":\n                            df_latent = result.get_latent_df(\n                                epoch=-1, split=s, modality=key\n                            )\n                        else:\n                            df_latent = result.get_latent_df(\n                                epoch=epoch, split=s, modality=key\n                            )\n                        df_latent[\"modality\"] = key\n                        df_latent[\"sample_ids\"] = (\n                            df_latent.index\n                        )  # Each sample can occur multiple times in latent space\n                        latent_data.append(df_latent)\n                        if hasattr(ds, \"metadata\") and ds.metadata is not None:\n                            df = ds.metadata.copy()\n                            df[\"sample_ids\"] = df.index.astype(str)\n                            df[\"split\"] = s\n                            df[\"modality\"] = key\n                            clin_data.append(df)\n\n            if latent_data and clin_data:\n                latent_data = pd.concat(latent_data, axis=0, ignore_index=True)\n                clin_data = pd.concat(clin_data, axis=0, ignore_index=True)\n                if \"sample_ids\" in clin_data.columns:\n                    clin_data = clin_data.drop_duplicates(\n                        subset=\"sample_ids\"\n                    ).set_index(\"sample_ids\")\n            else:\n                latent_data = pd.DataFrame()\n                clin_data = pd.DataFrame()\n\n            ## Label options\n            if param is None:\n                modality = list(result.model.keys())[\n                    0\n                ]  # Take the first since configs are same for all sub-VAEs\n                model = result.model.get(modality, None)\n                if model is None:\n                    raise ValueError(\n                        f\"Model for modality {modality} not found in result.model\"\n                    )\n                param = model.config.data_config.annotation_columns\n\n            if labels is None and param is None:\n                labels = [\"all\"] * latent_data[\"sample_ids\"].unique().shape[0]\n\n            if labels is None and isinstance(param, str):\n                if param == \"all\":\n                    param = list(clin_data.columns)\n                else:\n                    raise ValueError(\n                        \"Please provide parameter to plot as a list not as string. If you want to plot all parameters, set param to 'all' and labels to None.\"\n                    )\n\n            if labels is not None and param is not None:\n                raise ValueError(\n                    \"Please provide either labels or param, not both. If you want to plot all parameters, set param to 'all' and labels to None.\"\n                )\n\n            if labels is not None and param is None:\n                if isinstance(labels, pd.Series):\n                    param = [labels.name]\n                    # Order by index of latent_data first, fill missing with \"unknown\"\n                    labels = labels.reindex(  # ty: ignore\n                        latent_data[\"sample_ids\"],  # ty: ignore\n                        fill_value=\"unknown\",  # ty: ignore\n                    ).tolist()\n                else:\n                    param = [\"user_label\"]  # Default label if none provided\n            if not isinstance(param, list):\n                raise ValueError(f\"param: should be converted to list, got: {param}\")\n            for p in param:\n                if p in clin_data.columns:\n                    labels: List = clin_data.loc[\n                        latent_data[\"sample_ids\"], p\n                    ].tolist()  # ty: ignore\n                else:\n                    if clin_data.shape[0] == len(labels):  # ty: ignore\n                        clin_data[p] = labels\n                    else:\n                        clin_data[p] = [\"all\"] * clin_data.shape[0]\n\n                if plot_type == \"2D-scatter\":\n                    ## Make 2D Embedding with UMAP\n                    if (\n                        latent_data.drop(\n                            columns=[\"sample_ids\", \"modality\"]\n                        ).shape[  # ty: ignore\n                            1\n                        ]  # ty: ignore\n                        &gt; 2\n                    ):\n                        reducer = UMAP(n_components=2)\n                        embedding = pd.DataFrame(\n                            reducer.fit_transform(\n                                latent_data.drop(\n                                    columns=[\"sample_ids\", \"modality\"]\n                                )  # ty: ignore\n                            )\n                        )\n                        embedding.columns = [\"DIM1\", \"DIM2\"]\n                        embedding[\"sample_ids\"] = latent_data[\"sample_ids\"]\n                        embedding[\"modality\"] = latent_data[\"modality\"]\n                    else:\n                        embedding = latent_data\n\n                    # Merge with clinical data via sample_ids\n                    clin_data[\"sample_ids\"] = clin_data.index.astype(str)\n                    clin_data.index = clin_data.index.astype(str)  # Add this line\n                    embedding[\"sample_ids\"] = embedding[\"sample_ids\"].astype(str)\n\n                    embedding = embedding.merge(\n                        clin_data.drop(columns=[\"modality\"]),  # ty: ignore\n                        left_on=\"sample_ids\",\n                        right_index=True,\n                        how=\"left\",\n                    )\n\n                    self.plots[\"2D-scatter\"][epoch][split][p] = (\n                        self._plot_translate_latent(\n                            embedding=embedding,\n                            color_param=p,\n                            style_param=\"modality\",\n                        )\n                    )\n\n                    fig = self.plots[\"2D-scatter\"][epoch][split][p].figure\n                    # show_figure(fig)\n                    plt.show()\n\n                if plot_type == \"Ridgeline\":\n                    ## Make ridgeline plot\n                    if len(labels) != latent_data.shape[0]:  # ty: ignore\n                        if labels[0] == \"all\":  # ty: ignore\n                            labels = [\"all\"] * latent_data.shape[0]  # ty: ignore\n                        else:\n                            raise ValueError(\n                                \"Labels must match the number of samples in the latent space.\"\n                            )\n\n                    self.plots[\"Ridgeline\"][epoch][split][p] = (\n                        self._plot_latent_ridge_multi(\n                            lat_space=latent_data.drop(\n                                columns=[\"sample_ids\"]\n                            ),  # ty: ignore\n                            labels=labels,\n                            modality=\"modality\",\n                            param=p,\n                        )\n                    )\n\n                    fig = self.plots[\"Ridgeline\"][epoch][split][p].figure\n                    show_figure(fig)\n                    plt.show()\n\n    def show_weights(self) -&gt; None:\n        ## TODO\n        raise NotImplementedError(\n            \"Weight visualization for X-Modalix is not implemented.\"\n        )\n\n    @no_type_check\n    def show_image_translation(  # ty: ignore\n        self,\n        result: Result,\n        from_key: str,\n        to_key: str,\n        n_sample_per_class: int = 3,\n        param: Optional[str] = None,\n    ) -&gt; None:  # ty: ignore\n        \"\"\"Visualizes image translation results for a given dataset.\n\n        Split by displaying a grid of original, translated, and reference images,grouped by class values.\n        Args:\n            result:The result object containing datasets and reconstructions.\n            from_key: The source modality key (not directly used in visualization, but relevant for context).\n            to_key: The target modality key. Must correspond to an image dataset (must contain \"IMG\").\n            split: The dataset split to visualize (\"test\", \"train\", or \"valid\"). Default is \"test\".\n            n_sample_per_class: Number of samples to display per class value. Default is 3.\n            param: The metadata column name used to group samples by class.\n        Raises\n            ValueError: If `to_key` does not correspond to an image dataset.\n        \"\"\"\n\n        if \"img\" not in to_key:\n            raise ValueError(\n                f\"You provided as 'to_key' {to_key} a non-image dataset. \"\n                \"Image translation grid visualization is only possible for translation to IMG data type.\"\n            )\n        else:\n            split = \"test\"  # Currently only test split is supported\n            ## Get n samples per class\n            if split == \"test\":\n                meta = result.datasets.test.datasets[to_key].metadata\n                paired_sample_ids = result.datasets.test.paired_sample_ids\n\n            # Restrict meta to only paired sample ids\n            meta = meta.loc[paired_sample_ids]\n\n            if param is None:\n                param = \"user-label\"\n                meta[param] = (\n                    \"all\"  # Default to all samples if no parameter is provided\n                )\n\n            # Get possible class values\n            class_values = meta[param].unique()\n            if len(class_values) &gt; 10:\n                # Make warning\n                warnings.warn(\n                    f\"Found {len(class_values)} class values for parameter '{param}'. Only first 10 will be used to limit figure size\"\n                )\n                class_values = class_values[:10]\n\n            # Build dictionary of sample_ids per class value (max n_sample_per_class per class)\n            sample_per_class = {\n                val: meta[meta[param] == val]\n                .sample(\n                    n=min(n_sample_per_class, (meta[param] == val).sum()),\n                    random_state=42,\n                )\n                .index.tolist()\n                for val in class_values\n            }\n\n            print(f\"Sample per class: {sample_per_class}\")\n\n            # Lookup of sample indices per modality\n            sample_ids_per_key = dict()\n\n            for key in result.sample_ids.get(epoch=-1, split=\"test\").keys():\n                sample_ids_per_key[key] = result.sample_ids.get(epoch=-1, split=\"test\")[\n                    key\n                ]\n            # Original\n            sample_ids_per_key[\"original\"] = result.datasets.test.datasets[\n                to_key\n            ].sample_ids\n\n            ## Generate Image Grid\n            # Number of test (or train or valid) samples from all values in sample_per_class dictionary\n            n_test_samples = sum(len(indices) for indices in sample_per_class.values())\n\n            # #\n            col_labels = []\n            for class_value in sample_per_class:\n                col_labels.extend(\n                    [\n                        class_value + \" \" + split + \"-sample:\" + s\n                        for s in sample_per_class[class_value]\n                    ]\n                )\n\n            row_labels = [\"Original\", \"Translated\", \"Reference\"]\n\n            fig, axes = plt.subplots(\n                ncols=n_test_samples,  # Number of classes\n                nrows=3,  # Original, translated, reference\n                figsize=(n_test_samples * 2, 3 * 2),\n            )\n\n            for i, ax in enumerate(axes.flat):\n                row = int(i / n_test_samples)\n                # test_sample = sample_idx_list[i % n_test_samples]\n                # print(f\"Row: {row}, Column: {i % n_test_samples}\")\n                # print(f\"Current sample: {col_labels[i % n_test_samples]}\")\n\n                if row == 0:\n                    if split == \"test\":\n                        idx_original = list(sample_ids_per_key[\"original\"]).index(\n                            col_labels[i % n_test_samples].split(\"sample:\")[1]\n                        )\n                        img_temp = result.datasets.test.datasets[to_key][idx_original][\n                            1\n                        ].squeeze()  # Stored as Tuple (index, tensor, sample_id)\n\n                    # Original image\n                    ax.imshow(np.asarray(img_temp))\n                    ax.axis(\"off\")\n                    # Sample label\n                    ax.text(\n                        0.5,\n                        1.1,\n                        col_labels[i],\n                        va=\"bottom\",\n                        ha=\"center\",\n                        # rotation='vertical',\n                        rotation=45,\n                        transform=ax.transAxes,\n                    )\n                    # Row label\n                    if i % n_test_samples == 0:\n                        ax.text(\n                            -0.1,\n                            0.5,\n                            row_labels[0],\n                            va=\"center\",\n                            ha=\"right\",\n                            transform=ax.transAxes,\n                        )\n\n                if row == 1:\n                    # Translated image\n                    idx_translated = list(sample_ids_per_key[\"translation\"]).index(\n                        col_labels[i % n_test_samples].split(\"sample:\")[1]\n                    )\n                    ax.imshow(\n                        result.reconstructions.get(epoch=-1, split=split)[\n                            \"translation\"\n                        ][idx_translated].squeeze()\n                    )\n                    ax.axis(\"off\")\n                    # Row label\n                    if i % n_test_samples == 0:\n                        ax.text(\n                            -0.1,\n                            0.5,\n                            row_labels[1],\n                            va=\"center\",\n                            ha=\"right\",\n                            transform=ax.transAxes,\n                        )\n\n                if row == 2:\n                    # Reference image reconstruction\n                    idx_reference = list(\n                        sample_ids_per_key[f\"reference_{to_key}_to_{to_key}\"]\n                    ).index(col_labels[i % n_test_samples].split(\"sample:\")[1])\n                    ax.imshow(\n                        result.reconstructions.get(epoch=-1, split=split)[\n                            f\"reference_{to_key}_to_{to_key}\"\n                        ][idx_reference].squeeze()\n                    )\n                    ax.axis(\"off\")\n                    # Row label\n                    if i % n_test_samples == 0:\n                        ax.text(\n                            -0.1,\n                            0.5,\n                            row_labels[2],\n                            va=\"center\",\n                            ha=\"right\",\n                            transform=ax.transAxes,\n                        )\n\n            self.plots[\"Image-translation\"][to_key][split][param] = fig\n            # show_figure(fig)\n            plt.show()\n\n    @no_type_check\n    def show_2D_translation(\n        self,\n        result: Result,\n        translated_modality: str,\n        split: str = \"test\",\n        param: Optional[str] = None,\n        reducer: str = \"UMAP\",\n    ) -&gt; None:\n        ## TODO add similar labels/param logic from other visualizations\n        dataset = result.datasets\n\n        ## Overwrite original datasets with new_datasets if available after predict with other data\n        if dataset is None:\n            dataset = DatasetContainer()\n\n        if bool(result.new_datasets.test):\n            dataset.test = result.new_datasets.test\n\n        if split not in [\"train\", \"valid\", \"test\", \"all\"]:\n            raise ValueError(f\"Unknown split: {split}\")\n\n        if dataset.test is None:\n            raise ValueError(\"test of dataset is None\")\n\n        if split == \"test\":\n            df_processed = dataset.test._to_df(modality=translated_modality)\n        else:\n            raise NotImplementedError(\n                \"2D translation visualization is currently only implemented for the 'test' split since reconstruction is only performed on test-split.\"\n            )\n\n        # Get translated reconstruction\n        tensor_list = result.reconstructions.get(epoch=-1, split=split)[  # ty: ignore\n            \"translation\"\n        ]  # ty: ignore\n        print(f\"len of tensor-list: {len(tensor_list)}\")\n        tensor_ids = result.sample_ids.get(epoch=-1, split=split)[\"translation\"]\n        print(f\"len of tensor_ids: {len(tensor_ids)}\")\n\n        # Flatten each tensor and collect as rows (for image case)\n        rows = [\n            t.flatten().cpu().numpy() if isinstance(t, torch.Tensor) else t.flatten()\n            for t in tensor_list\n        ]\n\n        # Create DataFrame\n        df_translate_flat = pd.DataFrame(\n            rows,\n            columns=[\"Feature_\" + str(i) for i in range(len(rows[0]))],\n            index=tensor_ids,\n        )\n\n        if reducer == \"UMAP\":\n            reducer_model = UMAP(n_components=2)\n        elif reducer == \"PCA\":\n            reducer_model = PCA(n_components=2)\n        elif reducer == \"TSNE\":\n            reducer_model = TSNE(n_components=2)\n\n        # making sure of index alignemnt\n        common_ids = df_processed.index.intersection(df_translate_flat.index)\n        df_processed = df_processed.loc[common_ids]\n        df_translate_flat = df_translate_flat.loc[common_ids]\n        df_translate_flat = df_translate_flat.reindex(df_processed.index)\n        df_translate_flat.index = pd.Index([i for i in range(len(common_ids))])\n        X = np.vstack([df_processed.values, df_translate_flat.values])\n        df_red_comb = pd.DataFrame(reducer_model.fit_transform(X))\n\n        # df_comb = pd.concat(\n        #     [df_processed, df_translate_flat], axis=0, ignore_index=True\n        # )\n\n        df_red_comb[\"origin\"] = [\"input\"] * df_processed.shape[0] + [\n            \"translated\"\n        ] * df_translate_flat.shape[0]\n\n        # df_red_comb = pd.DataFrame(\n        #     reducer_model.fit_transform(\n        #         pd.concat([df_processed, df_translate_flat], axis=0)\n        #     )\n        # )\n\n        labels = (\n            list(\n                result.datasets.test.datasets[translated_modality].metadata[param]\n            )  # ty: ignore\n            * 2\n        )\n        df_red_comb[param] = (\n            labels + labels[0 : df_red_comb.shape[0] - len(labels)]\n        )  ## TODO fix for not matching lengths\n\n        g = sns.FacetGrid(\n            df_red_comb,\n            col=\"origin\",\n            hue=param,\n            sharex=True,\n            sharey=True,\n            height=8,\n            aspect=1,\n        )\n        g.map_dataframe(sns.scatterplot, x=0, y=1, alpha=0.7)\n        g.add_legend()\n        g.set_axis_labels(reducer + \" DIM 1\", reducer + \" DIM 2\")\n        g.set_titles(col_template=\"{col_name}\")\n\n        self.plots[\"2D-translation\"][translated_modality][split][param] = g\n        plt.show()\n\n    ## Utilities specific for X-Modalix\n    @staticmethod\n    def _plot_translate_latent(\n        embedding,\n        color_param,\n        style_param=None,\n    ):\n        \"\"\"Creates a 2D visualization of the 2D embedding of the latent space.\n        Args:\n            embedding: embedding on which is visualized. Assumes prior 2D dimension reduction.\n            color_params: Clinical parameter to color scatter plot\n            style_param: Parameter e.g. \"Translate\" to facet scatter plot\n        Returns:\n            fig: Figure handle\n\n        \"\"\"\n        labels = list(embedding[color_param])\n        # logger = getlogger(cfg)\n        numeric = False\n        if not isinstance(labels[0], str):\n            if len(np.unique(labels)) &gt; 3:\n                # TODO Decide if numeric to category should be optional in new Package\n                # print(\n                #     f\"The provided label column is numeric and converted to categories.\"\n                # )\n                # labels = pd.qcut(\n                #     labels, q=4, labels=[\"1stQ\", \"2ndQ\", \"3rdQ\", \"4thQ\"]\n                # ).astype(str)\n                # else:\n                numeric = True\n            else:\n                labels = [str(x) for x in labels]\n\n        # check if label or embedding is longerm and duplicate the shorter one\n        if len(labels) &lt; embedding.shape[0]:\n            print(\n                \"Given labels do not have the same length as given sample size. Labels will be duplicated.\"\n            )\n            labels = [\n                label\n                for label in labels\n                for _ in range(embedding.shape[0] // len(labels))\n            ]\n        elif len(labels) &gt; embedding.shape[0]:\n            labels = list(set(labels))\n\n        if style_param is not None:\n            embedding[color_param] = labels\n            if numeric:\n                palette = \"bwr\"\n            else:\n                palette = None\n            plot = sns.relplot(\n                data=embedding,\n                x=\"DIM1\",\n                y=\"DIM2\",\n                hue=color_param,\n                palette=palette,\n                col=style_param,\n                style=style_param,\n                markers=True,\n                alpha=0.4,\n                ec=\"black\",\n                height=10,\n                aspect=1,\n                s=150,\n            )\n\n        return plot\n\n    @staticmethod\n    def _plot_latent_ridge_multi(\n        lat_space: pd.DataFrame,\n        modality: Optional[str] = None,\n        labels: Optional[Union[list, pd.Series, None]] = None,\n        param: Optional[Union[str, None]] = None,\n    ) -&gt; sns.FacetGrid:\n        \"\"\"Creates a ridge line plot of latent space dimension where each row shows the density of a latent dimension and groups (ridges).\n        Args:\n            lat_space: DataFrame containing the latent space intensities for samples (rows) and latent dimensions (columns)\n            labels: List of labels for each sample. If None, all samples are considered as one group.\n            param: Clinical parameter to create groupings and coloring of ridges. Must be a column name (str) of clin_data\n        Returns:\n            g (sns.FacetGrid): FacetGrid object containing the ridge line plot\n        \"\"\"\n        sns.set_theme(\n            style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)}\n        )  ## Necessary to enforce overplotting\n\n        df = pd.melt(\n            lat_space,\n            id_vars=modality,  # ty: ignore\n            var_name=\"latent dim\",\n            value_name=\"latent intensity\",\n        )\n        # print(df)\n        df[\"sample\"] = len(lat_space.drop(columns=modality).columns) * list(\n            lat_space.index\n        )\n\n        if labels is None:\n            param = \"all\"\n            labels = [\"all\"] * len(df)\n\n        # print(labels[0])\n        if not isinstance(labels[0], str):\n            if len(np.unique(labels)) &gt; 3:\n                # Change all non-float labels to NaN\n                labels = [x if isinstance(x, float) else float(\"nan\") for x in labels]\n                labels = pd.qcut(\n                    x=pd.Series(labels),\n                    q=4,\n                    labels=[\"1stQ\", \"2ndQ\", \"3rdQ\", \"4thQ\"],\n                ).astype(str)\n            else:\n                labels = [str(x) for x in labels]\n\n        df[param] = len(lat_space.drop(columns=modality).columns) * labels  # type: ignore\n\n        exclude_missing_info = (df[param] == \"unknown\") | (df[param] == \"nan\")\n\n        xmin = (\n            df.loc[~exclude_missing_info, [\"latent intensity\", \"latent dim\", param]]\n            .groupby([param, \"latent dim\"], observed=False)\n            .quantile(0.05)\n            .min()\n        )\n        xmax = (\n            df.loc[~exclude_missing_info, [\"latent intensity\", \"latent dim\", param]]\n            .groupby([param, \"latent dim\"], observed=False)\n            .quantile(0.9)\n            .max()\n        )\n\n        if len(np.unique(df[param])) &gt; 8:\n            cat_pal = sns.husl_palette(len(np.unique(df[param])))\n        else:\n            cat_pal = sns.color_palette(n_colors=len(np.unique(df[param])))\n\n        g = sns.FacetGrid(\n            df[~exclude_missing_info],\n            row=\"latent dim\",\n            col=modality,\n            hue=param,\n            aspect=12,\n            height=0.8,\n            xlim=(xmin.iloc[0], xmax.iloc[0]),\n            palette=cat_pal,\n        )\n\n        g.map_dataframe(\n            sns.kdeplot,\n            \"latent intensity\",\n            bw_adjust=0.5,\n            clip_on=True,\n            fill=True,\n            alpha=0.5,\n            warn_singular=False,\n            ec=\"k\",\n            lw=1,\n        )\n\n        def label(data, color, label, text=\"latent dim\"):\n            ax = plt.gca()\n            label_text = data[text].unique()[0]\n            ax.text(\n                0.0,\n                0.2,\n                label_text,\n                fontweight=\"bold\",\n                ha=\"right\",\n                va=\"center\",\n                transform=ax.transAxes,\n            )\n\n        g.map_dataframe(label, text=\"latent dim\")\n\n        g.set(xlim=(xmin.iloc[0], xmax.iloc[0]))\n        # Set the subplots to overlap\n        g.figure.subplots_adjust(hspace=-0.5)\n\n        # Remove axes details that don't play well with overlap\n        g.set_titles(\"\")\n        g.set(yticks=[], ylabel=\"\")\n        g.despine(bottom=True, left=True)\n\n        for i, m in enumerate(df[modality].unique()):\n            g.fig.get_axes()[i].set_title(m)\n\n        g.add_legend()\n\n        plt.close()\n        return g\n\n    def _plot_evaluation(\n        self,\n        result: Result,\n    ) -&gt; dict:\n        \"\"\"Plots the evaluation results from the Result object.\n\n        Args:\n            result: The Result object containing evaluation data.\n\n        Returns:\n            The generated dictionary containing the evaluation plots.\n        \"\"\"\n        ## Plot all results\n\n        ml_plots = dict()\n        plt.ioff()\n\n        for c in pd.unique(result.embedding_evaluation.CLINIC_PARAM):\n            ml_plots[c] = dict()\n            for m in pd.unique(\n                result.embedding_evaluation.loc[\n                    result.embedding_evaluation.CLINIC_PARAM == c, \"metric\"\n                ]\n            ):  # ty: ignore\n                ml_plots[c][m] = dict()\n                for alg in pd.unique(\n                    result.embedding_evaluation.loc[\n                        (result.embedding_evaluation.CLINIC_PARAM == c)\n                        &amp; (result.embedding_evaluation.metric == m),\n                        \"ML_ALG\",\n                    ]\n                ):  # ty: ignore\n                    data = result.embedding_evaluation[\n                        (result.embedding_evaluation.metric == m)\n                        &amp; (result.embedding_evaluation.CLINIC_PARAM == c)\n                        &amp; (result.embedding_evaluation.ML_ALG == alg)\n                    ]\n\n                    sns_plot = sns.catplot(\n                        data=data,\n                        x=\"score_split\",\n                        y=\"value\",\n                        col=\"ML_TASK\",\n                        row=\"MODALITY\",\n                        hue=\"score_split\",\n                        kind=\"bar\",\n                    )\n\n                    min_y = data.value.min()\n                    if min_y &gt; 0:\n                        min_y = 0\n\n                    ml_plots[c][m][alg] = sns_plot.set(ylim=(min_y, None))\n\n        self.plots[\"ML_Evaluation\"] = ml_plots\n\n        return ml_plots\n</code></pre>"},{"location":"api/visualize/#autoencodix.visualize.XModalVisualizer.show_image_translation","title":"<code>show_image_translation(result, from_key, to_key, n_sample_per_class=3, param=None)</code>","text":"<p>Visualizes image translation results for a given dataset.</p> <p>Split by displaying a grid of original, translated, and reference images,grouped by class values. Args:     result:The result object containing datasets and reconstructions.     from_key: The source modality key (not directly used in visualization, but relevant for context).     to_key: The target modality key. Must correspond to an image dataset (must contain \"IMG\").     split: The dataset split to visualize (\"test\", \"train\", or \"valid\"). Default is \"test\".     n_sample_per_class: Number of samples to display per class value. Default is 3.     param: The metadata column name used to group samples by class. Raises     ValueError: If <code>to_key</code> does not correspond to an image dataset.</p> Source code in <code>src/autoencodix/visualize/_xmodal_visualizer.py</code> <pre><code>@no_type_check\ndef show_image_translation(  # ty: ignore\n    self,\n    result: Result,\n    from_key: str,\n    to_key: str,\n    n_sample_per_class: int = 3,\n    param: Optional[str] = None,\n) -&gt; None:  # ty: ignore\n    \"\"\"Visualizes image translation results for a given dataset.\n\n    Split by displaying a grid of original, translated, and reference images,grouped by class values.\n    Args:\n        result:The result object containing datasets and reconstructions.\n        from_key: The source modality key (not directly used in visualization, but relevant for context).\n        to_key: The target modality key. Must correspond to an image dataset (must contain \"IMG\").\n        split: The dataset split to visualize (\"test\", \"train\", or \"valid\"). Default is \"test\".\n        n_sample_per_class: Number of samples to display per class value. Default is 3.\n        param: The metadata column name used to group samples by class.\n    Raises\n        ValueError: If `to_key` does not correspond to an image dataset.\n    \"\"\"\n\n    if \"img\" not in to_key:\n        raise ValueError(\n            f\"You provided as 'to_key' {to_key} a non-image dataset. \"\n            \"Image translation grid visualization is only possible for translation to IMG data type.\"\n        )\n    else:\n        split = \"test\"  # Currently only test split is supported\n        ## Get n samples per class\n        if split == \"test\":\n            meta = result.datasets.test.datasets[to_key].metadata\n            paired_sample_ids = result.datasets.test.paired_sample_ids\n\n        # Restrict meta to only paired sample ids\n        meta = meta.loc[paired_sample_ids]\n\n        if param is None:\n            param = \"user-label\"\n            meta[param] = (\n                \"all\"  # Default to all samples if no parameter is provided\n            )\n\n        # Get possible class values\n        class_values = meta[param].unique()\n        if len(class_values) &gt; 10:\n            # Make warning\n            warnings.warn(\n                f\"Found {len(class_values)} class values for parameter '{param}'. Only first 10 will be used to limit figure size\"\n            )\n            class_values = class_values[:10]\n\n        # Build dictionary of sample_ids per class value (max n_sample_per_class per class)\n        sample_per_class = {\n            val: meta[meta[param] == val]\n            .sample(\n                n=min(n_sample_per_class, (meta[param] == val).sum()),\n                random_state=42,\n            )\n            .index.tolist()\n            for val in class_values\n        }\n\n        print(f\"Sample per class: {sample_per_class}\")\n\n        # Lookup of sample indices per modality\n        sample_ids_per_key = dict()\n\n        for key in result.sample_ids.get(epoch=-1, split=\"test\").keys():\n            sample_ids_per_key[key] = result.sample_ids.get(epoch=-1, split=\"test\")[\n                key\n            ]\n        # Original\n        sample_ids_per_key[\"original\"] = result.datasets.test.datasets[\n            to_key\n        ].sample_ids\n\n        ## Generate Image Grid\n        # Number of test (or train or valid) samples from all values in sample_per_class dictionary\n        n_test_samples = sum(len(indices) for indices in sample_per_class.values())\n\n        # #\n        col_labels = []\n        for class_value in sample_per_class:\n            col_labels.extend(\n                [\n                    class_value + \" \" + split + \"-sample:\" + s\n                    for s in sample_per_class[class_value]\n                ]\n            )\n\n        row_labels = [\"Original\", \"Translated\", \"Reference\"]\n\n        fig, axes = plt.subplots(\n            ncols=n_test_samples,  # Number of classes\n            nrows=3,  # Original, translated, reference\n            figsize=(n_test_samples * 2, 3 * 2),\n        )\n\n        for i, ax in enumerate(axes.flat):\n            row = int(i / n_test_samples)\n            # test_sample = sample_idx_list[i % n_test_samples]\n            # print(f\"Row: {row}, Column: {i % n_test_samples}\")\n            # print(f\"Current sample: {col_labels[i % n_test_samples]}\")\n\n            if row == 0:\n                if split == \"test\":\n                    idx_original = list(sample_ids_per_key[\"original\"]).index(\n                        col_labels[i % n_test_samples].split(\"sample:\")[1]\n                    )\n                    img_temp = result.datasets.test.datasets[to_key][idx_original][\n                        1\n                    ].squeeze()  # Stored as Tuple (index, tensor, sample_id)\n\n                # Original image\n                ax.imshow(np.asarray(img_temp))\n                ax.axis(\"off\")\n                # Sample label\n                ax.text(\n                    0.5,\n                    1.1,\n                    col_labels[i],\n                    va=\"bottom\",\n                    ha=\"center\",\n                    # rotation='vertical',\n                    rotation=45,\n                    transform=ax.transAxes,\n                )\n                # Row label\n                if i % n_test_samples == 0:\n                    ax.text(\n                        -0.1,\n                        0.5,\n                        row_labels[0],\n                        va=\"center\",\n                        ha=\"right\",\n                        transform=ax.transAxes,\n                    )\n\n            if row == 1:\n                # Translated image\n                idx_translated = list(sample_ids_per_key[\"translation\"]).index(\n                    col_labels[i % n_test_samples].split(\"sample:\")[1]\n                )\n                ax.imshow(\n                    result.reconstructions.get(epoch=-1, split=split)[\n                        \"translation\"\n                    ][idx_translated].squeeze()\n                )\n                ax.axis(\"off\")\n                # Row label\n                if i % n_test_samples == 0:\n                    ax.text(\n                        -0.1,\n                        0.5,\n                        row_labels[1],\n                        va=\"center\",\n                        ha=\"right\",\n                        transform=ax.transAxes,\n                    )\n\n            if row == 2:\n                # Reference image reconstruction\n                idx_reference = list(\n                    sample_ids_per_key[f\"reference_{to_key}_to_{to_key}\"]\n                ).index(col_labels[i % n_test_samples].split(\"sample:\")[1])\n                ax.imshow(\n                    result.reconstructions.get(epoch=-1, split=split)[\n                        f\"reference_{to_key}_to_{to_key}\"\n                    ][idx_reference].squeeze()\n                )\n                ax.axis(\"off\")\n                # Row label\n                if i % n_test_samples == 0:\n                    ax.text(\n                        -0.1,\n                        0.5,\n                        row_labels[2],\n                        va=\"center\",\n                        ha=\"right\",\n                        transform=ax.transAxes,\n                    )\n\n        self.plots[\"Image-translation\"][to_key][split][param] = fig\n        # show_figure(fig)\n        plt.show()\n</code></pre>"}]}