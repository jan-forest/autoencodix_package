{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c90fee75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in handle_direct_user_data with data: <class 'autoencodix.data.datapackage.DataPackage'>\n",
      "--- Running Pairing-Aware Split ---\n",
      "Training each modality model...\n",
      "Training modality: rna\n",
      "Training modality: rna\n",
      "Epoch 1 - Train Loss: 8.4255\n",
      "Sub-losses: recon_loss: 8.4255, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 5.1928\n",
      "Sub-losses: recon_loss: 5.1928, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 7.6681\n",
      "Sub-losses: recon_loss: 7.6567, var_loss: 0.0114, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 2 - Valid Loss: 5.7376\n",
      "Sub-losses: recon_loss: 5.7361, var_loss: 0.0015, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 3 - Train Loss: 8.5381\n",
      "Sub-losses: recon_loss: 8.2019, var_loss: 0.3362, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 3 - Valid Loss: 5.1187\n",
      "Sub-losses: recon_loss: 5.0709, var_loss: 0.0478, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Training modality: protein\n",
      "Training modality: protein\n",
      "Epoch 1 - Train Loss: 8.5400\n",
      "Sub-losses: recon_loss: 8.5399, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 4.9206\n",
      "Sub-losses: recon_loss: 4.9206, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 7.7576\n",
      "Sub-losses: recon_loss: 7.7385, var_loss: 0.0191, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 2 - Valid Loss: 4.3700\n",
      "Sub-losses: recon_loss: 4.3680, var_loss: 0.0020, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 3 - Train Loss: 7.3891\n",
      "Sub-losses: recon_loss: 6.9086, var_loss: 0.4805, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 3 - Valid Loss: 4.3608\n",
      "Sub-losses: recon_loss: 4.3010, var_loss: 0.0598, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Training modality: atac\n",
      "Training modality: atac\n",
      "Epoch 1 - Train Loss: 5.4983\n",
      "Sub-losses: recon_loss: 5.4982, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 7.2965\n",
      "Sub-losses: recon_loss: 7.2965, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 5.7865\n",
      "Sub-losses: recon_loss: 5.7766, var_loss: 0.0099, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 2 - Valid Loss: 6.6937\n",
      "Sub-losses: recon_loss: 6.6924, var_loss: 0.0013, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 3 - Train Loss: 6.2593\n",
      "Sub-losses: recon_loss: 5.9667, var_loss: 0.2926, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 3 - Valid Loss: 7.5693\n",
      "Sub-losses: recon_loss: 7.5290, var_loss: 0.0403, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Training modality: cytokine\n",
      "Training modality: cytokine\n",
      "Epoch 1 - Train Loss: 6.3578\n",
      "Sub-losses: recon_loss: 6.3578, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 3.4262\n",
      "Sub-losses: recon_loss: 3.4262, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 5.9437\n",
      "Sub-losses: recon_loss: 5.9310, var_loss: 0.0127, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 2 - Valid Loss: 2.7794\n",
      "Sub-losses: recon_loss: 2.7775, var_loss: 0.0019, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 3 - Train Loss: 5.7153\n",
      "Sub-losses: recon_loss: 5.4134, var_loss: 0.3019, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 3 - Valid Loss: 3.2402\n",
      "Sub-losses: recon_loss: 3.1761, var_loss: 0.0641, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Training modality: metabolite\n",
      "Training modality: metabolite\n",
      "Epoch 1 - Train Loss: 8.7634\n",
      "Sub-losses: recon_loss: 8.7634, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 3.9133\n",
      "Sub-losses: recon_loss: 3.9133, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 8.6715\n",
      "Sub-losses: recon_loss: 8.6550, var_loss: 0.0165, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 2 - Valid Loss: 4.7811\n",
      "Sub-losses: recon_loss: 4.7797, var_loss: 0.0014, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 3 - Train Loss: 8.5535\n",
      "Sub-losses: recon_loss: 8.1002, var_loss: 0.4533, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 3 - Valid Loss: 4.3375\n",
      "Sub-losses: recon_loss: 4.2919, var_loss: 0.0455, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Found 49 common samples for the stacked autoencoder.\n",
      "Found 7 common samples for the stacked autoencoder.\n",
      "finished training each modality model\n",
      "Epoch 1 - Train Loss: 158.7507\n",
      "Sub-losses: recon_loss: 158.7507, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 95.5126\n",
      "Sub-losses: recon_loss: 95.5126, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 155.0117\n",
      "Sub-losses: recon_loss: 154.9996, var_loss: 0.0121, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 2 - Valid Loss: 99.9265\n",
      "Sub-losses: recon_loss: 99.9193, var_loss: 0.0073, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 3 - Train Loss: 155.6679\n",
      "Sub-losses: recon_loss: 155.3170, var_loss: 0.3509, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 3 - Valid Loss: 96.4424\n",
      "Sub-losses: recon_loss: 96.2611, var_loss: 0.1813, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Found 14 common samples for the stacked autoencoder.\n",
      "<autoencodix.data._numeric_dataset.NumericDataset object at 0x32b628d30>\n",
      "Successfully created annotated latent space object (adata_latent).\n",
      "Modality rna: train=58, valid=7, test=24, total=89\n",
      "Modality protein: train=59, valid=7, test=23, total=89\n",
      "Modality atac: train=58, valid=7, test=24, total=89\n",
      "Modality cytokine: train=57, valid=7, test=25, total=89\n",
      "Modality metabolite: train=57, valid=7, test=25, total=89\n",
      "Integration test passed!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import autoencodix as acx\n",
    "from autoencodix.data.datapackage import DataPackage\n",
    "from autoencodix.configs.stackix_config import StackixConfig\n",
    "from autoencodix.configs.default_config import DataCase\n",
    "\n",
    "# --- Step 1: generate synthetic data ---\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "modalities = [\"rna\", \"protein\", \"atac\", \"cytokine\", \"metabolite\"]\n",
    "\n",
    "n_paired = int(n_samples * 0.65)\n",
    "paired_ids = [f\"sample_{i}\" for i in range(n_paired)]\n",
    "remaining_ids = [f\"sample_{i}\" for i in range(n_paired, n_samples)]\n",
    "\n",
    "data_frames = {}\n",
    "for mod in modalities:\n",
    "    mod_ids = paired_ids + list(np.random.choice(remaining_ids, size=int(len(remaining_ids)*0.7), replace=False))\n",
    "    df = pd.DataFrame(\n",
    "        np.random.randn(len(mod_ids), 10),\n",
    "        index=mod_ids,\n",
    "        columns=[f\"{mod}_feature_{i}\" for i in range(10)]\n",
    "    )\n",
    "    data_frames[mod] = df\n",
    "\n",
    "dp = DataPackage(\n",
    "    multi_bulk=data_frames,\n",
    "    annotation={mod: df.copy() for mod, df in data_frames.items()}\n",
    ")\n",
    "\n",
    "stackix_config = StackixConfig(\n",
    "    data_case=DataCase.MULTI_BULK,\n",
    "    requires_paired=False\n",
    ")\n",
    "\n",
    "stackix = acx.Stackix(data=dp, config=stackix_config)\n",
    "result = stackix.run()\n",
    "\n",
    "for mod in modalities:\n",
    "    train_len = len(result.datasets.train.datasets[mod].sample_ids)\n",
    "    valid_len = len(result.datasets.valid.datasets[mod].sample_ids)\n",
    "    test_len = len(result.datasets.test.datasets[mod].sample_ids)\n",
    "    total_len = train_len + valid_len + test_len\n",
    "    print(f\"Modality {mod}: train={train_len}, valid={valid_len}, test={test_len}, total={total_len}\")\n",
    "    assert total_len == len(data_frames[mod]), \"All samples must be assigned\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This checks if a sample_id does only occur in one split.\n",
    "Ths should be the case because of our pairing aware splitting,\n",
    "so if one sample occurs in multiple data modalites, this sample\n",
    "needs to be in the same split for these data modalites to prevent\n",
    "leakage. If the sample does only occur in one data modality, however,\n",
    "it can also be in only one split, so the following test makes sense\n",
    "\n",
    "\"\"\"\n",
    "all_sample_ids = set().union(*[df.index for df in data_frames.values()])\n",
    "\n",
    "for sid in all_sample_ids:\n",
    "    splits = set()\n",
    "    for split_name, dataset in zip(\n",
    "        [\"train\", \"valid\", \"test\"],\n",
    "        [result.datasets.train, result.datasets.valid, result.datasets.test]\n",
    "    ):\n",
    "        for mod in modalities:\n",
    "            if sid in dataset.datasets[mod].sample_ids:\n",
    "                splits.add(split_name)\n",
    "    assert len(splits) == 1, f\"Sample {sid} appears in multiple splits across modalities\"\n",
    "\n",
    "print(\"Integration test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b901a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST 1: Single Global Annotation ===\n",
      "in handle_direct_user_data with data: <class 'autoencodix.data.datapackage.DataPackage'>\n",
      "--- Running Pairing-Aware Split ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'annotation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 275\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m✓ TEST 3 PASSED: Partial annotation coverage handled correctly\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[43mtest_single_global_annotation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     test_multiple_per_modality_annotations()\n\u001b[1;32m    277\u001b[0m     test_mixed_annotation_coverage()\n",
      "Cell \u001b[0;32mIn[2], line 53\u001b[0m, in \u001b[0;36mtest_single_global_annotation\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m stackix_config \u001b[38;5;241m=\u001b[39m StackixConfig(\n\u001b[1;32m     48\u001b[0m     data_case\u001b[38;5;241m=\u001b[39mDataCase\u001b[38;5;241m.\u001b[39mMULTI_BULK,\n\u001b[1;32m     49\u001b[0m     requires_paired\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     52\u001b[0m stackix \u001b[38;5;241m=\u001b[39m acx\u001b[38;5;241m.\u001b[39mStackix(data\u001b[38;5;241m=\u001b[39mdp, config\u001b[38;5;241m=\u001b[39mstackix_config)\n\u001b[0;32m---> 53\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mstackix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Test 1: All samples in each modality must be assigned to a split\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTest 1.1: All modality samples assigned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_pipeline.py:954\u001b[0m, in \u001b[0;36mBasePipeline.run\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m    942\u001b[0m     \u001b[38;5;28mself\u001b[39m, data: Optional[Union[DatasetContainer, DataPackage]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    943\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Result:\n\u001b[1;32m    944\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Executes the complete pipeline from preprocessing to visualization.\u001b[39;00m\n\u001b[1;32m    945\u001b[0m \n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    Runs all pipeline steps in sequence and returns the result.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;124;03m        Complete pipeline results.\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 954\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    955\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit()\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(data\u001b[38;5;241m=\u001b[39mdata)\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_pipeline.py:437\u001b[0m, in \u001b[0;36mBasePipeline.preprocess\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_user_data()\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocessed_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mraw_user_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_user_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mdatasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datasets\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/data/_stackix_preprocessor.py:57\u001b[0m, in \u001b[0;36mStackixPreprocessor.preprocess\u001b[0;34m(self, raw_user_data)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess\u001b[39m(\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m, raw_user_data: Optional[DataPackage] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DatasetContainer:\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute preprocessing steps for Stackix architecture.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m    Args\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m        TypeError: If datapackage is None after preprocessing\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_datapackage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_general_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_user_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_container \u001b[38;5;241m=\u001b[39m DatasetContainer()\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_preprocessor.py:141\u001b[0m, in \u001b[0;36mBasePreprocessor._general_preprocess\u001b[0;34m(self, raw_user_data, predict_new_data)\u001b[0m\n\u001b[1;32m    139\u001b[0m process_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_process_function(datacase\u001b[38;5;241m=\u001b[39mdatacase)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m process_function:\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_user_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mraw_user_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported data case: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatacase\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_preprocessor.py:341\u001b[0m, in \u001b[0;36mBasePreprocessor._process_multi_bulk_case\u001b[0;34m(self, raw_user_data)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpostsplit_processor\u001b[39m(\n\u001b[1;32m    337\u001b[0m     split_data: Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]],\n\u001b[1;32m    338\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postsplit_multi_bulk(split_data\u001b[38;5;241m=\u001b[39msplit_data)\n\u001b[0;32m--> 341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data_case\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_package\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodality_processors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmulti_bulk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpresplit_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostsplit_processor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_preprocessor.py:220\u001b[0m, in \u001b[0;36mBasePreprocessor._process_data_case\u001b[0;34m(self, data_package, modality_processors)\u001b[0m\n\u001b[1;32m    218\u001b[0m         processed_modality_data \u001b[38;5;241m=\u001b[39m presplit_processor(modality_data)\n\u001b[1;32m    219\u001b[0m         clean_package[modality_key] \u001b[38;5;241m=\u001b[39m processed_modality_data\n\u001b[0;32m--> 220\u001b[0m split_packages, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_data_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_package\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_package\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m processed_splits \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m modality_key, (_, postsplit_processor) \u001b[38;5;129;01min\u001b[39;00m modality_processors\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_preprocessor.py:884\u001b[0m, in \u001b[0;36mBasePreprocessor._split_data_package\u001b[0;34m(self, data_package)\u001b[0m\n\u001b[1;32m    877\u001b[0m data_package_splitter \u001b[38;5;241m=\u001b[39m DataPackageSplitter(\n\u001b[1;32m    878\u001b[0m     data_package\u001b[38;5;241m=\u001b[39mdata_package,\n\u001b[1;32m    879\u001b[0m     config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig,\n\u001b[1;32m    880\u001b[0m     indices\u001b[38;5;241m=\u001b[39msplit_indices_config,\n\u001b[1;32m    881\u001b[0m )\n\u001b[1;32m    883\u001b[0m \u001b[38;5;66;03m# 4. Perform the actual split using the synchronized indices.\u001b[39;00m\n\u001b[0;32m--> 884\u001b[0m split_datasets \u001b[38;5;241m=\u001b[39m \u001b[43mdata_package_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;66;03m# 5. Return both the split data and the indices used, just like your old method.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m split_datasets, split_indices_config\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/data/_datapackage_splitter.py:147\u001b[0m, in \u001b[0;36mDataPackageSplitter.split\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m         result[split] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     result[split] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_split_data_package\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindices\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices,\n\u001b[1;32m    149\u001b[0m     }\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/data/_datapackage_splitter.py:93\u001b[0m, in \u001b[0;36mDataPackageSplitter._split_data_package\u001b[0;34m(self, split)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m     split_data[key] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     94\u001b[0m         modality: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexing(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[key][modality][split])\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m modality, data \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     96\u001b[0m     }\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataPackage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_data)\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/data/_datapackage_splitter.py:94\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     split_data[key] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m---> 94\u001b[0m         modality: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indexing(data, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m[modality][split])\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m modality, data \u001b[38;5;129;01min\u001b[39;00m value\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m     96\u001b[0m     }\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataPackage(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_data)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'annotation'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import autoencodix as acx\n",
    "from autoencodix.data.datapackage import DataPackage\n",
    "from autoencodix.configs.stackix_config import StackixConfig\n",
    "from autoencodix.configs.default_config import DataCase\n",
    "\n",
    "\n",
    "def test_single_global_annotation():\n",
    "    \"\"\"Test with a single global annotation file containing all samples.\"\"\"\n",
    "    print(\"\\n=== TEST 1: Single Global Annotation ===\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    modalities = [\"rna\", \"protein\", \"atac\", \"cytokine\", \"metabolite\"]\n",
    "    \n",
    "    # Create paired and unpaired samples\n",
    "    n_paired = int(n_samples * 0.65)\n",
    "    paired_ids = [f\"sample_{i}\" for i in range(n_paired)]\n",
    "    remaining_ids = [f\"sample_{i}\" for i in range(n_paired, n_samples)]\n",
    "    \n",
    "    # Build modality data frames with partial overlap\n",
    "    data_frames = {}\n",
    "    for mod in modalities:\n",
    "        mod_ids = paired_ids + list(\n",
    "            np.random.choice(remaining_ids, size=int(len(remaining_ids)*0.7), replace=False)\n",
    "        )\n",
    "        df = pd.DataFrame(\n",
    "            np.random.randn(len(mod_ids), 10),\n",
    "            index=mod_ids,\n",
    "            columns=[f\"{mod}_feature_{i}\" for i in range(10)]\n",
    "        )\n",
    "        data_frames[mod] = df\n",
    "    \n",
    "    # Create ONE global annotation containing all unique sample IDs\n",
    "    all_sample_ids = sorted(set().union(*[df.index for df in data_frames.values()]))\n",
    "    global_annotation = pd.DataFrame(\n",
    "        {\"metadata\": np.random.choice([\"A\", \"B\", \"C\"], size=len(all_sample_ids))},\n",
    "        index=all_sample_ids\n",
    "    )\n",
    "    \n",
    "    dp = DataPackage(\n",
    "        multi_bulk=data_frames,\n",
    "        annotation={\"global\": global_annotation}  # Single global annotation\n",
    "    )\n",
    "    \n",
    "    stackix_config = StackixConfig(\n",
    "        data_case=DataCase.MULTI_BULK,\n",
    "        requires_paired=False\n",
    "    )\n",
    "    \n",
    "    stackix = acx.Stackix(data=dp, config=stackix_config)\n",
    "    result = stackix.run()\n",
    "    \n",
    "    # Test 1: All samples in each modality must be assigned to a split\n",
    "    print(\"\\nTest 1.1: All modality samples assigned\")\n",
    "    for mod in modalities:\n",
    "        train_len = len(result.datasets.train.datasets[mod].sample_ids)\n",
    "        valid_len = len(result.datasets.valid.datasets[mod].sample_ids)\n",
    "        test_len = len(result.datasets.test.datasets[mod].sample_ids)\n",
    "        total_len = train_len + valid_len + test_len\n",
    "        print(f\"  {mod}: train={train_len}, valid={valid_len}, test={test_len}, total={total_len}\")\n",
    "        assert total_len == len(data_frames[mod]), f\"All samples in {mod} must be assigned\"\n",
    "    \n",
    "    # Test 2: Pairing-aware splitting - each sample appears in only one split\n",
    "    print(\"\\nTest 1.2: Pairing-aware splitting (no leakage)\")\n",
    "    all_sample_ids_set = set().union(*[df.index for df in data_frames.values()])\n",
    "    for sid in all_sample_ids_set:\n",
    "        splits = set()\n",
    "        for split_name, dataset in zip(\n",
    "            [\"train\", \"valid\", \"test\"],\n",
    "            [result.datasets.train, result.datasets.valid, result.datasets.test]\n",
    "        ):\n",
    "            for mod in modalities:\n",
    "                if sid in dataset.datasets[mod].sample_ids:\n",
    "                    splits.add(split_name)\n",
    "        assert len(splits) == 1, f\"Sample {sid} appears in multiple splits: {splits}\"\n",
    "    print(\"  ✓ All samples appear in exactly one split across modalities\")\n",
    "    \n",
    "    # Test 3: Annotation split contains all assigned samples\n",
    "    print(\"\\nTest 1.3: Global annotation split consistency\")\n",
    "    anno_train = result.datasets.train.annotation[\"global\"]\n",
    "    anno_valid = result.datasets.valid.annotation[\"global\"]\n",
    "    anno_test = result.datasets.test.annotation[\"global\"]\n",
    "    \n",
    "    # Collect all sample IDs from annotation splits\n",
    "    anno_train_ids = set(anno_train.index)\n",
    "    anno_valid_ids = set(anno_valid.index)\n",
    "    anno_test_ids = set(anno_test.index)\n",
    "    \n",
    "    print(f\"  Annotation splits: train={len(anno_train_ids)}, valid={len(anno_valid_ids)}, test={len(anno_test_ids)}\")\n",
    "    \n",
    "    # Check no overlap between annotation splits\n",
    "    assert len(anno_train_ids & anno_valid_ids) == 0, \"Train/valid annotation overlap\"\n",
    "    assert len(anno_train_ids & anno_test_ids) == 0, \"Train/test annotation overlap\"\n",
    "    assert len(anno_valid_ids & anno_test_ids) == 0, \"Valid/test annotation overlap\"\n",
    "    print(\"  ✓ No overlap between annotation splits\")\n",
    "    \n",
    "    # Verify annotation IDs match modality IDs\n",
    "    all_modality_ids = set()\n",
    "    for mod in modalities:\n",
    "        all_modality_ids.update(result.datasets.train.datasets[mod].sample_ids)\n",
    "        all_modality_ids.update(result.datasets.valid.datasets[mod].sample_ids)\n",
    "        all_modality_ids.update(result.datasets.test.datasets[mod].sample_ids)\n",
    "    \n",
    "    all_annotation_ids = anno_train_ids | anno_valid_ids | anno_test_ids\n",
    "    assert all_modality_ids == all_annotation_ids, \"Annotation and modality IDs must match\"\n",
    "    print(\"  ✓ Annotation IDs match modality IDs exactly\")\n",
    "    \n",
    "    print(\"\\n✓ TEST 1 PASSED: Single global annotation works correctly\\n\")\n",
    "\n",
    "\n",
    "def test_multiple_per_modality_annotations():\n",
    "    \"\"\"Test with separate annotation files for each modality.\"\"\"\n",
    "    print(\"\\n=== TEST 2: Multiple Per-Modality Annotations ===\")\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    n_samples = 100\n",
    "    modalities = [\"rna\", \"protein\", \"atac\"]\n",
    "    \n",
    "    # Create paired and unpaired samples\n",
    "    n_paired = int(n_samples * 0.6)\n",
    "    paired_ids = [f\"sample_{i}\" for i in range(n_paired)]\n",
    "    remaining_ids = [f\"sample_{i}\" for i in range(n_paired, n_samples)]\n",
    "    \n",
    "    # Build modality data frames with partial overlap\n",
    "    data_frames = {}\n",
    "    annotations = {}\n",
    "    \n",
    "    for mod in modalities:\n",
    "        # Each modality has different sample coverage\n",
    "        mod_ids = paired_ids + list(\n",
    "            np.random.choice(remaining_ids, size=int(len(remaining_ids)*0.6), replace=False)\n",
    "        )\n",
    "        \n",
    "        # Modality data\n",
    "        df = pd.DataFrame(\n",
    "            np.random.randn(len(mod_ids), 10),\n",
    "            index=mod_ids,\n",
    "            columns=[f\"{mod}_feature_{i}\" for i in range(10)]\n",
    "        )\n",
    "        data_frames[mod] = df\n",
    "        \n",
    "        # Per-modality annotation (matching child key)\n",
    "        anno_df = pd.DataFrame(\n",
    "            {\"label\": np.random.choice([\"typeA\", \"typeB\"], size=len(mod_ids))},\n",
    "            index=mod_ids\n",
    "        )\n",
    "        annotations[mod] = anno_df\n",
    "    \n",
    "    dp = DataPackage(\n",
    "        multi_bulk=data_frames,\n",
    "        annotation=annotations  # Multiple annotations, one per modality\n",
    "    )\n",
    "    \n",
    "    stackix_config = StackixConfig(\n",
    "        data_case=DataCase.MULTI_BULK,\n",
    "        requires_paired=False\n",
    "    )\n",
    "    \n",
    "    stackix = acx.Stackix(data=dp, config=stackix_config)\n",
    "    result = stackix.run()\n",
    "    \n",
    "    # Test 1: All samples assigned\n",
    "    print(\"\\nTest 2.1: All modality samples assigned\")\n",
    "    for mod in modalities:\n",
    "        train_len = len(result.datasets.train.datasets[mod].sample_ids)\n",
    "        valid_len = len(result.datasets.valid.datasets[mod].sample_ids)\n",
    "        test_len = len(result.datasets.test.datasets[mod].sample_ids)\n",
    "        total_len = train_len + valid_len + test_len\n",
    "        print(f\"  {mod}: train={train_len}, valid={valid_len}, test={test_len}, total={total_len}\")\n",
    "        assert total_len == len(data_frames[mod]), f\"All samples in {mod} must be assigned\"\n",
    "    \n",
    "    # Test 2: Pairing-aware splitting\n",
    "    print(\"\\nTest 2.2: Pairing-aware splitting (no leakage)\")\n",
    "    all_sample_ids = set().union(*[df.index for df in data_frames.values()])\n",
    "    for sid in all_sample_ids:\n",
    "        splits = set()\n",
    "        for split_name, dataset in zip(\n",
    "            [\"train\", \"valid\", \"test\"],\n",
    "            [result.datasets.train, result.datasets.valid, result.datasets.test]\n",
    "        ):\n",
    "            for mod in modalities:\n",
    "                if sid in dataset.datasets[mod].sample_ids:\n",
    "                    splits.add(split_name)\n",
    "        assert len(splits) == 1, f\"Sample {sid} appears in multiple splits: {splits}\"\n",
    "    print(\"  ✓ All samples appear in exactly one split across modalities\")\n",
    "    \n",
    "    # Test 3: Each annotation matches its modality\n",
    "    print(\"\\nTest 2.3: Per-modality annotation consistency\")\n",
    "    for mod in modalities:\n",
    "        # Get modality sample IDs from each split\n",
    "        mod_train_ids = set(result.datasets.train.datasets[mod].sample_ids)\n",
    "        mod_valid_ids = set(result.datasets.valid.datasets[mod].sample_ids)\n",
    "        mod_test_ids = set(result.datasets.test.datasets[mod].sample_ids)\n",
    "        \n",
    "        # Get annotation sample IDs from each split\n",
    "        anno_train_ids = set(result.datasets.train.annotation[mod].index)\n",
    "        anno_valid_ids = set(result.datasets.valid.annotation[mod].index)\n",
    "        anno_test_ids = set(result.datasets.test.annotation[mod].index)\n",
    "        \n",
    "        # Verify they match exactly\n",
    "        assert mod_train_ids == anno_train_ids, f\"{mod}: train IDs don't match\"\n",
    "        assert mod_valid_ids == anno_valid_ids, f\"{mod}: valid IDs don't match\"\n",
    "        assert mod_test_ids == anno_test_ids, f\"{mod}: test IDs don't match\"\n",
    "        \n",
    "        print(f\"  {mod}: ✓ Annotation splits match modality splits\")\n",
    "        \n",
    "        # Verify no overlap in annotation splits\n",
    "        assert len(anno_train_ids & anno_valid_ids) == 0, f\"{mod}: train/valid overlap\"\n",
    "        assert len(anno_train_ids & anno_test_ids) == 0, f\"{mod}: train/test overlap\"\n",
    "        assert len(anno_valid_ids & anno_test_ids) == 0, f\"{mod}: valid/test overlap\"\n",
    "    \n",
    "    print(\"\\n✓ TEST 2 PASSED: Multiple per-modality annotations work correctly\\n\")\n",
    "\n",
    "\n",
    "def test_mixed_annotation_coverage():\n",
    "    \"\"\"Test where annotation doesn't cover all modality samples (should drop uncovered).\"\"\"\n",
    "    print(\"\\n=== TEST 3: Partial Annotation Coverage ===\")\n",
    "    \n",
    "    np.random.seed(123)\n",
    "    modalities = [\"rna\", \"protein\"]\n",
    "    \n",
    "    # Create samples\n",
    "    all_ids = [f\"sample_{i}\" for i in range(50)]\n",
    "    annotation_ids = all_ids[:40]  # Annotation only covers 40 samples\n",
    "    \n",
    "    data_frames = {}\n",
    "    for mod in modalities:\n",
    "        # Modalities have all 50 samples\n",
    "        df = pd.DataFrame(\n",
    "            np.random.randn(len(all_ids), 5),\n",
    "            index=all_ids,\n",
    "            columns=[f\"{mod}_f{i}\" for i in range(5)]\n",
    "        )\n",
    "        data_frames[mod] = df\n",
    "    \n",
    "    # Global annotation only covers 40 samples\n",
    "    global_annotation = pd.DataFrame(\n",
    "        {\"info\": [\"x\"] * len(annotation_ids)},\n",
    "        index=annotation_ids\n",
    "    )\n",
    "    \n",
    "    dp = DataPackage(\n",
    "        multi_bulk=data_frames,\n",
    "        annotation={\"meta\": global_annotation}\n",
    "    )\n",
    "    \n",
    "    stackix_config = StackixConfig(\n",
    "        data_case=DataCase.MULTI_BULK,\n",
    "        requires_paired=False\n",
    "    )\n",
    "    \n",
    "    stackix = acx.Stackix(data=dp, config=stackix_config)\n",
    "    result = stackix.run()\n",
    "    \n",
    "    # Test: Only annotated samples should be in splits\n",
    "    print(\"\\nTest 3.1: Only annotated samples included\")\n",
    "    for mod in modalities:\n",
    "        all_split_ids = set()\n",
    "        all_split_ids.update(result.datasets.train.datasets[mod].sample_ids)\n",
    "        all_split_ids.update(result.datasets.valid.datasets[mod].sample_ids)\n",
    "        all_split_ids.update(result.datasets.test.datasets[mod].sample_ids)\n",
    "        \n",
    "        print(f\"  {mod}: {len(all_split_ids)} samples (expected 40)\")\n",
    "        assert len(all_split_ids) == 40, f\"{mod} should only have annotated samples\"\n",
    "        assert all_split_ids == set(annotation_ids), f\"{mod} IDs must match annotation\"\n",
    "    \n",
    "    print(\"  ✓ Unannotated samples correctly dropped\")\n",
    "    \n",
    "    print(\"\\n✓ TEST 3 PASSED: Partial annotation coverage handled correctly\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_single_global_annotation()\n",
    "    test_multiple_per_modality_annotations()\n",
    "    test_mixed_annotation_coverage()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALL TESTS PASSED! ✓\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcceb32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
