{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df474bf2",
   "metadata": {},
   "source": [
    "# How To Use XModalix\n",
    "\n",
    "X-Modalix is our implementation of a cross-modal autoencoder.  \n",
    "This tutorial follows the structure of `Getting Started - Vanillix`, but is less extensive because our pipeline works similarly across different architectures. Here, we focus only on X-Modalix specifics.\n",
    "\n",
    "**AUTOENCODIX** supports far more functionality than shown here, so weâ€™ll also point to advanced tutorials where relevant.  \n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "> This tutorial only shows the specifics of the XModalix pipeline. If you're unfamiliar with general concepts,  \n",
    "> we recommend following the `Getting Started - Vanillix` tutorial first.\n",
    "\n",
    "## XModalix Theory\n",
    "\n",
    "The core idea of a cross-modal autoencoder is to align the latent spaces of different data modalities.  \n",
    "Once the latent distributions are aligned, we can translate between modalities by feeding data from one modality into its encoder and then passing the latent space to the decoder of the target modality.\n",
    "\n",
    "The underlying models are separate VAEsâ€”one for each data modalityâ€”which are trained in parallel. Their latent spaces are closely aligned so that modalities are hard to discriminate, but sample variation is preserved.  \n",
    "To achieve this alignment, the VAE loss function is extended with the following terms:\n",
    "\n",
    "- *Adversarial* loss term: In parallel with the VAEs, a third neural network (latent space classifier) is trained to discriminate the embeddings of different modalities. Analogous to a GAN, the inverse of this classifier loss is added to the VAE loss to enforce latent space alignment.  \n",
    "\n",
    "- *Paired* loss term: Alignment is further enforced by minimizing the distance between samples with paired measurements across modalities. While unpaired samples are allowed, we recommend that most samples are paired (present in all modalities).  \n",
    "\n",
    "- *Class-based* loss term: This semi-supervised term minimizes distances among samples of the same class. Class membership is defined in the annotation data (e.g., cancer type, cell type). In each iteration, the class mean in the latent space is computed, and distances from individual samples to the class mean are minimized. See our `Preprint [TODO]` for more details.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jan-forest/autoencodix/729f3e8172a722e7c3681298719d9d4b59e313d9/images/xmodalix_scheme.svg\" alt=\"xmodalix_scheme\" width=\"1200\"/>\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "As a showcase for data modality translation with X-Modalix, we will use cancer gene expression from TCGA in combination with handwritten digits from the [MNIST dataset](https://keras.io/api/datasets/mnist/).  \n",
    "Our goal is to translate gene expression signatures of five selected cancer subtypes into images of digits, with each cancer subtype class mapped to a digit between 0â€“4.  \n",
    "\n",
    "In practice, these images could be histopathological images or any other data modality.  \n",
    "Before we show data preparation and X-Modalix training, we provide background on cross-modal VAEs as proposed by [Yang & Uhler](https://arxiv.org/abs/1902.03515).  \n",
    "\n",
    "While following this showcase, you will learn how to:\n",
    "\n",
    "1. **Initialize** the pipeline and run it. <br><br>\n",
    "2. Understand the X-Modalix-specific **pipeline steps** (paired vs. unpaired data). <br><br>\n",
    "3. Access X-Modalix-specific **results** (sub-results for modality autoencoders). <br><br>\n",
    "4. **Visualize** and **evaluate** the outputs. <br><br>\n",
    "5. Apply **custom parameters**. <br><br>\n",
    "6. **Save, load, and reuse** a trained pipeline. <br><br>\n",
    "\n",
    "Letâ€™s get started! ðŸš€\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2465f4",
   "metadata": {},
   "source": [
    "## 1) Initialize and Run XModalix\n",
    "\n",
    "In this example, we read our input data from files.  \n",
    "The file locations are defined in the config via a section called `DataConfig` (see details in the Python code below).  \n",
    "\n",
    "We highlight a few custom config parameters for XModalix. For a deeper dive into the config object, see [1]:\n",
    "\n",
    "- `pretrain_epochs`: Before training the full X-Modalix, the sub-VAEs of each modality can be pretrained. This parameter can be set globally for all modalities or individually per modality and specifies the number of pretraining epochs.  \n",
    "- `gamma`: Hyperparameter to weight the adversarial loss term.  \n",
    "- `delta_pair`: Hyperparameter to weight the paired loss term.  \n",
    "- `delta_class`: Hyperparameter to weight the class-based loss term.  \n",
    "\n",
    "### â—â— Requirements: Getting Tutorial Data â—â—\n",
    "\n",
    "You can use the following bash commands to download the data and set up the correct folder structure.  \n",
    "**Assumption:** you are in the root of `autoencodix_package`.\n",
    "\n",
    "```bash\n",
    "mkdir -p data\n",
    "cd data\n",
    "wget \"https://cloud.scadsai.uni-leipzig.de/index.php/s/bq64MaQyZGZfN64/download/XModalix-Tut-data.zip\"\n",
    "unzip XModalix-Tut-data.zip\n",
    "```\n",
    "\n",
    "### Extra 2: Get correct path\n",
    "We assume you are in the root of the package. The following code ensures that the correct paths are used.\n",
    "[1] Tutorials/DeepDives/ConfigTutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4da076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "p = os.getcwd()\n",
    "d = \"autoencodix_package\"\n",
    "if d not in p:\n",
    "    raise FileNotFoundError(f\"'{d}' not found in path: {p}\")\n",
    "os.chdir(os.sep.join(p.split(os.sep)[: p.split(os.sep).index(d) + 1]))\n",
    "print(f\"Changed to: {os.getcwd()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad61c1a7",
   "metadata": {},
   "source": [
    "### Extra 3: Data Assumptions\n",
    "\n",
    "X-Modalix supports two special cases:  \n",
    "(a) image data, and  \n",
    "(b) unpaired data.  \n",
    "\n",
    "This introduces specific requirements for the input data. In the config object, the `DataConfig` must be populated with the relevant information. Example code:\n",
    "\n",
    "```python\n",
    "imganno_file = os.path.join(\"data/XModalix-Tut-data/tcga_mappings.txt\")\n",
    "rna_file = os.path.join(\"data/XModalix-Tut-data/combined_rnaseq_formatted.parquet\")\n",
    "img_root = os.path.join(\"data/XModalix-Tut-data/images/tcga_fake\")\n",
    "clin_file = os.path.join(\"data/XModalix-Tut-data/combined_clin_formatted.parquet\")\n",
    "\n",
    "dc = DataConfig(\n",
    "    data_info={\n",
    "        \"img\": DataInfo(\n",
    "            file_path=img_root,\n",
    "            data_type=\"IMG\",\n",
    "            scaling=\"MINMAX\",\n",
    "            translate_direction=\"to\",\n",
    "            pretrain_epochs=3,\n",
    "            # extra_anno_file=imganno_file,\n",
    "        ),\n",
    "        \"rna\": DataInfo(\n",
    "            file_path=rna_file,\n",
    "            data_type=\"NUMERIC\",\n",
    "            scaling=\"MINMAX\",\n",
    "            translate_direction=\"from\",\n",
    "        ),\n",
    "        \"anno\": DataInfo(\n",
    "            file_path=clin_file,\n",
    "            data_type=\"ANNOTATION\",\n",
    "            sep=\"\\t\"\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "```\n",
    "**Image Data Requirements**\n",
    "\n",
    "- Specify the root folder where the images are located. The preprocessor will automatically read all images and convert them into tensors.  \n",
    "- Supported image extensions (case-insensitive): `\".jpg\"`, `\".jpeg\"`, `\".png\"`, `\".tif\"`, `\".tiff\"`.  \n",
    "- A mapping between each image filename (without directories) and a `sample_id` is required, along with any additional metadata.  \n",
    "\n",
    "To provide this mapping, you must supply a **global annotation file** that contains all `sample_id`s and metadata columns, including one column with the image paths.  \n",
    "- Use the config parameter `img_path_col` to define the name of the column containing image paths (default: `\"img_paths\"`).  \n",
    "- See your `clin_file` for an example.\n",
    "\n",
    "**Translation Direction**\n",
    "\n",
    "- X-Modalix trains a shared latent alignment, enabling translation between all modalities.  \n",
    "- To keep results consistent, users should define a `translate_direction` in the config.  \n",
    "- If you want to change the direction or translate a different pair, you can specify this in the `predict` step (see [Section 3](#3-access-xmodalix-results)).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed3befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import autoencodix as acx\n",
    "\n",
    "from autoencodix.configs.xmodalix_config import XModalixConfig\n",
    "from autoencodix.configs.default_config import DataConfig, DataInfo, DataCase\n",
    "print(f\"Current directory: {os.getcwd()}\")\n",
    "\n",
    "clin_file = os.path.join(\"./data/XModalix-Tut-data/combined_clin_formatted.parquet\")\n",
    "clin_df = pd.read_parquet(clin_file)\n",
    "print(\"First five image paths:\")\n",
    "print(clin_df[\"img_paths\"].to_list()[0:5])\n",
    "print(\"\\n\")\n",
    "clin_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fed079a",
   "metadata": {},
   "source": [
    "Another way is to provide a image-specific annotation file in with the `extra_anno_file` parameter of the DataInfo object in the DataConfig.\n",
    "Here you need to map the sample_id (index) to the image path and can add addtional metadata columns like we did in our `imganno_file`.   \n",
    "> We recommend the first option of a global annotation file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11dfba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "imganno_file = os.path.join(\"data/XModalix-Tut-data/tcga_mappings.txt\")\n",
    "img_anno_df = pd.read_csv(imganno_file, sep=\"\\t\", index_col=0)\n",
    "img_anno_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96e186f",
   "metadata": {},
   "source": [
    "## 1) Initialize and Run XModalix\n",
    "\n",
    "In this section, we demonstrate how to set up the configuration to account for the data requirements and highlight the custom XModalix parameters.  \n",
    "\n",
    "For a more in-depth guide on the configuration parameters and input data, see:  \n",
    "\n",
    "[1] `Tutorials/DeepDives/ConfigTutorial.ipynb`  \n",
    "[2] `Tutorials/DeepDives/InputDataTutorials.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593e79b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rna_file = os.path.join(\"data/XModalix-Tut-data/combined_rnaseq_formatted.parquet\")\n",
    "img_root = os.path.join(\"data/XModalix-Tut-data/images/tcga_fake\")\n",
    "\n",
    "xmodalix_config = XModalixConfig(\n",
    "    checkpoint_interval=100,\n",
    "    class_param=\"CANCER_TYPE\",\n",
    "    epochs=100,\n",
    "    beta=0.1,\n",
    "    gamma=10,\n",
    "    delta_class=100,\n",
    "    delta_pair=300,\n",
    "    latent_dim=6,\n",
    "    k_filter=1000,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.0005,\n",
    "    requires_paired=False,\n",
    "    loss_reduction=\"sum\",\n",
    "    data_case=DataCase.IMG_TO_BULK,\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"img\": DataInfo(\n",
    "                file_path=img_root,\n",
    "                img_height_resize=32,\n",
    "                img_width_resize=32,\n",
    "                data_type=\"IMG\",\n",
    "                scaling=\"STANDARD\",\n",
    "                translate_direction=\"to\",\n",
    "                pretrain_epochs=50,\n",
    "            ),\n",
    "            \"rna\": DataInfo(\n",
    "                file_path=rna_file,\n",
    "                data_type=\"NUMERIC\",\n",
    "                scaling=\"STANDARD\",\n",
    "                pretrain_epochs=0,\n",
    "                translate_direction=\"from\",\n",
    "            ),\n",
    "            \"anno\": DataInfo(file_path=clin_file, data_type=\"ANNOTATION\", sep=\"\\t\"),\n",
    "        },\n",
    "        annotation_columns=[\"CANCER_TYPE_ACRONYM\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "xmodalix = acx.XModalix(config=xmodalix_config)\n",
    "result = xmodalix.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b25ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmodalix.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf72ae",
   "metadata": {},
   "source": [
    "## 2) XModalix Specific Steps\n",
    "\n",
    "XModalix does not introduce additional pipeline steps. However, its preprocessing and training differ slightly from Varix and Vanillix. XModalix processes each data modality individually and therefore supports unpaired data input.  \n",
    "\n",
    "It is also possible to re-run the `predict` step and change the translation pairs or direction by providing the `from_key` and `to_key` arguments with the string keys that define your data modalities (e.g., `\"rna\"` and `\"img\"` in our case). See the code below for details.\n",
    "\n",
    "Furthermore, the visualization and evaluation steps produce outputs that differ somewhat from the other pipelines, which we will focus on in [Section 4](#4-visualize-and-evaluate-xmodalix).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initially, we translated from RNA to image. Now we can use the trained model\n",
    "# to re-run the translation with switched pairs.\n",
    "# ATTENTION: This will overwrite the existing result object, so we create a backup first.\n",
    "from copy import deepcopy\n",
    "\n",
    "backup_result = deepcopy(result)\n",
    "result_flipped = xmodalix.predict(from_key=\"img\", to_key=\"rna\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c18f33",
   "metadata": {},
   "source": [
    "## 3) Access XModalix Results\n",
    "\n",
    "Accessing the results works slightly differently for `XModalix` because we have:\n",
    "\n",
    "- Multiple latent spaces (one for each data modality).\n",
    "\n",
    "- Not just reconstructions, but also `translations` and `reference translations`.  \n",
    "  *Reference translations* are reconstructions within the same data modality as the translation split.  \n",
    "  For example, if we translate from `rna` to `img`, the reference translation is the reconstruction from `img` to `img`.  \n",
    "  These translations can be accessed via the `reconstructions` attribute of the `result` object.  \n",
    "  First, use the usual `TrainingDynamics` API via `.get()`, which returns a dict containing each data modalityâ€™s translation and reference translation.\n",
    "\n",
    "- Multiple losses.  \n",
    "  These are accessed via the `sub_losses` attribute of the `result` object. This is a `Dict` of `TrainingDynamics`.  \n",
    "  First, select the loss type youâ€™re interested in, then work with the standard `TrainingDynamics` API.\n",
    "\n",
    "See the code below for examples and [3] for more details:\n",
    "\n",
    "[3] `Tutorials/DeepDives/PipelineOutputTutorial.ipynb`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599265d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Find available loss types\")\n",
    "print(backup_result.sub_losses.keys())\n",
    "print(\"Get adver_loss for training\")\n",
    "backup_result.sub_losses.get(\"adver_loss\").get(split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b822c090",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Get reconstruction keys\")\n",
    "# Frist define split and epoch you're interested in\n",
    "# usually test split (there are no epochs, so by default this is always epoch=-1)\n",
    "recons = backup_result.reconstructions.get(split=\"test\", epoch=-1)\n",
    "print(recons.keys())\n",
    "print(\"Getting Translation\")\n",
    "trans = recons.get(\"translation\")\n",
    "print(f\"shape of translation: {trans.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161f5101",
   "metadata": {},
   "source": [
    "**We can acutally plot a sample image here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce547d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sample = trans[0, 0, :, :]\n",
    "plt.imshow(sample)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d69d35",
   "metadata": {},
   "source": [
    "Accessing the latent spaces works analogously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3783d19",
   "metadata": {},
   "source": [
    "## 4) Visualize XModalix\n",
    "\n",
    "XModalix results can be inspected by similar plots as other architectures:\n",
    "- loss curves via `show_loss(plot_type=\"absolute|relative\")`\n",
    "- latent spaces via `show_latent_space(plot_type=\"Ridgeline|2D-scatter\")`\n",
    "Latent space plotting will plot for each VAE per modality and can be used to check if latent space alignment across modality was achieved during combined training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97f38d1",
   "metadata": {},
   "source": [
    "### Loss curves\n",
    "Loss curves can be plotted like for all autoencoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmodalix.visualizer.show_loss(plot_type=\"absolute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d28995",
   "metadata": {},
   "source": [
    "In `aggregated_sub_losses` all sub-VAE's are aggregated collecting their reconstruction losses as well as KL-divergence loss term. \n",
    "\n",
    "All sub-losses are stored in sub_loss attribute of the result where you can access individual VAE's like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ce246",
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_result.sub_losses.get(\"multi_bulk.rna.recon_loss\").get(split=\"train\")[0:5] ## First five epochs of training reconstruction loss for RNA modality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba738b1",
   "metadata": {},
   "source": [
    "Due to the complexity of loss terms in XModalix, a relative contribution of loss terms to overall is useful to find best loss term weight combination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce455323",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmodalix.visualizer.show_loss(plot_type=\"relative\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d9717",
   "metadata": {},
   "source": [
    "### Latent space alignment\n",
    "Latent space can be visualized as 2D representation or Ridgeline representation as before. But now for each modality. A good alignment will be visible if they match and show similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e0df3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmodalix.visualizer.show_latent_space(\n",
    "\tresult=backup_result,\n",
    "\tplot_type=\"2D-scatter\"\n",
    "\t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca28955b",
   "metadata": {},
   "source": [
    "### Specific XModalix visualizations\n",
    "To inspect translation quality of XModalix reconstructions we provide two plots:\n",
    "1) `show_2D_translation` shows a 2D representation created by e.g. UMAP of the high-dimensional input data vs. the reconstructed translation using the other modality as input\n",
    "2) `show_image_translation` if your target modality for translation is an image, this can be used to visually compare original images to translated images (cross-modality) and reference images of the image VAE inside the XModalix.\n",
    "A \"good\" XModalix can be seen in 1) if 2D representation of input and translation show high structural similarity and in 2) if translated images look similar to original images from the test-set which was not part of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06124374",
   "metadata": {},
   "source": [
    "#### 2D translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c089fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = xmodalix.visualizer.show_2D_translation(\n",
    "\tresult=backup_result,\n",
    "\ttranslated_modality=\"img.img\",\n",
    "\tparam=\"CANCER_TYPE_ACRONYM\",\n",
    "\treducer=\"PCA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9b0418",
   "metadata": {},
   "source": [
    "#### Image translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a127d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = xmodalix.visualizer.show_image_translation(  # ty: ignore\n",
    "        result=backup_result,\n",
    "        from_key =\"mult_bulk.rna\",\n",
    "        to_key = \"img.img\",\n",
    "\t\tn_sample_per_class = 3,\n",
    "        param = \"CANCER_TYPE\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb203808",
   "metadata": {},
   "source": [
    "## 5) Evaluate \n",
    "\n",
    "### 5.1 Embedding evaluation \n",
    "Beyond visual inspection and representation of XModalix results, you can evaluate VAE embeddings as before with the `evaluate` step. \n",
    "\n",
    "This will perform embedding evaluatation for downstream tasks similarly as for a normal VAE (`varix`). \n",
    "\n",
    "Since XModalix is composed of as many VAE's as modalities, each embedding will be evaluated independently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5cc0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "my_classifier = SVC(kernel=\"rbf\", probability=True) # You can provide any sklearn classifier here\n",
    "my_metric = \"roc_auc_ovo\" # You can provide any sklearn metric here\n",
    "\n",
    "params = [\"CANCER_TYPE_ACRONYM\"]\n",
    "result = xmodalix.evaluate(\n",
    "\tml_model_class = my_classifier,\n",
    "\tparams=params,\n",
    "\tmetric_class = my_metric,\n",
    "\treference_methods= [\"PCA\"] # We compare XModalix embeddings to PCA embeddings\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df65aa0",
   "metadata": {},
   "source": [
    "Then we can visualize the evaluation results as bar plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd579c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = xmodalix.visualizer.show_evaluation(\n",
    "\tparam=params[0], \t# Only one param at a time\n",
    "\tmetric = my_metric,\t\t# Needs to be specificied in case multiple metrics were used\n",
    "\tml_alg=str(my_classifier),\t# Needs to be specificied in case multiple classifiers were used\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20633f0",
   "metadata": {},
   "source": [
    "We can see that both image VAE (bottom, right) and RNA VAE (top, right) latent space embeddings have similar predictive power for cancer type classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c1bba6",
   "metadata": {},
   "source": [
    "### 5.2 Comparison to a Imagix\n",
    "Since XModalix VAE's for each modalities are coupled by a joint loss, we offer a function `xmodalix.evaluator.pure_vae_comparison()` for the image case which will compare translated (`from` -> `to` modality) and reference (`to` -> `to`) image reconstructions of XModalix with a pure image VAE (Imagix) reconstruction capability. \n",
    "\n",
    "Prior comparison this pure Imagix needs to be trained on the same data and ideally with a similar config as the XModalix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e44dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.configs.default_config import DefaultConfig\n",
    "\n",
    "imagix_config = \tDefaultConfig(\n",
    "    checkpoint_interval=100,\n",
    "    epochs=100,\n",
    "    beta=0.1,\n",
    "    latent_dim=6,\n",
    "    batch_size=512,\n",
    "    learning_rate=0.0005,\n",
    "    loss_reduction=\"sum\",\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"img\": DataInfo(\n",
    "                file_path=img_root,\n",
    "                img_height_resize=32,\n",
    "                img_width_resize=32,\n",
    "                data_type=\"IMG\",\n",
    "                scaling=\"STANDARD\",\n",
    "            ),\n",
    "            \"anno\": DataInfo(file_path=clin_file, data_type=\"ANNOTATION\", sep=\"\\t\"),\n",
    "        },\n",
    "        annotation_columns=[\"CANCER_TYPE_ACRONYM\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagix = acx.Imagix(config=imagix_config)\n",
    "imagix_result = imagix.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c075642b",
   "metadata": {},
   "source": [
    "Before we make the comparison, we can quickly check the imagix reconstruction capability with `imagix.visualizer.show_image_recon_grid()` showing original and reconstructed images from the test-split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f599d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_imagix = imagix.visualizer.show_image_recon_grid(\n",
    "\tresult=imagix_result,\n",
    "\tn_samples = 10, # Number of (random) test samples to show\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6ae9a",
   "metadata": {},
   "source": [
    "Looks good with small deviations. Let's compare with the XModalix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7ccc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = \"CANCER_TYPE\" # Aggregate over a metadata parameter\n",
    "fig_pure_comparison, df_comparison = xmodalix.evaluator.pure_vae_comparison(\n",
    "\txmodalix_result = backup_result, # The XModalix for comparison\n",
    "\tpure_vae_result = imagix.result, # The pure VAE (Imagix) for comparison\n",
    "\tto_key = \"img.img\", # The translated modality key\n",
    "\tparam = param # The metadata parameter to aggregate over. If None, all test samples are shown individually\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca3a0e",
   "metadata": {},
   "source": [
    "A visualization is created:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c49836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_pure_comparison.get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02aef37",
   "metadata": {},
   "source": [
    "## 6) Apply Custom Parameters\n",
    "Due to the more complex training, we have more config parameters to customize. Here we go over the different `loss terms` and `pretraining`.\n",
    "```python\n",
    "    gamma: float = Field(\n",
    "        default=10.0,\n",
    "        ge=0,\n",
    "        description=\"Gamma weighting factor for Adversial Loss Term i.e. for XModal Classfier training\",\n",
    "    )\n",
    "    delta_pair: float = Field(\n",
    "        default=5.0,\n",
    "        ge=0,\n",
    "        description=\"Delta weighting factor for paired loss term in XModale Training\",\n",
    "    )\n",
    "    delta_class: float = Field(\n",
    "        default=5.0,\n",
    "        ge=0,\n",
    "        description=\"Delta weighting factor for class loss term in XModale Training\",\n",
    "    )\n",
    "```\n",
    "The correct parametrization of these terms depends heavily on your data. We provide sensible default values in our `XModalixConfig` class, which are a good starting point. You can investigate the loss plots to see how much each loss contributes. For example, if your goal is to strongly preserve class information in the latent embedding, you would set `delta_class` higher. If you care more about generalization, you might set `gamma` higher.\n",
    "\n",
    "Another important factor is pretraining. Before training a modality within the full XModalix architecture, we can pretrain that modality in its subnetwork without the XModalix constraints (adversarial, paired, class loss). This can be controlled via the `pretrain_epochs` parameter, which can be set either globally for all data modalities or individually per modality. Both options are shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f47d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencodix as acx\n",
    "from autoencodix.configs.xmodalix_config import XModalixConfig\n",
    "from autoencodix.configs.default_config import DataConfig, DataInfo, DataCase\n",
    "\n",
    "\n",
    "xmodalix_config = XModalixConfig(\n",
    "    class_param=\"CANCER_TYPE_ACRONYM\",\n",
    "    gamma=2,\n",
    "    delta_class=10.0,  # increase this if you care about class info in embedding\n",
    "    delta_pair=3.0,\n",
    "    requires_paired=False,\n",
    "    pretrain_epochs=2,  # can be overridden on data level, see below\n",
    "    data_case=DataCase.IMG_TO_BULK,\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"img\": DataInfo(\n",
    "                file_path=\"YOUR_PATH\",\n",
    "                data_type=\"IMG\",\n",
    "                translate_direction=\"to\",\n",
    "                pretrain_epochs=2,  # this overrides global pretraining_epochs\n",
    "            ),\n",
    "            \"rna\": DataInfo(\n",
    "                file_path=\"YOUR_PATH\",\n",
    "                data_type=\"NUMERIC\",\n",
    "                translate_direction=\"from\",\n",
    "            ),\n",
    "            \"anno\": DataInfo(file_path=\"YOUR_PATH\", data_type=\"ANNOTATION\", sep=\"\\t\"),\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d665447",
   "metadata": {},
   "source": [
    "## 7) Save, Load and Re-use XModalix\n",
    "\n",
    "This works for `XModalix` as for any other models, by using the `save` and `load` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3573f6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "outpath = os.path.join(\"xmodalix\")\n",
    "xmodalix.save(file_path=outpath, save_all=False)\n",
    "\n",
    "folder = os.path.dirname(outpath)\n",
    "pkl_files = glob.glob(os.path.join(folder, \"*.pkl\"))\n",
    "model_files = glob.glob(os.path.join(folder, \"*.pth\"))\n",
    "\n",
    "print(\"PKL files:\", pkl_files)\n",
    "print(\"Model files:\", model_files)\n",
    "\n",
    "# the load functionality automatically will build the pipeline object out of the three saved files\n",
    "xmodalix_loaded = acx.XModalix.load(outpath)\n",
    "# now you can use the model to predict with a different pair again:\n",
    "testdata = backup_result.datasets\n",
    "r = xmodalix_loaded.predict(data=testdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencodix_package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
