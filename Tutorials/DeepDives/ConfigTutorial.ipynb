{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3271f9b8",
   "metadata": {},
   "source": [
    "# Config Deep Dive\n",
    "Our config class is the central point to customize our  `AUTOENCODIX` pipelines. This notebook is more a reference document than a tutorial as we mainly will list config parameters with explanations and default values and only have a few coding parts.\n",
    "\n",
    "**IMPORTANT**\n",
    "> This tutorial explains specific concepts of our config class. If you're unfamilar with general concepts,  \n",
    "> we recommend to follow the `Getting Started - Vanillix` Tutorial first.\n",
    "\n",
    "## What You'll Learn\n",
    "We'll cover the following points:\n",
    "- The two ways to provide a config:\n",
    "  - as instance of our config class\n",
    "  - as a `YAML` file\n",
    "- Which config parameters you should set\n",
    "- The two parts of our config:\n",
    "  - main config\n",
    "  - data config\n",
    "- Pipeline specific config parameters\n",
    "- Default config reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b65fae",
   "metadata": {},
   "source": [
    "## 1) How to Provide a Config\n",
    "The main way is to pass an instance of our `DefaultConfig` class to the pipeline object as you've seen many times in the pipeline tutorials.\n",
    "#### 1.1 Provide Config as a Class Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ebce55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in handle_direct_user_data with data: <class 'autoencodix.data.datapackage.DataPackage'>\n"
     ]
    }
   ],
   "source": [
    "import autoencodix as acx\n",
    "from autoencodix.configs.default_config import DefaultConfig, DataCase\n",
    "from autoencodix.configs import VarixConfig\n",
    "from autoencodix.utils.example_data import EXAMPLE_MULTI_BULK\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "config = VarixConfig(\n",
    "    latent_dim=8, scaling=\"MINMAX\", data_case=DataCase.MULTI_BULK, epochs=10\n",
    ")\n",
    "varix = acx.Varix(config=config, data=EXAMPLE_MULTI_BULK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bf77e",
   "metadata": {},
   "source": [
    "#### 1.2 Provide the Config as a YAML file\n",
    "In our GitHub repo, we prepared a directory called `configs` with sample yaml files.  \n",
    "We can easily load the values into our config class with the `model_validate` method as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b32c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to: /Users/maximilianjoas/development/autoencodix_package\n",
      "reading parquet: data/raw/mini/bulk/clinical_sample_data.parquet\n",
      "anno key: paired\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianjoas/development/autoencodix_package/src/autoencodix/base/_base_trainer.py:126: UserWarning: increased batch_size to 33 for validset, to avoid dropping samples and having batches (makes trainingdynamics messy with missing samples per epoch) of size one (fails for Models with BachNorm)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 28.4991\n",
      "Sub-losses: recon_loss: 28.4991, var_loss: 0.0000, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 10695960690688.0000\n",
      "Sub-losses: recon_loss: 10695960690688.0000, var_loss: 13216.0996, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 136.1494\n",
      "Sub-losses: recon_loss: 112.1679, var_loss: 23.9815, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 2 - Valid Loss: 174949908480.0000\n",
      "Sub-losses: recon_loss: 174941552640.0000, var_loss: 8359107.0000, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 3 - Train Loss: 6245.3555\n",
      "Sub-losses: recon_loss: 32.6682, var_loss: 6212.6875, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 3 - Valid Loss: 17513809920.0000\n",
      "Sub-losses: recon_loss: 17256136704.0000, var_loss: 257672928.0000, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n"
     ]
    }
   ],
   "source": [
    "# first be sure to be in root\n",
    "import os\n",
    "\n",
    "p = os.getcwd()\n",
    "d = \"autoencodix_package\"\n",
    "if d not in p:\n",
    "    raise FileNotFoundError(f\"'{d}' not found in path: {p}\")\n",
    "os.chdir(os.sep.join(p.split(os.sep)[: p.split(os.sep).index(d) + 1]))\n",
    "print(f\"Changed to: {os.getcwd()}\")\n",
    "\n",
    "# now we can load the config\n",
    "custom_config = VarixConfig.model_validate(\n",
    "    {\n",
    "        **yaml.safe_load(Path(\"configs/multi_bulk.yaml\").read_text()),\n",
    "        \"learning_rate\": 0.77,\n",
    "    }\n",
    ")\n",
    "# and pass to a pipeline\n",
    "varix = acx.Varix(config=custom_config)\n",
    "r = varix.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6fe79",
   "metadata": {},
   "source": [
    "**Note** \n",
    "> We can also go the reverse way and save our config object to a yaml file as shown below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc7cf6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved config to /Users/maximilianjoas/development/autoencodix_package/Tutorials/DeepDives/default_config.yaml\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"default_config.yaml\")\n",
    "\n",
    "# Convert to plain Python dict first\n",
    "config_dict = config.model_dump()\n",
    "# Write YAML with nice formatting\n",
    "with output_path.open(\"w\") as f:\n",
    "    yaml.dump(config_dict, f, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "print(f\"✅ Saved config to {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be346beb",
   "metadata": {},
   "source": [
    "## 2) Which Config Parameter to Set\n",
    "\n",
    "To make **AUTOENCODIX** easily usable, we provide sensible defaults per pipeline, so you don't need to think of and set 30+ config params before starting.  \n",
    "However, there are a few parameters you should consider — and depending on the pipeline and data type, a few mandatory ones you’ll need to set.\n",
    "\n",
    "\n",
    "### 2.1) Mandatory Config Parameters\n",
    "*(These depend on your pipeline — see specific documentation for details.)*\n",
    "\n",
    "\n",
    "### 2.2) Parameters to Consider\n",
    "\n",
    "Although we set sensible defaults per pipeline for these parameters, it might make sense to adjust the following values:\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `latent_dim` | `int` | Dimension of the latent space |\n",
    "| `epochs` | `int` | Number of training epochs |\n",
    "| `batch_size` | `int` | Number of samples per batch (must be >1 due to BatchNorm) |\n",
    "| `filtering` | `Literal['VAR', 'MAD', 'CORR', 'VARCORR', 'NOFILT', 'NONZEROVAR']` | Feature filtering method |\n",
    "| `scaling` | `Literal['STANDARD', 'MINMAX', 'ROBUST', 'MAXABS', 'NONE']` | How to scale your input data; should be compatible with your loss function |\n",
    "| `k_filter` | `Optional[int]` | Number of features to keep |\n",
    "| `learning_rate` | `float` | Learning rate for optimization |\n",
    "| `reconstruction_loss` | `Literal['mse', 'bce']` | Type of reconstruction loss: `mse` = Mean Squared Error, `bce` = Binary Cross-Entropy |\n",
    "| `default_vae_loss` | `Literal['kl', 'mmd']` | Type of VAE loss: `kl` = Kullback-Leibler Divergence, `mmd` = Maximum Mean Discrepancy |\n",
    "| `beta` | `float` | β weighting factor for VAE loss |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9636ab",
   "metadata": {},
   "source": [
    "## 3) General Config and Data Config\n",
    "Our config class consists of two parts: (a) a general part where we set global parameters and (b) a `DataConfig` where we can set params for each data type indivdually.  \n",
    "Inside the `DataConfig` we have a `DataInfo` object where we can set the parameters, see a minimal code example below.  \n",
    "You can a find a full list of parameters in [section 5.1](#51-dataconfig--configuration-parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa2b676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in handle_direct_user_data with data: <class 'autoencodix.data.datapackage.DataPackage'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import autoencodix as acx\n",
    "from autoencodix.configs.default_config import DataConfig, DataInfo, DataCase\n",
    "from autoencodix.configs import VarixConfig\n",
    "from autoencodix.utils.example_data import raw_protein, raw_rna, annotation\n",
    "from autoencodix.data import DataPackage\n",
    "\n",
    "\n",
    "my_dp = DataPackage(\n",
    "    multi_bulk={\"rna\": raw_rna, \"protein\": raw_protein}, annotation={\"anno\": annotation}\n",
    ")\n",
    "data_config = DataConfig(\n",
    "    data_info={\n",
    "        \"rna\": DataInfo(scaling=\"MINMAX\", data_type=\"NUMERIC\"),\n",
    "        \"protein\": DataInfo(scaling=\"MINMAX\", data_type=\"NUMERIC\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "bulk_config = VarixConfig(data_config=data_config, data_case=DataCase.MULTI_BULK)\n",
    "varix = acx.Varix(config=bulk_config, data=my_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057235c",
   "metadata": {},
   "source": [
    "## 4) Pipeline Specific Configs\n",
    "We have one `DefaultConfig` class that defines all configurable parameters and sets sensible defaults.\n",
    "However, not all pipelines (Varix, XModalix, etc) should have the same sensible defaults.\n",
    "Thus each pipeline has its own config class that inherits from `DefaultConfig` and overrides for some default values.\n",
    "Be aware of that when you build a config object that you use the appropriate config for the pipeline.\n",
    "The config can be imported from `autoencodix.configs`. The naming is <pipeline-name>Config for example: `VarixConfig` or `OntixConfig`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec11183",
   "metadata": {},
   "source": [
    "## 5)  Configuration Parameters Reference\n",
    "We list all configurable parameters sorted into functional sections below:\n",
    "\n",
    "#### **Data Configuration**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `data_config` | `autoencodix.configs.default_config.DataConfig` | Contains detailed information about each data modality (see `DataConfig` section) |\n",
    "| `img_path_col` | `str` | When working with images, defines the column name containing image paths per sample |\n",
    "| `requires_paired` | `Optional[bool]` | Indicates whether samples for xmodalix are paired (based on sample ID) |\n",
    "| `data_case` | `Optional[DataCase]` | Data case for the model (auto-determined) |\n",
    "| `k_filter` | `Optional[int]` | Number of features to keep |\n",
    "| `scaling` | `Literal['STANDARD', 'MINMAX', 'ROBUST', 'MAXABS', 'NONE']` | Global scaling setting (can be overridden per modality) |\n",
    "| `skip_preprocessing` | `bool` | Skip scaling, filtering, and cleaning |\n",
    "| `class_param` | `Optional[str]` | Optional column name for class labels |\n",
    "\n",
    "\n",
    "#### **Model Architecture**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `latent_dim` | `int` | Dimension of the latent space |\n",
    "| `n_layers` | `int` | Number of encoder/decoder layers (excluding latent layer) |\n",
    "| `enc_factor` | `int` | Encoder dimension scaling factor |\n",
    "| `input_dim` | `int` | Input feature dimension |\n",
    "| `drop_p` | `float` | Dropout probability |\n",
    "| `save_memory` | `bool` | Skip storing `TrainingDynamics` to save memory |\n",
    "\n",
    "\n",
    "#### **Training Hyperparameters**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `learning_rate` | `float` | Learning rate for optimization |\n",
    "| `batch_size` | `int` | Samples per batch (>1 required due to BatchNorm) |\n",
    "| `epochs` | `int` | Number of training epochs |\n",
    "| `weight_decay` | `float` | L2 regularization factor |\n",
    "| `reconstruction_loss` | `Literal['mse', 'bce']` | Type of reconstruction loss |\n",
    "| `default_vae_loss` | `Literal['kl', 'mmd']` | Type of VAE loss |\n",
    "| `loss_reduction` | `Literal['sum', 'mean']` | Loss reduction mode in PyTorch |\n",
    "| `beta` | `float` | β weight for VAE loss |\n",
    "| `beta_mi` | `float` | β weight for mutual information term |\n",
    "| `beta_tc` | `float` | β weight for total correlation term |\n",
    "| `beta_dimKL` | `float` | β weight for dimension-wise KL |\n",
    "| `use_mss` | `bool` | Use minibatch stratified sampling for disentangled VAE loss |\n",
    "| `gamma` | `float` | γ weight for adversarial loss (XModalix classifier) |\n",
    "| `delta_pair` | `float` | δ weight for paired loss (XModalix training) |\n",
    "| `delta_class` | `float` | δ weight for class loss (XModalix training) |\n",
    "| `anneal_function` | `Literal['5phase-constant', '3phase-linear', '3phase-log', 'logistic-mid', 'logistic-early', 'logistic-late', 'no-annealing']` | Annealing function strategy for VAE loss |\n",
    "| `pretrain_epochs` | `int` | Number of pretraining epochs (can differ per modality) |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Device & Performance**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `device` | `Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']` | Compute device |\n",
    "| `n_gpus` | `int` | Number of GPUs to use |\n",
    "| `checkpoint_interval` | `int` | Checkpoint save interval |\n",
    "| `float_precision` | `Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16']` | Floating-point precision |\n",
    "| `gpu_strategy` | `Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']` | GPU parallelization strategy |\n",
    "\n",
    "\n",
    "#### **Data Splits & Reproducibility**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `train_ratio` | `float` | Training split ratio |\n",
    "| `test_ratio` | `float` | Test split ratio |\n",
    "| `valid_ratio` | `float` | Validation split ratio |\n",
    "| `min_samples_per_split` | `int` | Minimum samples per split |\n",
    "| `reproducible` | `bool` | Ensure reproducibility |\n",
    "| `global_seed` | `int` | Global random seed |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f4512",
   "metadata": {},
   "source": [
    "#### 5.1 DataConfig — Configuration Parameters\n",
    "\n",
    "---\n",
    "\n",
    "##### **DataConfig**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `data_info` | `Dict[str, DataInfo]` | Dictionary mapping modality names (e.g. `\"RNA\"`, `\"IMG\"`) to their `DataInfo` configuration |\n",
    "| `require_common_cells` | `Optional[bool]` | Whether to require that all data modalities share a common set of cells/samples |\n",
    "| `annotation_columns` | `Optional[List[str]]` | List of column names from the annotation file to include as metadata |\n",
    "\n",
    "###### **DataInfo**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `file_path` | `str` | Path to the raw data file |\n",
    "| `data_type` | `Literal['NUMERIC', 'CATEGORICAL', 'IMG', 'ANNOTATION']` | Type of data modality |\n",
    "| `scaling` | `Literal['STANDARD', 'MINMAX', 'ROBUST', 'MAXABS', 'NONE', 'NOTSET']` | Overrides the globally set scaling method for this modality |\n",
    "| `filtering` | `Literal['VAR', 'MAD', 'CORR', 'VARCORR', 'NOFILT', 'NONZEROVAR']` | Feature filtering method |\n",
    "| `sep` | `Optional[str]` | Delimiter for CSV/TSV input files (passed to `pandas.read_csv`) |\n",
    "| `extra_anno_file` | `Optional[str]` | Path to an additional annotation file |\n",
    "\n",
    "\n",
    "**Single-Cell Specific Parameters**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `is_single_cell` | `bool` | Whether the dataset represents single-cell data |\n",
    "| `min_cells` | `float` | Minimum fraction of cells in which a gene must be expressed to be kept (filters rare genes) |\n",
    "| `min_genes` | `float` | Minimum fraction of genes a cell must express to be kept (filters low-quality cells) |\n",
    "| `selected_layers` | `List[str]` | Layers to include from the single-cell dataset; must always include `\"X\"` |\n",
    "| `is_X` | `bool` | Whether the data originates from the `\"X\"` matrix only |\n",
    "| `normalize_counts` | `bool` | Whether to normalize single-cell counts by total expression per cell |\n",
    "| `log_transform` | `bool` | Whether to apply `log1p` transformation after normalization |\n",
    "| `k_filter` | `Optional[int]` | Automatically set based on global config; do not override manually |\n",
    "\n",
    "\n",
    "**Image-Specific Parameters**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `img_width_resize` | `Optional[int]` | Target width for image resizing (must equal height) |\n",
    "| `img_height_resize` | `Optional[int]` | Target height for image resizing (must equal width) |\n",
    "\n",
    "\n",
    "**XModalix & Translation Parameters**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `translate_direction` | `Optional[Literal['from', 'to']]` | Defines translation direction in cross-modal (XModalix) training |\n",
    "| `pretrain_epochs` | `int` | Number of pretraining epochs specific to this modality (overrides global pretraining setting) |\n",
    "\n",
    "\n",
    "**Validation Rules**\n",
    "\n",
    "- `selected_layers` must always contain `\"X\"`.  \n",
    "- `img_width_resize` and `img_height_resize` must be **positive integers** and **equal** (enforces square resizing).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60c8cf",
   "metadata": {},
   "source": [
    "## 6) List Config Parameters Dynamically.\n",
    "\n",
    "If want to see all available parameters directly via Python, you can call `<config-instance>.print_schema()` as shown belos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76139a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DefaultConfig Configuration Parameters:\n",
      "--------------------------------------------------\n",
      "\n",
      "data_config:\n",
      "  Type: <class 'autoencodix.configs.default_config.DataConfig'>\n",
      "  Default: data_info={} require_common_cells=False annotation_columns=None\n",
      "  Description: No description available\n",
      "\n",
      "img_path_col:\n",
      "  Type: <class 'str'>\n",
      "  Default: img_paths\n",
      "  Description: When working with images, we except a column in your annotation file that specifies the path of the image for a particular sample. Here you can define the name of this column\n",
      "\n",
      "requires_paired:\n",
      "  Type: typing.Optional[bool]\n",
      "  Default: PydanticUndefined\n",
      "  Description: Indicator if the samples for the xmodalix are paired, based on some sample id\n",
      "\n",
      "data_case:\n",
      "  Type: typing.Optional[autoencodix.configs.default_config.DataCase]\n",
      "  Default: PydanticUndefined\n",
      "  Description: Data case for the model, will be determined automatically\n",
      "\n",
      "k_filter:\n",
      "  Type: typing.Optional[int]\n",
      "  Default: 20\n",
      "  Description: Number of features to keep\n",
      "\n",
      "scaling:\n",
      "  Type: typing.Literal['STANDARD', 'MINMAX', 'ROBUST', 'MAXABS', 'NONE']\n",
      "  Default: STANDARD\n",
      "  Description: Setting the scaling here for all data modalities, can per overruled by setting scaling at data modality level per data modality\n",
      "\n",
      "skip_preprocessing:\n",
      "  Type: <class 'bool'>\n",
      "  Default: False\n",
      "  Description: If set don't scale, filter or clean the input data.\n",
      "\n",
      "class_param:\n",
      "  Type: typing.Optional[str]\n",
      "  Default: None\n",
      "  Description: No description available\n",
      "\n",
      "latent_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 16\n",
      "  Description: Dimension of the latent space\n",
      "\n",
      "n_layers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of layers in encoder/decoder, without latent layer. If 0, is only the latent layer.\n",
      "\n",
      "enc_factor:\n",
      "  Type: <class 'int'>\n",
      "  Default: 4\n",
      "  Description: Scaling factor for encoder dimensions\n",
      "\n",
      "input_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 10000\n",
      "  Description: Input dimension\n",
      "\n",
      "drop_p:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Dropout probability\n",
      "\n",
      "save_memory:\n",
      "  Type: <class 'bool'>\n",
      "  Default: False\n",
      "  Description: If set to True we don't store TrainingDynamics\n",
      "\n",
      "learning_rate:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.001\n",
      "  Description: Learning rate for optimization\n",
      "\n",
      "batch_size:\n",
      "  Type: <class 'int'>\n",
      "  Default: 32\n",
      "  Description: Number of samples per batch, has to be > 1, because we use BatchNorm() Layer\n",
      "\n",
      "epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of training epochs\n",
      "\n",
      "weight_decay:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.01\n",
      "  Description: L2 regularization factor\n",
      "\n",
      "reconstruction_loss:\n",
      "  Type: typing.Literal['mse', 'bce']\n",
      "  Default: mse\n",
      "  Description: Type of reconstruction loss\n",
      "\n",
      "default_vae_loss:\n",
      "  Type: typing.Literal['kl', 'mmd']\n",
      "  Default: kl\n",
      "  Description: Type of VAE loss\n",
      "\n",
      "loss_reduction:\n",
      "  Type: typing.Literal['sum', 'mean']\n",
      "  Default: sum\n",
      "  Description: Loss reduction in PyTorch i.e in torch.nn.functional.binary_cross_entropy_with_logits(reduction=loss_reduction)\n",
      "\n",
      "beta:\n",
      "  Type: <class 'float'>\n",
      "  Default: 1\n",
      "  Description: Beta weighting factor for VAE loss\n",
      "\n",
      "beta_mi:\n",
      "  Type: <class 'float'>\n",
      "  Default: 1\n",
      "  Description: Beta weighting factor for mutual information term in disentangled VAE loss\n",
      "\n",
      "beta_tc:\n",
      "  Type: <class 'float'>\n",
      "  Default: 1\n",
      "  Description: Beta weighting factor for total correlation term in disentangled VAE loss\n",
      "\n",
      "beta_dimKL:\n",
      "  Type: <class 'float'>\n",
      "  Default: 1\n",
      "  Description: Beta weighting factor for dimension-wise KL in disentangled VAE loss\n",
      "\n",
      "use_mss:\n",
      "  Type: <class 'bool'>\n",
      "  Default: True\n",
      "  Description: Using minibatch stratified sampling for disentangled VAE loss calculation (faster estimation)\n",
      "\n",
      "gamma:\n",
      "  Type: <class 'float'>\n",
      "  Default: 10.0\n",
      "  Description: Gamma weighting factor for Adversial Loss Term i.e. for XModalix Classfier training\n",
      "\n",
      "delta_pair:\n",
      "  Type: <class 'float'>\n",
      "  Default: 5.0\n",
      "  Description: Delta weighting factor for paired loss term in XModalix Training\n",
      "\n",
      "delta_class:\n",
      "  Type: <class 'float'>\n",
      "  Default: 5.0\n",
      "  Description: Delta weighting factor for class loss term in XModalix Training\n",
      "\n",
      "min_samples_per_split:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Minimum number of samples per split\n",
      "\n",
      "anneal_function:\n",
      "  Type: typing.Literal['5phase-constant', '3phase-linear', '3phase-log', 'logistic-mid', 'logistic-early', 'logistic-late', 'no-annealing']\n",
      "  Default: logistic-mid\n",
      "  Description: Annealing function strategy for VAE loss scheduling\n",
      "\n",
      "pretrain_epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 0\n",
      "  Description: Number of pretraining epochs, can be overwritten in DataInfo to have different number of pretraining epochs for each data modality\n",
      "\n",
      "device:\n",
      "  Type: typing.Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']\n",
      "  Default: auto\n",
      "  Description: Device to use\n",
      "\n",
      "n_gpus:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Number of GPUs to use\n",
      "\n",
      "n_workers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 0\n",
      "  Description: Number of data loading workers\n",
      "\n",
      "checkpoint_interval:\n",
      "  Type: <class 'int'>\n",
      "  Default: 10\n",
      "  Description: Interval for saving checkpoints\n",
      "\n",
      "float_precision:\n",
      "  Type: typing.Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16']\n",
      "  Default: 32\n",
      "  Description: Floating point precision\n",
      "\n",
      "gpu_strategy:\n",
      "  Type: typing.Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']\n",
      "  Default: auto\n",
      "  Description: GPU parallelization strategy\n",
      "\n",
      "train_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.7\n",
      "  Description: Ratio of data for training\n",
      "\n",
      "test_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.2\n",
      "  Description: Ratio of data for testing\n",
      "\n",
      "valid_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Ratio of data for validation\n",
      "\n",
      "reproducible:\n",
      "  Type: <class 'bool'>\n",
      "  Default: False\n",
      "  Description: Whether to ensure reproducibility\n",
      "\n",
      "global_seed:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Global random seed\n"
     ]
    }
   ],
   "source": [
    "config = DefaultConfig()\n",
    "config.print_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bd8bd8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencodix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
