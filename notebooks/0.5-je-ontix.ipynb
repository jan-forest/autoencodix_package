{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ewald/Github/autoencodix_package/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/ewald/Github/autoencodix_package/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from autoencodix.utils.example_data import (\n",
    "    EXAMPLE_MULTI_BULK,\n",
    "    EXAMPLE_MULTI_SC,\n",
    "    EXAMPLE_PROCESSED_DATA,\n",
    ")\n",
    "\n",
    "# EXAMPLE_DATA hold PyTorch Datasets (child with extra info) with metdata for train, test and valid splits\n",
    "processed_data = EXAMPLE_PROCESSED_DATA\n",
    "raw_bulk = EXAMPLE_MULTI_BULK\n",
    "raw_sc = EXAMPLE_MULTI_SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencodix as acx\n",
    "from autoencodix.utils.default_config import DefaultConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ont_lvl1 = dict()\n",
    "ont_lvl2 = dict()\n",
    "\n",
    "ont_lvl1[\"pwy-1\"] = [\"sub-pwy-1\", \"sub-pwy-2\"]\n",
    "ont_lvl1[\"pwy-2\"] = [\"sub-pwy-3\", \"sub-pwy-2\"]\n",
    "\n",
    "ont_lvl2[\"sub-pwy-1\"] = [\"gene-1\", \"gene-2\"]\n",
    "ont_lvl2[\"sub-pwy-2\"] = [\"gene-3\", \"gene-4\"]\n",
    "ont_lvl2[\"sub-pwy-3\"] = [\"gene-2\", \"gene-6\"]\n",
    "\n",
    "ontology_tuple = ( ont_lvl1, ont_lvl2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontix = acx.Ontix(ontologies=ontology_tuple, config=DefaultConfig(), user_data=processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontix.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n",
      "Mask layer cannot be calculated. Output layer list does not match next layer dimension\n",
      "Returning zero mask\n",
      "Mask layer cannot be calculated. Output layer list does not match next layer dimension\n",
      "Returning zero mask\n",
      "You provided an ontology with no connections between layers in the decoder. Please check your ontology definition.\n",
      "[30, 16, 16, 16, 16]\n",
      "2 3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of masks does not match number of decoder layers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43montix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/autoencodix_package/src/autoencodix/utils/_utils.py:128\u001b[0m, in \u001b[0;36mconfig_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m parameter must be of type DefaultConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m         )\n\u001b[1;32m    126\u001b[0m     config \u001b[38;5;241m=\u001b[39m user_config\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/autoencodix_package/src/autoencodix/base/_base_pipeline.py:362\u001b[0m, in \u001b[0;36mBasePipeline.fit\u001b[0;34m(self, config, **kwargs)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     ontologies \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loss_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43montologies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43montologies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m trainer_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult\u001b[38;5;241m.\u001b[39mupdate(trainer_result)\n",
      "File \u001b[0;32m~/Github/autoencodix_package/src/autoencodix/trainers/_general_trainer.py:52\u001b[0m, in \u001b[0;36mGeneralTrainer.__init__\u001b[0;34m(self, trainset, validset, result, config, model_type, loss_type, ontologies)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     44\u001b[0m     trainset: Optional[BaseDataset],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m     ontologies: Optional[\u001b[38;5;28mtuple\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# Addition to Varix, mandotory for Ontix\u001b[39;00m\n\u001b[1;32m     51\u001b[0m ):\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43montologies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43montologies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/autoencodix_package/src/autoencodix/base/_base_trainer.py:107\u001b[0m, in \u001b[0;36mBaseTrainer.__init__\u001b[0;34m(self, trainset, validset, result, config, model_type, loss_type, ontologies)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Model and optimizer setup\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dim \u001b[38;5;241m=\u001b[39m cast(BaseDataset, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainset)\u001b[38;5;241m.\u001b[39mget_input_dim()\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_model_architecture\u001b[49m\u001b[43m(\u001b[49m\u001b[43montologies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43montologies\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fabric \u001b[38;5;241m=\u001b[39m Fabric(\n\u001b[1;32m    110\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mdevice,\n\u001b[1;32m    111\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mn_gpus,\n\u001b[1;32m    112\u001b[0m     precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mfloat_precision,\n\u001b[1;32m    113\u001b[0m     strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mgpu_strategy,\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[1;32m    118\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mlearning_rate,\n\u001b[1;32m    119\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mweight_decay,\n\u001b[1;32m    120\u001b[0m )\n",
      "File \u001b[0;32m~/Github/autoencodix_package/src/autoencodix/base/_base_trainer.py:166\u001b[0m, in \u001b[0;36mBaseTrainer._init_model_architecture\u001b[0;34m(self, ontologies)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_type(config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config, input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dim)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m## Ontix specific\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43montologies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43montologies\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/autoencodix_package/src/autoencodix/modeling/_ontix_architecture.py:74\u001b[0m, in \u001b[0;36mOntixArchitecture.__init__\u001b[0;34m(self, config, input_dim, ontologies)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoder))\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of masks does not match number of decoder layers\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmasks):\n",
      "\u001b[0;31mValueError\u001b[0m: Number of masks does not match number of decoder layers"
     ]
    }
   ],
   "source": [
    "ontix.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attributes of ontix:\n",
      "_abc_impl\n",
      "_data_splitter\n",
      "_dataset_type\n",
      "_datasets\n",
      "_evaluator\n",
      "_fill_data_info\n",
      "_loss_type\n",
      "_model_type\n",
      "_preprocessor\n",
      "_preprocessor_type\n",
      "_trainer_type\n",
      "_validate_config_data\n",
      "_validate_container\n",
      "_validate_raw_user_data\n",
      "_validate_user_data\n",
      "_visualizer\n",
      "config\n",
      "evaluate\n",
      "fit\n",
      "ontologies\n",
      "predict\n",
      "preprocess\n",
      "preprocessed_data\n",
      "raw_user_data\n",
      "result\n",
      "run\n",
      "sample_latent_space\n",
      "show_result\n",
      "visualize\n"
     ]
    }
   ],
   "source": [
    "# Print all available attributes of ontix\n",
    "print(\"Attributes of ontix:\")\n",
    "for attr in dir(ontix):\n",
    "\tif not attr.startswith(\"__\"):\n",
    "\t\tprint(attr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
