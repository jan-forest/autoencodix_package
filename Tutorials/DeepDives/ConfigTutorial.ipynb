{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3271f9b8",
   "metadata": {},
   "source": [
    "# Config Deep Dive\n",
    "\n",
    "Our config class is the central point to customize our `AUTOENCODIX` pipelines. This notebook is more of a reference document than a tutorial, as we mainly list config parameters with explanations and default values and only include a few coding examples.\n",
    "\n",
    "**IMPORTANT**\n",
    "> This tutorial explains specific concepts of our config class. If you're unfamiliar with general concepts,  \n",
    "> we recommend following the `Getting Started - Vanillix` Tutorial first.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "We'll cover the following points:\n",
    "\n",
    "- The two ways to provide a config:\n",
    "  - as an instance of our config class\n",
    "  - as a `YAML` file\n",
    "- Which config parameters you should set\n",
    "- The two parts of our config:\n",
    "  - main config\n",
    "  - data config\n",
    "- Pipeline-specific config parameters\n",
    "- Default config reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b65fae",
   "metadata": {},
   "source": [
    "## 1) How to Provide a Config\n",
    "\n",
    "The main way is to pass an instance of our `DefaultConfig` class to the pipeline object, as you've seen many times in the pipeline tutorials.\n",
    "\n",
    "#### 1.1 Provide Config as a Class Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ebce55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in handle_direct_user_data with data: <class 'autoencodix.data.datapackage.DataPackage'>\n"
     ]
    }
   ],
   "source": [
    "import autoencodix as acx\n",
    "from autoencodix.configs.default_config import DefaultConfig, DataCase\n",
    "from autoencodix.configs import VarixConfig\n",
    "from autoencodix.utils.example_data import EXAMPLE_MULTI_BULK\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "config = VarixConfig(\n",
    "    latent_dim=8, scaling=\"MINMAX\", data_case=DataCase.MULTI_BULK, epochs=10\n",
    ")\n",
    "varix = acx.Varix(config=config, data=EXAMPLE_MULTI_BULK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bf77e",
   "metadata": {},
   "source": [
    "#### 1.2 Provide the Config as a YAML file\n",
    "\n",
    "In our GitHub repo, we prepared a directory called `configs` with sample YAML files.  \n",
    "We can easily load the values into our config class with the `model_validate` method, as shown below:\n",
    "\n",
    "#### ❗❗ Requirements: Getting Tutorial Data ❗❗\n",
    "To follow along, please download the date from the link below (1GB), if you've already done the `Ontix` Tutorial, you've probably already downloaded the data.\n",
    "\n",
    "https://cloud.scadsai.uni-leipzig.de/index.php/s/QXYnieKY8AA3Zta/download/OntixTutorialData.zip\n",
    "\n",
    "After downloading:\n",
    "- from the root of the repository, create the folders `data/raw` if not created yet\n",
    "- move the donwloaded files there\n",
    "\n",
    "### Extra 2: Get correct path\n",
    "We assume you are in the root of the package. The following code ensures that the correct paths are used.\n",
    "[1] Tutorials/DeepDives/ConfigTutorial.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b32c088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed to: /Users/maximilianjoas/development/autoencodix_package\n",
      "reading parquet: data/raw/combined_rnaseq_formatted.parquet\n",
      "reading parquet: data/raw/combined_meth_formatted.parquet\n",
      "reading parquet: data/raw/combined_clin_formatted.parquet\n",
      "anno key: paired\n",
      "Epoch 1 - Train Loss: 47.9436\n",
      "Sub-losses: recon_loss: 27.6438, var_loss: 20.2998, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 18.8278\n",
      "Sub-losses: recon_loss: 18.8271, var_loss: 0.0007, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 359.4296\n",
      "Sub-losses: recon_loss: 20.3551, var_loss: 339.0744, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 2 - Valid Loss: 43.4410\n",
      "Sub-losses: recon_loss: 18.7178, var_loss: 24.7232, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 3 - Train Loss: 9603.1596\n",
      "Sub-losses: recon_loss: 20.6843, var_loss: 9582.4751, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 3 - Valid Loss: 18.5837\n",
      "Sub-losses: recon_loss: 18.5836, var_loss: 0.0001, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n"
     ]
    }
   ],
   "source": [
    "# first be sure to be in root\n",
    "import os\n",
    "\n",
    "p = os.getcwd()\n",
    "d = \"autoencodix_package\"\n",
    "if d not in p:\n",
    "    raise FileNotFoundError(f\"'{d}' not found in path: {p}\")\n",
    "os.chdir(os.sep.join(p.split(os.sep)[: p.split(os.sep).index(d) + 1]))\n",
    "print(f\"Changed to: {os.getcwd()}\")\n",
    "\n",
    "# now we can load the config\n",
    "custom_config = VarixConfig.model_validate(\n",
    "    {\n",
    "        **yaml.safe_load(Path(\"configs/multi_bulk.yaml\").read_text()),\n",
    "        \"learning_rate\": 0.77,\n",
    "    }\n",
    ")\n",
    "# and pass to a pipeline\n",
    "varix = acx.Varix(config=custom_config)\n",
    "r = varix.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc6fe79",
   "metadata": {},
   "source": [
    "**Note**  \n",
    "> We can also go the reverse way and save our config object to a YAML file, as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc7cf6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved config to /Users/maximilianjoas/development/autoencodix_package/default_config.yaml\n"
     ]
    }
   ],
   "source": [
    "output_path = Path(\"default_config.yaml\")\n",
    "\n",
    "# Convert to plain Python dict first\n",
    "config_dict = config.model_dump()\n",
    "# Write YAML with nice formatting\n",
    "with output_path.open(\"w\") as f:\n",
    "    yaml.dump(config_dict, f, sort_keys=False, default_flow_style=False)\n",
    "\n",
    "print(f\"✅ Saved config to {output_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be346beb",
   "metadata": {},
   "source": [
    "## 2) Which Config Parameter to Set\n",
    "\n",
    "To make **AUTOENCODIX** easily usable, we provide sensible defaults per pipeline, so you don't need to think of and set 30+ config params before starting.  \n",
    "However, there are a few parameters you should consider — and depending on the pipeline and data type, a few mandatory ones you’ll need to set.\n",
    "\n",
    "\n",
    "### 2.1) Mandatory Config Parameters\n",
    "*(These depend on your pipeline — see specific documentation for details.)*\n",
    "\n",
    "\n",
    "### 2.2) Parameters to Consider\n",
    "\n",
    "Although we set sensible defaults per pipeline for these parameters, it might make sense to adjust the following values:\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `latent_dim` | `int` | Dimension of the latent space |\n",
    "| `epochs` | `int` | Number of training epochs |\n",
    "| `batch_size` | `int` | Number of samples per batch (must be >1 due to BatchNorm) |\n",
    "| `filtering` | `Literal['VAR', 'MAD', 'CORR', 'VARCORR', 'NOFILT', 'NONZEROVAR']` | Feature filtering method |\n",
    "| `scaling` | `Literal['STANDARD', 'MINMAX', 'ROBUST', 'MAXABS', 'NONE']` | How to scale your input data; should be compatible with your loss function |\n",
    "| `k_filter` | `Optional[int]` | Number of features to keep |\n",
    "| `learning_rate` | `float` | Learning rate for optimization |\n",
    "| `reconstruction_loss` | `Literal['mse', 'bce']` | Type of reconstruction loss: `mse` = Mean Squared Error, `bce` = Binary Cross-Entropy |\n",
    "| `default_vae_loss` | `Literal['kl', 'mmd']` | Type of VAE loss: `kl` = Kullback-Leibler Divergence, `mmd` = Maximum Mean Discrepancy |\n",
    "| `beta` | `float` | β weighting factor for VAE loss |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9636ab",
   "metadata": {},
   "source": [
    "## 3) General Config and Data Config\n",
    "Our config class consists of two parts: (a) a general part where we set global parameters, and (b) a `DataConfig` where we can set parameters for each data type individually.  \n",
    "Inside the `DataConfig` we have a `DataInfo` object where we can set the parameters; see a minimal code example below.  \n",
    "You can find a full list of parameters in [section 5.1](#51-dataconfig--configuration-parameters).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa2b676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in handle_direct_user_data with data: <class 'autoencodix.data.datapackage.DataPackage'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import autoencodix as acx\n",
    "from autoencodix.configs.default_config import DataConfig, DataInfo, DataCase\n",
    "from autoencodix.configs import VarixConfig\n",
    "from autoencodix.utils.example_data import raw_protein, raw_rna, annotation\n",
    "from autoencodix.data import DataPackage\n",
    "\n",
    "\n",
    "my_dp = DataPackage(\n",
    "    multi_bulk={\"rna\": raw_rna, \"protein\": raw_protein}, annotation={\"anno\": annotation}\n",
    ")\n",
    "data_config = DataConfig(\n",
    "    data_info={\n",
    "        \"rna\": DataInfo(scaling=\"MINMAX\", data_type=\"NUMERIC\"),\n",
    "        \"protein\": DataInfo(scaling=\"MINMAX\", data_type=\"NUMERIC\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "bulk_config = VarixConfig(data_config=data_config, data_case=DataCase.MULTI_BULK, epochs=30)\n",
    "varix = acx.Varix(config=bulk_config, data=my_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057235c",
   "metadata": {},
   "source": [
    "## 4) Pipeline Specific Configs\n",
    "We have one `DefaultConfig` class that defines all configurable parameters and sets sensible defaults.  \n",
    "However, not all pipelines (Varix, XModalix, etc.) should have the same sensible defaults.  \n",
    "Thus, each pipeline has its own config class that inherits from `DefaultConfig` and overrides some default values.  \n",
    "Be aware that when you build a config object, you use the appropriate config for the pipeline.  \n",
    "The config can be imported from `autoencodix.configs`. The naming convention is `<pipeline-name>Config`, for example: `VarixConfig` or `OntixConfig`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec11183",
   "metadata": {},
   "source": [
    "## 5)  Configuration Parameters Reference\n",
    "We list all configurable parameters sorted into functional sections below:\n",
    "\n",
    "#### **Data Configuration**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `data_config` | `autoencodix.configs.default_config.DataConfig` | Contains detailed information about each data modality (see `DataConfig` section) |\n",
    "| `img_path_col` | `str` | When working with images, defines the column name containing image paths per sample |\n",
    "| `requires_paired` | `Optional[bool]` | Indicates whether samples for xmodalix are paired (based on sample ID) |\n",
    "| `data_case` | `Optional[DataCase]` | Data case for the model (auto-determined) |\n",
    "| `k_filter` | `Optional[int]` | Number of features to keep |\n",
    "| `scaling` | `Literal['STANDARD', 'MINMAX', 'ROBUST', 'MAXABS', 'NONE']` | Global scaling setting (can be overridden per modality) |\n",
    "| `skip_preprocessing` | `bool` | Skip scaling, filtering, and cleaning |\n",
    "| `class_param` | `Optional[str]` | Optional column name for class labels |\n",
    "\n",
    "\n",
    "#### **Model Architecture**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `latent_dim` | `int` | Dimension of the latent space |\n",
    "| `n_layers` | `int` | Number of encoder/decoder layers (excluding latent layer) |\n",
    "| `enc_factor` | `int` | Encoder dimension scaling factor |\n",
    "| `input_dim` | `int` | Input feature dimension |\n",
    "| `drop_p` | `float` | Dropout probability |\n",
    "| `save_memory` | `bool` | Skip storing `TrainingDynamics` to save memory |\n",
    "\n",
    "\n",
    "#### **Training Hyperparameters**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `learning_rate` | `float` | Learning rate for optimization |\n",
    "| `batch_size` | `int` | Samples per batch (>1 required due to BatchNorm) |\n",
    "| `epochs` | `int` | Number of training epochs |\n",
    "| `weight_decay` | `float` | L2 regularization factor |\n",
    "| `reconstruction_loss` | `Literal['mse', 'bce']` | Type of reconstruction loss |\n",
    "| `default_vae_loss` | `Literal['kl', 'mmd']` | Type of VAE loss |\n",
    "| `loss_reduction` | `Literal['sum', 'mean']` | Loss reduction mode in PyTorch |\n",
    "| `beta` | `float` | β weight for VAE loss |\n",
    "| `beta_mi` | `float` | β weight for mutual information term |\n",
    "| `beta_tc` | `float` | β weight for total correlation term |\n",
    "| `beta_dimKL` | `float` | β weight for dimension-wise KL |\n",
    "| `use_mss` | `bool` | Use minibatch stratified sampling for disentangled VAE loss |\n",
    "| `gamma` | `float` | γ weight for adversarial loss (XModalix classifier) |\n",
    "| `delta_pair` | `float` | δ weight for paired loss (XModalix training) |\n",
    "| `delta_class` | `float` | δ weight for class loss (XModalix training) |\n",
    "| `anneal_function` | `Literal['5phase-constant', '3phase-linear', '3phase-log', 'logistic-mid', 'logistic-early', 'logistic-late', 'no-annealing']` | Annealing function strategy for VAE loss |\n",
    "| `pretrain_epochs` | `int` | Number of pretraining epochs (can differ per modality) |\n",
    "\n",
    "---\n",
    "\n",
    "#### **Device & Performance**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `device` | `Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']` | Compute device |\n",
    "| `n_gpus` | `int` | Number of GPUs to use |\n",
    "| `checkpoint_interval` | `int` | Checkpoint save interval |\n",
    "| `float_precision` | `Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16']` | Floating-point precision |\n",
    "| `gpu_strategy` | `Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']` | GPU parallelization strategy |\n",
    "\n",
    "\n",
    "#### **Data Splits & Reproducibility**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `train_ratio` | `float` | Training split ratio |\n",
    "| `test_ratio` | `float` | Test split ratio |\n",
    "| `valid_ratio` | `float` | Validation split ratio |\n",
    "| `min_samples_per_split` | `int` | Minimum samples per split |\n",
    "| `reproducible` | `bool` | Ensure reproducibility |\n",
    "| `global_seed` | `int` | Global random seed |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f4512",
   "metadata": {},
   "source": [
    "#### 5.1 DataConfig — Configuration Parameters\n",
    "\n",
    "---\n",
    "\n",
    "##### **DataConfig**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `data_info` | `Dict[str, DataInfo]` | Dictionary mapping modality names (e.g. `\"RNA\"`, `\"IMG\"`) to their `DataInfo` configuration |\n",
    "| `require_common_cells` | `Optional[bool]` | Whether to require that all data modalities share a common set of cells/samples |\n",
    "| `annotation_columns` | `Optional[List[str]]` | List of column names from the annotation file to include as metadata |\n",
    "\n",
    "###### **DataInfo**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `file_path` | `str` | Path to the raw data file |\n",
    "| `data_type` | `Literal['NUMERIC', 'CATEGORICAL', 'IMG', 'ANNOTATION']` | Type of data modality |\n",
    "| `scaling` | `Literal['STANDARD', 'MINMAX', 'ROBUST', 'MAXABS', 'NONE', 'NOTSET']` | Overrides the globally set scaling method for this modality |\n",
    "| `filtering` | `Literal['VAR', 'MAD', 'CORR', 'VARCORR', 'NOFILT', 'NONZEROVAR']` | Feature filtering method |\n",
    "| `sep` | `Optional[str]` | Delimiter for CSV/TSV input files (passed to `pandas.read_csv`) |\n",
    "| `extra_anno_file` | `Optional[str]` | Path to an additional annotation file |\n",
    "\n",
    "\n",
    "**Single-Cell Specific Parameters**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `is_single_cell` | `bool` | Whether the dataset represents single-cell data |\n",
    "| `min_cells` | `float` | Minimum fraction of cells in which a gene must be expressed to be kept (filters rare genes) |\n",
    "| `min_genes` | `float` | Minimum fraction of genes a cell must express to be kept (filters low-quality cells) |\n",
    "| `selected_layers` | `List[str]` | Layers to include from the single-cell dataset; must always include `\"X\"` |\n",
    "| `is_X` | `bool` | Whether the data originates from the `\"X\"` matrix only |\n",
    "| `normalize_counts` | `bool` | Whether to normalize single-cell counts by total expression per cell |\n",
    "| `log_transform` | `bool` | Whether to apply `log1p` transformation after normalization |\n",
    "| `k_filter` | `Optional[int]` | Automatically set based on global config; do not override manually |\n",
    "\n",
    "\n",
    "**Image-Specific Parameters**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `img_width_resize` | `Optional[int]` | Target width for image resizing (must equal height) |\n",
    "| `img_height_resize` | `Optional[int]` | Target height for image resizing (must equal width) |\n",
    "\n",
    "\n",
    "**XModalix & Translation Parameters**\n",
    "\n",
    "| **Parameter** | **Type** | **Description** |\n",
    "|----------------|-----------|-----------------|\n",
    "| `translate_direction` | `Optional[Literal['from', 'to']]` | Defines translation direction in cross-modal (XModalix) training |\n",
    "| `pretrain_epochs` | `int` | Number of pretraining epochs specific to this modality (overrides global pretraining setting) |\n",
    "\n",
    "\n",
    "**Validation Rules**\n",
    "\n",
    "- `selected_layers` must always contain `\"X\"`.  \n",
    "- `img_width_resize` and `img_height_resize` must be **positive integers** and **equal** (enforces square resizing).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb60c8cf",
   "metadata": {},
   "source": [
    "## 6) List Config Parameters Dynamically\n",
    "\n",
    "If you want to see all available parameters directly via Python, you can call `<config-instance>.print_schema()` as shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e76139a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DefaultConfig Configuration Parameters:\n",
      "--------------------------------------------------\n",
      "\n",
      "data_config:\n",
      "  Type: <class 'autoencodix.configs.default_config.DataConfig'>\n",
      "  Default: data_info={} require_common_cells=False annotation_columns=None\n",
      "  Description: No description available\n",
      "\n",
      "img_path_col:\n",
      "  Type: <class 'str'>\n",
      "  Default: img_paths\n",
      "  Description: When working with images, we except a column in your annotation file that specifies the path of the image for a particular sample. Here you can define the name of this column\n",
      "\n",
      "requires_paired:\n",
      "  Type: typing.Optional[bool]\n",
      "  Default: PydanticUndefined\n",
      "  Description: Indicator if the samples for the xmodalix are paired, based on some sample id\n",
      "\n",
      "data_case:\n",
      "  Type: typing.Optional[autoencodix.configs.default_config.DataCase]\n",
      "  Default: PydanticUndefined\n",
      "  Description: Data case for the model, will be determined automatically\n",
      "\n",
      "k_filter:\n",
      "  Type: typing.Optional[int]\n",
      "  Default: 20\n",
      "  Description: Number of features to keep\n",
      "\n",
      "scaling:\n",
      "  Type: typing.Literal['STANDARD', 'MINMAX', 'ROBUST', 'MAXABS', 'NONE', 'LOG1P']\n",
      "  Default: STANDARD\n",
      "  Description: Setting the scaling here for all data modalities, can per overruled by setting scaling at data modality level per data modality\n",
      "\n",
      "skip_preprocessing:\n",
      "  Type: <class 'bool'>\n",
      "  Default: False\n",
      "  Description: If set don't scale, filter or clean the input data.\n",
      "\n",
      "class_param:\n",
      "  Type: typing.Optional[str]\n",
      "  Default: None\n",
      "  Description: No description available\n",
      "\n",
      "latent_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 16\n",
      "  Description: Dimension of the latent space\n",
      "\n",
      "hidden_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 16\n",
      "  Description: Hidden dimension of image_vae, applies only to image_vae\n",
      "\n",
      "n_layers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of layers in encoder/decoder, without latent layer. If 0, is only the latent layer.\n",
      "\n",
      "enc_factor:\n",
      "  Type: <class 'float'>\n",
      "  Default: 4\n",
      "  Description: Scaling factor for encoder dimensions\n",
      "\n",
      "maskix_hidden_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 256\n",
      "  Description: The Maskix implementation follows https://doi.org/10.1093/bioinformatics/btae020. The authors use a hidden dimension 0f 256 for their neural network, so we set this as default\n",
      "\n",
      "maskix_swap_prob:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.4\n",
      "  Description: For the Maskix input_data masinkg, we sample a probablity if samples within one gene should be swapt. This is done with a Bernoulli distribution, maskix_swap_prob is the probablity passed to the bernoulli distribution \n",
      "\n",
      "drop_p:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Dropout probability\n",
      "\n",
      "save_memory:\n",
      "  Type: <class 'bool'>\n",
      "  Default: False\n",
      "  Description: If set to True we don't store TrainingDynamics\n",
      "\n",
      "learning_rate:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.001\n",
      "  Description: Learning rate for optimization\n",
      "\n",
      "batch_size:\n",
      "  Type: <class 'int'>\n",
      "  Default: 32\n",
      "  Description: Number of samples per batch, has to be > 1, because we use BatchNorm() Layer\n",
      "\n",
      "epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of training epochs\n",
      "\n",
      "weight_decay:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.01\n",
      "  Description: L2 regularization factor\n",
      "\n",
      "reconstruction_loss:\n",
      "  Type: typing.Literal['mse', 'bce']\n",
      "  Default: mse\n",
      "  Description: Type of reconstruction loss\n",
      "\n",
      "default_vae_loss:\n",
      "  Type: typing.Literal['kl', 'mmd']\n",
      "  Default: kl\n",
      "  Description: Type of VAE loss\n",
      "\n",
      "loss_reduction:\n",
      "  Type: typing.Literal['sum', 'mean']\n",
      "  Default: sum\n",
      "  Description: Loss reduction in PyTorch i.e in torch.nn.functional.binary_cross_entropy_with_logits(reduction=loss_reduction)\n",
      "\n",
      "beta:\n",
      "  Type: <class 'float'>\n",
      "  Default: 1\n",
      "  Description: Beta weighting factor for VAE loss\n",
      "\n",
      "beta_mi:\n",
      "  Type: <class 'float'>\n",
      "  Default: 1\n",
      "  Description: Beta weighting factor for mutual information term in disentangled VAE loss\n",
      "\n",
      "beta_tc:\n",
      "  Type: <class 'float'>\n",
      "  Default: 1\n",
      "  Description: Beta weighting factor for total correlation term in disentangled VAE loss\n",
      "\n",
      "beta_dimKL:\n",
      "  Type: <class 'float'>\n",
      "  Default: 1\n",
      "  Description: Beta weighting factor for dimension-wise KL in disentangled VAE loss\n",
      "\n",
      "use_mss:\n",
      "  Type: <class 'bool'>\n",
      "  Default: True\n",
      "  Description: Using minibatch stratified sampling for disentangled VAE loss calculation (faster estimation)\n",
      "\n",
      "gamma:\n",
      "  Type: <class 'float'>\n",
      "  Default: 10.0\n",
      "  Description: Gamma weighting factor for Adversial Loss Term i.e. for XModalix Classfier training\n",
      "\n",
      "delta_pair:\n",
      "  Type: <class 'float'>\n",
      "  Default: 5.0\n",
      "  Description: Delta weighting factor for paired loss term in XModalix Training\n",
      "\n",
      "delta_class:\n",
      "  Type: <class 'float'>\n",
      "  Default: 5.0\n",
      "  Description: Delta weighting factor for class loss term in XModalix Training\n",
      "\n",
      "delta_mask_predictor:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.7\n",
      "  Description: Delt weighting factor of the mask predictin loss term for the Maskix\n",
      "\n",
      "delta_mask_corrupted:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.75\n",
      "  Description: For the Maskix: if >0.5 this gives more weight for the correct reconstruction of corrupted input\n",
      "\n",
      "min_samples_per_split:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Minimum number of samples per split\n",
      "\n",
      "anneal_function:\n",
      "  Type: typing.Literal['5phase-constant', '3phase-linear', '3phase-log', 'logistic-mid', 'logistic-early', 'logistic-late', 'no-annealing']\n",
      "  Default: logistic-mid\n",
      "  Description: Annealing function strategy for VAE loss scheduling\n",
      "\n",
      "pretrain_epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 0\n",
      "  Description: Number of pretraining epochs, can be overwritten in DataInfo to have different number of pretraining epochs for each data modality\n",
      "\n",
      "device:\n",
      "  Type: typing.Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']\n",
      "  Default: auto\n",
      "  Description: Device to use\n",
      "\n",
      "n_gpus:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Number of GPUs to use\n",
      "\n",
      "checkpoint_interval:\n",
      "  Type: <class 'int'>\n",
      "  Default: 10\n",
      "  Description: Interval for saving checkpoints\n",
      "\n",
      "float_precision:\n",
      "  Type: typing.Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16']\n",
      "  Default: 32\n",
      "  Description: Floating point precision\n",
      "\n",
      "gpu_strategy:\n",
      "  Type: typing.Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']\n",
      "  Default: auto\n",
      "  Description: GPU parallelization strategy\n",
      "\n",
      "train_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.7\n",
      "  Description: Ratio of data for training\n",
      "\n",
      "test_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.2\n",
      "  Description: Ratio of data for testing\n",
      "\n",
      "valid_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Ratio of data for validation\n",
      "\n",
      "reproducible:\n",
      "  Type: <class 'bool'>\n",
      "  Default: False\n",
      "  Description: Whether to ensure reproducibility\n",
      "\n",
      "global_seed:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Global random seed\n"
     ]
    }
   ],
   "source": [
    "config = DefaultConfig()\n",
    "config.print_schema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoencodix_package",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
