{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODIX PACKAGE HANDBOOK\n",
    "This notebook demonstrates the usage of the autoencodix package.\n",
    "For now it serves as an internal guideline with the goal to:\n",
    "- test the package from a user perspective\n",
    "- serve as a first draft of user documentation\n",
    "- serve a developer guideline \n",
    "  - developer guide will be derrived from this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 Generate mock data\n",
    "When  development proceeds this section should be used to  show how to use different datatypes\n",
    "for now we only use a mock numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_data = np.random.rand(100, 10)\n",
    "sample_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 General Pipeline Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1036f1750>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/maximilianjoas/development/autoencodix_package/.venv/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import autoencodix as acx\n",
    "from autoencodix.utils.default_config import DefaultConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 0, Loss: 2.3556206822395325\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 1, Loss: 2.308934450149536\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 2, Loss: 2.213356852531433\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "output.reconstruction: torch.Size([20, 10])\n",
      "cpu not relevant here\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 0, Loss: 2.3556206822395325\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 1, Loss: 2.308934450149536\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "batch: 0\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 1\n",
      "model_outputs.reconstruction: torch.Size([32, 10])\n",
      "batch: 2\n",
      "model_outputs.reconstruction: torch.Size([6, 10])\n",
      "Epoch: 2, Loss: 2.213356852531433\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([32, 10])\n",
      "output.reconstruction: torch.Size([6, 10])\n",
      "output.reconstruction: torch.Size([10, 10])\n",
      "\n",
      "Warning: The following parameters are not valid for predict:\n",
      "Invalid parameters: data\n",
      "Valid parameters are: config\n",
      "output.reconstruction: torch.Size([20, 10])\n"
     ]
    }
   ],
   "source": [
    "#### --------------------------------------------\n",
    "# TODO user prepares data or config\n",
    "### INITIALIZATION ### --------------------------\n",
    "# Use Vanillix Pipeline interface\n",
    "# needs to be initialized with data\n",
    "# data should be a numpy array, pandas dataframe or AnnData object\n",
    "# possible to pass a custom Config object\n",
    "van = acx.Vanillix(data=sample_data)\n",
    "# ------------------------------------------------\n",
    "### DATA PROCESSING ### --------------------------\n",
    "# job of old make data\n",
    "# populates self._features attrbute with torch tensor\n",
    "# populates self._datasets attribute with torch dataset\n",
    "# (important for training with dataloader)\n",
    "# possible to pass a custom Config object, or keyword arguments\n",
    "van.preprocess()\n",
    "# ------------------------------------------------\n",
    "### MODEL TRAINING ### --------------------------\n",
    "# job of old make model\n",
    "# calls self.Trainer class to init and train model\n",
    "# populates self._model attribute with trained model\n",
    "# populates self.result attribute with training results (model, losses, etc)\n",
    "van.fit()\n",
    "# ------------------------------------------------\n",
    "### PREDICTION ### -------------------------------\n",
    "# job of old make predict\n",
    "# if no data is passed, used the test split from preprocessing\n",
    "# otherwise, uses the data passed, and preprocesses it\n",
    "# updates self.result attribute with predictions (latent space, reconstructions, etc)\n",
    "van.predict()\n",
    "# ------------------------------------------------\n",
    "### EVALUATION ### -------------------------------\n",
    "# job of old make ml_task\n",
    "# populates self.result attribute with ml task results\n",
    "van.evaluate()  # not implemented yet\n",
    "# ------------------------------------------------\n",
    "### VISUALIZATION ### ---------------------------\n",
    "# job of old make visualize\n",
    "# populates self.result attribute with visualizations\n",
    "van.visualize()\n",
    "# show visualizations for notebook use\n",
    "van.show_result()\n",
    "# --------------------------\n",
    "# --------------------------\n",
    "# run all steps in the pipeline\n",
    "result_object = van.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 10) (10, 10) (20, 10)\n"
     ]
    }
   ],
   "source": [
    "recons = result_object.reconstructions.get(split=\"train\", epoch=2)\n",
    "recons_val = result_object.reconstructions.get(split=\"valid\", epoch=2)\n",
    "recons_test = result_object.reconstructions.get(split=\"test\", epoch=-1)\n",
    "print(recons.shape, recons_val.shape, recons_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70, 16) (10, 16) (20, 16)\n"
     ]
    }
   ],
   "source": [
    "latents = result_object.latentspaces.get(split=\"train\", epoch=2)\n",
    "latents_val = result_object.latentspaces.get(split=\"valid\", epoch=2)\n",
    "latents_test = result_object.latentspaces.get(split=\"test\", epoch=-1)\n",
    "print(latents.shape, latents_val.shape, latents_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a custom train, test, valid split\n",
    "When you pass the data to the pipeline, autoencodix, internally splits the data for you based on the train,test, valid ratios provided in the config (defaults are 70%/10%/20% train/valid/test).\n",
    "You can either pass custom ratios (see next section) or provide the indices directly as shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.367237627506256\n",
      "Epoch: 1, Loss: 2.1364158391952515\n",
      "Epoch: 2, Loss: 2.140314042568207\n"
     ]
    }
   ],
   "source": [
    "sample_data = np.random.rand(100, 10)\n",
    "custom_train_indices = np.arange(75)  # we won't allow overlap between splits\n",
    "custom_valid_indices = np.arange(75, 80)\n",
    "custom_test_indices = np.arange(80, 100)\n",
    "\n",
    "# the custom split needs to be a dictionary with keys \"train\", \"valid\", and \"test\" and indices of the samples to be included in each split as numpy arrays\n",
    "custom_split = {\n",
    "    \"train\": custom_train_indices,\n",
    "    \"valid\": custom_valid_indices,\n",
    "    \"test\": custom_test_indices,\n",
    "}\n",
    "van = acx.Vanillix(data=sample_data, custom_splits=custom_split)\n",
    "van.preprocess()\n",
    "van.fit(epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to pass empty splits, but depending on how you'll use the autoencodix pipeline, this will throw an error at some point. So it is possible to call `fit` with only training data, but if you want to call `predict` and don't provide new data, this won't work without a data in the test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using predict with new data\n",
    "The standard case is to train the model with the train data and then predict with the test split.\n",
    "However, it is possible to pass new data to the predict method to perform inference on this data with the already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: The following parameters are not valid for predict:\n",
      "Invalid parameters: data\n",
      "Valid parameters are: config\n"
     ]
    }
   ],
   "source": [
    "new_unseen_data = np.random.rand(10, 10)\n",
    "van.predict(data=new_unseen_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the result of the pipeline\n",
    "Each step in the pipeline writes its results in the result object of the Vanillix instance.\n",
    "In this section we explore how to access and make sense of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Object Public Attributes:\n",
      "------------------------------\n",
      "latentspaces: TrainingDynamics object\n",
      "reconstructions: TrainingDynamics object\n",
      "mus: TrainingDynamics object\n",
      "sigmas: TrainingDynamics object\n",
      "losses: TrainingDynamics object\n",
      "preprocessed_data: Tensor of shape (100, 10)\n",
      "model: _FabricModule\n",
      "model_checkpoints: TrainingDynamics object\n",
      "datasets: DatasetContainer(train=<autoencodix.data._numeric_dataset.NumericDataset object at 0x1053f1e70>, valid=<autoencodix.data._numeric_dataset.NumericDataset object at 0x10626cd90>, test=<autoencodix.data._numeric_dataset.NumericDataset object at 0x10626ce20>)\n"
     ]
    }
   ],
   "source": [
    "result = van.result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TrainingDynamics object in result\n",
    "The training dynamics object has the followinf form:\n",
    "<epoch><split><data>\n",
    "So if you want to access the train loss for the 5th epoch, you would:\n",
    "`result.lossss.get(epoch=5, split=\"train\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7134380141894022\n",
      "[0.27492604 0.24243712 0.23720634]\n",
      "{0: {'train': array(0.78907921), 'valid': array(0.27492604)}, 1: {'train': array(0.71213861), 'valid': array(0.24243712)}, 2: {'train': array(0.71343801), 'valid': array(0.23720634)}}\n"
     ]
    }
   ],
   "source": [
    "loss_train_ep2 = result.losses.get(epoch=2, split=\"train\")\n",
    "print(loss_train_ep2)\n",
    "valid_loss = result.losses.get(split=\"valid\")\n",
    "print(valid_loss)\n",
    "print(result.losses.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this schema works for every TrainingDynamics instance in the results object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Pipeline usage with custom parameters\n",
    "Here we show how to customize the above shown pipeline with a user config or with keyword arguments.\n",
    "In future iterations we want to allow to read a config from a file, this will be also demonstrated here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n",
      "Epoch: 0, Loss: 1.9614873230457306\n",
      "Epoch: 1, Loss: 1.382771223783493\n",
      "Epoch: 2, Loss: 0.977891355752945\n",
      "Epoch: 3, Loss: 0.6883653551340103\n",
      "Epoch: 4, Loss: 0.5179779827594757\n",
      "cpu not relevant here\n",
      "Epoch: 0, Loss: 8780462592.772186\n",
      "Epoch: 1, Loss: 1348435297.7003174\n",
      "Epoch: 2, Loss: 5917.90837097168\n",
      "Epoch: 3, Loss: 10487.504081726074\n",
      "Epoch: 4, Loss: 3092.8131675720215\n"
     ]
    }
   ],
   "source": [
    "# Use Vanillix Pipeline interface\n",
    "# needs to be initialized with data\n",
    "# data should be a numpy array, pandas dataframe or AnnData object\n",
    "# possible to pass a custom Config object\n",
    "van = acx.Vanillix(data=sample_data)\n",
    "# job of old make data\n",
    "# populates self._features attrbute with torch tensor\n",
    "# populates self._datasets attribute with torch dataset\n",
    "# (important for training with dataloader)\n",
    "# possible to pass a custom Config object, or keyword arguments\n",
    "van.preprocess()\n",
    "# job of old make model\n",
    "# calls self.Trainer class to init and train model\n",
    "# populates self._model attribute with trained model\n",
    "# populates self.result attribute with training results (losses, etc)\n",
    "# van.fit()\n",
    "\"\"\" \n",
    "Each step can be run separately, with custom parameters, these parameters\n",
    "can be passed as keyword arguments, or as a Config object\n",
    "\"\"\"\n",
    "van.fit(learning_rate=0.01, batch_size=32, epochs=5)  # or like this:\n",
    "my_config = DefaultConfig(learning_rate=130.0, batch_size=32, epochs=5)\n",
    "van.fit(config=my_config)  # config has to be an keyword argument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.1  How to relevant keyword arguments for pipeline methods\n",
    "It can be hard to know what keyword arguments are valid for each step,\n",
    "so we show:\n",
    "- how to get a list of allowed keyword arguments\n",
    "- what happens if you pass non-allowed keyword arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size',\n",
      " 'checkpoint_interval',\n",
      " 'config',\n",
      " 'device',\n",
      " 'epochs',\n",
      " 'global_seed',\n",
      " 'gpu_strategy',\n",
      " 'learning_rate',\n",
      " 'n_gpus',\n",
      " 'n_workers',\n",
      " 'reconstruction_loss',\n",
      " 'reproducible',\n",
      " 'weight_decay'}\n"
     ]
    }
   ],
   "source": [
    "# for each config method, we can call a valid_params method\n",
    "van = acx.Vanillix(data=sample_data)\n",
    "fit_params = (\n",
    "    van.fit.valid_params\n",
    ")  # returns a set of keyword arguments that are actually used in the fit method\n",
    "\n",
    "import pprint\n",
    "\n",
    "pprint.pprint(fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get even more verbose info about the keyword args, you can run the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Keyword Arguments:\n",
      "--------------------------------------------------\n",
      "\n",
      "learning_rate:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.001\n",
      "  Description: Learning rate for optimization\n",
      "\n",
      "batch_size:\n",
      "  Type: <class 'int'>\n",
      "  Default: 32\n",
      "  Description: Number of samples per batch\n",
      "\n",
      "epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of training epochs\n",
      "\n",
      "weight_decay:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.01\n",
      "  Description: L2 regularization factor\n",
      "\n",
      "reconstruction_loss:\n",
      "  Type: typing.Literal['mse', 'bce']\n",
      "  Default: mse\n",
      "  Description: Type of reconstruction loss\n",
      "\n",
      "device:\n",
      "  Type: typing.Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']\n",
      "  Default: auto\n",
      "  Description: Device to use\n",
      "\n",
      "n_gpus:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Number of GPUs to use\n",
      "\n",
      "n_workers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 2\n",
      "  Description: Number of data loading workers\n",
      "\n",
      "checkpoint_interval:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Interval for saving checkpoints\n",
      "\n",
      "gpu_strategy:\n",
      "  Type: typing.Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']\n",
      "  Default: auto\n",
      "  Description: GPU parallelization strategy\n",
      "\n",
      "reproducible:\n",
      "  Type: <class 'bool'>\n",
      "  Default: True\n",
      "  Description: Whether to ensure reproducibility\n",
      "\n",
      "global_seed:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Global random seed\n"
     ]
    }
   ],
   "source": [
    "# when you want to have more info about the params, you can get type hints from the config object\n",
    "my_config = DefaultConfig()\n",
    "conig_values = my_config.get_params()\n",
    "my_config.print_schema(filter_params=fit_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you pass not supported parameters you get a warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: The following parameters are not valid for fit:\n",
      "Invalid parameters: epochds\n",
      "Valid parameters are: batch_size, checkpoint_interval, config, device, epochs, global_seed, gpu_strategy, learning_rate, n_gpus, n_workers, reconstruction_loss, reproducible, weight_decay\n",
      "cpu not relevant here\n",
      "Epoch: 0, Loss: 2.2837421894073486\n",
      "Epoch: 1, Loss: 2.2954598665237427\n",
      "Epoch: 2, Loss: 2.3972206711769104\n"
     ]
    }
   ],
   "source": [
    "# if you use an unsupported keyword argument, you will get a warning\n",
    "# as you see the default value from the DefaultConfig is not overwritten and the training will take 100 epochs (not 10)\n",
    "van.preprocess()\n",
    "van.fit(epochds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02.2 How to get information about the default config parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DefaultConfig Configuration Parameters:\n",
      "--------------------------------------------------\n",
      "\n",
      "latent_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 16\n",
      "  Description: Dimension of the latent space\n",
      "\n",
      "n_layers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of layers in encoder/decoder\n",
      "\n",
      "enc_factor:\n",
      "  Type: <class 'int'>\n",
      "  Default: 4\n",
      "  Description: Scaling factor for encoder dimensions\n",
      "\n",
      "input_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 10000\n",
      "  Description: Input dimension\n",
      "\n",
      "drop_p:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Dropout probability\n",
      "\n",
      "learning_rate:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.001\n",
      "  Description: Learning rate for optimization\n",
      "\n",
      "batch_size:\n",
      "  Type: <class 'int'>\n",
      "  Default: 32\n",
      "  Description: Number of samples per batch\n",
      "\n",
      "epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of training epochs\n",
      "\n",
      "weight_decay:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.01\n",
      "  Description: L2 regularization factor\n",
      "\n",
      "reconstruction_loss:\n",
      "  Type: typing.Literal['mse', 'bce']\n",
      "  Default: mse\n",
      "  Description: Type of reconstruction loss\n",
      "\n",
      "default_vae_loss:\n",
      "  Type: typing.Literal['kl']\n",
      "  Default: kl\n",
      "  Description: Type of VAE loss\n",
      "\n",
      "min_samples_per_split:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Minimum number of samples per split\n",
      "\n",
      "device:\n",
      "  Type: typing.Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']\n",
      "  Default: auto\n",
      "  Description: Device to use\n",
      "\n",
      "n_gpus:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Number of GPUs to use\n",
      "\n",
      "n_workers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 2\n",
      "  Description: Number of data loading workers\n",
      "\n",
      "checkpoint_interval:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Interval for saving checkpoints\n",
      "\n",
      "float_precision:\n",
      "  Type: typing.Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16']\n",
      "  Default: 32\n",
      "  Description: Floating point precision\n",
      "\n",
      "gpu_strategy:\n",
      "  Type: typing.Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']\n",
      "  Default: auto\n",
      "  Description: GPU parallelization strategy\n",
      "\n",
      "train_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.7\n",
      "  Description: Ratio of data for training\n",
      "\n",
      "test_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.2\n",
      "  Description: Ratio of data for testing\n",
      "\n",
      "valid_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Ratio of data for validation\n",
      "\n",
      "reproducible:\n",
      "  Type: <class 'bool'>\n",
      "  Default: True\n",
      "  Description: Whether to ensure reproducibility\n",
      "\n",
      "global_seed:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Global random seed\n"
     ]
    }
   ],
   "source": [
    "# if you want to see what config parameters are used in the default config you can do it like:\n",
    "default_config = DefaultConfig()\n",
    "default_config.print_schema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02.3 Documentation Config class\n",
    "You can update the config with your own values by:\n",
    "- passing arguments as:\n",
    "    - dict\n",
    "    - single arguments\n",
    "- passing a file (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "\n",
    "# METHOD 1: override the default config with a dictionary\n",
    "my_args = {\"learning_rate\": 0.0234, \"batch_size\": 13, \"epochs\": 12}\n",
    "my_config = DefaultConfig(**my_args)\n",
    "# METHOD 2: override signle parameters\n",
    "my_new_conig = DefaultConfig(latent_dim=23, n_gpus=13)\n",
    "\n",
    "# METHOD 3: from a file: TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Use the Varix model\n",
    "Now we show how easy it is to use a variational autoencoder instead of a vanilla version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n",
      "Epoch: 0, Loss: 1.3720149099826813\n",
      "Epoch: 1, Loss: 1.1976171135902405\n",
      "Epoch: 2, Loss: 1.221748024225235\n",
      "\n",
      "Warning: The following parameters are not valid for predict:\n",
      "Invalid parameters: data\n",
      "Valid parameters are: config\n"
     ]
    }
   ],
   "source": [
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "import autoencodix as acx\n",
    "import numpy as np\n",
    "\n",
    "sample_data = np.random.rand(100, 10)\n",
    "my_config = DefaultConfig(learning_rate=0.001, epochs=3, checkpoint_interval=1)\n",
    "varix = acx.Varix(data=sample_data, config=my_config)\n",
    "result = varix.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine Variational result\n",
    "Here, we have more info in our results object than in the Vanillix case. We have the learned paramters mu and logvar of the normal distirbution, in addition to the losses and reconstructions. We provide also the sampled latentspaces at each epoch and split.\n",
    "\n",
    "You can resample new latenspaces (shown in next section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20, 16)\n"
     ]
    }
   ],
   "source": [
    "# we did not train for the test split, so we don't need to pass an epoch\n",
    "# technically the epoch is -1\n",
    "mu_test_ep_last = result.latentspaces.get(split=\"test\")\n",
    "print(mu_test_ep_last.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different loss types\n",
    "For our variation autoencoder, the total loss consists of a reconstruction loss and a distribution loss i.e. kl-divergence. To investigate these losses, the result_obj has the attribute `sub_losses`. This is a `LossRegistry` withe the name of the loss as key and the value is of class `TrainingDynamics` and can be accessed as shown for the Vanillix part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: dict_keys(['recon_loss', 'var_loss'])\n",
      "[0.42741003 0.44090185 0.37895077]\n"
     ]
    }
   ],
   "source": [
    "sub_losses = result.sub_losses\n",
    "print(f\"keys: {sub_losses.keys()}\")\n",
    "recon_dyn = sub_losses.get(key=\"recon_loss\")\n",
    "print(recon_dyn.get(split=\"train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample new latentspaces\n",
    "You might want to use the trained model and the fitted parameters mu, and logvar to sample latentspaces. Therefore, the Varix pipeline has the additional method `sample_latent_space`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.4227, -1.1264, -0.5146, -0.6423,  1.3555,  0.1267,  1.1256, -0.8374,\n",
      "          1.2268, -1.7507,  0.9605, -0.4691,  0.9137, -0.8082, -0.5185, -0.1135],\n",
      "        [ 0.0871, -0.7650, -0.7087,  1.9782,  0.5053,  0.0163,  0.5845, -0.4165,\n",
      "          0.2073,  1.6614, -0.8132,  0.6487, -1.5413, -0.4115,  0.0680,  0.1220],\n",
      "        [ 0.5332,  0.1249,  1.5426,  1.3922,  0.3982,  0.5452,  0.5251, -0.1709,\n",
      "          0.2392, -0.6901,  0.6993, -2.4067,  0.7687,  0.4694, -1.3835, -1.3028],\n",
      "        [ 0.4925,  0.2083, -0.3846,  0.7595, -0.3062, -0.2996, -0.4743,  1.3784,\n",
      "          1.0943, -0.0518, -0.3465, -0.3751,  0.3677,  0.3754, -0.2108, -0.4754],\n",
      "        [-0.1330,  1.1649,  0.2671,  0.2378, -3.0322, -1.7903, -0.1169,  1.0738,\n",
      "         -0.9661,  0.0313,  0.2592, -1.3385,  0.7877, -0.0342,  1.4808,  2.4948]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "sampled = varix.sample_latent_space()\n",
    "\n",
    "print(sampled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3581,  0.8946,  2.3437, -1.0793,  1.5991, -0.6819,  1.3511,  0.7896,\n",
      "         -0.6627,  1.4723,  1.3740, -0.8405, -0.3541, -0.3976,  1.8037, -0.3309],\n",
      "        [-2.7742,  1.3001, -0.7447, -1.5788, -1.3535,  0.3174, -0.5517,  0.9802,\n",
      "         -1.0824,  1.4315, -0.8219, -1.6757,  0.4232, -1.0295, -1.2279,  0.2959],\n",
      "        [-0.5726,  1.2144,  1.3754, -0.1848,  1.2447, -0.2668, -1.7038,  1.4772,\n",
      "          0.6938,  0.8111,  0.5638, -1.3457,  1.3015, -2.0530, -1.4406, -0.6207],\n",
      "        [-0.5844,  1.0119,  1.3542,  0.8185,  0.4677,  0.6580, -0.4376, -1.5302,\n",
      "          0.3264,  0.5532,  1.9664, -1.2441,  1.4664, -1.4824,  1.2571, -1.7927],\n",
      "        [ 1.2330, -0.1874, -0.8378,  1.5088, -0.8935, -0.0678, -0.0589, -0.2272,\n",
      "          0.4520, -1.1687,  0.4746,  0.1345, -0.5463, -0.4620,  0.9897, -0.3327]],\n",
      "       device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# you can also select a specific epoch and split to sample from (default is last epoch and test split)\n",
    "sampled = varix.sample_latent_space(epoch=2, split=\"valid\")\n",
    "print(sampled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8121, -0.2599, -1.1656],\n",
      "        [-1.5269,  0.5212, -0.3416],\n",
      "        [ 1.1872,  1.8373,  0.4524]], device='mps:0')\n",
      "tensor([[ 2.3787,  0.1305, -0.7975],\n",
      "        [-0.3502, -0.2842,  1.0856],\n",
      "        [-2.3673,  1.2124,  1.5476]], device='mps:0')\n",
      "tensor([[ 0.0609,  2.3645,  0.2055],\n",
      "        [-0.3573, -1.5928,  0.5545],\n",
      "        [-2.5494, -0.6906, -0.8733]], device='mps:0')\n",
      "tensor([[ 0.9513,  1.5320,  1.5767],\n",
      "        [ 0.4684, -1.0381,  0.9011],\n",
      "        [-1.0837,  1.9147,  0.4768]], device='mps:0')\n",
      "tensor([[-0.0046, -1.3439, -0.2800],\n",
      "        [ 1.0443, -0.2889, -0.1052],\n",
      "        [ 0.5079,  1.7101,  0.0815]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# or sample multiple times\n",
    "for _ in range(5):\n",
    "    sampled = varix.sample_latent_space()\n",
    "    print(sampled[:3, :3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to add a new architecture\n",
    "If you want to add a new architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOS\n",
    "- show how to update and work with the config object (later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SANDBOX \n",
    "testing Varix and losses, especially sub_losses in result_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_FabricModule(\n",
       "  (_forward_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (_original_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varix._trainer._model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_losses = result.sub_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencodix.utils._result import Result, LossRegistry\n",
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "from autoencodix.utils._traindynamics import TrainingDynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_result = Result()\n",
    "sample_model = None\n",
    "sample_datasets = {\"train\": None, \"valid\": None, \"test\": None}\n",
    "sample_recon_data = {\n",
    "    0: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "    100: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "}\n",
    "sample_var_loss_data = {\n",
    "    0: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "    2: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "    100: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},\n",
    "}\n",
    "sample_recon_data_dyn = TrainingDynamics(_data=sample_recon_data)\n",
    "sample_var_loss_data_dyn = TrainingDynamics(_data=sample_var_loss_data)\n",
    "sample_losses = LossRegistry(\n",
    "    _losses={\"recon_loss\": sample_recon_data_dyn, \"var_loss\": sample_var_loss_data_dyn}\n",
    ")\n",
    "filled_result = Result(\n",
    "    model=sample_model, datasets=sample_datasets, sub_losses=sample_losses\n",
    ")\n",
    "\n",
    "recon_loss_data1 = {\n",
    "    0: {\"valid\": 0.1, \"train\": 0.2, \"test\": 0.3},\n",
    "    1: {\"valid\": 0.4, \"train\": 0.5, \"test\": 0.6},\n",
    "    2: {\"valid\": 0.7, \"train\": 0.8, \"test\": 0.9},\n",
    "}\n",
    "var_loss_data1 = {\n",
    "    0: {\"valid\": 0.11, \"train\": 0.21, \"test\": 0.31},\n",
    "    1: {\"valid\": 0.31, \"train\": 0.41, \"test\": 0.51},\n",
    "    2: {\"valid\": 0.51, \"train\": 0.61, \"test\": 0.71},\n",
    "}\n",
    "\n",
    "recon_dynamics1 = TrainingDynamics(_data=recon_loss_data1)\n",
    "var_dynamics1 = TrainingDynamics(_data=var_loss_data1)\n",
    "sample_registry = LossRegistry(\n",
    "    _losses={\"recon_loss\": recon_dynamics1, \"var_loss\": var_dynamics1}\n",
    ")\n",
    "sample_result1 = Result(sub_losses=sample_registry)\n",
    "recon_loss_data2 = {\n",
    "    0: {\"valid\": 0.15, \"train\": 0.25},\n",
    "    1: {\"valid\": 0.45, \"train\": 0.55, \"test\": None},\n",
    "    2: {\"valid\": None, \"train\": 0.85, \"test\": 0.95},\n",
    "}\n",
    "var_loss_data2 = {\n",
    "    0: {\"valid\": 0.16, \"train\": 0.26, \"test\": 0.36},\n",
    "    1: {\"valid\": 0.36, \"train\": 0.46},\n",
    "    2: None,\n",
    "}  # should not be updated\n",
    "\n",
    "recon_dynamics2 = TrainingDynamics(_data=recon_loss_data2)\n",
    "var_dynamics2 = TrainingDynamics(_data=var_loss_data2)\n",
    "sample_registry2 = LossRegistry(\n",
    "    _losses={\"recon_loss\": recon_dynamics2, \"var_loss\": var_dynamics2}\n",
    ")\n",
    "sample_result2 = Result(sub_losses=sample_registry2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_update = {\n",
    "    0: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},  # totally overwrite\n",
    "    2: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},  # keep all\n",
    "    3: {},  # update with partial data\n",
    "    4: None,  # udate with all\n",
    "    5: {\n",
    "        \"train\": None,\n",
    "        \"valid\": 0.0,\n",
    "        \"test\": 0.0,\n",
    "    },  # update with partial data (keep test)\n",
    "    6: {\"train\": 0.0},  # update with partial data (keep train), add vlaid and test\n",
    "    7: {\"train\": 0},  # update with partial data {train: 0.1}\n",
    "    100: {\"train\": 0.0, \"valid\": 0.0, \"test\": 0.0},  # not in to update\n",
    "}\n",
    "\n",
    "update_with = {\n",
    "    0: {\"train\": 0.1, \"valid\": 0.2, \"test\": 0.3},\n",
    "    1: {\"train\": 0.4, \"valid\": 0.5, \"test\": 0.6},\n",
    "    2: {},\n",
    "    3: {\"train\": 0.7, \"valid\": 0.8},\n",
    "    4: {\"train\": 0.9, \"valid\": 1.0, \"test\": 1.1},\n",
    "    5: {\"train\": 0.12, \"valid\": 0.22},\n",
    "    6: {\"valid\": 0.32, \"test\": 0.42},\n",
    "    7: {\"train\": 0.1},\n",
    "}\n",
    "\n",
    "expected_result = {\n",
    "    0: {\"train\": np.array(0.1), \"valid\": np.array(0.2), \"test\": np.array(0.3)},\n",
    "    1: {\"train\": np.array(0.4), \"valid\": np.array(0.5), \"test\": np.array(0.6)},\n",
    "    2: {\"train\": np.array(0.0), \"valid\": np.array(0.0), \"test\": np.array(0.0)},\n",
    "    3: {\"train\": np.array(0.7), \"valid\": np.array(0.8)},\n",
    "    4: {\"train\": np.array(0.9), \"valid\": np.array(1.0), \"test\": np.array(1.1)},\n",
    "    5: {\"train\": np.array(0.12), \"valid\": np.array(0.22), \"test\": np.array(0.0)},\n",
    "    6: {\"train\": np.array(0.0), \"valid\": np.array(0.32), \"test\": np.array(0.42)},\n",
    "    7: {\"train\": np.array(0.1)},\n",
    "    100: {\"train\": np.array(0.0), \"valid\": np.array(0.0), \"test\": np.array(0.0)},\n",
    "}\n",
    "\n",
    "to_update_dyn = TrainingDynamics(_data=to_update)\n",
    "to_update_registry = LossRegistry(\n",
    "    _losses={\n",
    "        \"recon_loss\": to_update_dyn,\n",
    "        \"var_loss\": to_update_dyn,\n",
    "        \"more_loss\": TrainingDynamics(_data={}),\n",
    "    }\n",
    ")\n",
    "to_update_result = Result(sub_losses=to_update_registry)\n",
    "\n",
    "\n",
    "update_with_dyn = TrainingDynamics(_data=update_with)\n",
    "update_with_registry = LossRegistry(\n",
    "    _losses={\n",
    "        \"recon_loss\": update_with_dyn,\n",
    "        \"var_loss\": TrainingDynamics(_data={}),\n",
    "        \"other_loss\": TrainingDynamics(_data={}),\n",
    "    }\n",
    ")\n",
    "update_with_result = Result(sub_losses=update_with_registry)\n",
    "to_update_result.update(update_with_result)\n",
    "after_update = dict(\n",
    "    sorted(to_update_result.sub_losses.get(key=\"recon_loss\").get().items())\n",
    ")\n",
    "expected_recon_dyn = TrainingDynamics(_data=expected_result)\n",
    "expected_varloss_dyn = TrainingDynamics(_data=to_update)\n",
    "expected_moreloss_dyn = TrainingDynamics(_data={})\n",
    "expected_other_dyn = TrainingDynamics(_data={})\n",
    "expected_loss_registry = LossRegistry(\n",
    "    _losses={\n",
    "        \"recon_loss\": expected_recon_dyn,\n",
    "        \"var_loss\": expected_varloss_dyn,\n",
    "        \"more_loss\": expected_moreloss_dyn,\n",
    "        \"other_loss\": expected_other_dyn,\n",
    "    }\n",
    ")\n",
    "expected_result = Result(sub_losses=expected_loss_registry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_loss\n",
      "<class 'dict'>\n",
      "var_loss\n",
      "<class 'dict'>\n",
      "more_loss\n",
      "<class 'dict'>\n",
      "other_loss\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "to_update_result.sub_losses._losses\n",
    "for lossname, dynamics in expected_result.sub_losses.losses():\n",
    "    print(lossname)\n",
    "    print(type(dynamics._data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_loss\n",
      "{0: {'train': array(0.1), 'valid': array(0.2), 'test': array(0.3)}, 1: {'train': array(0.4), 'valid': array(0.5), 'test': array(0.6)}, 2: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}, 3: {'valid': array(0.8), 'train': array(0.7)}, 4: {'valid': array(1.), 'test': array(1.1), 'train': array(0.9)}, 5: {'valid': array(0.22), 'test': array(0.), 'train': array(0.12)}, 6: {'train': array(0.), 'valid': array(0.32), 'test': array(0.42)}, 7: {'train': array(0.1)}, 100: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}}\n",
      "var_loss\n",
      "{0: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}, 2: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}, 5: {'valid': array(0.), 'test': array(0.)}, 6: {'train': array(0.)}, 7: {'train': array(0)}, 100: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}}\n",
      "more_loss\n",
      "{}\n",
      "other_loss\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "for lossname, dynamics in to_update_result.sub_losses.losses():\n",
    "    print(lossname)\n",
    "    print(dynamics.get())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_loss\n",
      "{0: {'train': array(0.1), 'valid': array(0.2), 'test': array(0.3)}, 1: {'train': array(0.4), 'valid': array(0.5), 'test': array(0.6)}, 2: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}, 3: {'train': array(0.7), 'valid': array(0.8)}, 4: {'train': array(0.9), 'valid': array(1.), 'test': array(1.1)}, 5: {'train': array(0.12), 'valid': array(0.22), 'test': array(0.)}, 6: {'train': array(0.), 'valid': array(0.32), 'test': array(0.42)}, 7: {'train': array(0.1)}, 100: {'train': array(0.), 'valid': array(0.), 'test': array(0.)}}\n",
      "var_loss\n",
      "{0: {'train': 0.0, 'valid': 0.0, 'test': 0.0}, 2: {'train': 0.0, 'valid': 0.0, 'test': 0.0}, 3: {}, 4: None, 5: {'train': None, 'valid': 0.0, 'test': 0.0}, 6: {'train': 0.0}, 7: {'train': 0}, 100: {'train': 0.0, 'valid': 0.0, 'test': 0.0}}\n",
      "more_loss\n",
      "{}\n",
      "other_loss\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "for lossname, dynamics in expected_result.sub_losses.losses():\n",
    "    print(lossname)\n",
    "    print(dynamics.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'train': array(0.26), 'valid': array(0.16), 'test': array(0.36)},\n",
       " 2: {'train': 0.0, 'valid': 0.0, 'test': 0.0},\n",
       " 100: {'train': 0.0, 'valid': 0.0, 'test': 0.0},\n",
       " 1: {'valid': array(0.36), 'train': array(0.46)}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(filled_result.sub_losses._losses[\"var_loss\"].get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetContainer(train=<autoencodix.data._numeric_dataset.NumericDataset object at 0x15ff96b30>, valid=<autoencodix.data._numeric_dataset.NumericDataset object at 0x15ff96bc0>, test=<autoencodix.data._numeric_dataset.NumericDataset object at 0x15ff96e00>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# empty does not overwrite anything\n",
    "result.update(empty_result)\n",
    "result.losses.get()\n",
    "result.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'valid': array(0.1), 'train': array(0.2), 'test': array(0.3)},\n",
       " 1: {'valid': array(0.4), 'train': array(0.5), 'test': array(0.6)},\n",
       " 2: {'valid': array(0.7), 'train': array(0.8), 'test': array(0.9)}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# case one update empty result\n",
    "# with: - empty result, - sample result1, - sample result2\n",
    "empty_result.update(other=sample_result1)\n",
    "empty_result.sub_losses._losses[\"recon_loss\"].get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_result.update(other=sample_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# case two update filled result\n",
    "# with: - filled result, - sample result1, - sample result2, empty result\n",
    "filled_result.update(other=sample_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_losses == updated_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LossRegistry(_losses={'recon_loss': TrainingDynamics(), 'var_loss': TrainingDynamics()})\n"
     ]
    }
   ],
   "source": [
    "updated_losses = result.sub_losses\n",
    "print(updated_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recon_loss: TrainingDynamics()\n",
      "{0: {'train': array(0.44560597), 'valid': array(0.24305786)}, 1: {'train': array(0.40050308), 'valid': array(0.23968849)}, 2: {'train': array(0.39850321), 'valid': array(0.2438124)}}\n",
      "var_loss: TrainingDynamics()\n",
      "{0: {'train': array(0.03078574), 'valid': array(0.00519827)}, 1: {'train': array(0.02884311), 'valid': array(0.00536202)}, 2: {'train': array(0.02645038), 'valid': array(0.00561051)}}\n"
     ]
    }
   ],
   "source": [
    "for name, loss in updated_losses.losses():\n",
    "    print(f\"{name}: {loss}\")\n",
    "    print(loss.get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_FabricModule(\n",
       "  (_forward_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (_original_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varix._trainer._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DefaultConfig Configuration Parameters:\n",
      "--------------------------------------------------\n",
      "\n",
      "latent_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 16\n",
      "  Description: Dimension of the latent space\n",
      "\n",
      "n_layers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of layers in encoder/decoder\n",
      "\n",
      "enc_factor:\n",
      "  Type: <class 'int'>\n",
      "  Default: 4\n",
      "  Description: Scaling factor for encoder dimensions\n",
      "\n",
      "input_dim:\n",
      "  Type: <class 'int'>\n",
      "  Default: 10000\n",
      "  Description: Input dimension\n",
      "\n",
      "drop_p:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Dropout probability\n",
      "\n",
      "learning_rate:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.001\n",
      "  Description: Learning rate for optimization\n",
      "\n",
      "batch_size:\n",
      "  Type: <class 'int'>\n",
      "  Default: 32\n",
      "  Description: Number of samples per batch\n",
      "\n",
      "epochs:\n",
      "  Type: <class 'int'>\n",
      "  Default: 3\n",
      "  Description: Number of training epochs\n",
      "\n",
      "weight_decay:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.01\n",
      "  Description: L2 regularization factor\n",
      "\n",
      "reconstruction_loss:\n",
      "  Type: typing.Literal['mse', 'bce']\n",
      "  Default: mse\n",
      "  Description: Type of reconstruction loss\n",
      "\n",
      "default_vae_loss:\n",
      "  Type: typing.Literal['kl', 'mmd']\n",
      "  Default: kl\n",
      "  Description: Type of VAE loss\n",
      "\n",
      "loss_reduction:\n",
      "  Type: typing.Literal['sum', 'mean']\n",
      "  Default: mean\n",
      "  Description: Loss reduction in PyTorch i.e in torch.nn.functional.binary_cross_entropy_with_logits(reduction=loss_reduction)\n",
      "\n",
      "beta:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Beta weighting factor for VAE loss\n",
      "\n",
      "min_samples_per_split:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Minimum number of samples per split\n",
      "\n",
      "device:\n",
      "  Type: typing.Literal['cpu', 'cuda', 'gpu', 'tpu', 'mps', 'auto']\n",
      "  Default: auto\n",
      "  Description: Device to use\n",
      "\n",
      "n_gpus:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Number of GPUs to use\n",
      "\n",
      "n_workers:\n",
      "  Type: <class 'int'>\n",
      "  Default: 2\n",
      "  Description: Number of data loading workers\n",
      "\n",
      "checkpoint_interval:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Interval for saving checkpoints\n",
      "\n",
      "float_precision:\n",
      "  Type: typing.Literal['transformer-engine', 'transformer-engine-float16', '16-true', '16-mixed', 'bf16-true', 'bf16-mixed', '32-true', '64-true', '64', '32', '16', 'bf16']\n",
      "  Default: 32\n",
      "  Description: Floating point precision\n",
      "\n",
      "gpu_strategy:\n",
      "  Type: typing.Literal['auto', 'dp', 'ddp', 'ddp_spawn', 'ddp_find_unused_parameters_true', 'xla', 'deepspeed', 'fsdp']\n",
      "  Default: auto\n",
      "  Description: GPU parallelization strategy\n",
      "\n",
      "train_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.7\n",
      "  Description: Ratio of data for training\n",
      "\n",
      "test_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.2\n",
      "  Description: Ratio of data for testing\n",
      "\n",
      "valid_ratio:\n",
      "  Type: <class 'float'>\n",
      "  Default: 0.1\n",
      "  Description: Ratio of data for validation\n",
      "\n",
      "reproducible:\n",
      "  Type: <class 'bool'>\n",
      "  Default: True\n",
      "  Description: Whether to ensure reproducibility\n",
      "\n",
      "global_seed:\n",
      "  Type: <class 'int'>\n",
      "  Default: 1\n",
      "  Description: Global random seed\n"
     ]
    }
   ],
   "source": [
    "DefaultConfig().print_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = DefaultConfig()\n",
    "config.n_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model architecture layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "varix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_FabricModule(\n",
       "  (_forward_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (_original_module): VarixArchitecture(\n",
       "    (_encoder): Sequential(\n",
       "      (0): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (_decoder): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): Dropout(p=0.1, inplace=False)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (6): Dropout(p=0.1, inplace=False)\n",
       "      (7): ReLU()\n",
       "      (8): Linear(in_features=16, out_features=10, bias=True)\n",
       "    )\n",
       "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varix._trainer._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = DefaultConfig()\n",
    "config.n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n",
      "[100, 50, 25, 12, 2]\n",
      "Epoch: 0, Loss: 0.5027862191200256\n",
      "_FabricModule(\n",
      "  (_forward_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (_logvar): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=50, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=12, bias=True)\n",
      "      (9): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "      (11): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=12, bias=True)\n",
      "      (1): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=12, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=50, bias=True)\n",
      "      (9): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "      (11): ReLU()\n",
      "      (12): Linear(in_features=50, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (_logvar): Linear(in_features=12, out_features=2, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=50, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=12, bias=True)\n",
      "      (9): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "      (11): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=2, out_features=12, bias=True)\n",
      "      (1): BatchNorm1d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=12, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=25, out_features=50, bias=True)\n",
      "      (9): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (10): Dropout(p=0.1, inplace=False)\n",
      "      (11): ReLU()\n",
      "      (12): Linear(in_features=50, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ENCODER FACTOR: 1\n",
      "cpu not relevant here\n",
      "[100, 100, 100, 16]\n",
      "Epoch: 0, Loss: 0.5166383385658264\n",
      "_FabricModule(\n",
      "  (_forward_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=100, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=100, bias=True)\n",
      "      (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (5): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=100, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ENCODER FACTOR: 2\n",
      "cpu not relevant here\n",
      "[100, 50, 25, 16]\n",
      "Epoch: 0, Loss: 0.46474137902259827\n",
      "_FabricModule(\n",
      "  (_forward_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=25, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=25, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=50, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=25, bias=True)\n",
      "      (1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=25, out_features=50, bias=True)\n",
      "      (5): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=50, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=25, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=25, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=50, bias=True)\n",
      "      (1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=50, out_features=25, bias=True)\n",
      "      (5): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=25, bias=True)\n",
      "      (1): BatchNorm1d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=25, out_features=50, bias=True)\n",
      "      (5): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=50, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ENCODER FACTOR: 3\n",
      "cpu not relevant here\n",
      "[100, 33, 16, 16]\n",
      "Epoch: 0, Loss: 0.5135508179664612\n",
      "_FabricModule(\n",
      "  (_forward_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=33, bias=True)\n",
      "      (1): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=33, out_features=16, bias=True)\n",
      "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=16, out_features=33, bias=True)\n",
      "      (5): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=33, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VarixArchitecture(\n",
      "    (_mu): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (_logvar): Linear(in_features=16, out_features=16, bias=True)\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=33, bias=True)\n",
      "      (1): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=33, out_features=16, bias=True)\n",
      "      (5): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): Dropout(p=0.1, inplace=False)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=16, out_features=33, bias=True)\n",
      "      (5): BatchNorm1d(33, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): Dropout(p=0.1, inplace=False)\n",
      "      (7): ReLU()\n",
      "      (8): Linear(in_features=33, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from autoencodix.trainers._general_trainer import GeneralTrainer\n",
    "import autoencodix as acx\n",
    "from autoencodix.utils.default_config import DefaultConfig\n",
    "import numpy as np\n",
    "sample_data = np.random.rand(10, 100)\n",
    "\n",
    "varix = acx.Varix(data=sample_data, config=DefaultConfig(n_layers=3, epochs=1, enc_factor=2,latent_dim=2))\n",
    "varix.preprocess()\n",
    "varix.fit()\n",
    "print(varix._trainer._model)\n",
    "for enc_factor in [1, 2, 3]:\n",
    "    print(f\"ENCODER FACTOR: {enc_factor}\")\n",
    "    varix = acx.Varix(data=sample_data, config=DefaultConfig(n_layers=2, epochs=1, enc_factor=enc_factor))\n",
    "    varix.preprocess()\n",
    "    varix.fit()\n",
    "    print(varix._trainer._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(varix._trainer._model._decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "empty = nn.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu not relevant here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.5317338109016418\n",
      "_FabricModule(\n",
      "  (_forward_module): VanillixArchitecture(\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=16, bias=True)\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (_original_module): VanillixArchitecture(\n",
      "    (_encoder): Sequential(\n",
      "      (0): Linear(in_features=100, out_features=16, bias=True)\n",
      "    )\n",
      "    (_decoder): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "van = acx.Vanillix(data=sample_data, config=DefaultConfig(n_layers=0, epochs=1))\n",
    "van.preprocess()\n",
    "van.fit()\n",
    "print(van._trainer._model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
