{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a5214ae8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# How To Use Stackix\n",
    "Stackeix is our implementation of a stacked/hierachial autoencoder.  \n",
    "This tutorial follows the structure of our `Getting Started - Vanillix`, but is much less extenisve, because\n",
    "our pipeline works similar for different architecture, so here we focus only on Stackix specifics.\n",
    "\n",
    "**AUTOENCODIX** supports far more functionality than shown here, so weâ€™ll also point to advanced tutorials where relevant.  \n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "> This tutorial only shows the specifics of the Stackix pipeline. If you're unfamilar with general concepts,  \n",
    "> we recommend to follow the `Getting Started - Vanillix` Tutorial first.\n",
    "\n",
    "## Stackix Theory\n",
    "\n",
    "In our implementation, we train one variational autoencoder (VAE) per data modality end-to-end. The latent spaces of these *outer* autoencoders are then concatenated and used as input to another *inner* autoencoder. We perform downstream visualization and evaluation on this **meta-latent space**.  \n",
    "\n",
    "For unpaired data, each outer autoencoder is trained on all available samples of its modality, even if the corresponding sample is missing in the other modalities. When constructing the input for the inner autoencoder, we drop non-overlapping samples to ensure consistency across modalities.  \n",
    "\n",
    "This approach has two goals:  \n",
    "1. To produce more informative latent spaces for each modality.  \n",
    "2. To provide a richer, joint representation for the inner autoencoder.  \n",
    "\n",
    "We also store the dropped samples and their indices. This allows us to reconstruct the full dataset: the output of the inner autoencoder can be passed back through the outer autoencoders, where decoding recovers the original modalities.  \n",
    "\n",
    "\n",
    "![image.png](attachment:image.png). . \n",
    "\n",
    "---\n",
    "## What You'll Learn\n",
    "\n",
    "Youâ€™ll learn how to:\n",
    "\n",
    "1. **Initialize** the pipeline and run the pipeline. <br> <br>\n",
    "2. Understand the Stackix sepecific **pipeline steps** (paired vs. unpaired data). <br><br>\n",
    "3. Access the Varix specific **results** (sub-results for \"outer\" autoencoders). <br><br>\n",
    "4. **Visualize** outputs effectively. <br><br>\n",
    "5. Apply **custom parameters**. <br><br>\n",
    "6. **Save, load, and reuse** a trained pipeline. <br><br>\n",
    "\n",
    "---\n",
    "\n",
    "Letâ€™s get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c582ab",
   "metadata": {},
   "source": [
    "## 1) Initialize and Run Stackix\n",
    "We set a few custom parameters of the config file. For a deep dive into the config object see: \n",
    "\n",
    "`Tutorials/DeepDives/ConfigTutorial.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff368c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      "multi_bulk:\n",
      "  transcriptomics: 500 samples Ã— 100 features\n",
      "  proteomics: 500 samples Ã— 80 features\n",
      "annotation:\n",
      "  transcriptomics: 500 samples Ã— 3 features\n",
      "  proteomics: 500 samples Ã— 3 features\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Starting Pipeline\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "in handle_direct_user_data with data: <class 'autoencodix.data.datapackage.DataPackage'>\n",
      "--- Running Pairing-Aware Split ---\n",
      "Identified 500 fully paired samples across all modalities.\n",
      "Identified 0 samples present in at least one, but not all, modalities.\n",
      "Successfully generated synchronized indices for all modalities.\n",
      "Training each modality model...\n",
      "Training modality: transcriptomics\n",
      "Training modality: transcriptomics\n",
      "Epoch 1 - Train Loss: 5609.8924\n",
      "Sub-losses: recon_loss: 5609.8845, var_loss: 0.0079, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 520.0080\n",
      "Sub-losses: recon_loss: 520.0076, var_loss: 0.0004, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 4969.3672\n",
      "Sub-losses: recon_loss: 4969.3538, var_loss: 0.0134, anneal_factor: 0.0001, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Valid Loss: 552.0918\n",
      "Sub-losses: recon_loss: 552.0905, var_loss: 0.0013, anneal_factor: 0.0001, effective_beta_factor: 0.0000\n",
      "Epoch 3 - Train Loss: 4751.8258\n",
      "Sub-losses: recon_loss: 4751.7945, var_loss: 0.0313, anneal_factor: 0.0002, effective_beta_factor: 0.0000\n",
      "Epoch 3 - Valid Loss: 562.9235\n",
      "Sub-losses: recon_loss: 562.9200, var_loss: 0.0035, anneal_factor: 0.0002, effective_beta_factor: 0.0000\n",
      "Epoch 4 - Train Loss: 4648.8221\n",
      "Sub-losses: recon_loss: 4648.7567, var_loss: 0.0653, anneal_factor: 0.0004, effective_beta_factor: 0.0000\n",
      "Epoch 4 - Valid Loss: 536.8811\n",
      "Sub-losses: recon_loss: 536.8728, var_loss: 0.0083, anneal_factor: 0.0004, effective_beta_factor: 0.0000\n",
      "Epoch 5 - Train Loss: 4427.3965\n",
      "Sub-losses: recon_loss: 4427.2376, var_loss: 0.1589, anneal_factor: 0.0009, effective_beta_factor: 0.0001\n",
      "Epoch 5 - Valid Loss: 556.0576\n",
      "Sub-losses: recon_loss: 556.0385, var_loss: 0.0190, anneal_factor: 0.0009, effective_beta_factor: 0.0001\n",
      "Epoch 6 - Train Loss: 4246.5968\n",
      "Sub-losses: recon_loss: 4246.2776, var_loss: 0.3192, anneal_factor: 0.0018, effective_beta_factor: 0.0002\n",
      "Epoch 6 - Valid Loss: 525.4267\n",
      "Sub-losses: recon_loss: 525.3819, var_loss: 0.0449, anneal_factor: 0.0018, effective_beta_factor: 0.0002\n",
      "Epoch 7 - Train Loss: 4154.6645\n",
      "Sub-losses: recon_loss: 4154.0245, var_loss: 0.6400, anneal_factor: 0.0039, effective_beta_factor: 0.0004\n",
      "Epoch 7 - Valid Loss: 513.2637\n",
      "Sub-losses: recon_loss: 513.1770, var_loss: 0.0866, anneal_factor: 0.0039, effective_beta_factor: 0.0004\n",
      "Epoch 8 - Train Loss: 4071.0426\n",
      "Sub-losses: recon_loss: 4069.7017, var_loss: 1.3409, anneal_factor: 0.0080, effective_beta_factor: 0.0008\n",
      "Epoch 8 - Valid Loss: 483.6313\n",
      "Sub-losses: recon_loss: 483.4180, var_loss: 0.2133, anneal_factor: 0.0080, effective_beta_factor: 0.0008\n",
      "Epoch 9 - Train Loss: 3970.9014\n",
      "Sub-losses: recon_loss: 3967.4734, var_loss: 3.4280, anneal_factor: 0.0167, effective_beta_factor: 0.0017\n",
      "Epoch 9 - Valid Loss: 508.6526\n",
      "Sub-losses: recon_loss: 508.1294, var_loss: 0.5232, anneal_factor: 0.0167, effective_beta_factor: 0.0017\n",
      "Epoch 10 - Train Loss: 4027.3040\n",
      "Sub-losses: recon_loss: 4018.3876, var_loss: 8.9164, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 10 - Valid Loss: 496.3611\n",
      "Sub-losses: recon_loss: 495.2250, var_loss: 1.1361, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 11 - Train Loss: 3903.1681\n",
      "Sub-losses: recon_loss: 3886.9073, var_loss: 16.2608, anneal_factor: 0.0696, effective_beta_factor: 0.0070\n",
      "Epoch 11 - Valid Loss: 491.1547\n",
      "Sub-losses: recon_loss: 488.5589, var_loss: 2.5958, anneal_factor: 0.0696, effective_beta_factor: 0.0070\n",
      "Epoch 12 - Train Loss: 3923.0472\n",
      "Sub-losses: recon_loss: 3887.3368, var_loss: 35.7105, anneal_factor: 0.1357, effective_beta_factor: 0.0136\n",
      "Epoch 12 - Valid Loss: 504.8520\n",
      "Sub-losses: recon_loss: 499.4753, var_loss: 5.3768, anneal_factor: 0.1357, effective_beta_factor: 0.0136\n",
      "Epoch 13 - Train Loss: 3893.4039\n",
      "Sub-losses: recon_loss: 3831.7692, var_loss: 61.6347, anneal_factor: 0.2477, effective_beta_factor: 0.0248\n",
      "Epoch 13 - Valid Loss: 485.2807\n",
      "Sub-losses: recon_loss: 477.0626, var_loss: 8.2181, anneal_factor: 0.2477, effective_beta_factor: 0.0248\n",
      "Epoch 14 - Train Loss: 3868.2794\n",
      "Sub-losses: recon_loss: 3786.1363, var_loss: 82.1432, anneal_factor: 0.4085, effective_beta_factor: 0.0408\n",
      "Epoch 14 - Valid Loss: 484.4265\n",
      "Sub-losses: recon_loss: 472.4406, var_loss: 11.9859, anneal_factor: 0.4085, effective_beta_factor: 0.0408\n",
      "Epoch 15 - Train Loss: 3973.5238\n",
      "Sub-losses: recon_loss: 3843.2139, var_loss: 130.3098, anneal_factor: 0.5915, effective_beta_factor: 0.0592\n",
      "Epoch 15 - Valid Loss: 483.8419\n",
      "Sub-losses: recon_loss: 467.1358, var_loss: 16.7062, anneal_factor: 0.5915, effective_beta_factor: 0.0592\n",
      "Epoch 16 - Train Loss: 3867.8018\n",
      "Sub-losses: recon_loss: 3726.8159, var_loss: 140.9859, anneal_factor: 0.7523, effective_beta_factor: 0.0752\n",
      "Epoch 16 - Valid Loss: 489.2818\n",
      "Sub-losses: recon_loss: 471.0419, var_loss: 18.2400, anneal_factor: 0.7523, effective_beta_factor: 0.0752\n",
      "Epoch 17 - Train Loss: 3882.3136\n",
      "Sub-losses: recon_loss: 3718.4631, var_loss: 163.8505, anneal_factor: 0.8643, effective_beta_factor: 0.0864\n",
      "Epoch 17 - Valid Loss: 480.8018\n",
      "Sub-losses: recon_loss: 464.8241, var_loss: 15.9777, anneal_factor: 0.8643, effective_beta_factor: 0.0864\n",
      "Epoch 18 - Train Loss: 3831.5854\n",
      "Sub-losses: recon_loss: 3690.7036, var_loss: 140.8819, anneal_factor: 0.9304, effective_beta_factor: 0.0930\n",
      "Epoch 18 - Valid Loss: 480.8932\n",
      "Sub-losses: recon_loss: 464.3478, var_loss: 16.5453, anneal_factor: 0.9304, effective_beta_factor: 0.0930\n",
      "Epoch 19 - Train Loss: 3827.1887\n",
      "Sub-losses: recon_loss: 3693.5688, var_loss: 133.6199, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 19 - Valid Loss: 494.6924\n",
      "Sub-losses: recon_loss: 480.2032, var_loss: 14.4892, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 20 - Train Loss: 3734.0012\n",
      "Sub-losses: recon_loss: 3605.3162, var_loss: 128.6851, anneal_factor: 0.9833, effective_beta_factor: 0.0983\n",
      "Epoch 20 - Valid Loss: 480.8520\n",
      "Sub-losses: recon_loss: 466.3126, var_loss: 14.5395, anneal_factor: 0.9833, effective_beta_factor: 0.0983\n",
      "Epoch 21 - Train Loss: 3833.9816\n",
      "Sub-losses: recon_loss: 3702.2674, var_loss: 131.7142, anneal_factor: 0.9920, effective_beta_factor: 0.0992\n",
      "Epoch 21 - Valid Loss: 466.6518\n",
      "Sub-losses: recon_loss: 452.1098, var_loss: 14.5420, anneal_factor: 0.9920, effective_beta_factor: 0.0992\n",
      "Epoch 22 - Train Loss: 3665.3828\n",
      "Sub-losses: recon_loss: 3531.2098, var_loss: 134.1730, anneal_factor: 0.9961, effective_beta_factor: 0.0996\n",
      "Epoch 22 - Valid Loss: 484.5035\n",
      "Sub-losses: recon_loss: 471.2411, var_loss: 13.2624, anneal_factor: 0.9961, effective_beta_factor: 0.0996\n",
      "Epoch 23 - Train Loss: 3778.5304\n",
      "Sub-losses: recon_loss: 3640.3718, var_loss: 138.1586, anneal_factor: 0.9982, effective_beta_factor: 0.0998\n",
      "Epoch 23 - Valid Loss: 478.9924\n",
      "Sub-losses: recon_loss: 465.2567, var_loss: 13.7358, anneal_factor: 0.9982, effective_beta_factor: 0.0998\n",
      "Epoch 24 - Train Loss: 3697.4668\n",
      "Sub-losses: recon_loss: 3572.2989, var_loss: 125.1679, anneal_factor: 0.9991, effective_beta_factor: 0.0999\n",
      "Epoch 24 - Valid Loss: 471.9397\n",
      "Sub-losses: recon_loss: 458.7978, var_loss: 13.1419, anneal_factor: 0.9991, effective_beta_factor: 0.0999\n",
      "Epoch 25 - Train Loss: 3707.4739\n",
      "Sub-losses: recon_loss: 3589.5990, var_loss: 117.8750, anneal_factor: 0.9996, effective_beta_factor: 0.1000\n",
      "Epoch 25 - Valid Loss: 485.8732\n",
      "Sub-losses: recon_loss: 473.4405, var_loss: 12.4328, anneal_factor: 0.9996, effective_beta_factor: 0.1000\n",
      "Epoch 26 - Train Loss: 3580.0159\n",
      "Sub-losses: recon_loss: 3461.2405, var_loss: 118.7754, anneal_factor: 0.9998, effective_beta_factor: 0.1000\n",
      "Epoch 26 - Valid Loss: 485.1238\n",
      "Sub-losses: recon_loss: 472.7646, var_loss: 12.3592, anneal_factor: 0.9998, effective_beta_factor: 0.1000\n",
      "Epoch 27 - Train Loss: 3700.9462\n",
      "Sub-losses: recon_loss: 3588.1581, var_loss: 112.7881, anneal_factor: 0.9999, effective_beta_factor: 0.1000\n",
      "Epoch 27 - Valid Loss: 464.6492\n",
      "Sub-losses: recon_loss: 452.9944, var_loss: 11.6548, anneal_factor: 0.9999, effective_beta_factor: 0.1000\n",
      "Training modality: proteomics\n",
      "Training modality: proteomics\n",
      "Epoch 1 - Train Loss: 6142.1183\n",
      "Sub-losses: recon_loss: 6142.1101, var_loss: 0.0081, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 582.6936\n",
      "Sub-losses: recon_loss: 582.6934, var_loss: 0.0003, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 5641.8488\n",
      "Sub-losses: recon_loss: 5641.8326, var_loss: 0.0162, anneal_factor: 0.0001, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Valid Loss: 603.5356\n",
      "Sub-losses: recon_loss: 603.5345, var_loss: 0.0011, anneal_factor: 0.0001, effective_beta_factor: 0.0000\n",
      "Epoch 3 - Train Loss: 5467.9058\n",
      "Sub-losses: recon_loss: 5467.8704, var_loss: 0.0355, anneal_factor: 0.0002, effective_beta_factor: 0.0000\n",
      "Epoch 3 - Valid Loss: 629.1970\n",
      "Sub-losses: recon_loss: 629.1936, var_loss: 0.0033, anneal_factor: 0.0002, effective_beta_factor: 0.0000\n",
      "Epoch 4 - Train Loss: 4971.2667\n",
      "Sub-losses: recon_loss: 4971.1869, var_loss: 0.0797, anneal_factor: 0.0004, effective_beta_factor: 0.0000\n",
      "Epoch 4 - Valid Loss: 554.0152\n",
      "Sub-losses: recon_loss: 554.0074, var_loss: 0.0078, anneal_factor: 0.0004, effective_beta_factor: 0.0000\n",
      "Epoch 5 - Train Loss: 4684.6028\n",
      "Sub-losses: recon_loss: 4684.4391, var_loss: 0.1637, anneal_factor: 0.0009, effective_beta_factor: 0.0001\n",
      "Epoch 5 - Valid Loss: 607.5368\n",
      "Sub-losses: recon_loss: 607.5194, var_loss: 0.0174, anneal_factor: 0.0009, effective_beta_factor: 0.0001\n",
      "Epoch 6 - Train Loss: 4655.0854\n",
      "Sub-losses: recon_loss: 4654.7615, var_loss: 0.3239, anneal_factor: 0.0018, effective_beta_factor: 0.0002\n",
      "Epoch 6 - Valid Loss: 576.3117\n",
      "Sub-losses: recon_loss: 576.2751, var_loss: 0.0366, anneal_factor: 0.0018, effective_beta_factor: 0.0002\n",
      "Epoch 7 - Train Loss: 4450.3938\n",
      "Sub-losses: recon_loss: 4449.6815, var_loss: 0.7123, anneal_factor: 0.0039, effective_beta_factor: 0.0004\n",
      "Epoch 7 - Valid Loss: 577.5719\n",
      "Sub-losses: recon_loss: 577.5005, var_loss: 0.0714, anneal_factor: 0.0039, effective_beta_factor: 0.0004\n",
      "Epoch 8 - Train Loss: 4194.3816\n",
      "Sub-losses: recon_loss: 4192.8712, var_loss: 1.5104, anneal_factor: 0.0080, effective_beta_factor: 0.0008\n",
      "Epoch 8 - Valid Loss: 577.6362\n",
      "Sub-losses: recon_loss: 577.4815, var_loss: 0.1547, anneal_factor: 0.0080, effective_beta_factor: 0.0008\n",
      "Epoch 9 - Train Loss: 4309.8588\n",
      "Sub-losses: recon_loss: 4306.6729, var_loss: 3.1860, anneal_factor: 0.0167, effective_beta_factor: 0.0017\n",
      "Epoch 9 - Valid Loss: 527.7432\n",
      "Sub-losses: recon_loss: 527.4297, var_loss: 0.3135, anneal_factor: 0.0167, effective_beta_factor: 0.0017\n",
      "Epoch 10 - Train Loss: 4200.8382\n",
      "Sub-losses: recon_loss: 4194.2826, var_loss: 6.5555, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 10 - Valid Loss: 538.0038\n",
      "Sub-losses: recon_loss: 537.2881, var_loss: 0.7157, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 11 - Train Loss: 4171.2261\n",
      "Sub-losses: recon_loss: 4156.5831, var_loss: 14.6430, anneal_factor: 0.0696, effective_beta_factor: 0.0070\n",
      "Epoch 11 - Valid Loss: 530.5583\n",
      "Sub-losses: recon_loss: 529.0470, var_loss: 1.5113, anneal_factor: 0.0696, effective_beta_factor: 0.0070\n",
      "Epoch 12 - Train Loss: 4009.9996\n",
      "Sub-losses: recon_loss: 3982.2936, var_loss: 27.7060, anneal_factor: 0.1357, effective_beta_factor: 0.0136\n",
      "Epoch 12 - Valid Loss: 521.9099\n",
      "Sub-losses: recon_loss: 518.9686, var_loss: 2.9413, anneal_factor: 0.1357, effective_beta_factor: 0.0136\n",
      "Epoch 13 - Train Loss: 3974.5507\n",
      "Sub-losses: recon_loss: 3921.9032, var_loss: 52.6474, anneal_factor: 0.2477, effective_beta_factor: 0.0248\n",
      "Epoch 13 - Valid Loss: 542.9483\n",
      "Sub-losses: recon_loss: 537.9376, var_loss: 5.0107, anneal_factor: 0.2477, effective_beta_factor: 0.0248\n",
      "Epoch 14 - Train Loss: 3996.8318\n",
      "Sub-losses: recon_loss: 3909.7469, var_loss: 87.0850, anneal_factor: 0.4085, effective_beta_factor: 0.0408\n",
      "Epoch 14 - Valid Loss: 536.8329\n",
      "Sub-losses: recon_loss: 527.6890, var_loss: 9.1439, anneal_factor: 0.4085, effective_beta_factor: 0.0408\n",
      "Epoch 15 - Train Loss: 3954.0083\n",
      "Sub-losses: recon_loss: 3825.5841, var_loss: 128.4242, anneal_factor: 0.5915, effective_beta_factor: 0.0592\n",
      "Epoch 15 - Valid Loss: 513.6464\n",
      "Sub-losses: recon_loss: 500.8037, var_loss: 12.8427, anneal_factor: 0.5915, effective_beta_factor: 0.0592\n",
      "Epoch 16 - Train Loss: 3935.3485\n",
      "Sub-losses: recon_loss: 3785.8005, var_loss: 149.5480, anneal_factor: 0.7523, effective_beta_factor: 0.0752\n",
      "Epoch 16 - Valid Loss: 529.3432\n",
      "Sub-losses: recon_loss: 513.4222, var_loss: 15.9210, anneal_factor: 0.7523, effective_beta_factor: 0.0752\n",
      "Epoch 17 - Train Loss: 3966.6768\n",
      "Sub-losses: recon_loss: 3779.8141, var_loss: 186.8627, anneal_factor: 0.8643, effective_beta_factor: 0.0864\n",
      "Epoch 17 - Valid Loss: 524.8805\n",
      "Sub-losses: recon_loss: 507.8089, var_loss: 17.0716, anneal_factor: 0.8643, effective_beta_factor: 0.0864\n",
      "Epoch 18 - Train Loss: 3944.2269\n",
      "Sub-losses: recon_loss: 3784.4498, var_loss: 159.7771, anneal_factor: 0.9304, effective_beta_factor: 0.0930\n",
      "Epoch 18 - Valid Loss: 531.4892\n",
      "Sub-losses: recon_loss: 513.7333, var_loss: 17.7559, anneal_factor: 0.9304, effective_beta_factor: 0.0930\n",
      "Epoch 19 - Train Loss: 3924.0750\n",
      "Sub-losses: recon_loss: 3757.7552, var_loss: 166.3198, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 19 - Valid Loss: 508.0324\n",
      "Sub-losses: recon_loss: 490.4751, var_loss: 17.5573, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 20 - Train Loss: 3778.6114\n",
      "Sub-losses: recon_loss: 3619.5998, var_loss: 159.0115, anneal_factor: 0.9833, effective_beta_factor: 0.0983\n",
      "Epoch 20 - Valid Loss: 510.0629\n",
      "Sub-losses: recon_loss: 492.5885, var_loss: 17.4744, anneal_factor: 0.9833, effective_beta_factor: 0.0983\n",
      "Epoch 21 - Train Loss: 3873.2333\n",
      "Sub-losses: recon_loss: 3723.4922, var_loss: 149.7410, anneal_factor: 0.9920, effective_beta_factor: 0.0992\n",
      "Epoch 21 - Valid Loss: 517.0883\n",
      "Sub-losses: recon_loss: 499.1979, var_loss: 17.8903, anneal_factor: 0.9920, effective_beta_factor: 0.0992\n",
      "Epoch 22 - Train Loss: 3843.3777\n",
      "Sub-losses: recon_loss: 3693.2923, var_loss: 150.0855, anneal_factor: 0.9961, effective_beta_factor: 0.0996\n",
      "Epoch 22 - Valid Loss: 491.8289\n",
      "Sub-losses: recon_loss: 473.8941, var_loss: 17.9347, anneal_factor: 0.9961, effective_beta_factor: 0.0996\n",
      "Epoch 23 - Train Loss: 3801.1101\n",
      "Sub-losses: recon_loss: 3654.8394, var_loss: 146.2707, anneal_factor: 0.9982, effective_beta_factor: 0.0998\n",
      "Epoch 23 - Valid Loss: 508.6148\n",
      "Sub-losses: recon_loss: 491.8533, var_loss: 16.7615, anneal_factor: 0.9982, effective_beta_factor: 0.0998\n",
      "Epoch 24 - Train Loss: 3679.5789\n",
      "Sub-losses: recon_loss: 3542.8816, var_loss: 136.6972, anneal_factor: 0.9991, effective_beta_factor: 0.0999\n",
      "Epoch 24 - Valid Loss: 501.4227\n",
      "Sub-losses: recon_loss: 485.8473, var_loss: 15.5754, anneal_factor: 0.9991, effective_beta_factor: 0.0999\n",
      "Epoch 25 - Train Loss: 3748.0014\n",
      "Sub-losses: recon_loss: 3619.1779, var_loss: 128.8234, anneal_factor: 0.9996, effective_beta_factor: 0.1000\n",
      "Epoch 25 - Valid Loss: 513.1085\n",
      "Sub-losses: recon_loss: 496.9098, var_loss: 16.1987, anneal_factor: 0.9996, effective_beta_factor: 0.1000\n",
      "Epoch 26 - Train Loss: 3748.9767\n",
      "Sub-losses: recon_loss: 3620.9734, var_loss: 128.0034, anneal_factor: 0.9998, effective_beta_factor: 0.1000\n",
      "Epoch 26 - Valid Loss: 493.7195\n",
      "Sub-losses: recon_loss: 477.7742, var_loss: 15.9454, anneal_factor: 0.9998, effective_beta_factor: 0.1000\n",
      "Epoch 27 - Train Loss: 3706.7410\n",
      "Sub-losses: recon_loss: 3583.5475, var_loss: 123.1935, anneal_factor: 0.9999, effective_beta_factor: 0.1000\n",
      "Epoch 27 - Valid Loss: 499.2827\n",
      "Sub-losses: recon_loss: 483.9935, var_loss: 15.2892, anneal_factor: 0.9999, effective_beta_factor: 0.1000\n",
      "['sample_0' 'sample_1' 'sample_10' 'sample_104' 'sample_105' 'sample_106'\n",
      " 'sample_107' 'sample_109' 'sample_11' 'sample_110' 'sample_111'\n",
      " 'sample_112' 'sample_115' 'sample_116' 'sample_117' 'sample_118'\n",
      " 'sample_119' 'sample_12' 'sample_120' 'sample_121' 'sample_122'\n",
      " 'sample_123' 'sample_125' 'sample_127' 'sample_128' 'sample_131'\n",
      " 'sample_132' 'sample_133' 'sample_134' 'sample_135' 'sample_136'\n",
      " 'sample_137' 'sample_138' 'sample_142' 'sample_144' 'sample_145'\n",
      " 'sample_146' 'sample_147' 'sample_148' 'sample_149' 'sample_15'\n",
      " 'sample_150' 'sample_151' 'sample_152' 'sample_153' 'sample_155'\n",
      " 'sample_156' 'sample_16' 'sample_160' 'sample_161' 'sample_162'\n",
      " 'sample_163' 'sample_164' 'sample_165' 'sample_166' 'sample_168'\n",
      " 'sample_17' 'sample_170' 'sample_171' 'sample_172' 'sample_173'\n",
      " 'sample_174' 'sample_177' 'sample_178' 'sample_179' 'sample_180'\n",
      " 'sample_182' 'sample_183' 'sample_184' 'sample_186' 'sample_188'\n",
      " 'sample_19' 'sample_191' 'sample_192' 'sample_194' 'sample_196'\n",
      " 'sample_197' 'sample_199' 'sample_2' 'sample_20' 'sample_200'\n",
      " 'sample_201' 'sample_204' 'sample_206' 'sample_207' 'sample_208'\n",
      " 'sample_209' 'sample_21' 'sample_210' 'sample_211' 'sample_212'\n",
      " 'sample_214' 'sample_215' 'sample_216' 'sample_217' 'sample_218'\n",
      " 'sample_219' 'sample_220' 'sample_221' 'sample_222' 'sample_223'\n",
      " 'sample_224' 'sample_225' 'sample_229' 'sample_231' 'sample_232'\n",
      " 'sample_233' 'sample_234' 'sample_235' 'sample_238' 'sample_239'\n",
      " 'sample_24' 'sample_240' 'sample_241' 'sample_242' 'sample_243'\n",
      " 'sample_248' 'sample_249' 'sample_250' 'sample_251' 'sample_252'\n",
      " 'sample_253' 'sample_255' 'sample_256' 'sample_257' 'sample_258'\n",
      " 'sample_260' 'sample_261' 'sample_262' 'sample_263' 'sample_264'\n",
      " 'sample_265' 'sample_266' 'sample_268' 'sample_27' 'sample_270'\n",
      " 'sample_272' 'sample_273' 'sample_274' 'sample_275' 'sample_276'\n",
      " 'sample_277' 'sample_278' 'sample_279' 'sample_280' 'sample_281'\n",
      " 'sample_282' 'sample_283' 'sample_284' 'sample_285' 'sample_286'\n",
      " 'sample_287' 'sample_288' 'sample_289' 'sample_29' 'sample_292'\n",
      " 'sample_293' 'sample_294' 'sample_296' 'sample_297' 'sample_299'\n",
      " 'sample_30' 'sample_300' 'sample_301' 'sample_302' 'sample_303'\n",
      " 'sample_304' 'sample_305' 'sample_306' 'sample_307' 'sample_309'\n",
      " 'sample_310' 'sample_312' 'sample_313' 'sample_314' 'sample_315'\n",
      " 'sample_316' 'sample_317' 'sample_318' 'sample_319' 'sample_32'\n",
      " 'sample_320' 'sample_325' 'sample_326' 'sample_327' 'sample_328'\n",
      " 'sample_329' 'sample_330' 'sample_331' 'sample_333' 'sample_334'\n",
      " 'sample_335' 'sample_336' 'sample_338' 'sample_339' 'sample_34'\n",
      " 'sample_340' 'sample_343' 'sample_344' 'sample_346' 'sample_347'\n",
      " 'sample_349' 'sample_35' 'sample_350' 'sample_351' 'sample_352'\n",
      " 'sample_353' 'sample_356' 'sample_357' 'sample_358' 'sample_36'\n",
      " 'sample_360' 'sample_361' 'sample_363' 'sample_365' 'sample_366'\n",
      " 'sample_367' 'sample_368' 'sample_37' 'sample_370' 'sample_371'\n",
      " 'sample_376' 'sample_377' 'sample_380' 'sample_381' 'sample_383'\n",
      " 'sample_385' 'sample_386' 'sample_387' 'sample_388' 'sample_391'\n",
      " 'sample_393' 'sample_394' 'sample_397' 'sample_398' 'sample_399'\n",
      " 'sample_40' 'sample_400' 'sample_401' 'sample_402' 'sample_405'\n",
      " 'sample_408' 'sample_41' 'sample_411' 'sample_412' 'sample_414'\n",
      " 'sample_415' 'sample_416' 'sample_417' 'sample_418' 'sample_419'\n",
      " 'sample_42' 'sample_420' 'sample_421' 'sample_422' 'sample_424'\n",
      " 'sample_425' 'sample_426' 'sample_428' 'sample_43' 'sample_430'\n",
      " 'sample_431' 'sample_432' 'sample_433' 'sample_434' 'sample_436'\n",
      " 'sample_438' 'sample_439' 'sample_441' 'sample_442' 'sample_443'\n",
      " 'sample_444' 'sample_446' 'sample_447' 'sample_448' 'sample_45'\n",
      " 'sample_450' 'sample_451' 'sample_452' 'sample_453' 'sample_454'\n",
      " 'sample_456' 'sample_458' 'sample_459' 'sample_460' 'sample_461'\n",
      " 'sample_462' 'sample_463' 'sample_465' 'sample_466' 'sample_467'\n",
      " 'sample_468' 'sample_47' 'sample_470' 'sample_471' 'sample_475'\n",
      " 'sample_478' 'sample_48' 'sample_480' 'sample_482' 'sample_483'\n",
      " 'sample_485' 'sample_487' 'sample_489' 'sample_490' 'sample_491'\n",
      " 'sample_492' 'sample_493' 'sample_494' 'sample_496' 'sample_5'\n",
      " 'sample_50' 'sample_51' 'sample_52' 'sample_54' 'sample_55' 'sample_56'\n",
      " 'sample_57' 'sample_58' 'sample_59' 'sample_6' 'sample_61' 'sample_62'\n",
      " 'sample_63' 'sample_64' 'sample_65' 'sample_68' 'sample_69' 'sample_70'\n",
      " 'sample_71' 'sample_72' 'sample_73' 'sample_74' 'sample_75' 'sample_76'\n",
      " 'sample_77' 'sample_8' 'sample_82' 'sample_83' 'sample_84' 'sample_85'\n",
      " 'sample_87' 'sample_88' 'sample_89' 'sample_9' 'sample_90' 'sample_94'\n",
      " 'sample_96' 'sample_97' 'sample_98']\n",
      "['sample_0' 'sample_1' 'sample_10' 'sample_104' 'sample_105' 'sample_106'\n",
      " 'sample_107' 'sample_109' 'sample_11' 'sample_110' 'sample_111'\n",
      " 'sample_112' 'sample_115' 'sample_116' 'sample_117' 'sample_118'\n",
      " 'sample_119' 'sample_12' 'sample_120' 'sample_121' 'sample_122'\n",
      " 'sample_123' 'sample_125' 'sample_127' 'sample_128' 'sample_131'\n",
      " 'sample_132' 'sample_133' 'sample_134' 'sample_135' 'sample_136'\n",
      " 'sample_137' 'sample_138' 'sample_142' 'sample_144' 'sample_145'\n",
      " 'sample_146' 'sample_147' 'sample_148' 'sample_149' 'sample_15'\n",
      " 'sample_150' 'sample_151' 'sample_152' 'sample_153' 'sample_155'\n",
      " 'sample_156' 'sample_16' 'sample_160' 'sample_161' 'sample_162'\n",
      " 'sample_163' 'sample_164' 'sample_165' 'sample_166' 'sample_168'\n",
      " 'sample_17' 'sample_170' 'sample_171' 'sample_172' 'sample_173'\n",
      " 'sample_174' 'sample_177' 'sample_178' 'sample_179' 'sample_180'\n",
      " 'sample_182' 'sample_183' 'sample_184' 'sample_186' 'sample_188'\n",
      " 'sample_19' 'sample_191' 'sample_192' 'sample_194' 'sample_196'\n",
      " 'sample_197' 'sample_199' 'sample_2' 'sample_20' 'sample_200'\n",
      " 'sample_201' 'sample_204' 'sample_206' 'sample_207' 'sample_208'\n",
      " 'sample_209' 'sample_21' 'sample_210' 'sample_211' 'sample_212'\n",
      " 'sample_214' 'sample_215' 'sample_216' 'sample_217' 'sample_218'\n",
      " 'sample_219' 'sample_220' 'sample_221' 'sample_222' 'sample_223'\n",
      " 'sample_224' 'sample_225' 'sample_229' 'sample_231' 'sample_232'\n",
      " 'sample_233' 'sample_234' 'sample_235' 'sample_238' 'sample_239'\n",
      " 'sample_24' 'sample_240' 'sample_241' 'sample_242' 'sample_243'\n",
      " 'sample_248' 'sample_249' 'sample_250' 'sample_251' 'sample_252'\n",
      " 'sample_253' 'sample_255' 'sample_256' 'sample_257' 'sample_258'\n",
      " 'sample_260' 'sample_261' 'sample_262' 'sample_263' 'sample_264'\n",
      " 'sample_265' 'sample_266' 'sample_268' 'sample_27' 'sample_270'\n",
      " 'sample_272' 'sample_273' 'sample_274' 'sample_275' 'sample_276'\n",
      " 'sample_277' 'sample_278' 'sample_279' 'sample_280' 'sample_281'\n",
      " 'sample_282' 'sample_283' 'sample_284' 'sample_285' 'sample_286'\n",
      " 'sample_287' 'sample_288' 'sample_289' 'sample_29' 'sample_292'\n",
      " 'sample_293' 'sample_294' 'sample_296' 'sample_297' 'sample_299'\n",
      " 'sample_30' 'sample_300' 'sample_301' 'sample_302' 'sample_303'\n",
      " 'sample_304' 'sample_305' 'sample_306' 'sample_307' 'sample_309'\n",
      " 'sample_310' 'sample_312' 'sample_313' 'sample_314' 'sample_315'\n",
      " 'sample_316' 'sample_317' 'sample_318' 'sample_319' 'sample_32'\n",
      " 'sample_320' 'sample_325' 'sample_326' 'sample_327' 'sample_328'\n",
      " 'sample_329' 'sample_330' 'sample_331' 'sample_333' 'sample_334'\n",
      " 'sample_335' 'sample_336' 'sample_338' 'sample_339' 'sample_34'\n",
      " 'sample_340' 'sample_343' 'sample_344' 'sample_346' 'sample_347'\n",
      " 'sample_349' 'sample_35' 'sample_350' 'sample_351' 'sample_352'\n",
      " 'sample_353' 'sample_356' 'sample_357' 'sample_358' 'sample_36'\n",
      " 'sample_360' 'sample_361' 'sample_363' 'sample_365' 'sample_366'\n",
      " 'sample_367' 'sample_368' 'sample_37' 'sample_370' 'sample_371'\n",
      " 'sample_376' 'sample_377' 'sample_380' 'sample_381' 'sample_383'\n",
      " 'sample_385' 'sample_386' 'sample_387' 'sample_388' 'sample_391'\n",
      " 'sample_393' 'sample_394' 'sample_397' 'sample_398' 'sample_399'\n",
      " 'sample_40' 'sample_400' 'sample_401' 'sample_402' 'sample_405'\n",
      " 'sample_408' 'sample_41' 'sample_411' 'sample_412' 'sample_414'\n",
      " 'sample_415' 'sample_416' 'sample_417' 'sample_418' 'sample_419'\n",
      " 'sample_42' 'sample_420' 'sample_421' 'sample_422' 'sample_424'\n",
      " 'sample_425' 'sample_426' 'sample_428' 'sample_43' 'sample_430'\n",
      " 'sample_431' 'sample_432' 'sample_433' 'sample_434' 'sample_436'\n",
      " 'sample_438' 'sample_439' 'sample_441' 'sample_442' 'sample_443'\n",
      " 'sample_444' 'sample_446' 'sample_447' 'sample_448' 'sample_45'\n",
      " 'sample_450' 'sample_451' 'sample_452' 'sample_453' 'sample_454'\n",
      " 'sample_456' 'sample_458' 'sample_459' 'sample_460' 'sample_461'\n",
      " 'sample_462' 'sample_463' 'sample_465' 'sample_466' 'sample_467'\n",
      " 'sample_468' 'sample_47' 'sample_470' 'sample_471' 'sample_475'\n",
      " 'sample_478' 'sample_48' 'sample_480' 'sample_482' 'sample_483'\n",
      " 'sample_485' 'sample_487' 'sample_489' 'sample_490' 'sample_491'\n",
      " 'sample_492' 'sample_493' 'sample_494' 'sample_496' 'sample_5'\n",
      " 'sample_50' 'sample_51' 'sample_52' 'sample_54' 'sample_55' 'sample_56'\n",
      " 'sample_57' 'sample_58' 'sample_59' 'sample_6' 'sample_61' 'sample_62'\n",
      " 'sample_63' 'sample_64' 'sample_65' 'sample_68' 'sample_69' 'sample_70'\n",
      " 'sample_71' 'sample_72' 'sample_73' 'sample_74' 'sample_75' 'sample_76'\n",
      " 'sample_77' 'sample_8' 'sample_82' 'sample_83' 'sample_84' 'sample_85'\n",
      " 'sample_87' 'sample_88' 'sample_89' 'sample_9' 'sample_90' 'sample_94'\n",
      " 'sample_96' 'sample_97' 'sample_98']\n",
      "Found 350 common samples for the stacked autoencoder.\n",
      "['sample_100' 'sample_114' 'sample_13' 'sample_130' 'sample_139'\n",
      " 'sample_14' 'sample_141' 'sample_143' 'sample_167' 'sample_169'\n",
      " 'sample_176' 'sample_185' 'sample_187' 'sample_189' 'sample_193'\n",
      " 'sample_202' 'sample_227' 'sample_228' 'sample_236' 'sample_237'\n",
      " 'sample_245' 'sample_259' 'sample_295' 'sample_31' 'sample_311'\n",
      " 'sample_321' 'sample_324' 'sample_337' 'sample_341' 'sample_342'\n",
      " 'sample_345' 'sample_375' 'sample_384' 'sample_407' 'sample_429'\n",
      " 'sample_437' 'sample_44' 'sample_440' 'sample_455' 'sample_457'\n",
      " 'sample_469' 'sample_477' 'sample_484' 'sample_488' 'sample_495'\n",
      " 'sample_498' 'sample_60' 'sample_67' 'sample_93' 'sample_99']\n",
      "['sample_100' 'sample_114' 'sample_13' 'sample_130' 'sample_139'\n",
      " 'sample_14' 'sample_141' 'sample_143' 'sample_167' 'sample_169'\n",
      " 'sample_176' 'sample_185' 'sample_187' 'sample_189' 'sample_193'\n",
      " 'sample_202' 'sample_227' 'sample_228' 'sample_236' 'sample_237'\n",
      " 'sample_245' 'sample_259' 'sample_295' 'sample_31' 'sample_311'\n",
      " 'sample_321' 'sample_324' 'sample_337' 'sample_341' 'sample_342'\n",
      " 'sample_345' 'sample_375' 'sample_384' 'sample_407' 'sample_429'\n",
      " 'sample_437' 'sample_44' 'sample_440' 'sample_455' 'sample_457'\n",
      " 'sample_469' 'sample_477' 'sample_484' 'sample_488' 'sample_495'\n",
      " 'sample_498' 'sample_60' 'sample_67' 'sample_93' 'sample_99']\n",
      "Found 50 common samples for the stacked autoencoder.\n",
      "finished training each modality model\n",
      "Epoch 1 - Train Loss: 21490.0792\n",
      "Sub-losses: recon_loss: 21490.0691, var_loss: 0.0099, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 2319.1309\n",
      "Sub-losses: recon_loss: 2319.1304, var_loss: 0.0004, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 20810.4937\n",
      "Sub-losses: recon_loss: 20810.4729, var_loss: 0.0205, anneal_factor: 0.0001, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Valid Loss: 2285.2589\n",
      "Sub-losses: recon_loss: 2285.2579, var_loss: 0.0009, anneal_factor: 0.0001, effective_beta_factor: 0.0000\n",
      "Epoch 3 - Train Loss: 19952.8438\n",
      "Sub-losses: recon_loss: 19952.8030, var_loss: 0.0408, anneal_factor: 0.0002, effective_beta_factor: 0.0000\n",
      "Epoch 3 - Valid Loss: 2273.1372\n",
      "Sub-losses: recon_loss: 2273.1351, var_loss: 0.0021, anneal_factor: 0.0002, effective_beta_factor: 0.0000\n",
      "Epoch 4 - Train Loss: 19293.5183\n",
      "Sub-losses: recon_loss: 19293.4340, var_loss: 0.0844, anneal_factor: 0.0004, effective_beta_factor: 0.0000\n",
      "Epoch 4 - Valid Loss: 2251.3121\n",
      "Sub-losses: recon_loss: 2251.3073, var_loss: 0.0048, anneal_factor: 0.0004, effective_beta_factor: 0.0000\n",
      "Epoch 5 - Train Loss: 19050.4972\n",
      "Sub-losses: recon_loss: 19050.3252, var_loss: 0.1719, anneal_factor: 0.0009, effective_beta_factor: 0.0001\n",
      "Epoch 5 - Valid Loss: 2330.5760\n",
      "Sub-losses: recon_loss: 2330.5663, var_loss: 0.0097, anneal_factor: 0.0009, effective_beta_factor: 0.0001\n",
      "Epoch 6 - Train Loss: 18684.9252\n",
      "Sub-losses: recon_loss: 18684.5540, var_loss: 0.3712, anneal_factor: 0.0018, effective_beta_factor: 0.0002\n",
      "Epoch 6 - Valid Loss: 2186.1752\n",
      "Sub-losses: recon_loss: 2186.1548, var_loss: 0.0203, anneal_factor: 0.0018, effective_beta_factor: 0.0002\n",
      "Epoch 7 - Train Loss: 18330.2054\n",
      "Sub-losses: recon_loss: 18329.3986, var_loss: 0.8070, anneal_factor: 0.0039, effective_beta_factor: 0.0004\n",
      "Epoch 7 - Valid Loss: 2227.1011\n",
      "Sub-losses: recon_loss: 2227.0570, var_loss: 0.0441, anneal_factor: 0.0039, effective_beta_factor: 0.0004\n",
      "Epoch 8 - Train Loss: 18148.3843\n",
      "Sub-losses: recon_loss: 18146.7179, var_loss: 1.6667, anneal_factor: 0.0080, effective_beta_factor: 0.0008\n",
      "Epoch 8 - Valid Loss: 2143.3668\n",
      "Sub-losses: recon_loss: 2143.2758, var_loss: 0.0910, anneal_factor: 0.0080, effective_beta_factor: 0.0008\n",
      "Epoch 9 - Train Loss: 17917.1453\n",
      "Sub-losses: recon_loss: 17913.7126, var_loss: 3.4327, anneal_factor: 0.0167, effective_beta_factor: 0.0017\n",
      "Epoch 9 - Valid Loss: 2201.3691\n",
      "Sub-losses: recon_loss: 2201.1716, var_loss: 0.1975, anneal_factor: 0.0167, effective_beta_factor: 0.0017\n",
      "Epoch 10 - Train Loss: 17641.5377\n",
      "Sub-losses: recon_loss: 17634.1401, var_loss: 7.3976, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 10 - Valid Loss: 2156.9382\n",
      "Sub-losses: recon_loss: 2156.5212, var_loss: 0.4169, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 11 - Train Loss: 17654.0782\n",
      "Sub-losses: recon_loss: 17638.9231, var_loss: 15.1551, anneal_factor: 0.0696, effective_beta_factor: 0.0070\n",
      "Epoch 11 - Valid Loss: 2111.1140\n",
      "Sub-losses: recon_loss: 2110.2789, var_loss: 0.8351, anneal_factor: 0.0696, effective_beta_factor: 0.0070\n",
      "Epoch 12 - Train Loss: 17121.8386\n",
      "Sub-losses: recon_loss: 17092.9211, var_loss: 28.9176, anneal_factor: 0.1357, effective_beta_factor: 0.0136\n",
      "Epoch 12 - Valid Loss: 2104.5753\n",
      "Sub-losses: recon_loss: 2103.0696, var_loss: 1.5056, anneal_factor: 0.1357, effective_beta_factor: 0.0136\n",
      "Epoch 13 - Train Loss: 17271.2699\n",
      "Sub-losses: recon_loss: 17216.4651, var_loss: 54.8048, anneal_factor: 0.2477, effective_beta_factor: 0.0248\n",
      "Epoch 13 - Valid Loss: 2118.4428\n",
      "Sub-losses: recon_loss: 2115.6367, var_loss: 2.8061, anneal_factor: 0.2477, effective_beta_factor: 0.0248\n",
      "Epoch 14 - Train Loss: 17239.0565\n",
      "Sub-losses: recon_loss: 17152.7775, var_loss: 86.2791, anneal_factor: 0.4085, effective_beta_factor: 0.0408\n",
      "Epoch 14 - Valid Loss: 2162.5125\n",
      "Sub-losses: recon_loss: 2157.5155, var_loss: 4.9970, anneal_factor: 0.4085, effective_beta_factor: 0.0408\n",
      "Epoch 15 - Train Loss: 17206.5796\n",
      "Sub-losses: recon_loss: 17084.0465, var_loss: 122.5331, anneal_factor: 0.5915, effective_beta_factor: 0.0592\n",
      "Epoch 15 - Valid Loss: 2135.0863\n",
      "Sub-losses: recon_loss: 2128.1667, var_loss: 6.9196, anneal_factor: 0.5915, effective_beta_factor: 0.0592\n",
      "Epoch 16 - Train Loss: 17196.0186\n",
      "Sub-losses: recon_loss: 17038.9814, var_loss: 157.0371, anneal_factor: 0.7523, effective_beta_factor: 0.0752\n",
      "Epoch 16 - Valid Loss: 2136.4482\n",
      "Sub-losses: recon_loss: 2127.9520, var_loss: 8.4962, anneal_factor: 0.7523, effective_beta_factor: 0.0752\n",
      "Epoch 17 - Train Loss: 17175.8660\n",
      "Sub-losses: recon_loss: 17014.2896, var_loss: 161.5765, anneal_factor: 0.8643, effective_beta_factor: 0.0864\n",
      "Epoch 17 - Valid Loss: 2145.2750\n",
      "Sub-losses: recon_loss: 2135.4391, var_loss: 9.8359, anneal_factor: 0.8643, effective_beta_factor: 0.0864\n",
      "Epoch 18 - Train Loss: 16901.7944\n",
      "Sub-losses: recon_loss: 16731.0016, var_loss: 170.7927, anneal_factor: 0.9304, effective_beta_factor: 0.0930\n",
      "Epoch 18 - Valid Loss: 2124.4754\n",
      "Sub-losses: recon_loss: 2114.6652, var_loss: 9.8102, anneal_factor: 0.9304, effective_beta_factor: 0.0930\n",
      "Epoch 19 - Train Loss: 17019.2346\n",
      "Sub-losses: recon_loss: 16846.6725, var_loss: 172.5619, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 19 - Valid Loss: 2125.3618\n",
      "Sub-losses: recon_loss: 2115.9705, var_loss: 9.3913, anneal_factor: 0.9656, effective_beta_factor: 0.0966\n",
      "Epoch 20 - Train Loss: 16980.2859\n",
      "Sub-losses: recon_loss: 16805.0011, var_loss: 175.2846, anneal_factor: 0.9833, effective_beta_factor: 0.0983\n",
      "Epoch 20 - Valid Loss: 2127.1773\n",
      "Sub-losses: recon_loss: 2117.9971, var_loss: 9.1802, anneal_factor: 0.9833, effective_beta_factor: 0.0983\n",
      "Epoch 21 - Train Loss: 16966.9456\n",
      "Sub-losses: recon_loss: 16802.1466, var_loss: 164.7991, anneal_factor: 0.9920, effective_beta_factor: 0.0992\n",
      "Epoch 21 - Valid Loss: 2092.5097\n",
      "Sub-losses: recon_loss: 2084.0126, var_loss: 8.4971, anneal_factor: 0.9920, effective_beta_factor: 0.0992\n",
      "Epoch 22 - Train Loss: 16832.4139\n",
      "Sub-losses: recon_loss: 16682.8467, var_loss: 149.5675, anneal_factor: 0.9961, effective_beta_factor: 0.0996\n",
      "Epoch 22 - Valid Loss: 2112.8704\n",
      "Sub-losses: recon_loss: 2105.2904, var_loss: 7.5800, anneal_factor: 0.9961, effective_beta_factor: 0.0996\n",
      "Epoch 23 - Train Loss: 16801.5293\n",
      "Sub-losses: recon_loss: 16643.2850, var_loss: 158.2443, anneal_factor: 0.9982, effective_beta_factor: 0.0998\n",
      "Epoch 23 - Valid Loss: 2106.9051\n",
      "Sub-losses: recon_loss: 2098.8386, var_loss: 8.0665, anneal_factor: 0.9982, effective_beta_factor: 0.0998\n",
      "Epoch 24 - Train Loss: 16793.6300\n",
      "Sub-losses: recon_loss: 16654.6334, var_loss: 138.9965, anneal_factor: 0.9991, effective_beta_factor: 0.0999\n",
      "Epoch 24 - Valid Loss: 2127.7733\n",
      "Sub-losses: recon_loss: 2120.3243, var_loss: 7.4490, anneal_factor: 0.9991, effective_beta_factor: 0.0999\n",
      "Epoch 25 - Train Loss: 16733.0984\n",
      "Sub-losses: recon_loss: 16589.6656, var_loss: 143.4328, anneal_factor: 0.9996, effective_beta_factor: 0.1000\n",
      "Epoch 25 - Valid Loss: 2131.1288\n",
      "Sub-losses: recon_loss: 2124.0247, var_loss: 7.1041, anneal_factor: 0.9996, effective_beta_factor: 0.1000\n",
      "Epoch 26 - Train Loss: 16726.2390\n",
      "Sub-losses: recon_loss: 16596.0631, var_loss: 130.1757, anneal_factor: 0.9998, effective_beta_factor: 0.1000\n",
      "Epoch 26 - Valid Loss: 2107.8077\n",
      "Sub-losses: recon_loss: 2100.9597, var_loss: 6.8480, anneal_factor: 0.9998, effective_beta_factor: 0.1000\n",
      "Epoch 27 - Train Loss: 16598.9917\n",
      "Sub-losses: recon_loss: 16470.8433, var_loss: 128.1484, anneal_factor: 0.9999, effective_beta_factor: 0.1000\n",
      "Epoch 27 - Valid Loss: 2089.3931\n",
      "Sub-losses: recon_loss: 2082.6385, var_loss: 6.7547, anneal_factor: 0.9999, effective_beta_factor: 0.1000\n",
      "['sample_101' 'sample_102' 'sample_103' 'sample_108' 'sample_113'\n",
      " 'sample_124' 'sample_126' 'sample_129' 'sample_140' 'sample_154'\n",
      " 'sample_157' 'sample_158' 'sample_159' 'sample_175' 'sample_18'\n",
      " 'sample_181' 'sample_190' 'sample_195' 'sample_198' 'sample_203'\n",
      " 'sample_205' 'sample_213' 'sample_22' 'sample_226' 'sample_23'\n",
      " 'sample_230' 'sample_244' 'sample_246' 'sample_247' 'sample_25'\n",
      " 'sample_254' 'sample_26' 'sample_267' 'sample_269' 'sample_271'\n",
      " 'sample_28' 'sample_290' 'sample_291' 'sample_298' 'sample_3'\n",
      " 'sample_308' 'sample_322' 'sample_323' 'sample_33' 'sample_332'\n",
      " 'sample_348' 'sample_354' 'sample_355' 'sample_359' 'sample_362'\n",
      " 'sample_364' 'sample_369' 'sample_372' 'sample_373' 'sample_374'\n",
      " 'sample_378' 'sample_379' 'sample_38' 'sample_382' 'sample_389'\n",
      " 'sample_39' 'sample_390' 'sample_392' 'sample_395' 'sample_396'\n",
      " 'sample_4' 'sample_403' 'sample_404' 'sample_406' 'sample_409'\n",
      " 'sample_410' 'sample_413' 'sample_423' 'sample_427' 'sample_435'\n",
      " 'sample_445' 'sample_449' 'sample_46' 'sample_464' 'sample_472'\n",
      " 'sample_473' 'sample_474' 'sample_476' 'sample_479' 'sample_481'\n",
      " 'sample_486' 'sample_49' 'sample_497' 'sample_499' 'sample_53'\n",
      " 'sample_66' 'sample_7' 'sample_78' 'sample_79' 'sample_80' 'sample_81'\n",
      " 'sample_86' 'sample_91' 'sample_92' 'sample_95']\n",
      "['sample_101' 'sample_102' 'sample_103' 'sample_108' 'sample_113'\n",
      " 'sample_124' 'sample_126' 'sample_129' 'sample_140' 'sample_154'\n",
      " 'sample_157' 'sample_158' 'sample_159' 'sample_175' 'sample_18'\n",
      " 'sample_181' 'sample_190' 'sample_195' 'sample_198' 'sample_203'\n",
      " 'sample_205' 'sample_213' 'sample_22' 'sample_226' 'sample_23'\n",
      " 'sample_230' 'sample_244' 'sample_246' 'sample_247' 'sample_25'\n",
      " 'sample_254' 'sample_26' 'sample_267' 'sample_269' 'sample_271'\n",
      " 'sample_28' 'sample_290' 'sample_291' 'sample_298' 'sample_3'\n",
      " 'sample_308' 'sample_322' 'sample_323' 'sample_33' 'sample_332'\n",
      " 'sample_348' 'sample_354' 'sample_355' 'sample_359' 'sample_362'\n",
      " 'sample_364' 'sample_369' 'sample_372' 'sample_373' 'sample_374'\n",
      " 'sample_378' 'sample_379' 'sample_38' 'sample_382' 'sample_389'\n",
      " 'sample_39' 'sample_390' 'sample_392' 'sample_395' 'sample_396'\n",
      " 'sample_4' 'sample_403' 'sample_404' 'sample_406' 'sample_409'\n",
      " 'sample_410' 'sample_413' 'sample_423' 'sample_427' 'sample_435'\n",
      " 'sample_445' 'sample_449' 'sample_46' 'sample_464' 'sample_472'\n",
      " 'sample_473' 'sample_474' 'sample_476' 'sample_479' 'sample_481'\n",
      " 'sample_486' 'sample_49' 'sample_497' 'sample_499' 'sample_53'\n",
      " 'sample_66' 'sample_7' 'sample_78' 'sample_79' 'sample_80' 'sample_81'\n",
      " 'sample_86' 'sample_91' 'sample_92' 'sample_95']\n",
      "Found 100 common samples for the stacked autoencoder.\n",
      "<autoencodix.data._numeric_dataset.NumericDataset object at 0x34aa06e60>\n",
      "Successfully created annotated latent space object (adata_latent).\n"
     ]
    }
   ],
   "source": [
    "from autoencodix.utils.example_data import EXAMPLE_MULTI_BULK\n",
    "from autoencodix.configs.stackix_config import StackixConfig\n",
    "from autoencodix.configs.default_config import DataCase\n",
    "import autoencodix as acx\n",
    "\n",
    "# Stackix has its own config class\n",
    "# instead of passing a pandas DataFrame, we use a pre-defined DataPackage object directly.\n",
    "# this time with single cell data\n",
    "print(\"Input data:\")\n",
    "print(EXAMPLE_MULTI_BULK)\n",
    "print(\"-\" *50)\n",
    "my_config = StackixConfig(\n",
    "    epochs=27,\n",
    "    checkpoint_interval=5,\n",
    "    default_vae_loss=\"kl\", # kl or mmd possible\n",
    "    data_case=DataCase.MULTI_BULK,\n",
    ")\n",
    "print(\"\\n\")\n",
    "print(\"Starting Pipeline\")\n",
    "print(\"-\"*50)\n",
    "print(\"-\"*50)\n",
    "stackix = acx.Stackix(data=EXAMPLE_MULTI_BULK, config=my_config)\n",
    "result = stackix.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05906815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb84dfb",
   "metadata": {},
   "source": [
    "## 2) Stackix Specific Steps\n",
    "While Stackix has no extra/different steps in the pipeline. The prerpocessing and training step works a bit differntly than for the Varix or Vanillix. Furthermore is it possible to add paired and unpaired data (in comparison to only paired data for Vanillix and Varix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed79795",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
