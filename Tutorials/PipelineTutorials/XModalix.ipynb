{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df474bf2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# How To Use X-Modalix\n",
    "X-Modalix is our implementation of a cross modal autoencoder.  \n",
    "This tutorial follows the structure of our `Getting Started - Vanillix`, but is much less extenisve, because\n",
    "our pipeline works similar for different architecture, so here we focus only on `X-Modalix` specifics.\n",
    "\n",
    "**AUTOENCODIX** supports far more functionality than shown here, so we’ll also point to advanced tutorials where relevant.  \n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "> This tutorial only shows the specifics of the Stackix pipeline. If you're unfamilar with general concepts,  \n",
    "> we recommend to follow the `Getting Started - Vanillix` Tutorial first.\n",
    "\n",
    "\n",
    "## X-Modalix Theory\n",
    "\n",
    "The underlying models are two separate VAE's for each data modality which are trained in parallel. However, their latent spaces are very closely aligned in a way that data modalities can hardly be discriminated, but sample variation is kept.  \n",
    "By this alignment one can easily translate between data modalities. To get an aligned latent space, the loss function to train both VAE's is extended by the following terms and ideas:\n",
    "\n",
    "- *Adversarial* loss term: in parallel to both VAE a third neural network (latent space classifier) is trained to discriminate the latent space embeddings of both data modalities. Analogously to a GAN, the inverse loss of this latent space classifier is used as loss term in addition to the normal VAE loss function to enforce an alignment of latent spaces. \n",
    "- *Paired* loss term: Latent space alignment is further enforced by minimizing the distance between samples with paired measurements of each data modality. In our current implementation, all samples must have paired measurements to use the paired loss. \n",
    "- *Class-based* loss term: If one does not have paired measured samples, a semi-supervised training loss term can be used, which minimizes distance among each class of samples. In our example this will be the five cancer subtypes.\n",
    "\n",
    "\n",
    "---\n",
    "## What You'll Learn\n",
    "\n",
    " TODO explain showcase\n",
    "As a showcase for data modality translation by `x-modalix`, we will use cancer gene expression from TCGA as in previous notebooks in combination with handwritten digits from the [MNIST data set](https://keras.io/api/datasets/mnist/).  \n",
    "Our goal is to translate the gene expression signature of five selected cancer subtypes to images of digits, where each cancer subtype class is assigned to a digit between 0-4:  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/jan-forest/autoencodix/729f3e8172a722e7c3681298719d9d4b59e313d9/images/xmodalix_scheme.svg\" alt=\"ontix-ontologies\" width=\"1200\"/>\n",
    "In practice those images can be histopathological images or any other data modality. Before we show data preparation and `x-modalix` training, some background to the basic idea of a cross-modal VAE as proposed by [Yang & Uhler](https://arxiv.org/abs/1902.03515). \n",
    "\n",
    "\n",
    "While following the showcase you learn how to:\n",
    "\n",
    "1. **Initialize** the pipeline and run the pipeline. <br> <br>\n",
    "2. Understand the X-Modalix sepecific **pipeline steps** (paired vs. unpaired data). <br><br>\n",
    "3. Access the X-Modalix specific **results** (sub-results for modality autoencoders). <br><br>\n",
    "4. **Visualize** outputs. <br><br>\n",
    "5. Apply **custom parameters**. <br><br>\n",
    "6. **Save, load, and reuse** a trained pipeline. <br><br>\n",
    "\n",
    "---\n",
    "\n",
    "Let’s get started! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6613ccd7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
