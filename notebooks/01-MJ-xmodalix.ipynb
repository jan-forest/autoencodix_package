{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c97c74",
   "metadata": {},
   "source": [
    "## XModalix Port First Milestone: ImageVAE + Loader\n",
    "\n",
    "\n",
    "### Outcome\n",
    "- Have a image loader and image VAE\n",
    "- Train this in a notebook\n",
    "    - With c. elegans and MNIST images\n",
    "\n",
    "\n",
    "### Checks\n",
    "- Check loss curves\n",
    "- Check image recons\n",
    "- \n",
    "\n",
    "\n",
    "### Steps\n",
    "- Prepare datasets\n",
    "    - Maybe already done in 00 notebook\n",
    "- Prepare config\n",
    "- Write ImageDataset Class\n",
    "- Write / port ImageVAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1992a8",
   "metadata": {},
   "source": [
    "#### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3efd521e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianjoas/development/autoencodix_package/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/maximilianjoas/development/autoencodix_package/notebooks\n",
      "/Users/maximilianjoas/development/autoencodix_package\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from autoencodix.utils.default_config import DataConfig, DataInfo, DefaultConfig\n",
    "import autoencodix as acx\n",
    "\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "print(notebook_dir)\n",
    "os.chdir(notebook_dir)\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a5632",
   "metadata": {},
   "source": [
    "#### GLOBALS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581dd8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGROOT = os.path.join(\"data/images/ALY-2_SYS721/\")\n",
    "IMGMAPPING = os.path.join(\"data/ALY-2_SYS721_mappings.txt\")\n",
    "NUMFILE = os.path.join(\"data/AM3_NO2_raw_cell.tsv\")\n",
    "\n",
    "img_config = DefaultConfig(\n",
    "    checkpoint_interval=10,\n",
    "    class_param=\"early\",\n",
    "    epochs=30,\n",
    "    pretrain_epochs=0,\n",
    "    batch_size=64,\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"IMG\": DataInfo(\n",
    "                file_path=IMGROOT,\n",
    "                data_type=\"IMG\",\n",
    "                translate_direction=\"to\",\n",
    "                pretrain_epochs=20\n",
    "            ),\n",
    "            \"RNA\": DataInfo(\n",
    "                file_path=NUMFILE,\n",
    "                data_type=\"NUMERIC\",\n",
    "                translate_direction=\"from\",\n",
    "            ),\n",
    "            \"RNA2\": DataInfo(\n",
    "                file_path=NUMFILE,\n",
    "                data_type=\"NUMERIC\",\n",
    "            ),\n",
    "            \"ANNO\": DataInfo(\n",
    "                file_path=IMGMAPPING,\n",
    "                data_type=\"ANNOTATION\",\n",
    "            ),\n",
    "        },\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22a6a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing\n",
      "Checking data type: IMG\n",
      "Found image type in config\n",
      "current data info: file_path='data/images/ALY-2_SYS721/' data_type='IMG' scaling='STANDARD' filtering='VAR' sep=None extra_anno_file=None is_single_cell=False min_cells=0.05 min_genes=0.02 selected_layers=['X'] is_X=False normalize_counts=True log_transform=True k_filter=20 img_root=None img_width_resize=64 img_height_resize=64 translate_direction='to' pretrain_epochs=20\n",
      "Checking data type: RNA\n",
      "Checking data type: RNA2\n",
      "Checking data type: ANNO\n",
      "Given image size is possible, rescaling images to: 64x64\n",
      "reading annotation file: data/ALY-2_SYS721_mappings.txt\n",
      " n_samples: {'multi_sc': {'multi_sc': 0}, 'multi_bulk': {'RNA': 260, 'RNA2': 260}, 'annotation': {'paired': 260}, 'img': {'IMG': 260}, 'from_modality': {}, 'to_modality': {}, 'paired_count': {'paired_count': 260}}\n",
      "Converting 182 images to torch.float32 tensors...\n",
      "Converting 52 images to torch.float32 tensors...\n",
      "Converting 26 images to torch.float32 tensors...\n",
      "key: train, type: <class 'dict'>\n",
      "key: valid, type: <class 'dict'>\n",
      "key: test, type: <class 'dict'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maximilianjoas/development/autoencodix_package/src/autoencodix/utils/_imgreader.py:251: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support sep=None with delim_whitespace=False; you can avoid this warning by specifying engine='python'.\n",
      "  annotation = pd.read_csv(anno_file, sep=sep)\n"
     ]
    }
   ],
   "source": [
    "xmodalix = acx.XModalix(config=img_config)\n",
    "ds = xmodalix.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2b9f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check if we need to pretrain: multi_bulk.RNA\n",
      "pretrain epochs : 0\n",
      "No pretraining for multi_bulk.RNA\n",
      "Check if we need to pretrain: multi_bulk.RNA2\n",
      "pretrain epochs : 0\n",
      "No pretraining for multi_bulk.RNA2\n",
      "Check if we need to pretrain: img.IMG\n",
      "pretrain epochs : 20\n",
      "Starting Pretraining for: img.IMG with <class 'autoencodix.trainers._general_trainer.GeneralTrainer'>\n",
      "Epoch 1 - Train Loss: 3.7376\n",
      "Sub-losses: recon_loss: 3.7360, var_loss: 353.6103, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 1 - Valid Loss: 1.2470\n",
      "Sub-losses: recon_loss: 1.2470, var_loss: 0.0110, anneal_factor: 0.0000, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Train Loss: 3.1950\n",
      "Sub-losses: recon_loss: 3.1949, var_loss: 11.7053, anneal_factor: 0.0001, effective_beta_factor: 0.0000\n",
      "Epoch 2 - Valid Loss: 1.2343\n",
      "Sub-losses: recon_loss: 1.2343, var_loss: 0.1828, anneal_factor: 0.0001, effective_beta_factor: 0.0000\n",
      "Epoch 3 - Train Loss: 2.9293\n",
      "Sub-losses: recon_loss: 2.9291, var_loss: 11.1733, anneal_factor: 0.0002, effective_beta_factor: 0.0000\n",
      "Epoch 3 - Valid Loss: 1.2007\n",
      "Sub-losses: recon_loss: 1.2007, var_loss: 0.9519, anneal_factor: 0.0002, effective_beta_factor: 0.0000\n",
      "Epoch 4 - Train Loss: 2.7866\n",
      "Sub-losses: recon_loss: 2.7862, var_loss: 13.4326, anneal_factor: 0.0003, effective_beta_factor: 0.0000\n",
      "Epoch 4 - Valid Loss: 1.1248\n",
      "Sub-losses: recon_loss: 1.1248, var_loss: 2.4593, anneal_factor: 0.0003, effective_beta_factor: 0.0000\n",
      "Epoch 5 - Train Loss: 2.6818\n",
      "Sub-losses: recon_loss: 2.6808, var_loss: 15.6114, anneal_factor: 0.0007, effective_beta_factor: 0.0001\n",
      "Epoch 5 - Valid Loss: 1.0251\n",
      "Sub-losses: recon_loss: 1.0248, var_loss: 4.3337, anneal_factor: 0.0007, effective_beta_factor: 0.0001\n",
      "Epoch 6 - Train Loss: 2.5989\n",
      "Sub-losses: recon_loss: 2.5967, var_loss: 17.5078, anneal_factor: 0.0013, effective_beta_factor: 0.0001\n",
      "Epoch 6 - Valid Loss: 0.9397\n",
      "Sub-losses: recon_loss: 0.9390, var_loss: 6.0265, anneal_factor: 0.0013, effective_beta_factor: 0.0001\n",
      "Epoch 7 - Train Loss: 2.5274\n",
      "Sub-losses: recon_loss: 2.5228, var_loss: 18.6893, anneal_factor: 0.0025, effective_beta_factor: 0.0002\n",
      "Epoch 7 - Valid Loss: 0.8841\n",
      "Sub-losses: recon_loss: 0.8823, var_loss: 7.0131, anneal_factor: 0.0025, effective_beta_factor: 0.0002\n",
      "Epoch 8 - Train Loss: 2.4617\n",
      "Sub-losses: recon_loss: 2.4529, var_loss: 18.4256, anneal_factor: 0.0048, effective_beta_factor: 0.0005\n",
      "Epoch 8 - Valid Loss: 0.8435\n",
      "Sub-losses: recon_loss: 0.8402, var_loss: 6.8255, anneal_factor: 0.0048, effective_beta_factor: 0.0005\n",
      "Epoch 9 - Train Loss: 2.4059\n",
      "Sub-losses: recon_loss: 2.3909, var_loss: 16.0462, anneal_factor: 0.0093, effective_beta_factor: 0.0009\n",
      "Epoch 9 - Valid Loss: 0.8116\n",
      "Sub-losses: recon_loss: 0.8065, var_loss: 5.4669, anneal_factor: 0.0093, effective_beta_factor: 0.0009\n",
      "Epoch 10 - Train Loss: 2.3526\n",
      "Sub-losses: recon_loss: 2.3317, var_loss: 11.6167, anneal_factor: 0.0180, effective_beta_factor: 0.0018\n",
      "Epoch 10 - Valid Loss: 0.7911\n",
      "Sub-losses: recon_loss: 0.7852, var_loss: 3.3336, anneal_factor: 0.0180, effective_beta_factor: 0.0018\n",
      "Epoch 11 - Train Loss: 2.2996\n",
      "Sub-losses: recon_loss: 2.2782, var_loss: 6.2055, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 11 - Valid Loss: 0.7869\n",
      "Sub-losses: recon_loss: 0.7827, var_loss: 1.2094, anneal_factor: 0.0344, effective_beta_factor: 0.0034\n",
      "Epoch 12 - Train Loss: 2.2457\n",
      "Sub-losses: recon_loss: 2.2335, var_loss: 1.8849, anneal_factor: 0.0650, effective_beta_factor: 0.0065\n",
      "Epoch 12 - Valid Loss: 0.8098\n",
      "Sub-losses: recon_loss: 0.8090, var_loss: 0.1254, anneal_factor: 0.0650, effective_beta_factor: 0.0065\n",
      "Epoch 13 - Train Loss: 2.1983\n",
      "Sub-losses: recon_loss: 2.1954, var_loss: 0.2443, anneal_factor: 0.1192, effective_beta_factor: 0.0119\n",
      "Epoch 13 - Valid Loss: 0.7928\n",
      "Sub-losses: recon_loss: 0.7925, var_loss: 0.0291, anneal_factor: 0.1192, effective_beta_factor: 0.0119\n",
      "Epoch 14 - Train Loss: 2.1511\n",
      "Sub-losses: recon_loss: 2.1476, var_loss: 0.1666, anneal_factor: 0.2086, effective_beta_factor: 0.0209\n",
      "Epoch 14 - Valid Loss: 0.7548\n",
      "Sub-losses: recon_loss: 0.7541, var_loss: 0.0365, anneal_factor: 0.2086, effective_beta_factor: 0.0209\n",
      "Epoch 15 - Train Loss: 2.1067\n",
      "Sub-losses: recon_loss: 2.1029, var_loss: 0.1102, anneal_factor: 0.3392, effective_beta_factor: 0.0339\n",
      "Epoch 15 - Valid Loss: 0.7376\n",
      "Sub-losses: recon_loss: 0.7375, var_loss: 0.0040, anneal_factor: 0.3392, effective_beta_factor: 0.0339\n",
      "Epoch 16 - Train Loss: 2.0665\n",
      "Sub-losses: recon_loss: 2.0657, var_loss: 0.0152, anneal_factor: 0.5000, effective_beta_factor: 0.0500\n",
      "Epoch 16 - Valid Loss: 0.7155\n",
      "Sub-losses: recon_loss: 0.7153, var_loss: 0.0034, anneal_factor: 0.5000, effective_beta_factor: 0.0500\n",
      "Epoch 17 - Train Loss: 2.0301\n",
      "Sub-losses: recon_loss: 2.0295, var_loss: 0.0086, anneal_factor: 0.6608, effective_beta_factor: 0.0661\n",
      "Epoch 17 - Valid Loss: 0.6929\n",
      "Sub-losses: recon_loss: 0.6927, var_loss: 0.0026, anneal_factor: 0.6608, effective_beta_factor: 0.0661\n",
      "Epoch 18 - Train Loss: 1.9985\n",
      "Sub-losses: recon_loss: 1.9979, var_loss: 0.0078, anneal_factor: 0.7914, effective_beta_factor: 0.0791\n",
      "Epoch 18 - Valid Loss: 0.6764\n",
      "Sub-losses: recon_loss: 0.6762, var_loss: 0.0026, anneal_factor: 0.7914, effective_beta_factor: 0.0791\n",
      "Epoch 19 - Train Loss: 1.9666\n",
      "Sub-losses: recon_loss: 1.9659, var_loss: 0.0078, anneal_factor: 0.8808, effective_beta_factor: 0.0881\n",
      "Epoch 19 - Valid Loss: 0.6638\n",
      "Sub-losses: recon_loss: 0.6636, var_loss: 0.0026, anneal_factor: 0.8808, effective_beta_factor: 0.0881\n",
      "Epoch 20 - Train Loss: 1.9399\n",
      "Sub-losses: recon_loss: 1.9392, var_loss: 0.0078, anneal_factor: 0.9350, effective_beta_factor: 0.0935\n",
      "Epoch 20 - Valid Loss: 0.6503\n",
      "Sub-losses: recon_loss: 0.6501, var_loss: 0.0026, anneal_factor: 0.9350, effective_beta_factor: 0.0935\n",
      "--- Epoch 1/30 ---\n",
      "split: train, n_samples: 1348\n",
      "Epoch 1/30 - Train Loss: 0.1665\n",
      "split: valid, n_samples: 124\n",
      "Epoch 1/30 - Valid Loss: 0.3813\n",
      "--- Epoch 2/30 ---\n",
      "split: train, n_samples: 1293\n",
      "Epoch 2/30 - Train Loss: 0.1696\n",
      "split: valid, n_samples: 129\n",
      "Epoch 2/30 - Valid Loss: 0.3676\n",
      "--- Epoch 3/30 ---\n",
      "split: train, n_samples: 1361\n",
      "Epoch 3/30 - Train Loss: 0.1612\n",
      "no common ids\n",
      "no common ids\n",
      "split: valid, n_samples: 119\n",
      "Epoch 3/30 - Valid Loss: 0.4013\n",
      "--- Epoch 4/30 ---\n",
      "split: train, n_samples: 1319\n",
      "Epoch 4/30 - Train Loss: 0.1648\n",
      "split: valid, n_samples: 129\n",
      "Epoch 4/30 - Valid Loss: 0.3679\n",
      "--- Epoch 5/30 ---\n",
      "split: train, n_samples: 1294\n",
      "Epoch 5/30 - Train Loss: 0.1663\n",
      "no common ids\n",
      "no common ids\n",
      "split: valid, n_samples: 124\n",
      "Epoch 5/30 - Valid Loss: 0.3851\n",
      "--- Epoch 6/30 ---\n",
      "split: train, n_samples: 1329\n",
      "Epoch 6/30 - Train Loss: 0.1614\n",
      "no common ids\n",
      "no common ids\n",
      "split: valid, n_samples: 126\n",
      "Epoch 6/30 - Valid Loss: 0.3863\n",
      "--- Epoch 7/30 ---\n",
      "split: train, n_samples: 1315\n",
      "Epoch 7/30 - Train Loss: 0.1601\n",
      "split: valid, n_samples: 135\n",
      "Epoch 7/30 - Valid Loss: 0.3527\n",
      "--- Epoch 8/30 ---\n",
      "split: train, n_samples: 1312\n",
      "Epoch 8/30 - Train Loss: 0.1584\n",
      "split: valid, n_samples: 121\n",
      "Epoch 8/30 - Valid Loss: 0.3850\n",
      "--- Epoch 9/30 ---\n",
      "split: train, n_samples: 1286\n",
      "Epoch 9/30 - Train Loss: 0.1612\n",
      "split: valid, n_samples: 140\n",
      "Epoch 9/30 - Valid Loss: 0.3326\n",
      "--- Epoch 10/30 ---\n",
      "split: train, n_samples: 1315\n",
      "Epoch 10/30 - Train Loss: 0.1580\n",
      "split: valid, n_samples: 134\n",
      "Epoch 10/30 - Valid Loss: 0.3567\n",
      "Storing checkpoint for epoch 9...\n",
      "--- Epoch 11/30 ---\n",
      "split: train, n_samples: 1331\n",
      "Epoch 11/30 - Train Loss: 0.1556\n",
      "split: valid, n_samples: 126\n",
      "Epoch 11/30 - Valid Loss: 0.3733\n",
      "--- Epoch 12/30 ---\n",
      "split: train, n_samples: 1304\n",
      "Epoch 12/30 - Train Loss: 0.1567\n",
      "split: valid, n_samples: 126\n",
      "Epoch 12/30 - Valid Loss: 0.3686\n",
      "--- Epoch 13/30 ---\n",
      "split: train, n_samples: 1315\n",
      "Epoch 13/30 - Train Loss: 0.1550\n",
      "split: valid, n_samples: 129\n",
      "Epoch 13/30 - Valid Loss: 0.3558\n",
      "--- Epoch 14/30 ---\n",
      "split: train, n_samples: 1316\n",
      "Epoch 14/30 - Train Loss: 0.1527\n",
      "split: valid, n_samples: 130\n",
      "Epoch 14/30 - Valid Loss: 0.3573\n",
      "--- Epoch 15/30 ---\n",
      "split: train, n_samples: 1316\n",
      "Epoch 15/30 - Train Loss: 0.1540\n",
      "split: valid, n_samples: 126\n",
      "Epoch 15/30 - Valid Loss: 0.3697\n",
      "--- Epoch 16/30 ---\n",
      "split: train, n_samples: 1327\n",
      "Epoch 16/30 - Train Loss: 0.1522\n",
      "split: valid, n_samples: 127\n",
      "Epoch 16/30 - Valid Loss: 0.3568\n",
      "--- Epoch 17/30 ---\n",
      "split: train, n_samples: 1314\n",
      "Epoch 17/30 - Train Loss: 0.1529\n",
      "split: valid, n_samples: 129\n",
      "Epoch 17/30 - Valid Loss: 0.3523\n",
      "--- Epoch 18/30 ---\n",
      "split: train, n_samples: 1275\n",
      "Epoch 18/30 - Train Loss: 0.1570\n",
      "split: valid, n_samples: 126\n",
      "Epoch 18/30 - Valid Loss: 0.3719\n",
      "--- Epoch 19/30 ---\n",
      "split: train, n_samples: 1307\n",
      "Epoch 19/30 - Train Loss: 0.1533\n",
      "split: valid, n_samples: 129\n",
      "Epoch 19/30 - Valid Loss: 0.3516\n",
      "--- Epoch 20/30 ---\n",
      "split: train, n_samples: 1294\n",
      "Epoch 20/30 - Train Loss: 0.1537\n",
      "split: valid, n_samples: 118\n",
      "Epoch 20/30 - Valid Loss: 0.3930\n",
      "Storing checkpoint for epoch 19...\n",
      "--- Epoch 21/30 ---\n",
      "split: train, n_samples: 1350\n",
      "Epoch 21/30 - Train Loss: 0.1481\n",
      "split: valid, n_samples: 135\n",
      "Epoch 21/30 - Valid Loss: 0.3363\n",
      "--- Epoch 22/30 ---\n",
      "split: train, n_samples: 1333\n",
      "Epoch 22/30 - Train Loss: 0.1498\n",
      "split: valid, n_samples: 129\n",
      "Epoch 22/30 - Valid Loss: 0.3471\n",
      "--- Epoch 23/30 ---\n",
      "split: train, n_samples: 1309\n",
      "Epoch 23/30 - Train Loss: 0.1525\n",
      "split: valid, n_samples: 136\n",
      "Epoch 23/30 - Valid Loss: 0.3328\n",
      "--- Epoch 24/30 ---\n",
      "split: train, n_samples: 1349\n",
      "Epoch 24/30 - Train Loss: 0.1471\n",
      "split: valid, n_samples: 132\n",
      "Epoch 24/30 - Valid Loss: 0.3432\n",
      "--- Epoch 25/30 ---\n",
      "split: train, n_samples: 1313\n",
      "Epoch 25/30 - Train Loss: 0.1512\n",
      "split: valid, n_samples: 126\n",
      "Epoch 25/30 - Valid Loss: 0.3450\n",
      "--- Epoch 26/30 ---\n",
      "split: train, n_samples: 1307\n",
      "Epoch 26/30 - Train Loss: 0.1514\n",
      "split: valid, n_samples: 138\n",
      "Epoch 26/30 - Valid Loss: 0.3279\n",
      "--- Epoch 27/30 ---\n",
      "split: train, n_samples: 1305\n",
      "Epoch 27/30 - Train Loss: 0.1517\n",
      "split: valid, n_samples: 129\n",
      "Epoch 27/30 - Valid Loss: 0.3465\n",
      "--- Epoch 28/30 ---\n",
      "split: train, n_samples: 1311\n",
      "Epoch 28/30 - Train Loss: 0.1511\n",
      "split: valid, n_samples: 130\n",
      "Epoch 28/30 - Valid Loss: 0.3470\n",
      "--- Epoch 29/30 ---\n",
      "split: train, n_samples: 1323\n",
      "Epoch 29/30 - Train Loss: 0.1495\n",
      "split: valid, n_samples: 131\n",
      "Epoch 29/30 - Valid Loss: 0.3416\n",
      "--- Epoch 30/30 ---\n",
      "split: train, n_samples: 1309\n",
      "Epoch 30/30 - Train Loss: 0.1506\n",
      "split: valid, n_samples: 128\n",
      "Epoch 30/30 - Valid Loss: 0.3499\n",
      "Storing checkpoint for epoch 29...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "xmodalix.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d332cf2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xmodalix._trainer._trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16994eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline object saved successfully.\n",
      "Preprocessor saved successfully.\n",
      "Model state saved successfully.\n"
     ]
    }
   ],
   "source": [
    "xmodalix.save(\"xmodalix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9bb8a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load a pipeline from xmodalix.pkl...\n",
      "Pipeline object loaded successfully. Actual type: XModalix\n",
      "Preprocessor loaded successfully.\n",
      "Model state loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "xmodalix = acx.XModalix.load(\"xmodalix.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e25d9e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-modal prediction...\n",
      "Translating from 'multi_bulk.RNA' to 'img.IMG'...\n",
      "Prediction complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Result Object Public Attributes:\n",
       "------------------------------\n",
       "latentspaces: TrainingDynamics object\n",
       "sample_ids: TrainingDynamics object\n",
       "reconstructions: TrainingDynamics object\n",
       "mus: TrainingDynamics object\n",
       "sigmas: TrainingDynamics object\n",
       "losses: TrainingDynamics object\n",
       "sub_losses: LossRegistry(_losses={'total_loss': TrainingDynamics(), 'adver_loss': TrainingDynamics(), 'aggregated_sub_losses': TrainingDynamics(), 'paired_loss': TrainingDynamics(), 'class_loss': TrainingDynamics(), 'multi_bulk.RNA.recon_loss': TrainingDynamics(), 'multi_bulk.RNA.var_loss': TrainingDynamics(), 'multi_bulk.RNA.anneal_factor': TrainingDynamics(), 'multi_bulk.RNA.effective_beta_factor': TrainingDynamics(), 'multi_bulk.RNA.loss': TrainingDynamics(), 'multi_bulk.RNA2.recon_loss': TrainingDynamics(), 'multi_bulk.RNA2.var_loss': TrainingDynamics(), 'multi_bulk.RNA2.anneal_factor': TrainingDynamics(), 'multi_bulk.RNA2.effective_beta_factor': TrainingDynamics(), 'multi_bulk.RNA2.loss': TrainingDynamics(), 'img.IMG.recon_loss': TrainingDynamics(), 'img.IMG.var_loss': TrainingDynamics(), 'img.IMG.anneal_factor': TrainingDynamics(), 'img.IMG.effective_beta_factor': TrainingDynamics(), 'img.IMG.loss': TrainingDynamics(), 'clf_loss': TrainingDynamics()})\n",
       "preprocessed_data: Tensor of shape (0,)\n",
       "model: Module\n",
       "model_checkpoints: TrainingDynamics object\n",
       "datasets: DatasetContainer(train=<autoencodix.data._multimodal_dataset.MultiModalDataset object at 0x4688b30d0>, valid=<autoencodix.data._multimodal_dataset.MultiModalDataset object at 0x4687f3d30>, test=<autoencodix.data._multimodal_dataset.MultiModalDataset object at 0x4687f35b0>)\n",
       "new_datasets: DatasetContainer(train=None, valid=None, test=None)\n",
       "adata_latent: AnnData object with n_obs × n_vars = 107 × 16\n",
       "final_reconstruction: <autoencodix.data._multimodal_dataset.MultiModalDataset object at 0x468830c40>\n",
       "sub_results: Dict with 4 items\n",
       "sub_reconstructions: None"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xmodalix.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff399221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-modal prediction...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid translation keys: IMG => IMG, valid keys are: ['RNA', 'RNA2', 'IMG']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxmodalix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrom_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIMG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIMG\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/utils/_utils.py:227\u001b[0m, in \u001b[0;36mconfig_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    218\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWarning: Additional keyword arguments provided \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(potential_config_kwargs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    220\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile an explicit \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object was also passed to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThese additional arguments will be ignored as configuration overrides.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    222\u001b[0m         )\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# Call the original function with the correct arguments\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Pass: self, original *args, the determined config object,\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# and only the **kwargs that matched the function's signature.\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_specific_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_pipeline.py:493\u001b[0m, in \u001b[0;36mBasePipeline.predict\u001b[0;34m(self, data, config, from_key, to_key, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m original_input \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    491\u001b[0m predict_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_prediction_data(data\u001b[38;5;241m=\u001b[39mdata)\n\u001b[0;32m--> 493\u001b[0m predictor_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredict_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_latent_results(\n\u001b[1;32m    496\u001b[0m     predictor_results\u001b[38;5;241m=\u001b[39mpredictor_results, predict_data\u001b[38;5;241m=\u001b[39mpredict_data\n\u001b[1;32m    497\u001b[0m )\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_reconstruction(\n\u001b[1;32m    499\u001b[0m     predictor_results\u001b[38;5;241m=\u001b[39mpredictor_results,\n\u001b[1;32m    500\u001b[0m     original_input\u001b[38;5;241m=\u001b[39moriginal_input,\n\u001b[1;32m    501\u001b[0m     predict_data\u001b[38;5;241m=\u001b[39mpredict_data,\n\u001b[1;32m    502\u001b[0m )\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/base/_base_pipeline.py:594\u001b[0m, in \u001b[0;36mBasePipeline._generate_predictions\u001b[0;34m(self, predict_data, from_key, to_key)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate predictions using the trained model.\"\"\"\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_prediction_data(predict_data\u001b[38;5;241m=\u001b[39mpredict_data)\n\u001b[0;32m--> 594\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredict_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/trainers/_xmodal_trainer.py:435\u001b[0m, in \u001b[0;36mXModalTrainer.predict\u001b[0;34m(self, data, from_key, to_key, model)\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    432\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype of data has to be MultiModalDataset, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    433\u001b[0m     )\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting cross-modal prediction...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 435\u001b[0m predict_keys \u001b[38;5;241m=\u001b[39m \u001b[43mfind_translation_keys\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrained_modalities\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modality_dynamics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43mto_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m from_key, to_key \u001b[38;5;241m=\u001b[39m predict_keys[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m], predict_keys[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    442\u001b[0m from_modality \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modality_dynamics[from_key]\n",
      "File \u001b[0;32m~/development/autoencodix_package/src/autoencodix/utils/_utils.py:509\u001b[0m, in \u001b[0;36mfind_translation_keys\u001b[0;34m(config, trained_modalities, from_key, to_key)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;66;03m# if the users passes from_key and to_key and we don't find them, we raise an error\u001b[39;00m\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (from_key_final \u001b[38;5;129;01mand\u001b[39;00m to_key_final):\n\u001b[0;32m--> 509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid translation keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfrom_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m => \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mto_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, valid keys are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msimple_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         )\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m: from_key_final, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m: to_key_final}\n\u001b[1;32m    514\u001b[0m data_info \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mdata_config\u001b[38;5;241m.\u001b[39mdata_info\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid translation keys: IMG => IMG, valid keys are: ['RNA', 'RNA2', 'IMG']"
     ]
    }
   ],
   "source": [
    "xmodalix.predict(from_key=\"IMG\", to_key=\"IMG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd8dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = xmodalix.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fba39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = xmodalix.predict()\n",
    "res2 = xmodalix.predict(from_key=\"IMG\", to_key=\"IMG\")\n",
    "res = xmodalix.result # = res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd06b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sub_results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c070ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sub_losses.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d00e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sub_losses.get(\"total_loss\").get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7055f0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = result.reconstructions.get(split=\"train\", epoch=-1)[\"img.IMG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ddb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82a2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = img[0, :, :, :].squeeze()\n",
    "sample_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a426fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = None\n",
    "b = None\n",
    "if not bool(a and b):\n",
    "    print(\"at least one None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "bool(a and b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f69239",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.final_reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1272a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img_trans = result.reconstructions.get(split=\"test\")[0,0,:,:,:].squeeze()\n",
    "plt.imshow(img_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e493fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmodalix._trainer._modality_dynamics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45726c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d1777d",
   "metadata": {},
   "source": [
    "## Xmodal_loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cde5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.sub_losses.keys()\n",
    "result.sub_losses.get(\"img.IMG.var_loss\").get()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927d4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_losses(result):\n",
    "    \"\"\"\n",
    "    Generates and displays plots for all relevant losses from a Result object.\n",
    "\n",
    "    Args:\n",
    "        result: Your custom result object containing the sub_losses attribute.\n",
    "    \"\"\"\n",
    "    if not hasattr(result, \"sub_losses\"):\n",
    "        print(\n",
    "            \"Error: The provided result object does not have a 'sub_losses' attribute.\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # 1. Get all loss keys and filter out the factors\n",
    "    all_loss_keys = result.sub_losses.keys()\n",
    "    keys_to_plot = [key for key in all_loss_keys if not key.endswith(\"_factor\")]\n",
    "\n",
    "    if not keys_to_plot:\n",
    "        print(\"No valid loss keys found to plot.\")\n",
    "        return\n",
    "\n",
    "    # 2. Iterate through each loss and create a plot\n",
    "    for loss_name in keys_to_plot:\n",
    "        try:\n",
    "            # Retrieve the nested dictionary for the current loss\n",
    "            loss_data = result.sub_losses.get(loss_name).get()\n",
    "\n",
    "            # 3. Unpack the data into lists for plotting\n",
    "            epochs = sorted(loss_data.keys())\n",
    "            train_losses = [loss_data[epoch][\"train\"] for epoch in epochs]\n",
    "\n",
    "            # Check if validation data exists before trying to plot it\n",
    "            has_valid_data = all(\"valid\" in loss_data[epoch] for epoch in epochs)\n",
    "            if has_valid_data:\n",
    "                valid_losses = [loss_data[epoch][\"valid\"] for epoch in epochs]\n",
    "\n",
    "            # 4. Create the plot\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(epochs, train_losses, \"o-\", label=f\"Train ({loss_name})\")\n",
    "            if has_valid_data:\n",
    "                plt.plot(epochs, valid_losses, \"x-\", label=f\"Validation ({loss_name})\")\n",
    "\n",
    "            plt.title(f\"Training and Validation Loss: {loss_name}\")\n",
    "            plt.xlabel(\"Epoch\")\n",
    "            plt.ylabel(\"Loss\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "\n",
    "        except (AttributeError, TypeError, KeyError) as e:\n",
    "            print(f\"Could not plot '{loss_name}'. Error retrieving data: {e}\")\n",
    "\n",
    "\n",
    "# This is a mock setup to demonstrate how to use the function.\n",
    "print(\"Generating plots from mock result object...\")\n",
    "plot_losses(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6920d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_cl = xmodalix._trainer._loss_fn\n",
    "batch_dynamics = xmodalix._trainer._modality_dynamics\n",
    "batch = next(iter(xmodalix._trainer._trainloader))\n",
    "config = img_config\n",
    "print(config.loss_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_config.class_param = \"early\"\n",
    "for mod_name, mod_data in batch.items():\n",
    "    metadata_df = mod_data.get(\"metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6367990",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def _calc_class_loss(\n",
    "        self,\n",
    "        batch: Dict[str, Dict[str, Any]],\n",
    "        modality_dynamics: Dict[str, Dict[str, Any]],\n",
    "    ) -> torch.Tensor:\n",
    "        \n",
    "        device = next(iter(modality_dynamics.values()))[\"mp\"].latentspace.device\n",
    "\n",
    "        if not self.class_param:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "\n",
    "        # Step 1: Dynamically update maps and discover new classes from the batch\n",
    "        for mod_name, mod_data in batch.items():\n",
    "            metadata_df = mod_data.get(\"metadata\")\n",
    "            if metadata_df is None:\n",
    "                continue\n",
    "\n",
    "            batch_map = metadata_df[self.class_param]\n",
    "            self.sample_to_class_map.update(batch_map.to_dict())\n",
    "\n",
    "            for label in batch_map.unique():\n",
    "                if label not in self.class_means:\n",
    "                    self.class_means[label] = torch.zeros(self.latent_dim, device=device)\n",
    "\n",
    "        # Step 2: Calculate the loss for the current batch\n",
    "        total_class_loss = torch.tensor(0.0, device=device)\n",
    "        for mod_name, mod_data in batch.items():\n",
    "            if mod_data.get(\"metadata\") is None:\n",
    "                continue\n",
    "                \n",
    "            latents = modality_dynamics[mod_name][\"mp\"].latentspace\n",
    "            metadata_df = mod_data[\"metadata\"]\n",
    "            batch_class_labels = metadata_df[self.class_param].tolist()\n",
    "\n",
    "            target_means_list = [self.class_means[label] for label in batch_class_labels]\n",
    "            target_means_tensor = torch.stack(target_means_list).to(latents.device)\n",
    "            \n",
    "            distance = torch.abs(latents - target_means_tensor).mean(dim=1)\n",
    "            total_class_loss += self.reduction_fn(distance)\n",
    "\n",
    "        num_modalities_in_batch = len(batch)\n",
    "        return total_class_loss / num_modalities_in_batch if num_modalities_in_batch > 0 else total_class_loss\n",
    "\n",
    "    def update_class_means(self, epoch_dynamics: List[Dict], device):\n",
    "        \"\"\"\n",
    "        Method to be called by the Trainer at the end of an epoch to update the\n",
    "        target class mean vectors.\n",
    "        \"\"\"\n",
    "        if not self.class_param or not epoch_dynamics:\n",
    "            return\n",
    "\n",
    "        final_latents = defaultdict(list)\n",
    "        final_sample_ids = defaultdict(list)\n",
    "        for batch_data in epoch_dynamics:\n",
    "            for mod_name, data in batch_data[\"latentspaces\"].items():\n",
    "                final_latents[mod_name].append(data)\n",
    "            for mod_name, data in batch_data[\"sample_ids\"].items():\n",
    "                final_sample_ids[mod_name].append(data)\n",
    "\n",
    "        all_latents_df_list = []\n",
    "        for mod_name in final_latents.keys():\n",
    "            mod_latents = np.concatenate(final_latents[mod_name])\n",
    "            mod_ids = np.concatenate(final_sample_ids[mod_name])\n",
    "            all_latents_df_list.append(pd.DataFrame(mod_latents, index=mod_ids))\n",
    "\n",
    "        if not all_latents_df_list:\n",
    "            return\n",
    "\n",
    "        all_latents_df = pd.concat(all_latents_df_list)\n",
    "        all_latents_df[\"class_label\"] = all_latents_df.index.map(self.sample_to_class_map)\n",
    "\n",
    "        new_means_df = all_latents_df.groupby(\"class_label\").mean()\n",
    "\n",
    "        for label, mean_values in new_means_df.iterrows():\n",
    "            self.class_means[label] = torch.tensor(\n",
    "                mean_values.values, dtype=torch.float32, device=device\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bb7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e716e9",
   "metadata": {},
   "source": [
    "```python\n",
    "sample_ids = {}\n",
    "\n",
    "for k,v in batch.items():\n",
    "\n",
    "    print(v.keys())\n",
    "\n",
    "sample_ids\n",
    "\n",
    "\n",
    "\n",
    "output\n",
    "\n",
    "sample_ids = {}\n",
    "\n",
    "for k,v in batch.items():\n",
    "\n",
    "    print(v.keys())\n",
    "\n",
    "sample_ids\n",
    "\n",
    "\n",
    "\n",
    "with metadata lookin like\n",
    "\n",
    "batch[\"multi_bulk.RNA\"][\"metadata\"]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecdf790",
   "metadata": {},
   "outputs": [],
   "source": [
    "latenspaces = {}\n",
    "for k, v in batch_dynamics.items():\n",
    "    latenspaces[k] = v[\"mp\"].latentspace.detach()\n",
    "    # latenspaces[k] = v[\"la\"]\n",
    "latenspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"multi_bulk.RNA\"][\"metadata\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7655ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = {}\n",
    "for k, v in batch.items():\n",
    "    print(v.keys())\n",
    "sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa36988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "import torch\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def calculate_paired_loss(\n",
    "    latentspaces: dict[str, torch.Tensor],\n",
    "    sample_ids: dict[str, list],\n",
    "    reduction: str = \"mean\",\n",
    "    device: str = \"mps\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Calculates the paired distance loss across all pairs of modalities in a batch.\n",
    "\n",
    "    Args:\n",
    "        latentspaces: A dictionary mapping modality names to their latent space tensors.\n",
    "                      e.g., {'RNA': tensor_rna, 'ATAC': tensor_atac}\n",
    "        sample_ids: A dictionary mapping modality names to their list of sample IDs.\n",
    "        reduction: The type of reduction to apply ('mean' or 'sum').\n",
    "\n",
    "    Returns:\n",
    "        A single scalar tensor representing the total paired loss.\n",
    "    \"\"\"\n",
    "\n",
    "    total_paired_loss = torch.tensor(0.0, device=device)\n",
    "    modality_names = list(latentspaces.keys())\n",
    "\n",
    "    # 1. Iterate through all unique pairs of modalities\n",
    "    for mod_a, mod_b in itertools.combinations(modality_names, 2):\n",
    "        ids_a = sample_ids[mod_a]\n",
    "        ids_b = sample_ids[mod_b]\n",
    "\n",
    "        # 2. Find the intersection of sample IDs\n",
    "        common_ids = set(ids_a) & set(ids_b)\n",
    "\n",
    "        if not common_ids:\n",
    "            print(\"no common ids\")\n",
    "            continue  # No overlapping samples for this pair, skip\n",
    "\n",
    "        # 3. Create a mapping from sample ID to index for efficient lookup\n",
    "        id_to_idx_a = {sample_id: i for i, sample_id in enumerate(ids_a)}\n",
    "        id_to_idx_b = {sample_id: i for i, sample_id in enumerate(ids_b)}\n",
    "\n",
    "        # Get the corresponding indices for the common samples\n",
    "        indices_a = [id_to_idx_a[common_id] for common_id in common_ids]\n",
    "        indices_b = [id_to_idx_b[common_id] for common_id in common_ids]\n",
    "\n",
    "        # 4. Select the latent vectors for the paired samples\n",
    "        paired_latents_a = latentspaces[mod_a][indices_a]\n",
    "        paired_latents_b = latentspaces[mod_b][indices_b]\n",
    "\n",
    "        # 5. Calculate the distance between the aligned latent vectors\n",
    "        # L1 distance, averaged over latent dimensions and then over samples\n",
    "        distance = torch.abs(paired_latents_a - paired_latents_b).mean(dim=1)\n",
    "\n",
    "        if reduction == \"mean\":\n",
    "            pair_loss = distance.mean()\n",
    "        elif reduction == \"sum\":\n",
    "            pair_loss = distance.sum()\n",
    "        else:\n",
    "            raise ValueError(f\"Reduction must be 'mean' or 'sum', not {reduction}\")\n",
    "\n",
    "        # 6. Aggregate the loss\n",
    "        total_paired_loss += pair_loss\n",
    "\n",
    "    return total_paired_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec16ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_paired_loss(latentspaces=latenspaces, sample_ids=sample_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad54bb1",
   "metadata": {},
   "source": [
    "## Imagix\n",
    "If we want to use a standard image vae, we can use the Imagix pipeline. Note that only one image dataset is allowed here\n",
    "**NOTE**:\n",
    "set the datacase in config to DataCase.IMG_TO_IMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e54b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autoencodix as acx\n",
    "from autoencodix.utils.default_config import (\n",
    "    DefaultConfig,\n",
    "    DataConfig,\n",
    "    DataCase,\n",
    "    DataInfo,\n",
    ")\n",
    "import os\n",
    "\n",
    "IMGROOT = os.path.join(\"data/images/ALY-2_SYS721/\")\n",
    "IMGMAPPING = os.path.join(\"data/ALY-2_SYS721_mappings.txt\")\n",
    "NUMFILE = os.path.join(\"data/AM3_NO2_raw_cell.tsv\")\n",
    "\n",
    "img_config2 = DefaultConfig(\n",
    "    data_case=DataCase.IMG_TO_IMG,\n",
    "    data_config=DataConfig(\n",
    "        data_info={\n",
    "            \"IMG\": DataInfo(\n",
    "                file_path=IMGROOT,\n",
    "                data_type=\"IMG\",\n",
    "            ),\n",
    "            \"ANNO\": DataInfo(\n",
    "                file_path=IMGMAPPING,\n",
    "                data_type=\"ANNOTATION\",\n",
    "            ),\n",
    "        },\n",
    "    ),\n",
    ")\n",
    "\n",
    "imagix = acx.Imagix(config=img_config2)\n",
    "imagix.preprocess()\n",
    "imagix.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2af91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagix.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dba08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = imagix.result\n",
    "r.final_reconstruction\n",
    "sample_img = r.final_reconstruction.data[0, :, :, :]\n",
    "sample_img = sample_img.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(sample_img.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde26e3e",
   "metadata": {},
   "source": [
    "### Old, maybe reuse code for getting stats, but seemd to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5566fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple statistics tracking - just build the dict in your training loop\n",
    "\n",
    "# Initialize stats dict once before training\n",
    "stats = {}\n",
    "for modality in trainset.datasets.keys():\n",
    "    stats[modality] = {}\n",
    "    for sample_id in trainset.datasets[modality].sample_ids:\n",
    "        stats[modality][sample_id] = 0\n",
    "print(stats)\n",
    "# Training loop\n",
    "epochs = 100\n",
    "\n",
    "# Add this to your training loop\n",
    "for epoch in range(100):  # Just test first 5 epochs\n",
    "    batch_count = 0\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        batch_count += 1\n",
    "\n",
    "    print(f\"Epoch {epoch}: {batch_count} batches processed\")\n",
    "\n",
    "    # Also check sampler length\n",
    "    print(f\"Sampler length: {len(sampler)}\")\n",
    "    print(f\"Dataset length: {len(trainset)}\")\n",
    "    print(f\"Paired samples: {len(trainset.paired_sample_ids)}\")\n",
    "    print(f\"Unpaired samples: {len(trainset.unpaired_sample_ids)}\")\n",
    "    # # Reset stats for new epoch\n",
    "    # for modality in stats:\n",
    "    #     for sample_id in stats[modality]:\n",
    "    #         stats[modality][sample_id] = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # Update stats with current batch\n",
    "        for modality, data in batch.items():\n",
    "            if \"sample_ids\" in data:\n",
    "                for sample_id in data[\"sample_ids\"]:\n",
    "                    if sample_id in stats[modality]:\n",
    "                        stats[modality][sample_id] += 1\n",
    "\n",
    "        # Your training code here\n",
    "        # for modality, data in batch.items():\n",
    "        #     if data[\"data\"] is not None:\n",
    "        #         outputs = models[modality](data[\"data\"])\n",
    "\n",
    "    # Print simple summary at end of epoch\n",
    "    print(\"\\nSampling Stats:\")\n",
    "    for modality in stats:\n",
    "        counts = list(stats[modality].values())\n",
    "        seen = sum(1 for c in counts if c > 0)\n",
    "        total = len(counts)\n",
    "        avg = sum(counts) / len(counts) if counts else 0\n",
    "        print(f\"  {modality}: {seen}/{total} samples seen, avg: {avg:.2f}\")\n",
    "\n",
    "    # Optional: Print unseen samples\n",
    "    print(\"\\nUnseen samples:\")\n",
    "    for modality in stats:\n",
    "        unseen = [sid for sid, count in stats[modality].items() if count == 0]\n",
    "        if unseen:\n",
    "            print(\n",
    "                f\"  {modality}: {len(unseen)} unseen - {unseen[:5]}{'...' if len(unseen) > 5 else ''}\"\n",
    "            )\n",
    "\n",
    "# Access raw stats anytime:\n",
    "# stats = {\n",
    "#     \"img.IMG\": {\"T_98\": 3, \"T_138\": 2, \"T_183\": 1, ...},\n",
    "#     \"multi_bulk.RNA\": {\"T_98\": 4, \"T_138\": 3, \"T_173\": 2, ...},\n",
    "#     ...\n",
    "# }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
